Apache Jena Fuseki is a SPARQL server.  It can run as a standalone server, or embedded in an
application.
Fuseki provides the
SPARQL 1.1 protocols for query and update
as well as the
SPARQL Graph Store protocol.
Fuseki is integrated with TDB to provide a robust,
transactional, persistent storage layer. Fuseki also incorporates
Jena text query.
Contents

Download
Getting Started
Running Fuseki Server

As a standalone server
As a service
As a web application
Security with Apache Shiro


Running Fuseki Plain

Setup
As a Docker container
As an embedded SPARQL server
Security and data access control
Logging


Fuseki Configuration
Server Statistics and Metrics
How to Contribute
Client access

Use from Java


Extending Fuseki with Fuseki Modules
Links to Standards

The Jena users mailing is the place to get help with Fuseki.
Email support lists
Download Fuseki
Releases of Apache Jena Fuseki can be downloaded from:
Jena Downloads
Fuseki download files

  
      
          Filename
          Description
      
  
  
      
          apache-jena-fuseki-*VER*.zip
          The Fuseki server and UI
      
  

The Fuseki engine is also available as a Maven artifact:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-main</artifactId>
   <version>X.Y.Z</version>
</dependency>

and the UI is available as:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-ui</artifactId>
   <version>X.Y.Z</version>
</dependency>

A WAR file is also available from the Jena download page.
Previous releases
While previous releases are available, we strongly recommend that wherever
possible users use the latest official Apache releases of Jena in
preference to using any older versions of Jena.
Previous Apache Jena releases can be found in the Apache archive area
at https://archive.apache.org/dist/jena
Development Builds
Regular development builds of all of Jena are available
(these are not formal releases) from the
Apache snapshots maven repository.
This includes the packaged build of Fuseki.
How to Contribute
We welcome contributions towards making Jena a better platform for semantic
web and linked data applications.  We appreciate feature suggestions, bug
reports and patches for code or documentation.
See “Getting Involved” for ways to
contribute to Jena and Fuseki, including patches and making github
pull-requests.
Source code
The development codebase is available from git.
Development builds (not a formal release):
SNAPSHOT
Source code:
https://github.com/apache/jena/tree/main/jena-fuseki2
The Fuseki code is under “jena-fuseki2/”:

  
      
          Code
          Purpose
      
  
  
      
          jena-fuseki-main
          The Fuseki server
      
      
          jena-fuseki-core
          The Fuseki engine
      
      
          jena-fuseki-server
          Build the combined jar for Fuseki server
      
      
          jena-fuseki-access
          Data access control
      
      
          apache-jena-fuseki
          The download for Fuseki
      
      
          Other
          
      
      
          jena-fuseki-docker
          Build a docker container for Fuseki
      
      
          jena-fuseki-geosparql
          Integration for GeoSPARQL
      
      
          Webapp
          
      
      
          jena-fuseki-webapp
          Web application and command line startup
      
      
          jena-fuseki-fulljar
          Build the combined jar for Fuseki/UI server
      
      
          jena-fuseki-war
          Build the war file for Fuseki/UI server\n\nOn this page
    
  
    Contents
    Download Fuseki
      
        Previous releases
        Development Builds
      
    
    How to Contribute
      
        Source code
      
    
  

  
  
    Apache Jena Fuseki is a SPARQL server.  It can run as a standalone server, or embedded in an
application.
Fuseki provides the
SPARQL 1.1 protocols for query and update
as well as the
SPARQL Graph Store protocol.
Fuseki is integrated with TDB to provide a robust,
transactional, persistent storage layer. Fuseki also incorporates
Jena text query.
Contents

Download
Getting Started
Running Fuseki Server

As a standalone server
As a service
As a web application
Security with Apache Shiro


Running Fuseki Plain

Setup
As a Docker container
As an embedded SPARQL server
Security and data access control
Logging


Fuseki Configuration
Server Statistics and Metrics
How to Contribute
Client access

Use from Java


Extending Fuseki with Fuseki Modules
Links to Standards

The Jena users mailing is the place to get help with Fuseki.
Email support lists
Download Fuseki
Releases of Apache Jena Fuseki can be downloaded from:
Jena Downloads
Fuseki download files

  
      
          Filename
          Description
      
  
  
      
          apache-jena-fuseki-*VER*.zip
          The Fuseki server and UI
      
  

The Fuseki engine is also available as a Maven artifact:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-main</artifactId>
   <version>X.Y.Z</version>
</dependency>

and the UI is available as:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-ui</artifactId>
   <version>X.Y.Z</version>
</dependency>

A WAR file is also available from the Jena download page.
Previous releases
While previous releases are available, we strongly recommend that wherever
possible users use the latest official Apache releases of Jena in
preference to using any older versions of Jena.
Previous Apache Jena releases can be found in the Apache archive area
at https://archive.apache.org/dist/jena
Development Builds
Regular development builds of all of Jena are available
(these are not formal releases) from the
Apache snapshots maven repository.
This includes the packaged build of Fuseki.
How to Contribute
We welcome contributions towards making Jena a better platform for semantic
web and linked data applications.  We appreciate feature suggestions, bug
reports and patches for code or documentation.
See “Getting Involved” for ways to
contribute to Jena and Fuseki, including patches and making github
pull-requests.
Source code
The development codebase is available from git.
Development builds (not a formal release):
SNAPSHOT
Source code:
https://github.com/apache/jena/tree/main/jena-fuseki2
The Fuseki code is under “jena-fuseki2/”:

  
      
          Code
          Purpose
      
  
  
      
          jena-fuseki-main
          The Fuseki server
      
      
          jena-fuseki-core
          The Fuseki engine
      
      
          jena-fuseki-server
          Build the combined jar for Fuseki server
      
      
          jena-fuseki-access
          Data access control
      
      
          apache-jena-fuseki
          The download for Fuseki
      
      
          Other
          
      
      
          jena-fuseki-docker
          Build a docker container for Fuseki
      
      
          jena-fuseki-geosparql
          Integration for GeoSPARQL
      
      
          Webapp
          
      
      
          jena-fuseki-webapp
          Web application and command line startup
      
      
          jena-fuseki-fulljar
          Build the combined jar for Fuseki/UI server
      
      
          jena-fuseki-war
          Build the war file for Fuseki/UI server
      
  


  
  
  
    On this page
    
  
    Contents
    Download Fuseki
      
        Previous releases
        Development Builds
      
    
    How to Contribute
      
        Source code\n\n\n\nApache Jena
  A free and open source Java framework for building Semantic Web and Linked Data applications.
  
    Get started now!
    Download
  
  


  
    
      
        RDF
      
      
        RDF API
        Interact with the core API to create and read Resource Description Framework (RDF) graphs. Serialise your triples using popular formats such as RDF/XML or Turtle.
      
      
        ARQ (SPARQL)
        Query your RDF data using ARQ, a SPARQL 1.1 compliant engine. ARQ supports remote federated queries and free text search.
      
    
  
  
    
      
        Triple store
      
      
        TDB
        Persist your data using TDB, a native high performance triple store. TDB supports the full range of Jena APIs.
      
      
        Fuseki
        Expose your triples as a SPARQL end-point accessible over HTTP. Fuseki provides REST-style interaction with your RDF data.
      
    
  
  
    
      
        OWL
      
      
        Ontology API
        Work with models, RDFS and the Web Ontology Language (OWL) to add extra semantics to your RDF data.
      
      
        Inference API
        Reason over your data to expand and check the content of your triple store. Configure your own inference rules or use the built-in OWL and RDFS reasoners.\n\n\n\nApache Jena is packaged as downloads which contain the most commonly used portions of the systems:

apache-jena – contains the APIs, SPARQL engine, the TDB native RDF database and command line tools
apache-jena-fuseki – the Jena SPARQL server

Jena5 requires Java 17, or a later version of Java.
Jena jars are available from Maven.
You may verify the authenticity of artifacts below by using the PGP KEYS file.
Apache Jena Release
Source release: this forms the official release of Apache Jena. All binaries artifacts and maven binaries correspond to this source.

  
      
          Apache Jena Release
          SHA512
          Signature
      
  
  
      
          jena-5.4.0-source-release.zip
          SHA512
          PGP
      
  

Apache Jena Binary Distributions
The binary distribution of the Fuseki server:

  
      
          Apache Jena Fuseki
          SHA512
          Signature
      
  
  
      
          apache-jena-fuseki-5.4.0.tar.gz
          SHA512
          PGP
      
      
          apache-jena-fuseki-5.4.0.zip
          SHA512
          PGP
      
  

 
The binary distribution of libraries contains the APIs, SPARQL engine, the TDB native RDF database and a variety of command line scripts and tools for working with these systems.

  
      
          Apache Jena Commands
          SHA512
          Signature
      
  
  
      
          apache-jena-5.4.0.tar.gz
          SHA512
          PGP
      
      
          apache-jena-5.4.0.zip
          SHA512
          PGP
      
  

 
The binary distribution of Fuseki as a WAR file:

  
      
          Apache Jena Fuseki
          SHA512
          Signature
      
  
  
      
          jena-fuseki-war-5.4.0.war
          SHA512
          PGP
      
  

This can be run in any servlet application container supporting Jakarta Servlet 6.0
(part of Jakarta EE version 9), such as Apache Tomcat
10.x or later.
The server must be running on the required version of Java.
Apache Jena Download area
The source release and also the binaries are available from the
Apache Jena Download area.
Individual Modules
Apache Jena publishes a range of modules beyond those included in the binary distributions (code for all modules may be found in the source distribution).
Individual modules may be obtained using a dependency manager which can talk to Maven repositories, some modules are only available via Maven.
Maven
See “Using Jena with Apache Maven” for full details.
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>apache-jena-libs</artifactId>
   <type>pom</type>
   <version>X.Y.Z</version>
</dependency>
Source code
The development codebase is available from git.
https://gitbox.apache.org/repos/asf?p=jena.git
This is also available on GitHub:
https://github.com/apache/jena
Previous releases
While previous releases are available, we strongly recommend that wherever
possible users use the latest official Apache releases of Jena in
preference to using any older versions of Jena.
Previous Apache Jena releases can be found in the Apache archive area
at https://archive.apache.org/dist/jena.
Download Source
The Apache Software foundation uses CDN-distribution for Apache
projects and the current release of Jena.

The currently selected mirror is https://dlcdn.apache.org/.  If you encounter a problem with this mirror, please select another
mirror.

Other mirrors: 
https://dlcdn.apache.org/
https://dlcdn.apache.org/
(backup)\n\nOn this page
    
  
    
      
        Apache Jena Release
        Apache Jena Binary Distributions
        Apache Jena Download area
        Individual Modules
          
            Maven
            Source code
            Previous releases
          
        
      
    
    Download Source
  

  
  
    Apache Jena is packaged as downloads which contain the most commonly used portions of the systems:

apache-jena – contains the APIs, SPARQL engine, the TDB native RDF database and command line tools
apache-jena-fuseki – the Jena SPARQL server

Jena5 requires Java 17, or a later version of Java.
Jena jars are available from Maven.
You may verify the authenticity of artifacts below by using the PGP KEYS file.
Apache Jena Release
Source release: this forms the official release of Apache Jena. All binaries artifacts and maven binaries correspond to this source.

  
      
          Apache Jena Release
          SHA512
          Signature
      
  
  
      
          jena-5.4.0-source-release.zip
          SHA512
          PGP
      
  

Apache Jena Binary Distributions
The binary distribution of the Fuseki server:

  
      
          Apache Jena Fuseki
          SHA512
          Signature
      
  
  
      
          apache-jena-fuseki-5.4.0.tar.gz
          SHA512
          PGP
      
      
          apache-jena-fuseki-5.4.0.zip
          SHA512
          PGP
      
  

 
The binary distribution of libraries contains the APIs, SPARQL engine, the TDB native RDF database and a variety of command line scripts and tools for working with these systems.

  
      
          Apache Jena Commands
          SHA512
          Signature
      
  
  
      
          apache-jena-5.4.0.tar.gz
          SHA512
          PGP
      
      
          apache-jena-5.4.0.zip
          SHA512
          PGP
      
  

 
The binary distribution of Fuseki as a WAR file:

  
      
          Apache Jena Fuseki
          SHA512
          Signature
      
  
  
      
          jena-fuseki-war-5.4.0.war
          SHA512
          PGP
      
  

This can be run in any servlet application container supporting Jakarta Servlet 6.0
(part of Jakarta EE version 9), such as Apache Tomcat
10.x or later.
The server must be running on the required version of Java.
Apache Jena Download area
The source release and also the binaries are available from the
Apache Jena Download area.
Individual Modules
Apache Jena publishes a range of modules beyond those included in the binary distributions (code for all modules may be found in the source distribution).
Individual modules may be obtained using a dependency manager which can talk to Maven repositories, some modules are only available via Maven.
Maven
See “Using Jena with Apache Maven” for full details.
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>apache-jena-libs</artifactId>
   <type>pom</type>
   <version>X.Y.Z</version>
</dependency>
Source code
The development codebase is available from git.
https://gitbox.apache.org/repos/asf?p=jena.git
This is also available on GitHub:
https://github.com/apache/jena
Previous releases
While previous releases are available, we strongly recommend that wherever
possible users use the latest official Apache releases of Jena in
preference to using any older versions of Jena.
Previous Apache Jena releases can be found in the Apache archive area
at https://archive.apache.org/dist/jena.
Download Source
The Apache Software foundation uses CDN-distribution for Apache
projects and the current release of Jena.

The currently selected mirror is https://dlcdn.apache.org/.  If you encounter a problem with this mirror, please select another
mirror.

Other mirrors: 
https://dlcdn.apache.org/
https://dlcdn.apache.org/
(backup)




  
  
  
    On this page
    
  
    
      
        Apache Jena Release
        Apache Jena Binary Distributions
        Apache Jena Download area
        Individual Modules
          
            Maven
            Source code
            Previous releases
          
        
      
    
    Download Source\n\n\n\nApache Jena Fuseki is a SPARQL server.  It can run as a standalone server, or embedded in an
application.
Fuseki provides the
SPARQL 1.1 protocols for query and update
as well as the
SPARQL Graph Store protocol.
Fuseki is integrated with TDB to provide a robust,
transactional, persistent storage layer. Fuseki also incorporates
Jena text query.
Contents

Download
Getting Started
Running Fuseki Server

As a standalone server
As a service
As a web application
Security with Apache Shiro


Running Fuseki Plain

Setup
As a Docker container
As an embedded SPARQL server
Security and data access control
Logging


Fuseki Configuration
Server Statistics and Metrics
How to Contribute
Client access

Use from Java


Extending Fuseki with Fuseki Modules
Links to Standards

The Jena users mailing is the place to get help with Fuseki.
Email support lists
Download Fuseki
Releases of Apache Jena Fuseki can be downloaded from:
Jena Downloads
Fuseki download files

  
      
          Filename
          Description
      
  
  
      
          apache-jena-fuseki-*VER*.zip
          The Fuseki server and UI
      
  

The Fuseki engine is also available as a Maven artifact:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-main</artifactId>
   <version>X.Y.Z</version>
</dependency>

and the UI is available as:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-ui</artifactId>
   <version>X.Y.Z</version>
</dependency>

A WAR file is also available from the Jena download page.
Previous releases
While previous releases are available, we strongly recommend that wherever
possible users use the latest official Apache releases of Jena in
preference to using any older versions of Jena.
Previous Apache Jena releases can be found in the Apache archive area
at https://archive.apache.org/dist/jena
Development Builds
Regular development builds of all of Jena are available
(these are not formal releases) from the
Apache snapshots maven repository.
This includes the packaged build of Fuseki.
How to Contribute
We welcome contributions towards making Jena a better platform for semantic
web and linked data applications.  We appreciate feature suggestions, bug
reports and patches for code or documentation.
See “Getting Involved” for ways to
contribute to Jena and Fuseki, including patches and making github
pull-requests.
Source code
The development codebase is available from git.
Development builds (not a formal release):
SNAPSHOT
Source code:
https://github.com/apache/jena/tree/main/jena-fuseki2
The Fuseki code is under “jena-fuseki2/”:

  
      
          Code
          Purpose
      
  
  
      
          jena-fuseki-main
          The Fuseki server
      
      
          jena-fuseki-core
          The Fuseki engine
      
      
          jena-fuseki-server
          Build the combined jar for Fuseki server
      
      
          jena-fuseki-access
          Data access control
      
      
          apache-jena-fuseki
          The download for Fuseki
      
      
          Other
          
      
      
          jena-fuseki-docker
          Build a docker container for Fuseki
      
      
          jena-fuseki-geosparql
          Integration for GeoSPARQL
      
      
          Webapp
          
      
      
          jena-fuseki-webapp
          Web application and command line startup
      
      
          jena-fuseki-fulljar
          Build the combined jar for Fuseki/UI server
      
      
          jena-fuseki-war
          Build the war file for Fuseki/UI server\n\nOn this page
    
  
    Contents
    Download Fuseki
      
        Previous releases
        Development Builds
      
    
    How to Contribute
      
        Source code
      
    
  

  
  
    Apache Jena Fuseki is a SPARQL server.  It can run as a standalone server, or embedded in an
application.
Fuseki provides the
SPARQL 1.1 protocols for query and update
as well as the
SPARQL Graph Store protocol.
Fuseki is integrated with TDB to provide a robust,
transactional, persistent storage layer. Fuseki also incorporates
Jena text query.
Contents

Download
Getting Started
Running Fuseki Server

As a standalone server
As a service
As a web application
Security with Apache Shiro


Running Fuseki Plain

Setup
As a Docker container
As an embedded SPARQL server
Security and data access control
Logging


Fuseki Configuration
Server Statistics and Metrics
How to Contribute
Client access

Use from Java


Extending Fuseki with Fuseki Modules
Links to Standards

The Jena users mailing is the place to get help with Fuseki.
Email support lists
Download Fuseki
Releases of Apache Jena Fuseki can be downloaded from:
Jena Downloads
Fuseki download files

  
      
          Filename
          Description
      
  
  
      
          apache-jena-fuseki-*VER*.zip
          The Fuseki server and UI
      
  

The Fuseki engine is also available as a Maven artifact:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-main</artifactId>
   <version>X.Y.Z</version>
</dependency>

and the UI is available as:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-ui</artifactId>
   <version>X.Y.Z</version>
</dependency>

A WAR file is also available from the Jena download page.
Previous releases
While previous releases are available, we strongly recommend that wherever
possible users use the latest official Apache releases of Jena in
preference to using any older versions of Jena.
Previous Apache Jena releases can be found in the Apache archive area
at https://archive.apache.org/dist/jena
Development Builds
Regular development builds of all of Jena are available
(these are not formal releases) from the
Apache snapshots maven repository.
This includes the packaged build of Fuseki.
How to Contribute
We welcome contributions towards making Jena a better platform for semantic
web and linked data applications.  We appreciate feature suggestions, bug
reports and patches for code or documentation.
See “Getting Involved” for ways to
contribute to Jena and Fuseki, including patches and making github
pull-requests.
Source code
The development codebase is available from git.
Development builds (not a formal release):
SNAPSHOT
Source code:
https://github.com/apache/jena/tree/main/jena-fuseki2
The Fuseki code is under “jena-fuseki2/”:

  
      
          Code
          Purpose
      
  
  
      
          jena-fuseki-main
          The Fuseki server
      
      
          jena-fuseki-core
          The Fuseki engine
      
      
          jena-fuseki-server
          Build the combined jar for Fuseki server
      
      
          jena-fuseki-access
          Data access control
      
      
          apache-jena-fuseki
          The download for Fuseki
      
      
          Other
          
      
      
          jena-fuseki-docker
          Build a docker container for Fuseki
      
      
          jena-fuseki-geosparql
          Integration for GeoSPARQL
      
      
          Webapp
          
      
      
          jena-fuseki-webapp
          Web application and command line startup
      
      
          jena-fuseki-fulljar
          Build the combined jar for Fuseki/UI server
      
      
          jena-fuseki-war
          Build the war file for Fuseki/UI server
      
  


  
  
  
    On this page
    
  
    Contents
    Download Fuseki
      
        Previous releases
        Development Builds
      
    
    How to Contribute
      
        Source code\n\n\n\nThe following tutorials take a step-by-step approach to explaining aspects of
RDF and linked-data applications programming in Jena. For a more task-oriented
description, please see the getting started guide.

RDF core API tutorial
SPARQL tutorial
Using Jena with Eclipse
Manipulating SPARQL using ARQ

Jena tutorials in other languages
Quelques uns des tutoriels de Jena sont aussi disponibles en français. Vous
pouvez les voir en suivant ces liens:

Une introduction à RDF
Requêtes SPARQL utilisant l’API Java ARQ
Les entrées/sorties RDF
Une introduction à SPARQL

Os tutoriais a seguir explicam aspectos de RDF e da programação em Jena de aplicações linked-data. Veja também o guia
getting started - em inglês.

Uma introdução à API RDF
Tutorial SPARQL
Manipulando SPARQL usando ARQ
Usando o Jena com o Eclipse

Simplified Chinese:

RDF 和 Jena RDF API 入门

Greek:

Εφαρμογές του Jena API στο Σημασιολογικό Ιστό\n\nOn this page
    
  
    Jena tutorials in other languages
  

  
  
    The following tutorials take a step-by-step approach to explaining aspects of
RDF and linked-data applications programming in Jena. For a more task-oriented
description, please see the getting started guide.

RDF core API tutorial
SPARQL tutorial
Using Jena with Eclipse
Manipulating SPARQL using ARQ

Jena tutorials in other languages
Quelques uns des tutoriels de Jena sont aussi disponibles en français. Vous
pouvez les voir en suivant ces liens:

Une introduction à RDF
Requêtes SPARQL utilisant l’API Java ARQ
Les entrées/sorties RDF
Une introduction à SPARQL

Os tutoriais a seguir explicam aspectos de RDF e da programação em Jena de aplicações linked-data. Veja também o guia
getting started - em inglês.

Uma introdução à API RDF
Tutorial SPARQL
Manipulando SPARQL usando ARQ
Usando o Jena com o Eclipse

Simplified Chinese:

RDF 和 Jena RDF API 入门

Greek:

Εφαρμογές του Jena API στο Σημασιολογικό Ιστό


  
  
  
    On this page
    
  
    Jena tutorials in other languages\n\n\n\nApache Jena Fuseki is a SPARQL server.  It can run as a standalone server, or embedded in an
application.
Fuseki provides the
SPARQL 1.1 protocols for query and update
as well as the
SPARQL Graph Store protocol.
Fuseki is integrated with TDB to provide a robust,
transactional, persistent storage layer. Fuseki also incorporates
Jena text query.
Contents

Download
Getting Started
Running Fuseki Server

As a standalone server
As a service
As a web application
Security with Apache Shiro


Running Fuseki Plain

Setup
As a Docker container
As an embedded SPARQL server
Security and data access control
Logging


Fuseki Configuration
Server Statistics and Metrics
How to Contribute
Client access

Use from Java


Extending Fuseki with Fuseki Modules
Links to Standards

The Jena users mailing is the place to get help with Fuseki.
Email support lists
Download Fuseki
Releases of Apache Jena Fuseki can be downloaded from:
Jena Downloads
Fuseki download files

  
      
          Filename
          Description
      
  
  
      
          apache-jena-fuseki-*VER*.zip
          The Fuseki server and UI
      
  

The Fuseki engine is also available as a Maven artifact:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-main</artifactId>
   <version>X.Y.Z</version>
</dependency>

and the UI is available as:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-ui</artifactId>
   <version>X.Y.Z</version>
</dependency>

A WAR file is also available from the Jena download page.
Previous releases
While previous releases are available, we strongly recommend that wherever
possible users use the latest official Apache releases of Jena in
preference to using any older versions of Jena.
Previous Apache Jena releases can be found in the Apache archive area
at https://archive.apache.org/dist/jena
Development Builds
Regular development builds of all of Jena are available
(these are not formal releases) from the
Apache snapshots maven repository.
This includes the packaged build of Fuseki.
How to Contribute
We welcome contributions towards making Jena a better platform for semantic
web and linked data applications.  We appreciate feature suggestions, bug
reports and patches for code or documentation.
See “Getting Involved” for ways to
contribute to Jena and Fuseki, including patches and making github
pull-requests.
Source code
The development codebase is available from git.
Development builds (not a formal release):
SNAPSHOT
Source code:
https://github.com/apache/jena/tree/main/jena-fuseki2
The Fuseki code is under “jena-fuseki2/”:

  
      
          Code
          Purpose
      
  
  
      
          jena-fuseki-main
          The Fuseki server
      
      
          jena-fuseki-core
          The Fuseki engine
      
      
          jena-fuseki-server
          Build the combined jar for Fuseki server
      
      
          jena-fuseki-access
          Data access control
      
      
          apache-jena-fuseki
          The download for Fuseki
      
      
          Other
          
      
      
          jena-fuseki-docker
          Build a docker container for Fuseki
      
      
          jena-fuseki-geosparql
          Integration for GeoSPARQL
      
      
          Webapp
          
      
      
          jena-fuseki-webapp
          Web application and command line startup
      
      
          jena-fuseki-fulljar
          Build the combined jar for Fuseki/UI server
      
      
          jena-fuseki-war
          Build the war file for Fuseki/UI server\n\nOn this page
    
  
    Contents
    Download Fuseki
      
        Previous releases
        Development Builds
      
    
    How to Contribute
      
        Source code
      
    
  

  
  
    Apache Jena Fuseki is a SPARQL server.  It can run as a standalone server, or embedded in an
application.
Fuseki provides the
SPARQL 1.1 protocols for query and update
as well as the
SPARQL Graph Store protocol.
Fuseki is integrated with TDB to provide a robust,
transactional, persistent storage layer. Fuseki also incorporates
Jena text query.
Contents

Download
Getting Started
Running Fuseki Server

As a standalone server
As a service
As a web application
Security with Apache Shiro


Running Fuseki Plain

Setup
As a Docker container
As an embedded SPARQL server
Security and data access control
Logging


Fuseki Configuration
Server Statistics and Metrics
How to Contribute
Client access

Use from Java


Extending Fuseki with Fuseki Modules
Links to Standards

The Jena users mailing is the place to get help with Fuseki.
Email support lists
Download Fuseki
Releases of Apache Jena Fuseki can be downloaded from:
Jena Downloads
Fuseki download files

  
      
          Filename
          Description
      
  
  
      
          apache-jena-fuseki-*VER*.zip
          The Fuseki server and UI
      
  

The Fuseki engine is also available as a Maven artifact:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-main</artifactId>
   <version>X.Y.Z</version>
</dependency>

and the UI is available as:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-ui</artifactId>
   <version>X.Y.Z</version>
</dependency>

A WAR file is also available from the Jena download page.
Previous releases
While previous releases are available, we strongly recommend that wherever
possible users use the latest official Apache releases of Jena in
preference to using any older versions of Jena.
Previous Apache Jena releases can be found in the Apache archive area
at https://archive.apache.org/dist/jena
Development Builds
Regular development builds of all of Jena are available
(these are not formal releases) from the
Apache snapshots maven repository.
This includes the packaged build of Fuseki.
How to Contribute
We welcome contributions towards making Jena a better platform for semantic
web and linked data applications.  We appreciate feature suggestions, bug
reports and patches for code or documentation.
See “Getting Involved” for ways to
contribute to Jena and Fuseki, including patches and making github
pull-requests.
Source code
The development codebase is available from git.
Development builds (not a formal release):
SNAPSHOT
Source code:
https://github.com/apache/jena/tree/main/jena-fuseki2
The Fuseki code is under “jena-fuseki2/”:

  
      
          Code
          Purpose
      
  
  
      
          jena-fuseki-main
          The Fuseki server
      
      
          jena-fuseki-core
          The Fuseki engine
      
      
          jena-fuseki-server
          Build the combined jar for Fuseki server
      
      
          jena-fuseki-access
          Data access control
      
      
          apache-jena-fuseki
          The download for Fuseki
      
      
          Other
          
      
      
          jena-fuseki-docker
          Build a docker container for Fuseki
      
      
          jena-fuseki-geosparql
          Integration for GeoSPARQL
      
      
          Webapp
          
      
      
          jena-fuseki-webapp
          Web application and command line startup
      
      
          jena-fuseki-fulljar
          Build the combined jar for Fuseki/UI server
      
      
          jena-fuseki-war
          Build the war file for Fuseki/UI server
      
  


  
  
  
    On this page
    
  
    Contents
    Download Fuseki
      
        Previous releases
        Development Builds
      
    
    How to Contribute
      
        Source code\n\n\n\nDataset Transactions
Concurrency how-to Handling concurrent access to Jena models
Event handler how-to Responding to events
Stream manager how-to Redirecting URLs to local files
Model factory Creating Jena models of various kinds
RDF frames Viewing RDF statements as frame-like objects
Typed literals Creating and extracting RDF typed literals
SSE SPARQL Syntax Expressions
Repacking Jena jars
Jena Initialization\n\nOn this page
    
  
  
    
Dataset Transactions
Concurrency how-to Handling concurrent access to Jena models
Event handler how-to Responding to events
Stream manager how-to Redirecting URLs to local files
Model factory Creating Jena models of various kinds
RDF frames Viewing RDF statements as frame-like objects
Typed literals Creating and extracting RDF typed literals
SSE SPARQL Syntax Expressions
Repacking Jena jars
Jena Initialization


  
  
  
    On this page\n\n\n\nWhen you’ve been working with SPARQL you quickly find that static
queries are restrictive. Maybe you want to vary a value, perhaps add a
filter, alter the limit, etc etc. Being an impatient sort you dive in to
the query string, and it works. But what about little Bobby
Tables? And, even if you
sanitise your inputs, string manipulation is a fraught process and
syntax errors await you. Although it might seem harder than string
munging, the ARQ API is your friend in the long run.
Originally published on the Research Revealed project
blog
Inserting values (simple prepared statements)
Let’s begin with something simple. Suppose we wanted to restrict the
following query to a particular person:
   select * { ?person <http://xmlns.com/foaf/0.1/name> ?name }

String#replaceAll would work, but there is a safer way.
QueryExecutionFactory in most cases lets you supply a QuerySolution
with which you can prebind values.
   QuerySolutionMap initialBinding = new QuerySolutionMap();
   initialBinding.add("name", personResource);
   qe = QueryExecutionFactory.create(query, dataset, initialBinding);

This is often much simpler than the string equivalent since you don’t
have to escape quotes in literals. (Beware that this doesn’t work for
sparqlService, which is a great shame. It would be nice to spend some
time remedying that.)
Making a Query from Scratch
The previously mentioned limitation is due to the fact that prebinding
doesn’t actually change the query at all, but the execution of that
query. So what how do we really alter queries?
ARQ provides two ways to work with queries: at the syntax level (Query
and Element), or the algebra level (Op). The distinction is clear in
filters:
   SELECT ?s { ?s <http://example.com/val> ?val . FILTER ( ?val < 20 ) }

If you work at the syntax level you’ll find that this looks (in pseudo
code) like:
   (GROUP (PATTERN ( ?s <http://example.com/val> ?val )) (FILTER ( < ?val 20 ) ))

That is there’s a group containing a triple pattern and a filter, just
as you see in the query. The algebra is different, and we can see it
using arq.qparse --print op
   $ java arq.qparse --print op 'SELECT ?s { ?s <http://example.com/val> ?val . FILTER ( ?val < 20 ) }'
   (base <file:///...>
       (project (?s)
           (filter (< ?val 20)
               (bgp (triple ?s <http://example.com/val> ?val)))))

Here the filter contains the pattern, rather than sitting next to it.
This form makes it clear that the expression is filtering the pattern.
Let’s create that query from scratch using ARQ. We begin with some
common pieces: the triple to match, and the expression for the filter.
   // ?s ?p ?o .
   Triple pattern =
       Triple.create(Var.alloc("s"), Var.alloc("p"), Var.alloc("o"));
   // ( ?s < 20 )
   Expr e = new E_LessThan(new ExprVar("s"), new NodeValueInteger(20));

Triple should be familiar from jena. Var is an extension of Node
for variables. Expr is the root interface for expressions, those
things that appear in FILTER and LET.
First the syntax route:
   ElementTriplesBlock block = new ElementTriplesBlock(); // Make a BGP
   block.addTriple(pattern);                              // Add our pattern match
   ElementFilter filter = new ElementFilter(e);           // Make a filter matching the expression
   ElementGroup body = new ElementGroup();                // Group our pattern match and filter
   body.addElement(block);
   body.addElement(filter);

   Query q = QueryFactory.make();
   q.setQueryPattern(body);                               // Set the body of the query to our group
   q.setQuerySelectType();                                // Make it a select query
   q.addResultVar("s");                                   // Select ?s

Now the algebra:
   Op op;
   BasicPattern pat = new BasicPattern();                 // Make a pattern
   pat.add(pattern);                                      // Add our pattern match
   op = new OpBGP(pat);                                   // Make a BGP from this pattern
   op = OpFilter.filter(e, op);                           // Filter that pattern with our expression
   op = new OpProject(op, Arrays.asList(Var.alloc("s"))); // Reduce to just ?s
   Query q = OpAsQuery.asQuery(op);                       // Convert to a query
   q.setQuerySelectType();                                // Make is a select query

Notice that the query form (SELECT, CONSTRUCT, DESCRIBE, ASK) isn’t
part of the algebra, and we have to set this in the query (although
SELECT is the default). FROM and FROM NAMED are similarly absent.
Navigating and Tinkering: Visitors
You can also look around the algebra and syntax using visitors. Start by
extending OpVisitorBase (ElementVisitorBase) which stubs out the
interface so you can concentrate on the parts of interest, then walk
using OpWalker.walk(Op, OpVisitor)
(ElementWalker.walk(Element, ElementVisitor)). These work bottom up.
For some alterations, like manipulating triple matches in place,
visitors will do the trick. They provide a simple way to get to the
right parts of the query, and you can alter the pattern backing BGPs in
both the algebra and syntax. Mutation isn’t consistently available,
however, so don’t depend on it.
Transforming the Algebra
So far there is no obvious advantage in using the algebra. The real
power is visible in transformers, which allow you to reorganise an
algebra completely. ARQ makes extensive use of transformations to
simplify and optimise query execution.
In Research Revealed I wrote some code to take a number of constraints
and produce a query. There were a number of ways to do this, but one way
I found was to generate ops from each constraint and join the results:
   for (Constraint con: cons) {
       op = OpJoin.create(op, consToOp(cons)); // join
   }

The result was a perfectly correct mess, which is only barely readable
with just three conditions:
   (join
       (join
           (filter (< ?o0 20) (bgp (triple ?s <urn:ex:prop0> ?o0)))
           (filter (< ?o1 20) (bgp (triple ?s <urn:ex:prop1> ?o1))))
       (filter (< ?o2 20) (bgp (triple ?s <urn:ex:prop2> ?o2))))

Each of the constraints is a filter on a bgp. This can be made much more
readable by moving the filters out, and merging the triple patterns. We
can do this with the following Transform:
   class QueryCleaner extends TransformBase
   {
       @Override
       public Op transform(OpJoin join, Op left, Op right) {
           // Bail if not of the right form
           if (!(left instanceof OpFilter && right instanceof OpFilter)) return join;
           OpFilter leftF = (OpFilter) left;
           OpFilter rightF = (OpFilter) right;

           // Add all of the triple matches to the LHS BGP
           ((OpBGP) leftF.getSubOp()).getPattern().addAll(((OpBGP) rightF.getSubOp()).getPattern());
           // Add the RHS filter to the LHS
           leftF.getExprs().addAll(rightF.getExprs());
           return leftF;
       }
   }
   ...
   op = Transformer.transform(new QueryCleaner(), op); // clean query

This looks for joins of the form:
   (join
       (filter (exp1) (bgp1))
       (filter (exp2) (bgp2)))

And replaces it with:
   (filter (exp1 && exp2) (bgp1 && bgp2))

As we go through the original query all joins are removed, and the
result is:
   (filter (exprlist (< ?o0 20) (< ?o1 20) (< ?o2 20))
       (bgp
           (triple ?s <urn:ex:prop0> ?o0)
           (triple ?s <urn:ex:prop1> ?o1)
           (triple ?s <urn:ex:prop2> ?o2)
   ))

That completes this brief introduction. There is much more to ARQ, of
course, but hopefully you now have a taste for what it can do.\n\nOn this page
    
  
    Inserting values (simple prepared statements)
    Making a Query from Scratch
    Navigating and Tinkering: Visitors
    Transforming the Algebra
  

  
  
    When you’ve been working with SPARQL you quickly find that static
queries are restrictive. Maybe you want to vary a value, perhaps add a
filter, alter the limit, etc etc. Being an impatient sort you dive in to
the query string, and it works. But what about little Bobby
Tables? And, even if you
sanitise your inputs, string manipulation is a fraught process and
syntax errors await you. Although it might seem harder than string
munging, the ARQ API is your friend in the long run.
Originally published on the Research Revealed project
blog
Inserting values (simple prepared statements)
Let’s begin with something simple. Suppose we wanted to restrict the
following query to a particular person:
   select * { ?person <http://xmlns.com/foaf/0.1/name> ?name }

String#replaceAll would work, but there is a safer way.
QueryExecutionFactory in most cases lets you supply a QuerySolution
with which you can prebind values.
   QuerySolutionMap initialBinding = new QuerySolutionMap();
   initialBinding.add("name", personResource);
   qe = QueryExecutionFactory.create(query, dataset, initialBinding);

This is often much simpler than the string equivalent since you don’t
have to escape quotes in literals. (Beware that this doesn’t work for
sparqlService, which is a great shame. It would be nice to spend some
time remedying that.)
Making a Query from Scratch
The previously mentioned limitation is due to the fact that prebinding
doesn’t actually change the query at all, but the execution of that
query. So what how do we really alter queries?
ARQ provides two ways to work with queries: at the syntax level (Query
and Element), or the algebra level (Op). The distinction is clear in
filters:
   SELECT ?s { ?s <http://example.com/val> ?val . FILTER ( ?val < 20 ) }

If you work at the syntax level you’ll find that this looks (in pseudo
code) like:
   (GROUP (PATTERN ( ?s <http://example.com/val> ?val )) (FILTER ( < ?val 20 ) ))

That is there’s a group containing a triple pattern and a filter, just
as you see in the query. The algebra is different, and we can see it
using arq.qparse --print op
   $ java arq.qparse --print op 'SELECT ?s { ?s <http://example.com/val> ?val . FILTER ( ?val < 20 ) }'
   (base <file:///...>
       (project (?s)
           (filter (< ?val 20)
               (bgp (triple ?s <http://example.com/val> ?val)))))

Here the filter contains the pattern, rather than sitting next to it.
This form makes it clear that the expression is filtering the pattern.
Let’s create that query from scratch using ARQ. We begin with some
common pieces: the triple to match, and the expression for the filter.
   // ?s ?p ?o .
   Triple pattern =
       Triple.create(Var.alloc("s"), Var.alloc("p"), Var.alloc("o"));
   // ( ?s < 20 )
   Expr e = new E_LessThan(new ExprVar("s"), new NodeValueInteger(20));

Triple should be familiar from jena. Var is an extension of Node
for variables. Expr is the root interface for expressions, those
things that appear in FILTER and LET.
First the syntax route:
   ElementTriplesBlock block = new ElementTriplesBlock(); // Make a BGP
   block.addTriple(pattern);                              // Add our pattern match
   ElementFilter filter = new ElementFilter(e);           // Make a filter matching the expression
   ElementGroup body = new ElementGroup();                // Group our pattern match and filter
   body.addElement(block);
   body.addElement(filter);

   Query q = QueryFactory.make();
   q.setQueryPattern(body);                               // Set the body of the query to our group
   q.setQuerySelectType();                                // Make it a select query
   q.addResultVar("s");                                   // Select ?s

Now the algebra:
   Op op;
   BasicPattern pat = new BasicPattern();                 // Make a pattern
   pat.add(pattern);                                      // Add our pattern match
   op = new OpBGP(pat);                                   // Make a BGP from this pattern
   op = OpFilter.filter(e, op);                           // Filter that pattern with our expression
   op = new OpProject(op, Arrays.asList(Var.alloc("s"))); // Reduce to just ?s
   Query q = OpAsQuery.asQuery(op);                       // Convert to a query
   q.setQuerySelectType();                                // Make is a select query

Notice that the query form (SELECT, CONSTRUCT, DESCRIBE, ASK) isn’t
part of the algebra, and we have to set this in the query (although
SELECT is the default). FROM and FROM NAMED are similarly absent.
Navigating and Tinkering: Visitors
You can also look around the algebra and syntax using visitors. Start by
extending OpVisitorBase (ElementVisitorBase) which stubs out the
interface so you can concentrate on the parts of interest, then walk
using OpWalker.walk(Op, OpVisitor)
(ElementWalker.walk(Element, ElementVisitor)). These work bottom up.
For some alterations, like manipulating triple matches in place,
visitors will do the trick. They provide a simple way to get to the
right parts of the query, and you can alter the pattern backing BGPs in
both the algebra and syntax. Mutation isn’t consistently available,
however, so don’t depend on it.
Transforming the Algebra
So far there is no obvious advantage in using the algebra. The real
power is visible in transformers, which allow you to reorganise an
algebra completely. ARQ makes extensive use of transformations to
simplify and optimise query execution.
In Research Revealed I wrote some code to take a number of constraints
and produce a query. There were a number of ways to do this, but one way
I found was to generate ops from each constraint and join the results:
   for (Constraint con: cons) {
       op = OpJoin.create(op, consToOp(cons)); // join
   }

The result was a perfectly correct mess, which is only barely readable
with just three conditions:
   (join
       (join
           (filter (< ?o0 20) (bgp (triple ?s <urn:ex:prop0> ?o0)))
           (filter (< ?o1 20) (bgp (triple ?s <urn:ex:prop1> ?o1))))
       (filter (< ?o2 20) (bgp (triple ?s <urn:ex:prop2> ?o2))))

Each of the constraints is a filter on a bgp. This can be made much more
readable by moving the filters out, and merging the triple patterns. We
can do this with the following Transform:
   class QueryCleaner extends TransformBase
   {
       @Override
       public Op transform(OpJoin join, Op left, Op right) {
           // Bail if not of the right form
           if (!(left instanceof OpFilter && right instanceof OpFilter)) return join;
           OpFilter leftF = (OpFilter) left;
           OpFilter rightF = (OpFilter) right;

           // Add all of the triple matches to the LHS BGP
           ((OpBGP) leftF.getSubOp()).getPattern().addAll(((OpBGP) rightF.getSubOp()).getPattern());
           // Add the RHS filter to the LHS
           leftF.getExprs().addAll(rightF.getExprs());
           return leftF;
       }
   }
   ...
   op = Transformer.transform(new QueryCleaner(), op); // clean query

This looks for joins of the form:
   (join
       (filter (exp1) (bgp1))
       (filter (exp2) (bgp2)))

And replaces it with:
   (filter (exp1 && exp2) (bgp1 && bgp2))

As we go through the original query all joins are removed, and the
result is:
   (filter (exprlist (< ?o0 20) (< ?o1 20) (< ?o2 20))
       (bgp
           (triple ?s <urn:ex:prop0> ?o0)
           (triple ?s <urn:ex:prop1> ?o1)
           (triple ?s <urn:ex:prop2> ?o2)
   ))

That completes this brief introduction. There is much more to ARQ, of
course, but hopefully you now have a taste for what it can do.

  
  
  
    On this page
    
  
    Inserting values (simple prepared statements)
    Making a Query from Scratch
    Navigating and Tinkering: Visitors
    Transforming the Algebra\n\n\n\nPreface
This is a tutorial introduction to both W3C’s Resource Description Framework
(RDF) and Jena, a Java API for RDF.  It is written for the programmer who is
unfamiliar with RDF and who learns best by prototyping, or, for other
reasons, wishes to move quickly to implementation. Some familiarity
with both XML and Java is assumed.
Implementing too quickly, without first understanding the RDF data model,
leads to frustration and disappointment. Yet studying the data model
alone is dry stuff and often leads to tortuous metaphysical conundrums.  It
is better to approach understanding both the data model and how to use it in
parallel. Learn a bit of the data model and try it out. Then
learn a bit more and try that out. Then the theory informs the practice
and the practice the theory. The data model is quite simple, so this
approach does not take long.
RDF has an XML syntax and many who are familiar with XML will think of RDF
in terms of that syntax. This is a mistake. RDF should be
understood in terms of its data model. RDF data can be represented in
XML, but understanding the syntax is secondary to understanding the data
model.
An implementation of the Jena API, including the working source code for
all the examples used in this tutorial can be downloaded from
jena.apache.org/download/index.cgi.
Introduction
The Resource Description Framework (RDF) is a standard (technically a W3C
Recommendation) for describing resources. What is a resource?  That is
rather a deep question and the precise definition is still the subject of
debate.  For our purposes we can think of it as anything we can identify.
You are a resource, as is your home page, this tutorial, the number one and
the great white whale in Moby Dick.
Our examples in this tutorial will be about people.  They use an RDF representation of VCARDS.  RDF
is best thought of in the form of node and arc diagrams.  A simple vcard
might look like this in RDF:


The resource, John Smith, is shown as
an ellipse and is identified by a Uniform Resource Identifier (URI)1,  in this case
"http://.../JohnSmith".  If you try to access that resource using your
browser, you are unlikely to be successful; April the first jokes not
withstanding, you would be rather surprised if your browser were able to
deliver John Smith to your desk top. If you are unfamiliar with URI's,
think of them simply as rather strange looking names.
Resources have properties.  In these
examples we are interested in the sort of properties that would appear on
John Smith's business card. Figure 1 shows only one property, John
Smith's full name.  A property is represented by an arc, labeled with the
name of a property.  The name of a property is also a URI, but as URI's are
rather long and cumbersome, the diagram shows it in XML qname form.   The
part before the ':' is called a namespace prefix and represents a
namespace. The part after the ':' is called a local name and represents
a name in that namespace. Properties are usually represented in this
qname form when written as RDF XML and it is a convenient shorthand for
representing them in diagrams and in text. Strictly, however,
properties are identified by a URI. The nsprefix:localname form is a
shorthand for the URI of the namespace concatenated with the localname.
There is no requirement that the URI of a property resolve to anything when
accessed by a browser.
Each property has a value. In this case the value is a literal, which for now we can think of as a
strings of characters2.
Literals are shown in rectangles.
Jena is a Java API which can be used to create and manipulate RDF graphs
like this one. Jena has object classes to represent graphs, resources,
properties and literals. The interfaces representing resources,
properties and literals are called Resource, Property and Literal
respectively. In Jena, a graph is called a model and is represented by the Model
interface.
The code to create this graph, or model, is simple:
// some definitions
static String personURI    = "http://somewhere/JohnSmith";
static String fullName     = "John Smith";

// create an empty Model
Model model = ModelFactory.createDefaultModel();

// create the resource
Resource johnSmith = model.createResource(personURI);

// add the property
johnSmith.addProperty(VCARD.FN, fullName);
It begins with some constant definitions and then creates an empty Model
or model, using the ModelFactory method createDefaultModel()
to create a memory-based model. Jena contains other implementations
of the Model interface, e.g one which uses a relational database: these
types of Model are also available from ModelFactory.
The John Smith resource is then created and a property added to it.
The property is provided by a "constant" class VCARD which holds objects
representing all the definitions in the VCARD schema.  Jena provides constant
classes for other well known schemas, such as RDF and RDF schema themselves,
Dublin Core and OWL.
The working code for this example can be found in the /src-examples directory of
the Jena distribution as 
tutorial
1.
As an exercise, take this code and modify it to create a simple VCARD for
yourself.
The code to create the resource and add the property, can be more
compactly written in a cascading style:
Resource johnSmith =
      model.createResource(personURI)
           .addProperty(VCARD.FN, fullName);
Now let's add some more detail to the vcard, exploring some more features
of RDF and Jena.
In the first example, the property value was a literal. RDF
properties can also take other resources as their value.  Using a common RDF
technique, this example shows how to represent the different parts of John
Smith's name:


Here we have added a new property, vcard:N, to represent the structure of
John Smith's name.  There are several things of interest about this
Model. Note that the vcard:N property takes a resource as its value.
Note also that the ellipse representing the compound name has no URI.
It is known as an blank Node.
The Jena code to construct this example, is again very simple. First
some declarations and the creation of the empty model.
// some definitions
String personURI    = "http://somewhere/JohnSmith";
String givenName    = "John";
String familyName   = "Smith";
String fullName     = givenName + " " + familyName;

// create an empty Model
Model model = ModelFactory.createDefaultModel();

// create the resource
//   and add the properties cascading style
Resource johnSmith
  = model.createResource(personURI)
         .addProperty(VCARD.FN, fullName)
         .addProperty(VCARD.N,
                      model.createResource()
                           .addProperty(VCARD.Given, givenName)
                           .addProperty(VCARD.Family, familyName));
The working code for this example can be found as tutorial 2 in the /src-examples directory
of the Jena distribution.
Statements
Each arc in an RDF Model is called a statement. Each statement asserts a fact
about a resource. A statement has three parts:

  the  subject is the resource from
    which the arc leaves
  the  predicate is the property
    that labels the arc
  the  object is the resource or
    literal pointed to by the arc

A statement is sometimes called a triple,
because of its three parts.
An RDF Model is represented as a set of statements.  Each call of
addProperty in tutorial2 added another statement to the Model.
(Because a Model is set of statements, adding a duplicate of a statement has no
effect.)  The Jena model interface defines a listStatements()
method which returns an StmtIterator, a subtype of Java's
Iterator over all the statements in a Model.
StmtIterator has a method nextStatement()
which returns the next statement from the iterator (the same one that
next() would deliver, already cast to Statement).
The Statement interface provides accessor
methods to the subject, predicate and object of a statement.
Now we will use that interface to extend tutorial2 to list all the
statements created and print them out.  The complete code for this can be
found in tutorial 3.
// list the statements in the Model
StmtIterator iter = model.listStatements();

// print out the predicate, subject and object of each statement
while (iter.hasNext()) {
    Statement stmt      = iter.nextStatement();  // get next statement
    Resource  subject   = stmt.getSubject();     // get the subject
    Property  predicate = stmt.getPredicate();   // get the predicate
    RDFNode   object    = stmt.getObject();      // get the object

    System.out.print(subject.toString());
    System.out.print(" " + predicate.toString() + " ");
    if (object instanceof Resource) {
       System.out.print(object.toString());
    } else {
        // object is a literal
        System.out.print(" \"" + object.toString() + "\"");
    }

    System.out.println(" .");
}
Since the object of a statement can be either a resource or a literal, the
getObject() method returns an
object typed as RDFNode, which is a
common superclass of both Resource
and Literal.  The underlying object
is of the appropriate type, so the code uses instanceof to
determine which and
processes it accordingly.
When run, this program should produce output resembling:
http://somewhere/JohnSmith http://www.w3.org/2001/vcard-rdf/3.0#N 413f6415-c3b0-4259-b74d-4bd6e757eb60 .
413f6415-c3b0-4259-b74d-4bd6e757eb60 http://www.w3.org/2001/vcard-rdf/3.0#Family  "Smith" .
413f6415-c3b0-4259-b74d-4bd6e757eb60 http://www.w3.org/2001/vcard-rdf/3.0#Given  "John" .
http://somewhere/JohnSmith http://www.w3.org/2001/vcard-rdf/3.0#FN  "John Smith" .

Now you know why it is clearer to draw Models.  If you look carefully, you
will see that each line consists of three fields representing the subject,
predicate and object of each statement.  There are four arcs in the Model, so
there are four statements.  The "14df86:ecc3dee17b:-7fff" is an internal
identifier generated by Jena.  It is not a URI and should not be confused
with one.  It is simply an internal label used by the Jena implementation.
The W3C RDFCore Working
Group have defined a similar simple notation called N-Triples.  The name
means "triple notation".  We will see in the next section that Jena has an
N-Triples writer built in.
Writing RDF
Jena has methods for reading and writing RDF as XML. These can be
used to save an RDF model to a file and later read it back in again.
Tutorial 3 created a model and wrote it out in triple form. Tutorial 4 modifies tutorial 3 to write the
model in RDF XML form to the standard output stream. The code again, is
very simple: model.write can take an OutputStream
argument.
// now write the model in XML form to a file
model.write(System.out);
The output should look something like this:
<rdf:RDF
  xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#'
  xmlns:vcard='http://www.w3.org/2001/vcard-rdf/3.0#'
 >
  <rdf:Description rdf:about='http://somewhere/JohnSmith'>
    <vcard:FN>John Smith</vcard:FN>
    <vcard:N rdf:nodeID="A0"/>
  </rdf:Description>
  <rdf:Description rdf:nodeID="A0">
    <vcard:Given>John</vcard:Given>
    <vcard:Family>Smith</vcard:Family>
  </rdf:Description>
</rdf:RDF>
The RDF specifications specify how to represent RDF as XML. The RDF
XML syntax is quite complex.  The reader is referred to the primer being developed by the
RDFCore WG for a more detailed introduction.  However, let's take a quick look
at how to interpret the above.
RDF is usually embedded in an <rdf:RDF> element.  The element is
optional if there are other ways of knowing that some XML is RDF, but it is
usually present.  The RDF element defines the two namespaces used in the
document.  There is then an <rdf:Description> element which describes
the resource whose URI is "http://somewhere/JohnSmith".  If the rdf:about
attribute was missing, this element would represent a blank node.
The <vcard:FN> element describes a property of the resource.  The
property name is the "FN" in the vcard namespace.  RDF converts this to a URI
reference by concatenating the URI reference for the namespace prefix and
"FN", the local name part of the name.  This gives a URI reference of
"http://www.w3.org/2001/vcard-rdf/3.0#FN".  The value of the property
is the literal "John Smith".
The <vcard:N> element is a resource.  In this case the resource is
represented by a relative URI reference.  RDF converts this to an absolute
URI reference by concatenating it with the base URI of the current
document.
There is an error in this RDF XML; it does not exactly represent the Model
we created.  The blank node in the Model has been given a URI reference.  It
is no longer blank.  The RDF/XML syntax is not capable of representing all
RDF Models; for example it cannot represent a blank node which is the object
of two statements.  The 'dumb' writer we used to write this RDF/XML  makes no
attempt to write correctly the subset of Models which can be written
correctly.  It gives a URI to each blank node, making it no longer blank.
Jena has an extensible interface which allows new writers for different
serialization languages for RDF to be easily plugged in.  The above call
invoked the standard 'dumb' writer.  Jena also includes a more sophisticated
RDF/XML writer which can be invoked by using
RDFDataMgr.write function call:
// now write the model in a pretty form
RDFDataMgr.write(System.out, model, Lang.RDFXML);
This writer, the so called PrettyWriter, takes advantage of features of
the RDF/XML abbreviated syntax to write a Model more compactly.  It is also
able to preserve blank nodes where that is possible.  It is however, not
suitable for writing very large Models, as its performance is unlikely to be
acceptable.  To write large files and preserve blank nodes, write in
N-Triples format:
// now write the model in N-TRIPLES form
RDFDataMgr.write(System.out, model, Lang.NTRIPLES);
This will produce output similar to that of tutorial 3 which conforms to
the N-Triples specification.
Reading RDF
Tutorial 5 demonstrates reading the
statements recorded in RDF XML form into a model. With this tutorial,
we have provided a small database of vcards in RDF/XML form.  The following
code will read it in and write it out. Note that for this application to
run, the input file must be in the current directory.
// create an empty model
Model model = ModelFactory.createDefaultModel();

// use the RDFDataMgr to find the input file
InputStream in = RDFDataMgr.open( inputFileName );
if (in == null) {
    throw new IllegalArgumentException("File: " + inputFileName + " not found");
}

// read the RDF/XML file
model.read(in, null);

// write it to standard out
model.write(System.out);
The second argument to the read() method call is the URI which will
be used for resolving relative URI's.  As there are no relative URI
references in the test file, it is allowed to be empty. When run,  tutorial 5 will produce XML output which
looks like:
<rdf:RDF
  xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#'
  xmlns:vcard='http://www.w3.org/2001/vcard-rdf/3.0#'
 >
  <rdf:Description rdf:nodeID="A0">
    <vcard:Family>Smith</vcard:Family>
    <vcard:Given>John</vcard:Given>
  </rdf:Description>
  <rdf:Description rdf:about='http://somewhere/JohnSmith/'>
    <vcard:FN>John Smith</vcard:FN>
    <vcard:N rdf:nodeID="A0"/>
  </rdf:Description>
  <rdf:Description rdf:about='http://somewhere/SarahJones/'>
    <vcard:FN>Sarah Jones</vcard:FN>
    <vcard:N rdf:nodeID="A1"/>
  </rdf:Description>
  <rdf:Description rdf:about='http://somewhere/MattJones/'>
    <vcard:FN>Matt Jones</vcard:FN>
    <vcard:N rdf:nodeID="A2"/>
  </rdf:Description>
  <rdf:Description rdf:nodeID="A3">
    <vcard:Family>Smith</vcard:Family>
    <vcard:Given>Rebecca</vcard:Given>
  </rdf:Description>
  <rdf:Description rdf:nodeID="A1">
    <vcard:Family>Jones</vcard:Family>
    <vcard:Given>Sarah</vcard:Given>
  </rdf:Description>
  <rdf:Description rdf:nodeID="A2">
    <vcard:Family>Jones</vcard:Family>
    <vcard:Given>Matthew</vcard:Given>
  </rdf:Description>
  <rdf:Description rdf:about='http://somewhere/RebeccaSmith/'>
    <vcard:FN>Becky Smith</vcard:FN>
    <vcard:N rdf:nodeID="A3"/>
  </rdf:Description>
</rdf:RDF>
Controlling Prefixes
Explicit prefix definitions
In the previous section, we saw that the output XML declared a namespace
prefix vcard and used that prefix to abbreviate URIs. While RDF
uses only the full URIs, and not this shortened form, Jena provides ways
of controlling the namespaces used on output with its prefix mappings.
Here’s a simple example.
Model m = ModelFactory.createDefaultModel();
String nsA = "http://somewhere/else#";
String nsB = "http://nowhere/else#";
Resource root = m.createResource( nsA + "root" );
Property P = m.createProperty( nsA + "P" );
Property Q = m.createProperty( nsB + "Q" );
Resource x = m.createResource( nsA + "x" );
Resource y = m.createResource( nsA + "y" );
Resource z = m.createResource( nsA + "z" );
m.add( root, P, x ).add( root, P, y ).add( y, Q, z );
System.out.println( "# -- no special prefixes defined" );
m.write( System.out );
System.out.println( "# -- nsA defined" );
m.setNsPrefix( "nsA", nsA );
m.write( System.out );
System.out.println( "# -- nsA and cat defined" );
m.setNsPrefix( "cat", nsB );
m.write( System.out );
The output from this fragment is lots of RDF/XML, with
three different prefix mappings. First the default, with no
prefixes other than the standard ones:
# -- no special prefixes defined

<rdf:RDF
    xmlns:j.0="http://nowhere/else#"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:j.1="http://somewhere/else#" >
  <rdf:Description rdf:about="http://somewhere/else#root">
    <j.1:P rdf:resource="http://somewhere/else#x"/>
    <j.1:P rdf:resource="http://somewhere/else#y"/>
  </rdf:Description>
  <rdf:Description rdf:about="http://somewhere/else#y">
    <j.0:Q rdf:resource="http://somewhere/else#z"/>
  </rdf:Description>
</rdf:RDF>
We see that the rdf namespace is declared automatically, since it
is required for tags such as <rdf:RDF> and
<rdf:resource>. XML namespace declarations are also
needed for using the two properties P and Q, but since their
prefixes have not been introduced to the model in this example,
they get invented namespace names: j.0 and j.1.
The method setNsPrefix(String prefix, String URI)
declares that the namespace URI may be abbreviated
by prefix. Jena requires that prefix be
a legal XML namespace name, and that URI ends with a
non-name character. The RDF/XML writer will turn these prefix
declarations into XML namespace declarations and use them in its
output:
# -- nsA defined

<rdf:RDF
    xmlns:j.0="http://nowhere/else#"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:nsA="http://somewhere/else#" >
  <rdf:Description rdf:about="http://somewhere/else#root">
    <nsA:P rdf:resource="http://somewhere/else#x"/>
    <nsA:P rdf:resource="http://somewhere/else#y"/>
  </rdf:Description>
  <rdf:Description rdf:about="http://somewhere/else#y">
    <j.0:Q rdf:resource="http://somewhere/else#z"/>
  </rdf:Description>
</rdf:RDF>
is now used in the property tags. There’s no need for the prefix name
to have anything to do with the variables in the Jena code:
# -- nsA and cat defined

<rdf:RDF
    xmlns:cat="http://nowhere/else#"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:nsA="http://somewhere/else#" >
  <rdf:Description rdf:about="http://somewhere/else#root">
    <nsA:P rdf:resource="http://somewhere/else#x"/>
    <nsA:P rdf:resource="http://somewhere/else#y"/>
  </rdf:Description>
  <rdf:Description rdf:about="http://somewhere/else#y">
    <cat:Q rdf:resource="http://somewhere/else#z"/>
  </rdf:Description>
</rdf:RDF>
Both prefixes are used for output, and no generated prefixes are needed.
Implicit prefix definitions
As well as prefix declarations provided by calls to setNsPrefix,
Jena will remember the prefixes that were used in input to
model.read().
Take the output produced by the previous fragment, and paste it into
some file, with URL file:/tmp/fragment.rdf say. Then run the
code:
Model m2 = ModelFactory.createDefaultModel();
m2.read( "file:/tmp/fragment.rdf" );
m2.write( System.out );
You’ll see that the prefixes from the input are preserved in the output.
All the prefixes are written, even if they’re not used anywhere. You can
remove a prefix with removeNsPrefix(String prefix) if you
don’t want it in the output.
Since NTriples doesn't have any short way of writing URIs, it takes
no notice of prefixes on output and doesn't provide any on input. The
notation N3, also supported by Jena, does have short prefixed names,
and records them on input and uses them on output.
Jena has further operations on the prefix mappings that a model holds,
such as extracting a Java Map of the exiting mappings, or
adding a whole group of mappings at once; see the documentation for
PrefixMapping for details.
Jena RDF Packages
Jena is a Java API for semantic web applications.  The key RDF package for
the application developer is
org.apache.jena.rdf.model. The API has been defined
in terms of interfaces so that application code can work with different
implementations without change. This package contains interfaces for
representing models, resources, properties, literals, statements and all the
other key concepts of RDF, and a ModelFactory for creating models.  So that
application code remains independent of
the implementation, it is best if it uses interfaces wherever possible, not
specific class implementations.
The org.apache.jena.tutorial package contains the
working source code for all the examples used in this tutorial.
The org.apache.jena...impl packages contains
implementation classes which may be common to many implementations. For
example, they defines classes ResourceImpl,
PropertyImpl, and LiteralImpl which may be
used directly or subclassed by different implementations. Applications
should rarely, if ever, use these classes directly. For example, rather
than creating a new instance of ResourceImpl, it is better to
use the createResource method of whatever model is being
used. That way, if the model implementation has used an optimized
implementation of Resource, then no conversions between the two
types will be necessary.
Navigating a Model
So far, this tutorial has dealt mainly with creating, reading and writing
RDF Models. It is now time to deal with accessing information held in a
Model.
Given the URI of a resource, the resource object can be retrieved from a
model using the Model.getResource(String uri) method. This
method is defined to return a Resource object if one exists in the model, or
otherwise to create a new one. For example, to retrieve the John Smith
resource from the model read in from the file in tutorial 5:
// retrieve the John Smith vcard resource from the model
Resource vcard = model.getResource(johnSmithURI);
The Resource interface defines a number of methods for accessing the
properties of a resource. The Resource.getProperty(Property
p) method accesses a property of the resource. This method does
not follow the usual Java accessor convention in that the type of the object
returned is Statement, not the Property that you
might have expected.  Returning the whole statement allows the application to
access the value of the property using one of its accessor methods which
return the object of the statement. For example to retrieve the
resource which is the value of the vcard:N property:
// retrieve the value of the N property
Resource name = (Resource) vcard.getProperty(VCARD.N)
                                .getObject();
In general, the object of a statement could be a resource or a literal, so
the application code, knowing the value must be a resource, casts the
returned object. One of the things that Jena tries to do is to provide
type specific methods so the application does not have to cast and type
checking can be done at compile time. The code fragment above, can be
more conveniently written:
// retrieve the value of the N property
Resource name = vcard.getProperty(VCARD.N)
                     .getResource();
Similarly, the literal value of a property can be retrieved:
String fullName = vcard.getProperty(VCARD.FN)
                        .getString();
In this example, the vcard resource has only one vcard:FN and
one vcard:N property. RDF permits a resource to repeat a
property; for example Adam might have more than one nickname.  Let's give him
two:
// add two nickname properties to vcard
vcard.addProperty(VCARD.NICKNAME, "Smithy")
     .addProperty(VCARD.NICKNAME, "Adman");
As noted before, Jena represents an RDF Model as set of
statements, so adding a statement with the subject, predicate and object as
one already in the Model will have no effect.  Jena does not define which of
the two nicknames present in the Model will be returned.  The result of
calling vcard.getProperty(VCARD.NICKNAME) is indeterminate. Jena
will return one of the values, but there is no guarantee even that two
consecutive calls will return the same value.
If it is possible that a property may occur more than once, then the
Resource.listProperties(Property p) method can be used to return an iterator
which will list them all.  This method returns an iterator which returns
objects of type Statement.  We can list the nicknames like
this:
// set up the output
System.out.println("The nicknames of \""
                      + fullName + "\" are:");
// list the nicknames
StmtIterator iter = vcard.listProperties(VCARD.NICKNAME);
while (iter.hasNext()) {
    System.out.println("    " + iter.nextStatement()
                                    .getObject()
                                    .toString());
}
This code can be found in  tutorial 6.
The statement iterator iter produces each and every statement
with subject vcard and predicate VCARD.NICKNAME,
so looping over it allows us to fetch each statement by using
nextStatement(), get the object field, and convert it to
a string.
The code produces the following output when run:
The nicknames of "John Smith" are:
    Smithy
    Adman
All the properties of a resource can be listed by using the
listProperties() method without an argument.

Querying a Model
The previous section dealt with the case of navigating a model from a
resource with a known URI. This section deals with searching a
model. The core Jena API supports only a limited query primitive.  The
more powerful query facilities of SPARQL are described elsewhere.
The Model.listStatements() method, which lists all the
statements in a model, is perhaps the crudest way of querying a model.
Its use is not recommended on very large Models.
Model.listSubjects() is similar, but returns an iterator over
all resources that have properties, ie are the subject of some
statement.
Model.listSubjectsWithProperty(Property p, RDFNode
o) will return an iterator over all the resources which
have property p with value o. If we assume that
only vcard resources
will have vcard:FN property, and that in our data, all such
resources have such a property, then we can find all the vcards like this:
// list vcards
ResIterator iter = model.listSubjectsWithProperty(VCARD.FN);
while (iter.hasNext()) {
    Resource r = iter.nextResource();
    ...
}
All these query methods are simply syntactic sugar over a primitive query
method model.listStatements(Selector s).  This method returns an
iterator over all the statements in the model 'selected' by s.
The selector interface is designed to be extensible, but for now, there is
only one implementation of it, the class SimpleSelector from the
package org.apache.jena.rdf.model.  Using
SimpleSelector is one of the rare occasions in Jena when it is
necessary to use a specific class rather than an interface.  The
SimpleSelector constructor takes three arguments:
Selector selector = new SimpleSelector(subject, predicate, object);
This selector will select all statements with a subject that matches
subject, a predicate that matches predicate and an
object that matches object.  If a null is supplied
in any of the positions, it matches anything; otherwise they match corresponding
equal resources or literals. (Two resources are equal if they have equal URIs
or are the same blank node; two literals are the same if all their components
are equal.) Thus:
Selector selector = new SimpleSelector(null, null, null);
will select all the statements in a Model.
Selector selector = new SimpleSelector(null, VCARD.FN, null);
will select all the statements with VCARD.FN as their predicate, whatever
the subject or object. As a special shorthand,
listStatements( S, P, O )
is equivalent to
listStatements( new SimpleSelector( S, P, O ) )

The following code, which can be found in full in tutorial 7 lists the full names on all the
vcards in the database.
// select all the resources with a VCARD.FN property
ResIterator iter = model.listSubjectsWithProperty(VCARD.FN);
if (iter.hasNext()) {
    System.out.println("The database contains vcards for:");
    while (iter.hasNext()) {
        System.out.println("  " + iter.nextResource()
                                      .getProperty(VCARD.FN)
                                      .getString());
    }
} else {
    System.out.println("No vcards were found in the database");
}
This should produce output similar to the following:
The database contains vcards for:
  Sarah Jones
  John Smith
  Matt Jones
  Becky Smith
Your next exercise is to modify this code to use SimpleSelector
instead of listSubjectsWithProperty.
Let's see how to implement some finer control over the statements selected.
SimpleSelector can be subclassed and its selects method modified
to perform further filtering:
// select all the resources with a VCARD.FN property
// whose value ends with "Smith"
StmtIterator iter = model.listStatements(
    new SimpleSelector(null, VCARD.FN, (RDFNode) null) {
        public boolean selects(Statement s)
            {return s.getString().endsWith("Smith");}
    });
This sample code uses a neat Java technique of overriding a method
definition inline when creating an instance of the class.  Here the
selects(...) method checks to ensure that the full name ends
with "Smith".  It is important to note that filtering based on the subject,
predicate and object arguments takes place before the
selects(...) method is called, so the extra test will only be
applied to matching statements.
The full code can be found in tutorial
8 and produces output like this:
The database contains vcards for:
  John Smith
  Becky Smith
You might think that:
// do all filtering in the selects method
StmtIterator iter = model.listStatements(
  new
      SimpleSelector(null, null, (RDFNode) null) {
          public boolean selects(Statement s) {
              return (subject == null   || s.getSubject().equals(subject))
                  &amp;&amp; (predicate == null || s.getPredicate().equals(predicate))
                  &amp;&amp; (object == null    || s.getObject().equals(object)) ;
          }
     }
     });
is equivalent to:
StmtIterator iter =
  model.listStatements(new SimpleSelector(subject, predicate, object)
Whilst functionally they may be equivalent, the first form will list all
the statements in the Model and test each one individually, whilst the second
allows indexes maintained by the implementation to improve performance.  Try
it on a large Model and see for yourself, but make a cup of coffee first.
Operations on Models
Jena provides three operations for manipulating Models as a whole.  These
are the common set operations of union, intersection and difference.
The union of two Models is the union of the sets of statements which
represent each Model.  This is one of the key operations that the design of
RDF supports.  It enables data from disparate data sources to be merged.
Consider the following two Models:

and

When these are merged, the two http://...JohnSmith nodes are merged into
one and the duplicate vcard:FN arc is dropped to produce:


Let's look at the code to do this (the full code is in tutorial 9) and see what happens.
// read the RDF/XML files
model1.read(new InputStreamReader(in1), "");
model2.read(new InputStreamReader(in2), "");

// merge the Models
Model model = model1.union(model2);

// print the Model as RDF/XML
model.write(system.out, "RDF/XML-ABBREV");
The output produced by the pretty writer looks like this:
<rdf:RDF
    xmlns:rdf="<a href="http://www.w3.org/1999/02/22-rdf-syntax-ns#">http://www.w3.org/1999/02/22-rdf-syntax-ns#</a>"
    xmlns:vcard="http://www.w3.org/2001/vcard-rdf/3.0#">
  <rdf:Description rdf:about="http://somewhere/JohnSmith/">
    <vcard:EMAIL>
      <vcard:internet>
        <rdf:value>John@somewhere.com</rdf:value>
      </vcard:internet>
    </vcard:EMAIL>
    <vcard:N rdf:parseType="Resource">
      <vcard:Given>John</vcard:Given>
      <vcard:Family>Smith</vcard:Family>
    </vcard:N>
    <vcard:FN>John Smith</vcard:FN>
  </rdf:Description>
</rdf:RDF>
Even if you are unfamiliar with the details of the RDF/XML syntax, it
should be reasonably clear that the Models have merged as expected.  The
intersection and difference of the Models can be computed in a similar
manner, using the methods .intersection(Model) and
.difference(Model); see the
difference
and
intersection
Javadocs for more details.

Containers
RDF defines a special kind of resources for representing collections of
things. These resources are called containers. The members of a
container can be either literals or resources. There are three kinds of
container:

  a BAG is an unordered collection
  an ALT is an unordered collection intended to represent
  alternatives
  a SEQ is an ordered collection

A container is represented by a resource. That resource will have an
rdf:type property whose value should be one of rdf:Bag, rdf:Alt or rdf:Seq,
or a subclass of one of these, depending on the type of the container.
The first member of the container is the value of the container's rdf:_1
property; the second member of the container is the value of the container's
rdf:_2 property and so on. The rdf:_nnn properties are known as the
ordinal properties.
For example, the Model for a simple bag containing the vcards of the
Smith's might look like this:


Whilst the members of the bag are represented by the
properties rdf:_1, rdf:_2 etc the ordering of the properties is not
significant. We could switch the values of the rdf:_1 and rdf:_2
properties and the resulting Model would represent the same information.
Alt's are intended to represent alternatives. For
example, lets say a resource represented a software product. It might
have a property to indicate where it might be obtained from. The value
of that property might be an Alt collection containing various sites from
which it could be downloaded. Alt's are unordered except that the
rdf:_1 property has special significance. It represents the default
choice.
Whilst containers can be handled using the basic machinery of
resources and properties, Jena has explicit interfaces and implementation
classes to handle them. It is not a good idea to have an object
manipulating a container, and at the same time to modify the state of that
container using the lower level methods.
Let's modify tutorial 8 to create this bag:
// create a bag
Bag smiths = model.createBag();

// select all the resources with a VCARD.FN property
// whose value ends with "Smith"
StmtIterator iter = model.listStatements(
    new SimpleSelector(null, VCARD.FN, (RDFNode) null) {
        public boolean selects(Statement s) {
                return s.getString().endsWith("Smith");
        }
    });
// add the Smith's to the bag
while (iter.hasNext()) {
    smiths.add(iter.nextStatement().getSubject());
}
If we write out this Model, it contains something like the following:
<rdf:RDF
  xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#'
  xmlns:vcard='http://www.w3.org/2001/vcard-rdf/3.0#'
 >
...
  <rdf:Description rdf:nodeID="A3">
    <rdf:type rdf:resource='http://www.w3.org/1999/02/22-rdf-syntax-ns#Bag'/>
    <rdf:_1 rdf:resource='http://somewhere/JohnSmith/'/>
    <rdf:_2 rdf:resource='http://somewhere/RebeccaSmith/'/>
  </rdf:Description>
</rdf:RDF>
which represents the Bag resource.
The container interface provides an iterator to list the contents of a
container:
// print out the members of the bag
NodeIterator iter2 = smiths.iterator();
if (iter2.hasNext()) {
    System.out.println("The bag contains:");
    while (iter2.hasNext()) {
        System.out.println("  " +
            ((Resource) iter2.next())
                            .getProperty(VCARD.FN)
                            .getString());
    }
} else {
    System.out.println("The bag is empty");
}
which produces the following output:
The bag contains:
  John Smith
  Becky Smith
Executable example code can be found in 
tutorial 10, which glues together the fragments above into a complete
example.
The Jena classes offer methods for manipulating containers including
adding new members, inserting new members into the middle of a container and
removing existing members.  The Jena container classes currently ensure that
the list of ordinal properties used starts at rdf:_1 and is contiguous.
The RDFCore WG have relaxed this constraint, which allows partial
representation of containers.  This therefore is an area of Jena may be
changed in the future.
More about Literals and Datatypes
RDF literals are not just simple strings.  Literals may have a language
tag to indicate the language of the literal.  The literal "chat" with an
English language tag is considered different to the literal "chat" with a
French language tag.  This rather strange behaviour is an artefact of the
original RDF/XML syntax.
Further there are really two sorts of Literals.  In one, the string
component is just that, an ordinary string.  In the other the string
component is expected to be a well-balanced fragment of XML.  When an RDF
Model is written as RDF/XML a special construction using a
parseType='Literal' attribute is used to represent it.
In Jena, these attributes of a literal may be set when the literal is
constructed, e.g. in tutorial 11:
// create the resource
Resource r = model.createResource();

// add the property
r.addProperty(RDFS.label, model.createLiteral("chat", "en"))
 .addProperty(RDFS.label, model.createLiteral("chat", "fr"))
 .addProperty(RDFS.label, model.createLiteral("&lt;em&gt;chat&lt;/em&gt;", true));

// write out the Model
model.write(system.out);
produces
<rdf:RDF
  xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#'
  xmlns:rdfs='http://www.w3.org/2000/01/rdf-schema#'
 >
  <rdf:Description rdf:nodeID="A0">
    <rdfs:label xml:lang='en'>chat</rdfs:label>
    <rdfs:label xml:lang='fr'>chat</rdfs:label>
    <rdfs:label rdf:parseType='Literal'><em>chat</em></rdfs:label>
  </rdf:Description>
</rdf:RDF>
For two literals to be considered equal, they must either both be XML
literals or both be simple literals.  In addition, either both must have no
language tag, or if language tags are present they must be equal.  For simple
literals the strings must be equal.  XML literals have two notions of
equality.  The simple notion is that the conditions previously mentioned are
true and the strings are also equal.  The other notion is that they can be
equal if the canonicalization of their strings is equal.
Jena's interfaces also support typed literals. The old-fashioned way
(shown below) treats typed literals as shorthand for strings: typed
values are converted in the usual Java way to strings and these strings
are stored in the Model. For example, try (noting that for
simple literals, we can omit the model.createLiteral(...)
call):
// create the resource
Resource r = model.createResource();

// add the property
r.addProperty(RDFS.label, "11")
 .addProperty(RDFS.label, 11);

// write out the Model
model.write(system.out, "N-TRIPLE");
The output produced is:
_:A... <http://www.w3.org/2000/01/rdf-schema#label> "11" .
Since both literals are really just the string "11", then only one
statement is added.
The RDFCore WG has defined mechanisms for supporting datatypes in RDF.
Jena supports these using the typed literal mechanisms; they are
not discussed in this tutorial.
Glossary

Blank Node
Represents a resource, but does not indicate a URI for the
resource.  Blank nodes act like existentially qualified variables in first
order logic.
Dublin Core
A standard for metadata about web resources. Further
information can be found at the Dublin Core
web site.
Literal
A string of characters which can be the value of a property.
Object
The part of a triple which is the value of the statement.
Predicate
The property part of a triple.
Property
A property is an attribute of a resource. For example
DC.title is a property, as is RDF.type.
Resource
Some entity. It could be a web resource such as web page, or
it could be a concrete physical thing such as a tree or a car. It could be an
abstract idea such as chess or football. Resources are named by URI's.
Statement
An arc in an RDF Model, normally interpreted as a fact.
Subject
The resource which is the source of an arc in an RDF Model
Triple
A structure containing a subject, a predicate and an object.
Another term for a statement.

Footnotes

  The identifier of an RDF resource can
    include a fragment identifier, e.g.
    http://hostname/rdf/tutorial/#ch-Introduction, so, strictly speaking, an
    RDF resource is identified by a URI reference.
  As well as being a string of characters,
    literals also have an optional language encoding to represent the
    language of the string. For example the literal "two" might have a
    language encoding of "en" for English and the literal "deux" might have a
    language encoding of "fr" for France.\n\nOn this page
    
  
    Preface
    Introduction
    Statements
    Writing RDF
    Reading RDF
    Controlling Prefixes
      
        Explicit prefix definitions
      
    
    Jena RDF Packages
    Navigating a Model
    Querying a Model
    Operations on Models
    Containers
    More about Literals and Datatypes
    Glossary
    Footnotes
  

  
  
    Preface
This is a tutorial introduction to both W3C’s Resource Description Framework
(RDF) and Jena, a Java API for RDF.  It is written for the programmer who is
unfamiliar with RDF and who learns best by prototyping, or, for other
reasons, wishes to move quickly to implementation. Some familiarity
with both XML and Java is assumed.
Implementing too quickly, without first understanding the RDF data model,
leads to frustration and disappointment. Yet studying the data model
alone is dry stuff and often leads to tortuous metaphysical conundrums.  It
is better to approach understanding both the data model and how to use it in
parallel. Learn a bit of the data model and try it out. Then
learn a bit more and try that out. Then the theory informs the practice
and the practice the theory. The data model is quite simple, so this
approach does not take long.
RDF has an XML syntax and many who are familiar with XML will think of RDF
in terms of that syntax. This is a mistake. RDF should be
understood in terms of its data model. RDF data can be represented in
XML, but understanding the syntax is secondary to understanding the data
model.
An implementation of the Jena API, including the working source code for
all the examples used in this tutorial can be downloaded from
jena.apache.org/download/index.cgi.
Introduction
The Resource Description Framework (RDF) is a standard (technically a W3C
Recommendation) for describing resources. What is a resource?  That is
rather a deep question and the precise definition is still the subject of
debate.  For our purposes we can think of it as anything we can identify.
You are a resource, as is your home page, this tutorial, the number one and
the great white whale in Moby Dick.
Our examples in this tutorial will be about people.  They use an RDF representation of VCARDS.  RDF
is best thought of in the form of node and arc diagrams.  A simple vcard
might look like this in RDF:


The resource, John Smith, is shown as
an ellipse and is identified by a Uniform Resource Identifier (URI)1,  in this case
"http://.../JohnSmith".  If you try to access that resource using your
browser, you are unlikely to be successful; April the first jokes not
withstanding, you would be rather surprised if your browser were able to
deliver John Smith to your desk top. If you are unfamiliar with URI's,
think of them simply as rather strange looking names.
Resources have properties.  In these
examples we are interested in the sort of properties that would appear on
John Smith's business card. Figure 1 shows only one property, John
Smith's full name.  A property is represented by an arc, labeled with the
name of a property.  The name of a property is also a URI, but as URI's are
rather long and cumbersome, the diagram shows it in XML qname form.   The
part before the ':' is called a namespace prefix and represents a
namespace. The part after the ':' is called a local name and represents
a name in that namespace. Properties are usually represented in this
qname form when written as RDF XML and it is a convenient shorthand for
representing them in diagrams and in text. Strictly, however,
properties are identified by a URI. The nsprefix:localname form is a
shorthand for the URI of the namespace concatenated with the localname.
There is no requirement that the URI of a property resolve to anything when
accessed by a browser.
Each property has a value. In this case the value is a literal, which for now we can think of as a
strings of characters2.
Literals are shown in rectangles.
Jena is a Java API which can be used to create and manipulate RDF graphs
like this one. Jena has object classes to represent graphs, resources,
properties and literals. The interfaces representing resources,
properties and literals are called Resource, Property and Literal
respectively. In Jena, a graph is called a model and is represented by the Model
interface.
The code to create this graph, or model, is simple:
// some definitions
static String personURI    = "http://somewhere/JohnSmith";
static String fullName     = "John Smith";

// create an empty Model
Model model = ModelFactory.createDefaultModel();

// create the resource
Resource johnSmith = model.createResource(personURI);

// add the property
johnSmith.addProperty(VCARD.FN, fullName);
It begins with some constant definitions and then creates an empty Model
or model, using the ModelFactory method createDefaultModel()
to create a memory-based model. Jena contains other implementations
of the Model interface, e.g one which uses a relational database: these
types of Model are also available from ModelFactory.
The John Smith resource is then created and a property added to it.
The property is provided by a "constant" class VCARD which holds objects
representing all the definitions in the VCARD schema.  Jena provides constant
classes for other well known schemas, such as RDF and RDF schema themselves,
Dublin Core and OWL.
The working code for this example can be found in the /src-examples directory of
the Jena distribution as 
tutorial
1.
As an exercise, take this code and modify it to create a simple VCARD for
yourself.
The code to create the resource and add the property, can be more
compactly written in a cascading style:
Resource johnSmith =
      model.createResource(personURI)
           .addProperty(VCARD.FN, fullName);
Now let's add some more detail to the vcard, exploring some more features
of RDF and Jena.
In the first example, the property value was a literal. RDF
properties can also take other resources as their value.  Using a common RDF
technique, this example shows how to represent the different parts of John
Smith's name:


Here we have added a new property, vcard:N, to represent the structure of
John Smith's name.  There are several things of interest about this
Model. Note that the vcard:N property takes a resource as its value.
Note also that the ellipse representing the compound name has no URI.
It is known as an blank Node.
The Jena code to construct this example, is again very simple. First
some declarations and the creation of the empty model.
// some definitions
String personURI    = "http://somewhere/JohnSmith";
String givenName    = "John";
String familyName   = "Smith";
String fullName     = givenName + " " + familyName;

// create an empty Model
Model model = ModelFactory.createDefaultModel();

// create the resource
//   and add the properties cascading style
Resource johnSmith
  = model.createResource(personURI)
         .addProperty(VCARD.FN, fullName)
         .addProperty(VCARD.N,
                      model.createResource()
                           .addProperty(VCARD.Given, givenName)
                           .addProperty(VCARD.Family, familyName));
The working code for this example can be found as tutorial 2 in the /src-examples directory
of the Jena distribution.
Statements
Each arc in an RDF Model is called a statement. Each statement asserts a fact
about a resource. A statement has three parts:

  the  subject is the resource from
    which the arc leaves
  the  predicate is the property
    that labels the arc
  the  object is the resource or
    literal pointed to by the arc

A statement is sometimes called a triple,
because of its three parts.
An RDF Model is represented as a set of statements.  Each call of
addProperty in tutorial2 added another statement to the Model.
(Because a Model is set of statements, adding a duplicate of a statement has no
effect.)  The Jena model interface defines a listStatements()
method which returns an StmtIterator, a subtype of Java's
Iterator over all the statements in a Model.
StmtIterator has a method nextStatement()
which returns the next statement from the iterator (the same one that
next() would deliver, already cast to Statement).
The Statement interface provides accessor
methods to the subject, predicate and object of a statement.
Now we will use that interface to extend tutorial2 to list all the
statements created and print them out.  The complete code for this can be
found in tutorial 3.
// list the statements in the Model
StmtIterator iter = model.listStatements();

// print out the predicate, subject and object of each statement
while (iter.hasNext()) {
    Statement stmt      = iter.nextStatement();  // get next statement
    Resource  subject   = stmt.getSubject();     // get the subject
    Property  predicate = stmt.getPredicate();   // get the predicate
    RDFNode   object    = stmt.getObject();      // get the object

    System.out.print(subject.toString());
    System.out.print(" " + predicate.toString() + " ");
    if (object instanceof Resource) {
       System.out.print(object.toString());
    } else {
        // object is a literal
        System.out.print(" \"" + object.toString() + "\"");
    }

    System.out.println(" .");
}
Since the object of a statement can be either a resource or a literal, the
getObject() method returns an
object typed as RDFNode, which is a
common superclass of both Resource
and Literal.  The underlying object
is of the appropriate type, so the code uses instanceof to
determine which and
processes it accordingly.
When run, this program should produce output resembling:
http://somewhere/JohnSmith http://www.w3.org/2001/vcard-rdf/3.0#N 413f6415-c3b0-4259-b74d-4bd6e757eb60 .
413f6415-c3b0-4259-b74d-4bd6e757eb60 http://www.w3.org/2001/vcard-rdf/3.0#Family  "Smith" .
413f6415-c3b0-4259-b74d-4bd6e757eb60 http://www.w3.org/2001/vcard-rdf/3.0#Given  "John" .
http://somewhere/JohnSmith http://www.w3.org/2001/vcard-rdf/3.0#FN  "John Smith" .

Now you know why it is clearer to draw Models.  If you look carefully, you
will see that each line consists of three fields representing the subject,
predicate and object of each statement.  There are four arcs in the Model, so
there are four statements.  The "14df86:ecc3dee17b:-7fff" is an internal
identifier generated by Jena.  It is not a URI and should not be confused
with one.  It is simply an internal label used by the Jena implementation.
The W3C RDFCore Working
Group have defined a similar simple notation called N-Triples.  The name
means "triple notation".  We will see in the next section that Jena has an
N-Triples writer built in.
Writing RDF
Jena has methods for reading and writing RDF as XML. These can be
used to save an RDF model to a file and later read it back in again.
Tutorial 3 created a model and wrote it out in triple form. Tutorial 4 modifies tutorial 3 to write the
model in RDF XML form to the standard output stream. The code again, is
very simple: model.write can take an OutputStream
argument.
// now write the model in XML form to a file
model.write(System.out);
The output should look something like this:
<rdf:RDF
  xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#'
  xmlns:vcard='http://www.w3.org/2001/vcard-rdf/3.0#'
 >
  <rdf:Description rdf:about='http://somewhere/JohnSmith'>
    <vcard:FN>John Smith</vcard:FN>
    <vcard:N rdf:nodeID="A0"/>
  </rdf:Description>
  <rdf:Description rdf:nodeID="A0">
    <vcard:Given>John</vcard:Given>
    <vcard:Family>Smith</vcard:Family>
  </rdf:Description>
</rdf:RDF>
The RDF specifications specify how to represent RDF as XML. The RDF
XML syntax is quite complex.  The reader is referred to the primer being developed by the
RDFCore WG for a more detailed introduction.  However, let's take a quick look
at how to interpret the above.
RDF is usually embedded in an <rdf:RDF> element.  The element is
optional if there are other ways of knowing that some XML is RDF, but it is
usually present.  The RDF element defines the two namespaces used in the
document.  There is then an <rdf:Description> element which describes
the resource whose URI is "http://somewhere/JohnSmith".  If the rdf:about
attribute was missing, this element would represent a blank node.
The <vcard:FN> element describes a property of the resource.  The
property name is the "FN" in the vcard namespace.  RDF converts this to a URI
reference by concatenating the URI reference for the namespace prefix and
"FN", the local name part of the name.  This gives a URI reference of
"http://www.w3.org/2001/vcard-rdf/3.0#FN".  The value of the property
is the literal "John Smith".
The <vcard:N> element is a resource.  In this case the resource is
represented by a relative URI reference.  RDF converts this to an absolute
URI reference by concatenating it with the base URI of the current
document.
There is an error in this RDF XML; it does not exactly represent the Model
we created.  The blank node in the Model has been given a URI reference.  It
is no longer blank.  The RDF/XML syntax is not capable of representing all
RDF Models; for example it cannot represent a blank node which is the object
of two statements.  The 'dumb' writer we used to write this RDF/XML  makes no
attempt to write correctly the subset of Models which can be written
correctly.  It gives a URI to each blank node, making it no longer blank.
Jena has an extensible interface which allows new writers for different
serialization languages for RDF to be easily plugged in.  The above call
invoked the standard 'dumb' writer.  Jena also includes a more sophisticated
RDF/XML writer which can be invoked by using
RDFDataMgr.write function call:
// now write the model in a pretty form
RDFDataMgr.write(System.out, model, Lang.RDFXML);
This writer, the so called PrettyWriter, takes advantage of features of
the RDF/XML abbreviated syntax to write a Model more compactly.  It is also
able to preserve blank nodes where that is possible.  It is however, not
suitable for writing very large Models, as its performance is unlikely to be
acceptable.  To write large files and preserve blank nodes, write in
N-Triples format:
// now write the model in N-TRIPLES form
RDFDataMgr.write(System.out, model, Lang.NTRIPLES);
This will produce output similar to that of tutorial 3 which conforms to
the N-Triples specification.
Reading RDF
Tutorial 5 demonstrates reading the
statements recorded in RDF XML form into a model. With this tutorial,
we have provided a small database of vcards in RDF/XML form.  The following
code will read it in and write it out. Note that for this application to
run, the input file must be in the current directory.
// create an empty model
Model model = ModelFactory.createDefaultModel();

// use the RDFDataMgr to find the input file
InputStream in = RDFDataMgr.open( inputFileName );
if (in == null) {
    throw new IllegalArgumentException("File: " + inputFileName + " not found");
}

// read the RDF/XML file
model.read(in, null);

// write it to standard out
model.write(System.out);
The second argument to the read() method call is the URI which will
be used for resolving relative URI's.  As there are no relative URI
references in the test file, it is allowed to be empty. When run,  tutorial 5 will produce XML output which
looks like:
<rdf:RDF
  xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#'
  xmlns:vcard='http://www.w3.org/2001/vcard-rdf/3.0#'
 >
  <rdf:Description rdf:nodeID="A0">
    <vcard:Family>Smith</vcard:Family>
    <vcard:Given>John</vcard:Given>
  </rdf:Description>
  <rdf:Description rdf:about='http://somewhere/JohnSmith/'>
    <vcard:FN>John Smith</vcard:FN>
    <vcard:N rdf:nodeID="A0"/>
  </rdf:Description>
  <rdf:Description rdf:about='http://somewhere/SarahJones/'>
    <vcard:FN>Sarah Jones</vcard:FN>
    <vcard:N rdf:nodeID="A1"/>
  </rdf:Description>
  <rdf:Description rdf:about='http://somewhere/MattJones/'>
    <vcard:FN>Matt Jones</vcard:FN>
    <vcard:N rdf:nodeID="A2"/>
  </rdf:Description>
  <rdf:Description rdf:nodeID="A3">
    <vcard:Family>Smith</vcard:Family>
    <vcard:Given>Rebecca</vcard:Given>
  </rdf:Description>
  <rdf:Description rdf:nodeID="A1">
    <vcard:Family>Jones</vcard:Family>
    <vcard:Given>Sarah</vcard:Given>
  </rdf:Description>
  <rdf:Description rdf:nodeID="A2">
    <vcard:Family>Jones</vcard:Family>
    <vcard:Given>Matthew</vcard:Given>
  </rdf:Description>
  <rdf:Description rdf:about='http://somewhere/RebeccaSmith/'>
    <vcard:FN>Becky Smith</vcard:FN>
    <vcard:N rdf:nodeID="A3"/>
  </rdf:Description>
</rdf:RDF>
Controlling Prefixes
Explicit prefix definitions
In the previous section, we saw that the output XML declared a namespace
prefix vcard and used that prefix to abbreviate URIs. While RDF
uses only the full URIs, and not this shortened form, Jena provides ways
of controlling the namespaces used on output with its prefix mappings.
Here’s a simple example.
Model m = ModelFactory.createDefaultModel();
String nsA = "http://somewhere/else#";
String nsB = "http://nowhere/else#";
Resource root = m.createResource( nsA + "root" );
Property P = m.createProperty( nsA + "P" );
Property Q = m.createProperty( nsB + "Q" );
Resource x = m.createResource( nsA + "x" );
Resource y = m.createResource( nsA + "y" );
Resource z = m.createResource( nsA + "z" );
m.add( root, P, x ).add( root, P, y ).add( y, Q, z );
System.out.println( "# -- no special prefixes defined" );
m.write( System.out );
System.out.println( "# -- nsA defined" );
m.setNsPrefix( "nsA", nsA );
m.write( System.out );
System.out.println( "# -- nsA and cat defined" );
m.setNsPrefix( "cat", nsB );
m.write( System.out );
The output from this fragment is lots of RDF/XML, with
three different prefix mappings. First the default, with no
prefixes other than the standard ones:
# -- no special prefixes defined

<rdf:RDF
    xmlns:j.0="http://nowhere/else#"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:j.1="http://somewhere/else#" >
  <rdf:Description rdf:about="http://somewhere/else#root">
    <j.1:P rdf:resource="http://somewhere/else#x"/>
    <j.1:P rdf:resource="http://somewhere/else#y"/>
  </rdf:Description>
  <rdf:Description rdf:about="http://somewhere/else#y">
    <j.0:Q rdf:resource="http://somewhere/else#z"/>
  </rdf:Description>
</rdf:RDF>
We see that the rdf namespace is declared automatically, since it
is required for tags such as <rdf:RDF> and
<rdf:resource>. XML namespace declarations are also
needed for using the two properties P and Q, but since their
prefixes have not been introduced to the model in this example,
they get invented namespace names: j.0 and j.1.
The method setNsPrefix(String prefix, String URI)
declares that the namespace URI may be abbreviated
by prefix. Jena requires that prefix be
a legal XML namespace name, and that URI ends with a
non-name character. The RDF/XML writer will turn these prefix
declarations into XML namespace declarations and use them in its
output:
# -- nsA defined

<rdf:RDF
    xmlns:j.0="http://nowhere/else#"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:nsA="http://somewhere/else#" >
  <rdf:Description rdf:about="http://somewhere/else#root">
    <nsA:P rdf:resource="http://somewhere/else#x"/>
    <nsA:P rdf:resource="http://somewhere/else#y"/>
  </rdf:Description>
  <rdf:Description rdf:about="http://somewhere/else#y">
    <j.0:Q rdf:resource="http://somewhere/else#z"/>
  </rdf:Description>
</rdf:RDF>
is now used in the property tags. There’s no need for the prefix name
to have anything to do with the variables in the Jena code:
# -- nsA and cat defined

<rdf:RDF
    xmlns:cat="http://nowhere/else#"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:nsA="http://somewhere/else#" >
  <rdf:Description rdf:about="http://somewhere/else#root">
    <nsA:P rdf:resource="http://somewhere/else#x"/>
    <nsA:P rdf:resource="http://somewhere/else#y"/>
  </rdf:Description>
  <rdf:Description rdf:about="http://somewhere/else#y">
    <cat:Q rdf:resource="http://somewhere/else#z"/>
  </rdf:Description>
</rdf:RDF>
Both prefixes are used for output, and no generated prefixes are needed.
Implicit prefix definitions
As well as prefix declarations provided by calls to setNsPrefix,
Jena will remember the prefixes that were used in input to
model.read().
Take the output produced by the previous fragment, and paste it into
some file, with URL file:/tmp/fragment.rdf say. Then run the
code:
Model m2 = ModelFactory.createDefaultModel();
m2.read( "file:/tmp/fragment.rdf" );
m2.write( System.out );
You’ll see that the prefixes from the input are preserved in the output.
All the prefixes are written, even if they’re not used anywhere. You can
remove a prefix with removeNsPrefix(String prefix) if you
don’t want it in the output.
Since NTriples doesn't have any short way of writing URIs, it takes
no notice of prefixes on output and doesn't provide any on input. The
notation N3, also supported by Jena, does have short prefixed names,
and records them on input and uses them on output.
Jena has further operations on the prefix mappings that a model holds,
such as extracting a Java Map of the exiting mappings, or
adding a whole group of mappings at once; see the documentation for
PrefixMapping for details.
Jena RDF Packages
Jena is a Java API for semantic web applications.  The key RDF package for
the application developer is
org.apache.jena.rdf.model. The API has been defined
in terms of interfaces so that application code can work with different
implementations without change. This package contains interfaces for
representing models, resources, properties, literals, statements and all the
other key concepts of RDF, and a ModelFactory for creating models.  So that
application code remains independent of
the implementation, it is best if it uses interfaces wherever possible, not
specific class implementations.
The org.apache.jena.tutorial package contains the
working source code for all the examples used in this tutorial.
The org.apache.jena...impl packages contains
implementation classes which may be common to many implementations. For
example, they defines classes ResourceImpl,
PropertyImpl, and LiteralImpl which may be
used directly or subclassed by different implementations. Applications
should rarely, if ever, use these classes directly. For example, rather
than creating a new instance of ResourceImpl, it is better to
use the createResource method of whatever model is being
used. That way, if the model implementation has used an optimized
implementation of Resource, then no conversions between the two
types will be necessary.
Navigating a Model
So far, this tutorial has dealt mainly with creating, reading and writing
RDF Models. It is now time to deal with accessing information held in a
Model.
Given the URI of a resource, the resource object can be retrieved from a
model using the Model.getResource(String uri) method. This
method is defined to return a Resource object if one exists in the model, or
otherwise to create a new one. For example, to retrieve the John Smith
resource from the model read in from the file in tutorial 5:
// retrieve the John Smith vcard resource from the model
Resource vcard = model.getResource(johnSmithURI);
The Resource interface defines a number of methods for accessing the
properties of a resource. The Resource.getProperty(Property
p) method accesses a property of the resource. This method does
not follow the usual Java accessor convention in that the type of the object
returned is Statement, not the Property that you
might have expected.  Returning the whole statement allows the application to
access the value of the property using one of its accessor methods which
return the object of the statement. For example to retrieve the
resource which is the value of the vcard:N property:
// retrieve the value of the N property
Resource name = (Resource) vcard.getProperty(VCARD.N)
                                .getObject();
In general, the object of a statement could be a resource or a literal, so
the application code, knowing the value must be a resource, casts the
returned object. One of the things that Jena tries to do is to provide
type specific methods so the application does not have to cast and type
checking can be done at compile time. The code fragment above, can be
more conveniently written:
// retrieve the value of the N property
Resource name = vcard.getProperty(VCARD.N)
                     .getResource();
Similarly, the literal value of a property can be retrieved:
String fullName = vcard.getProperty(VCARD.FN)
                        .getString();
In this example, the vcard resource has only one vcard:FN and
one vcard:N property. RDF permits a resource to repeat a
property; for example Adam might have more than one nickname.  Let's give him
two:
// add two nickname properties to vcard
vcard.addProperty(VCARD.NICKNAME, "Smithy")
     .addProperty(VCARD.NICKNAME, "Adman");
As noted before, Jena represents an RDF Model as set of
statements, so adding a statement with the subject, predicate and object as
one already in the Model will have no effect.  Jena does not define which of
the two nicknames present in the Model will be returned.  The result of
calling vcard.getProperty(VCARD.NICKNAME) is indeterminate. Jena
will return one of the values, but there is no guarantee even that two
consecutive calls will return the same value.
If it is possible that a property may occur more than once, then the
Resource.listProperties(Property p) method can be used to return an iterator
which will list them all.  This method returns an iterator which returns
objects of type Statement.  We can list the nicknames like
this:
// set up the output
System.out.println("The nicknames of \""
                      + fullName + "\" are:");
// list the nicknames
StmtIterator iter = vcard.listProperties(VCARD.NICKNAME);
while (iter.hasNext()) {
    System.out.println("    " + iter.nextStatement()
                                    .getObject()
                                    .toString());
}
This code can be found in  tutorial 6.
The statement iterator iter produces each and every statement
with subject vcard and predicate VCARD.NICKNAME,
so looping over it allows us to fetch each statement by using
nextStatement(), get the object field, and convert it to
a string.
The code produces the following output when run:
The nicknames of "John Smith" are:
    Smithy
    Adman
All the properties of a resource can be listed by using the
listProperties() method without an argument.

Querying a Model
The previous section dealt with the case of navigating a model from a
resource with a known URI. This section deals with searching a
model. The core Jena API supports only a limited query primitive.  The
more powerful query facilities of SPARQL are described elsewhere.
The Model.listStatements() method, which lists all the
statements in a model, is perhaps the crudest way of querying a model.
Its use is not recommended on very large Models.
Model.listSubjects() is similar, but returns an iterator over
all resources that have properties, ie are the subject of some
statement.
Model.listSubjectsWithProperty(Property p, RDFNode
o) will return an iterator over all the resources which
have property p with value o. If we assume that
only vcard resources
will have vcard:FN property, and that in our data, all such
resources have such a property, then we can find all the vcards like this:
// list vcards
ResIterator iter = model.listSubjectsWithProperty(VCARD.FN);
while (iter.hasNext()) {
    Resource r = iter.nextResource();
    ...
}
All these query methods are simply syntactic sugar over a primitive query
method model.listStatements(Selector s).  This method returns an
iterator over all the statements in the model 'selected' by s.
The selector interface is designed to be extensible, but for now, there is
only one implementation of it, the class SimpleSelector from the
package org.apache.jena.rdf.model.  Using
SimpleSelector is one of the rare occasions in Jena when it is
necessary to use a specific class rather than an interface.  The
SimpleSelector constructor takes three arguments:
Selector selector = new SimpleSelector(subject, predicate, object);
This selector will select all statements with a subject that matches
subject, a predicate that matches predicate and an
object that matches object.  If a null is supplied
in any of the positions, it matches anything; otherwise they match corresponding
equal resources or literals. (Two resources are equal if they have equal URIs
or are the same blank node; two literals are the same if all their components
are equal.) Thus:
Selector selector = new SimpleSelector(null, null, null);
will select all the statements in a Model.
Selector selector = new SimpleSelector(null, VCARD.FN, null);
will select all the statements with VCARD.FN as their predicate, whatever
the subject or object. As a special shorthand,
listStatements( S, P, O )
is equivalent to
listStatements( new SimpleSelector( S, P, O ) )

The following code, which can be found in full in tutorial 7 lists the full names on all the
vcards in the database.
// select all the resources with a VCARD.FN property
ResIterator iter = model.listSubjectsWithProperty(VCARD.FN);
if (iter.hasNext()) {
    System.out.println("The database contains vcards for:");
    while (iter.hasNext()) {
        System.out.println("  " + iter.nextResource()
                                      .getProperty(VCARD.FN)
                                      .getString());
    }
} else {
    System.out.println("No vcards were found in the database");
}
This should produce output similar to the following:
The database contains vcards for:
  Sarah Jones
  John Smith
  Matt Jones
  Becky Smith
Your next exercise is to modify this code to use SimpleSelector
instead of listSubjectsWithProperty.
Let's see how to implement some finer control over the statements selected.
SimpleSelector can be subclassed and its selects method modified
to perform further filtering:
// select all the resources with a VCARD.FN property
// whose value ends with "Smith"
StmtIterator iter = model.listStatements(
    new SimpleSelector(null, VCARD.FN, (RDFNode) null) {
        public boolean selects(Statement s)
            {return s.getString().endsWith("Smith");}
    });
This sample code uses a neat Java technique of overriding a method
definition inline when creating an instance of the class.  Here the
selects(...) method checks to ensure that the full name ends
with "Smith".  It is important to note that filtering based on the subject,
predicate and object arguments takes place before the
selects(...) method is called, so the extra test will only be
applied to matching statements.
The full code can be found in tutorial
8 and produces output like this:
The database contains vcards for:
  John Smith
  Becky Smith
You might think that:
// do all filtering in the selects method
StmtIterator iter = model.listStatements(
  new
      SimpleSelector(null, null, (RDFNode) null) {
          public boolean selects(Statement s) {
              return (subject == null   || s.getSubject().equals(subject))
                  &amp;&amp; (predicate == null || s.getPredicate().equals(predicate))
                  &amp;&amp; (object == null    || s.getObject().equals(object)) ;
          }
     }
     });
is equivalent to:
StmtIterator iter =
  model.listStatements(new SimpleSelector(subject, predicate, object)
Whilst functionally they may be equivalent, the first form will list all
the statements in the Model and test each one individually, whilst the second
allows indexes maintained by the implementation to improve performance.  Try
it on a large Model and see for yourself, but make a cup of coffee first.
Operations on Models
Jena provides three operations for manipulating Models as a whole.  These
are the common set operations of union, intersection and difference.
The union of two Models is the union of the sets of statements which
represent each Model.  This is one of the key operations that the design of
RDF supports.  It enables data from disparate data sources to be merged.
Consider the following two Models:

and

When these are merged, the two http://...JohnSmith nodes are merged into
one and the duplicate vcard:FN arc is dropped to produce:


Let's look at the code to do this (the full code is in tutorial 9) and see what happens.
// read the RDF/XML files
model1.read(new InputStreamReader(in1), "");
model2.read(new InputStreamReader(in2), "");

// merge the Models
Model model = model1.union(model2);

// print the Model as RDF/XML
model.write(system.out, "RDF/XML-ABBREV");
The output produced by the pretty writer looks like this:
<rdf:RDF
    xmlns:rdf="<a href="http://www.w3.org/1999/02/22-rdf-syntax-ns#">http://www.w3.org/1999/02/22-rdf-syntax-ns#</a>"
    xmlns:vcard="http://www.w3.org/2001/vcard-rdf/3.0#">
  <rdf:Description rdf:about="http://somewhere/JohnSmith/">
    <vcard:EMAIL>
      <vcard:internet>
        <rdf:value>John@somewhere.com</rdf:value>
      </vcard:internet>
    </vcard:EMAIL>
    <vcard:N rdf:parseType="Resource">
      <vcard:Given>John</vcard:Given>
      <vcard:Family>Smith</vcard:Family>
    </vcard:N>
    <vcard:FN>John Smith</vcard:FN>
  </rdf:Description>
</rdf:RDF>
Even if you are unfamiliar with the details of the RDF/XML syntax, it
should be reasonably clear that the Models have merged as expected.  The
intersection and difference of the Models can be computed in a similar
manner, using the methods .intersection(Model) and
.difference(Model); see the
difference
and
intersection
Javadocs for more details.

Containers
RDF defines a special kind of resources for representing collections of
things. These resources are called containers. The members of a
container can be either literals or resources. There are three kinds of
container:

  a BAG is an unordered collection
  an ALT is an unordered collection intended to represent
  alternatives
  a SEQ is an ordered collection

A container is represented by a resource. That resource will have an
rdf:type property whose value should be one of rdf:Bag, rdf:Alt or rdf:Seq,
or a subclass of one of these, depending on the type of the container.
The first member of the container is the value of the container's rdf:_1
property; the second member of the container is the value of the container's
rdf:_2 property and so on. The rdf:_nnn properties are known as the
ordinal properties.
For example, the Model for a simple bag containing the vcards of the
Smith's might look like this:


Whilst the members of the bag are represented by the
properties rdf:_1, rdf:_2 etc the ordering of the properties is not
significant. We could switch the values of the rdf:_1 and rdf:_2
properties and the resulting Model would represent the same information.
Alt's are intended to represent alternatives. For
example, lets say a resource represented a software product. It might
have a property to indicate where it might be obtained from. The value
of that property might be an Alt collection containing various sites from
which it could be downloaded. Alt's are unordered except that the
rdf:_1 property has special significance. It represents the default
choice.
Whilst containers can be handled using the basic machinery of
resources and properties, Jena has explicit interfaces and implementation
classes to handle them. It is not a good idea to have an object
manipulating a container, and at the same time to modify the state of that
container using the lower level methods.
Let's modify tutorial 8 to create this bag:
// create a bag
Bag smiths = model.createBag();

// select all the resources with a VCARD.FN property
// whose value ends with "Smith"
StmtIterator iter = model.listStatements(
    new SimpleSelector(null, VCARD.FN, (RDFNode) null) {
        public boolean selects(Statement s) {
                return s.getString().endsWith("Smith");
        }
    });
// add the Smith's to the bag
while (iter.hasNext()) {
    smiths.add(iter.nextStatement().getSubject());
}
If we write out this Model, it contains something like the following:
<rdf:RDF
  xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#'
  xmlns:vcard='http://www.w3.org/2001/vcard-rdf/3.0#'
 >
...
  <rdf:Description rdf:nodeID="A3">
    <rdf:type rdf:resource='http://www.w3.org/1999/02/22-rdf-syntax-ns#Bag'/>
    <rdf:_1 rdf:resource='http://somewhere/JohnSmith/'/>
    <rdf:_2 rdf:resource='http://somewhere/RebeccaSmith/'/>
  </rdf:Description>
</rdf:RDF>
which represents the Bag resource.
The container interface provides an iterator to list the contents of a
container:
// print out the members of the bag
NodeIterator iter2 = smiths.iterator();
if (iter2.hasNext()) {
    System.out.println("The bag contains:");
    while (iter2.hasNext()) {
        System.out.println("  " +
            ((Resource) iter2.next())
                            .getProperty(VCARD.FN)
                            .getString());
    }
} else {
    System.out.println("The bag is empty");
}
which produces the following output:
The bag contains:
  John Smith
  Becky Smith
Executable example code can be found in 
tutorial 10, which glues together the fragments above into a complete
example.
The Jena classes offer methods for manipulating containers including
adding new members, inserting new members into the middle of a container and
removing existing members.  The Jena container classes currently ensure that
the list of ordinal properties used starts at rdf:_1 and is contiguous.
The RDFCore WG have relaxed this constraint, which allows partial
representation of containers.  This therefore is an area of Jena may be
changed in the future.
More about Literals and Datatypes
RDF literals are not just simple strings.  Literals may have a language
tag to indicate the language of the literal.  The literal "chat" with an
English language tag is considered different to the literal "chat" with a
French language tag.  This rather strange behaviour is an artefact of the
original RDF/XML syntax.
Further there are really two sorts of Literals.  In one, the string
component is just that, an ordinary string.  In the other the string
component is expected to be a well-balanced fragment of XML.  When an RDF
Model is written as RDF/XML a special construction using a
parseType='Literal' attribute is used to represent it.
In Jena, these attributes of a literal may be set when the literal is
constructed, e.g. in tutorial 11:
// create the resource
Resource r = model.createResource();

// add the property
r.addProperty(RDFS.label, model.createLiteral("chat", "en"))
 .addProperty(RDFS.label, model.createLiteral("chat", "fr"))
 .addProperty(RDFS.label, model.createLiteral("&lt;em&gt;chat&lt;/em&gt;", true));

// write out the Model
model.write(system.out);
produces
<rdf:RDF
  xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#'
  xmlns:rdfs='http://www.w3.org/2000/01/rdf-schema#'
 >
  <rdf:Description rdf:nodeID="A0">
    <rdfs:label xml:lang='en'>chat</rdfs:label>
    <rdfs:label xml:lang='fr'>chat</rdfs:label>
    <rdfs:label rdf:parseType='Literal'><em>chat</em></rdfs:label>
  </rdf:Description>
</rdf:RDF>
For two literals to be considered equal, they must either both be XML
literals or both be simple literals.  In addition, either both must have no
language tag, or if language tags are present they must be equal.  For simple
literals the strings must be equal.  XML literals have two notions of
equality.  The simple notion is that the conditions previously mentioned are
true and the strings are also equal.  The other notion is that they can be
equal if the canonicalization of their strings is equal.
Jena's interfaces also support typed literals. The old-fashioned way
(shown below) treats typed literals as shorthand for strings: typed
values are converted in the usual Java way to strings and these strings
are stored in the Model. For example, try (noting that for
simple literals, we can omit the model.createLiteral(...)
call):
// create the resource
Resource r = model.createResource();

// add the property
r.addProperty(RDFS.label, "11")
 .addProperty(RDFS.label, 11);

// write out the Model
model.write(system.out, "N-TRIPLE");
The output produced is:
_:A... <http://www.w3.org/2000/01/rdf-schema#label> "11" .
Since both literals are really just the string "11", then only one
statement is added.
The RDFCore WG has defined mechanisms for supporting datatypes in RDF.
Jena supports these using the typed literal mechanisms; they are
not discussed in this tutorial.
Glossary

Blank Node
Represents a resource, but does not indicate a URI for the
resource.  Blank nodes act like existentially qualified variables in first
order logic.
Dublin Core
A standard for metadata about web resources. Further
information can be found at the Dublin Core
web site.
Literal
A string of characters which can be the value of a property.
Object
The part of a triple which is the value of the statement.
Predicate
The property part of a triple.
Property
A property is an attribute of a resource. For example
DC.title is a property, as is RDF.type.
Resource
Some entity. It could be a web resource such as web page, or
it could be a concrete physical thing such as a tree or a car. It could be an
abstract idea such as chess or football. Resources are named by URI's.
Statement
An arc in an RDF Model, normally interpreted as a fact.
Subject
The resource which is the source of an arc in an RDF Model
Triple
A structure containing a subject, a predicate and an object.
Another term for a statement.

Footnotes

  The identifier of an RDF resource can
    include a fragment identifier, e.g.
    http://hostname/rdf/tutorial/#ch-Introduction, so, strictly speaking, an
    RDF resource is identified by a URI reference.
  As well as being a string of characters,
    literals also have an optional language encoding to represent the
    language of the string. For example the literal "two" might have a
    language encoding of "en" for English and the literal "deux" might have a
    language encoding of "fr" for France.


  
  
  
    On this page
    
  
    Preface
    Introduction
    Statements
    Writing RDF
    Reading RDF
    Controlling Prefixes
      
        Explicit prefix definitions
      
    
    Jena RDF Packages
    Navigating a Model
    Querying a Model
    Operations on Models
    Containers
    More about Literals and Datatypes
    Glossary
    Footnotes\n\n\n\nThe objective of this SPARQL tutorial is to give a fast course in
SPARQL. The tutorial covers the major features of the query
language through examples but does not aim to be complete.
If you are looking for a short introduction to SPARQL and Jena try
Search RDF data with SPARQL.  If you are looking to execute SPARQL queries in code and already known SPARQL then you likely want to read the ARQ Documentation instead.
SPARQL is a
query language and a
protocol for accessing
RDF designed by the
W3C RDF Data Access Working Group.
As a query language, SPARQL is “data-oriented” in that it only
queries the information held in the models; there is no inference
in the query language itself. Of course, the Jena model may be
‘smart’ in that it provides the impression that certain triples
exist by creating them on-demand, including OWL reasoning. SPARQL
does not do anything other than take the description of what the
application wants, in the form of a query, and returns that
information, in the form of a set of bindings or an RDF graph.
SPARQL tutorial

Preliminaries: data!
Executing a simple query
Basic patterns
Value constraints
Optional information
Alternatives
Named Graphs
Results

Other Material

The
SPARQL query language definition document
itself contains many examples.
Search RDF data with SPARQL
(by Phil McCarthy) - article published on IBM developer works about
SPARQL and Jena.
SPARQL reference card
(by Dave Beckett)

Detailed ARQ documentation\n\nOn this page
    
  
    SPARQL tutorial
    Other Material
  

  
  
    The objective of this SPARQL tutorial is to give a fast course in
SPARQL. The tutorial covers the major features of the query
language through examples but does not aim to be complete.
If you are looking for a short introduction to SPARQL and Jena try
Search RDF data with SPARQL.  If you are looking to execute SPARQL queries in code and already known SPARQL then you likely want to read the ARQ Documentation instead.
SPARQL is a
query language and a
protocol for accessing
RDF designed by the
W3C RDF Data Access Working Group.
As a query language, SPARQL is “data-oriented” in that it only
queries the information held in the models; there is no inference
in the query language itself. Of course, the Jena model may be
‘smart’ in that it provides the impression that certain triples
exist by creating them on-demand, including OWL reasoning. SPARQL
does not do anything other than take the description of what the
application wants, in the form of a query, and returns that
information, in the form of a set of bindings or an RDF graph.
SPARQL tutorial

Preliminaries: data!
Executing a simple query
Basic patterns
Value constraints
Optional information
Alternatives
Named Graphs
Results

Other Material

The
SPARQL query language definition document
itself contains many examples.
Search RDF data with SPARQL
(by Phil McCarthy) - article published on IBM developer works about
SPARQL and Jena.
SPARQL reference card
(by Dave Beckett)

Detailed ARQ documentation

  
  
  
    On this page
    
  
    SPARQL tutorial
    Other Material\n\n\n\nThis tutorial will guide you to set up Jena on your Eclipse. At the time of writing, the
latest version of Eclipse is 4.7.0. The version of Java used for this tutorial was Java
1.8.0_121. The operational system should not be a problem, so the only requirements are
Eclipse, Java 1.8.x, and git to checkout the Jena source code.
Setting up your environment
The first thing you will need to install is a Java JDK 1.8.x. The installation instructions
vary depending on the operating system, and will not be covered in this tutorial.
Once you have Java installed, you can proceed installing Eclipse. You can either download
an Eclipse distribution, or download the installer and choose one amongst the available
packages. For this tutorial, you will see instructions and screenshots taken from an
Eclipse IDE for Java Developers.
Eclipse comes with a bundled Apache Maven, but you may prefer to install it to another
directory and customize your local settings. As this is not a must have requirement, this
will not be covered in this tutorial.
Getting the source code
Follow the instructions from our Getting involved in Apache Jena
page to check out the code from the Git repository. Most developers will check out the code into
their Eclipse workspace folder. But you should be able to import it into Eclipse from a
different folder too, as will be shown in the next sections.
Do not forget to run mvn clean install as instructed, so that Eclipse will be able to
find all local artifacts with no issues.
Importing the source code into Eclipse
Eclipse comes, by default, with Maven integration. In the past you would have to install
and configure a plug-in for that. But assuming you followed the instructions from the previous
sections, you should be ready to import the source code.

In the previous picture, you can see an empty Eclipse workspace. The view was configured to
display working sets, and there is a Jena working set already created. This is not necessary
for this tutorial, but you may find it useful if you work on separate projects at the same
time (e.g. working on Apache Commons RDF and Apache Jena projects simultaneously).
Eclipse keeps, by default, your projects on the left hand side panel. Right click somewhere
on that panel and choose Import. Alternatively, you can navigate using the top menu
to File / Import.

That will open a menu dialog, where you should find several types of projects to
import into your workspace. For Jena, you must select import Existing Maven Projects,
under the Maven project category.

Clicking Next will bring you to another screen where you can choose the location
of Jena source code. Point it to the folder where you checked out the Jena source code
in the previous section of this tutorial.

Click Finish and Eclipse will start importing your project. This may take
a few minutes, depending on your computer resources. You can keep an eye at the Progress
tab, in the bottom panel, to see what is the status of the import process.
Once the project has been imported into your workspace, you should see something similar to the
following screenshot.


After the import process is complete, Eclipse will start building the project
automatically if you have it configured with the default settings, or you may have to
click on Project / Build All.
Eclipse will display a red icon on the project folders with build problems. We will see now how
to fix these build problems, so Eclipse can successfully build and run the project.

The build problems are related to a known issue due to how the project shades Google Guava classes.
The workaround is to make sure the jena-shaded-guava Maven module remains closed
in Eclipse. You can simply right click on the project, and choose Close. Its icon should
change, indicating it has been closed.

After doing that, it is good to trigger a Clean on all projects, so that
Eclipse can clean and re-build everything.

You may also need to update the Maven project settings, so that Eclipse
is aware that the project is closed and it will use a local artifact, rather than the
module in the workspace.

If you followed all steps, and there is nothing else running in your Eclipse
(check the Progress tab) then your Jena project should have been built with success.

If you would like to test Fuseki now, for example, you can expand the jena-fuseki-core
Maven module, navigate to the org.apache.jena.fuseki.cmd package, and run
FusekiCmd as a Java Application.

That should initialize Fuseki, and have it listening on http://localhost:3030.

Now you should also be able to debug Jena, modify the source code and build the
project again, or import or create other projects into your workspace, and use them
with the latest version of Jena.\n\nOn this page
    
  
    Setting up your environment
    Getting the source code
    Importing the source code into Eclipse
  

  
  
    This tutorial will guide you to set up Jena on your Eclipse. At the time of writing, the
latest version of Eclipse is 4.7.0. The version of Java used for this tutorial was Java
1.8.0_121. The operational system should not be a problem, so the only requirements are
Eclipse, Java 1.8.x, and git to checkout the Jena source code.
Setting up your environment
The first thing you will need to install is a Java JDK 1.8.x. The installation instructions
vary depending on the operating system, and will not be covered in this tutorial.
Once you have Java installed, you can proceed installing Eclipse. You can either download
an Eclipse distribution, or download the installer and choose one amongst the available
packages. For this tutorial, you will see instructions and screenshots taken from an
Eclipse IDE for Java Developers.
Eclipse comes with a bundled Apache Maven, but you may prefer to install it to another
directory and customize your local settings. As this is not a must have requirement, this
will not be covered in this tutorial.
Getting the source code
Follow the instructions from our Getting involved in Apache Jena
page to check out the code from the Git repository. Most developers will check out the code into
their Eclipse workspace folder. But you should be able to import it into Eclipse from a
different folder too, as will be shown in the next sections.
Do not forget to run mvn clean install as instructed, so that Eclipse will be able to
find all local artifacts with no issues.
Importing the source code into Eclipse
Eclipse comes, by default, with Maven integration. In the past you would have to install
and configure a plug-in for that. But assuming you followed the instructions from the previous
sections, you should be ready to import the source code.

In the previous picture, you can see an empty Eclipse workspace. The view was configured to
display working sets, and there is a Jena working set already created. This is not necessary
for this tutorial, but you may find it useful if you work on separate projects at the same
time (e.g. working on Apache Commons RDF and Apache Jena projects simultaneously).
Eclipse keeps, by default, your projects on the left hand side panel. Right click somewhere
on that panel and choose Import. Alternatively, you can navigate using the top menu
to File / Import.

That will open a menu dialog, where you should find several types of projects to
import into your workspace. For Jena, you must select import Existing Maven Projects,
under the Maven project category.

Clicking Next will bring you to another screen where you can choose the location
of Jena source code. Point it to the folder where you checked out the Jena source code
in the previous section of this tutorial.

Click Finish and Eclipse will start importing your project. This may take
a few minutes, depending on your computer resources. You can keep an eye at the Progress
tab, in the bottom panel, to see what is the status of the import process.
Once the project has been imported into your workspace, you should see something similar to the
following screenshot.


After the import process is complete, Eclipse will start building the project
automatically if you have it configured with the default settings, or you may have to
click on Project / Build All.
Eclipse will display a red icon on the project folders with build problems. We will see now how
to fix these build problems, so Eclipse can successfully build and run the project.

The build problems are related to a known issue due to how the project shades Google Guava classes.
The workaround is to make sure the jena-shaded-guava Maven module remains closed
in Eclipse. You can simply right click on the project, and choose Close. Its icon should
change, indicating it has been closed.

After doing that, it is good to trigger a Clean on all projects, so that
Eclipse can clean and re-build everything.

You may also need to update the Maven project settings, so that Eclipse
is aware that the project is closed and it will use a local artifact, rather than the
module in the workspace.

If you followed all steps, and there is nothing else running in your Eclipse
(check the Progress tab) then your Jena project should have been built with success.

If you would like to test Fuseki now, for example, you can expand the jena-fuseki-core
Maven module, navigate to the org.apache.jena.fuseki.cmd package, and run
FusekiCmd as a Java Application.

That should initialize Fuseki, and have it listening on http://localhost:3030.

Now you should also be able to debug Jena, modify the source code and build the
project again, or import or create other projects into your workspace, and use them
with the latest version of Jena.

  
  
  
    On this page
    
  
    Setting up your environment
    Getting the source code
    Importing the source code into Eclipse\n\n\n\nThis section contains detailed information about the various Jena
sub-systems, aimed at developers using Jena. For more general introductions,
please refer to the Getting started and Tutorial
sections.
Documentation index

The RDF API - the core RDF API in Jena
SPARQL - querying and updating RDF models using the SPARQL standards
Fuseki - SPARQL server which can present RDF data and answer SPARQL queries over HTTP
I/O - reading and writing RDF data
RDF Connection - a SPARQL API for local datasets and remote services
Assembler - describing recipes for constructing Jena models declaratively using RDF
Inference - using the Jena rules engine and other inference algorithms to derive consequences from RDF models
Ontology - support for handling OWL models in Jena
Data and RDFS - apply RDFS to graphs in a dataset
TDB2 - a fast persistent triple store that stores directly to disk
TDB - Original TDB database
SHACL - SHACL processor for Jena
ShEx - ShEx processor for Jena
Text Search - enhanced indexes using Lucene for more efficient searching of text literals in Jena models and datasets.
GeoSPARQL - support for GeoSPARQL
Permissions - a permissions wrapper around Jena RDF implementation
Tools - various command-line tools and utilities to help developers manage RDF data and other aspects of Jena
How-To’s - various topic-specific how-to documents
QueryBuilder - Classes to simplify the programmatic building of various query and update statements.
Extras - various modules that provide utilities and larger packages that make Apache Jena development or usage easier but that do not fall within the standard Jena framework.
Javadoc - JavaDoc generated from the Jena source\n\nOn this page
    
  
    Documentation index
  

  
  
    This section contains detailed information about the various Jena
sub-systems, aimed at developers using Jena. For more general introductions,
please refer to the Getting started and Tutorial
sections.
Documentation index

The RDF API - the core RDF API in Jena
SPARQL - querying and updating RDF models using the SPARQL standards
Fuseki - SPARQL server which can present RDF data and answer SPARQL queries over HTTP
I/O - reading and writing RDF data
RDF Connection - a SPARQL API for local datasets and remote services
Assembler - describing recipes for constructing Jena models declaratively using RDF
Inference - using the Jena rules engine and other inference algorithms to derive consequences from RDF models
Ontology - support for handling OWL models in Jena
Data and RDFS - apply RDFS to graphs in a dataset
TDB2 - a fast persistent triple store that stores directly to disk
TDB - Original TDB database
SHACL - SHACL processor for Jena
ShEx - ShEx processor for Jena
Text Search - enhanced indexes using Lucene for more efficient searching of text literals in Jena models and datasets.
GeoSPARQL - support for GeoSPARQL
Permissions - a permissions wrapper around Jena RDF implementation
Tools - various command-line tools and utilities to help developers manage RDF data and other aspects of Jena
How-To’s - various topic-specific how-to documents
QueryBuilder - Classes to simplify the programmatic building of various query and update statements.
Extras - various modules that provide utilities and larger packages that make Apache Jena development or usage easier but that do not fall within the standard Jena framework.
Javadoc - JavaDoc generated from the Jena source


  
  
  
    On this page
    
  
    Documentation index\n\n\n\nARQ is a query engine for Jena that
supports the
SPARQL RDF Query language.
SPARQL is the query language developed by the W3C
RDF Data Access Working Group.
ARQ Features

Standard SPARQL
Free text search via Lucene
SPARQL/Update
Access and extension of the SPARQL algebra
Support for custom filter functions, including javascript functions
Property functions for custom processing of semantic
relationships
Aggregation, GROUP BY and assignment as SPARQL extensions
Support for federated query
Support for extension to other storage systems
Client-support for remote access to any SPARQL endpoint

Introduction

A Brief Tutorial on SPARQL
Application API - covers the majority of
application usages
Frequently Asked Questions
ARQ Support
Application javadoc
Command line utilities
Querying remote SPARQL services

HTTP Authentication for ARQ


Logging
Explaining queries
Tutorial: manipulating SPARQL using ARQ
Basic federated query (SERVICE)
Property paths
GROUP BY and counting
SELECT expressions
Sub-SELECT
Negation

Features of ARQ that are legal SPARQL syntax

Conditions in FILTERs
Free text searches
Accessing lists (RDF collections)
Extension mechanisms

Custom Expression Functions
Property Functions


Library

Expression function library
Property function library


Writing SPARQL functions
Writing SPARQL functions in JavaScript
Custom execution of SERVICE
Constructing queries programmatically
Parameterized query strings
ARQ and the SPARQL algebra
Extending ARQ query execution and accessing different storage implementations
Custom aggregates
Caching and bulk-retrieval for SERVICE

Extensions
Feature of ARQ that go beyond SPARQL syntax.

LATERAL Join
RDF-star
Operators and functions
MOD
and IDIV for modulus and integer division.
LET variable assignment
Order results using a Collation
Construct Quad
Generate JSON from SPARQL

Update
ARQ supports the W3C standard SPARQL Update language.

SPARQL Update
The ARQ SPARQL/Update API

See Also

Fuseki - Server implementation of the SPARQL protocol.
TDB - A SPARQL database for Jena, a pure Java persistence layer for large graphs, high performance applications and embedded use.
RDFConnection, a unified API for SPARQL Query, Update and Graph Store Protocol.

W3C Documents

SPARQL Query Language specification
SPARQL Query Results JSON Format
SPARQL Protocol

Articles
Articles and documentation elsewhere:

Introducing SPARQL: Querying the Semantic Web
(xml.com article by Leigh Dodds)
Search RDF data with SPARQL
(by Phil McCarthy) - article published on IBM developer works about
SPARQL and Jena.
SPARQL reference card
(by Dave Beckett)
Parameterised Queries with SPARQL and ARQ
(by Leigh Dodds)
Writing an ARQ Extension Function
(by Leigh Dodds)

RDF Syntax Specifications

Turtle
N-Triples
TriG
N-Quads\n\nOn this page
    
  
    ARQ Features
    Introduction
    Extensions
    Update
    See Also
    W3C Documents
    Articles
    RDF Syntax Specifications
  

  
  
    ARQ is a query engine for Jena that
supports the
SPARQL RDF Query language.
SPARQL is the query language developed by the W3C
RDF Data Access Working Group.
ARQ Features

Standard SPARQL
Free text search via Lucene
SPARQL/Update
Access and extension of the SPARQL algebra
Support for custom filter functions, including javascript functions
Property functions for custom processing of semantic
relationships
Aggregation, GROUP BY and assignment as SPARQL extensions
Support for federated query
Support for extension to other storage systems
Client-support for remote access to any SPARQL endpoint

Introduction

A Brief Tutorial on SPARQL
Application API - covers the majority of
application usages
Frequently Asked Questions
ARQ Support
Application javadoc
Command line utilities
Querying remote SPARQL services

HTTP Authentication for ARQ


Logging
Explaining queries
Tutorial: manipulating SPARQL using ARQ
Basic federated query (SERVICE)
Property paths
GROUP BY and counting
SELECT expressions
Sub-SELECT
Negation

Features of ARQ that are legal SPARQL syntax

Conditions in FILTERs
Free text searches
Accessing lists (RDF collections)
Extension mechanisms

Custom Expression Functions
Property Functions


Library

Expression function library
Property function library


Writing SPARQL functions
Writing SPARQL functions in JavaScript
Custom execution of SERVICE
Constructing queries programmatically
Parameterized query strings
ARQ and the SPARQL algebra
Extending ARQ query execution and accessing different storage implementations
Custom aggregates
Caching and bulk-retrieval for SERVICE

Extensions
Feature of ARQ that go beyond SPARQL syntax.

LATERAL Join
RDF-star
Operators and functions
MOD
and IDIV for modulus and integer division.
LET variable assignment
Order results using a Collation
Construct Quad
Generate JSON from SPARQL

Update
ARQ supports the W3C standard SPARQL Update language.

SPARQL Update
The ARQ SPARQL/Update API

See Also

Fuseki - Server implementation of the SPARQL protocol.
TDB - A SPARQL database for Jena, a pure Java persistence layer for large graphs, high performance applications and embedded use.
RDFConnection, a unified API for SPARQL Query, Update and Graph Store Protocol.

W3C Documents

SPARQL Query Language specification
SPARQL Query Results JSON Format
SPARQL Protocol

Articles
Articles and documentation elsewhere:

Introducing SPARQL: Querying the Semantic Web
(xml.com article by Leigh Dodds)
Search RDF data with SPARQL
(by Phil McCarthy) - article published on IBM developer works about
SPARQL and Jena.
SPARQL reference card
(by Dave Beckett)
Parameterised Queries with SPARQL and ARQ
(by Leigh Dodds)
Writing an ARQ Extension Function
(by Leigh Dodds)

RDF Syntax Specifications

Turtle
N-Triples
TriG
N-Quads


  
  
  
    On this page
    
  
    ARQ Features
    Introduction
    Extensions
    Update
    See Also
    W3C Documents
    Articles
    RDF Syntax Specifications\n\n\n\nThis page details the setup of RDF I/O technology (RIOT).

Formats
Commands
Reading RDF in Jena
Writing RDF in Jena
Working with RDF Streams

Formats
The following RDF formats are supported by Jena. In addition, other syntaxes
can be integrated into both the parser and writer registries.

Turtle
JSON-LD
N-Triples
N-Quads
TriG
RDF/XML
TriX
RDF/JSON
RDF Binary

RDF/JSON is different from JSON-LD - it is a direct encoding of RDF triples in JSON.
See the description of RDF/JSON.
RDF Binary is a binary encoding of RDF (graphs and datasets) that can be useful
for fast parsing.  See RDF Binary.
Command line tools
There are scripts in Jena download to run these commands.

riot - parse, guessing the syntax from the file extension.
Assumes N-Quads/N-Triples from stdin.
turtle, ntriples, nquads, trig, rdfxml - parse a particular language

These can be called directly as Java programs:
The file extensions understood are:

  
      
           Extension 
            Language 
      
  
  
      
          .ttl
          Turtle
      
      
          .nt
          N-Triples
      
      
          .nq
          N-Quads
      
      
          .trig
          TriG
      
      
          .rdf
          RDF/XML
      
      
          .owl
          RDF/XML
      
      
          .jsonld
          JSON-LD
      
      
          .trdf
          RDF Thrift
      
      
          .rt
          RDF Thrift
      
      
          .rpb
          RDF Protobuf
      
      
          .pbrdf
          RDF Protobuf
      
      
          .rj
          RDF/JSON
      
      
          .trix
          TriX
      
  

.n3 is supported but only as a synonym for Turtle.
The TriX support is for the core TriX format.
In addition, if the extension is .gz the file is assumed to be gzip
compressed. The file name is examined for an inner extension. For
example, .nt.gz is gzip compressed N-Triples.
Jena does not support all possible compression formats itself, only
GZip and BZip2 are supported directly.  If you want to use an
alternative compression format you can do so by piping the output of the
relevant decompression utility into one of Jena’s commands e.g.
zstd -d < FILE.nq.zst | riot --syntax NQ ...

These scripts call java programs in the riotcmd package. For example:
java -cp ... riotcmd.riot file.ttl

This can be a mixture of files in different syntaxes when file extensions
are used to determine the file syntax type.
The scripts all accept the same arguments (type "riot --help" to
get command line reminders):

--syntax=NAME; Explicitly set the input syntax for all files.
--validate: Checking mode: same as --strict --sink --check=true.
--check=true/false: Run with checking of literals and IRIs either on or off.
--time: Output timing information.
--sink: No output.
--output=FORMAT: Output in a given syntax (streaming if possible).
--formatted=FORMAT: Output in a given syntax, using pretty printing.
--stream=FORMAT: Output in a given syntax, streaming (not all syntaxes can be streamed).

To aid in checking for errors in UTF8-encoded files, there is a
utility which reads a file of bytes as UTF8 and checks the encoding.

utf8 – read bytes as UTF8

Inference
RIOT support creation of inferred triples during the parsing
process:
riotcmd.infer --rdfs VOCAB FILE FILE ...

Output will contain the base data and triples inferred based on
RDF subclass, subproperty, domain and range declarations.\n\nOn this page
    
  
    Formats
    Command line tools
    Inference
  

  
  
    This page details the setup of RDF I/O technology (RIOT).

Formats
Commands
Reading RDF in Jena
Writing RDF in Jena
Working with RDF Streams

Formats
The following RDF formats are supported by Jena. In addition, other syntaxes
can be integrated into both the parser and writer registries.

Turtle
JSON-LD
N-Triples
N-Quads
TriG
RDF/XML
TriX
RDF/JSON
RDF Binary

RDF/JSON is different from JSON-LD - it is a direct encoding of RDF triples in JSON.
See the description of RDF/JSON.
RDF Binary is a binary encoding of RDF (graphs and datasets) that can be useful
for fast parsing.  See RDF Binary.
Command line tools
There are scripts in Jena download to run these commands.

riot - parse, guessing the syntax from the file extension.
Assumes N-Quads/N-Triples from stdin.
turtle, ntriples, nquads, trig, rdfxml - parse a particular language

These can be called directly as Java programs:
The file extensions understood are:

  
      
           Extension 
            Language 
      
  
  
      
          .ttl
          Turtle
      
      
          .nt
          N-Triples
      
      
          .nq
          N-Quads
      
      
          .trig
          TriG
      
      
          .rdf
          RDF/XML
      
      
          .owl
          RDF/XML
      
      
          .jsonld
          JSON-LD
      
      
          .trdf
          RDF Thrift
      
      
          .rt
          RDF Thrift
      
      
          .rpb
          RDF Protobuf
      
      
          .pbrdf
          RDF Protobuf
      
      
          .rj
          RDF/JSON
      
      
          .trix
          TriX
      
  

.n3 is supported but only as a synonym for Turtle.
The TriX support is for the core TriX format.
In addition, if the extension is .gz the file is assumed to be gzip
compressed. The file name is examined for an inner extension. For
example, .nt.gz is gzip compressed N-Triples.
Jena does not support all possible compression formats itself, only
GZip and BZip2 are supported directly.  If you want to use an
alternative compression format you can do so by piping the output of the
relevant decompression utility into one of Jena’s commands e.g.
zstd -d < FILE.nq.zst | riot --syntax NQ ...

These scripts call java programs in the riotcmd package. For example:
java -cp ... riotcmd.riot file.ttl

This can be a mixture of files in different syntaxes when file extensions
are used to determine the file syntax type.
The scripts all accept the same arguments (type "riot --help" to
get command line reminders):

--syntax=NAME; Explicitly set the input syntax for all files.
--validate: Checking mode: same as --strict --sink --check=true.
--check=true/false: Run with checking of literals and IRIs either on or off.
--time: Output timing information.
--sink: No output.
--output=FORMAT: Output in a given syntax (streaming if possible).
--formatted=FORMAT: Output in a given syntax, using pretty printing.
--stream=FORMAT: Output in a given syntax, streaming (not all syntaxes can be streamed).

To aid in checking for errors in UTF8-encoded files, there is a
utility which reads a file of bytes as UTF8 and checks the encoding.

utf8 – read bytes as UTF8

Inference
RIOT support creation of inferred triples during the parsing
process:
riotcmd.infer --rdfs VOCAB FILE FILE ...

Output will contain the base data and triples inferred based on
RDF subclass, subproperty, domain and range declarations.

  
  
  
    On this page
    
  
    Formats
    Command line tools
    Inference\n\n\n\nJena’s assembler provides a means of constructing Jena models
according to a recipe, where that recipe is itself stated in
RDF. This is the Assembler quickstart page. For more detailed
information, see the Assembler howto
or Inside assemblers.
What is an Assembler specification?
An Assembler specification is an RDF description of how to
construct a model and its associated resources, such as reasoners,
prefix mappings, and initial content. The Assembler vocabulary is
given in the Assembler schema,
and we’ll use the prefix ja for its identifiers.
What is an Assembler?
An Assembler is an object that implements the Assembler
interface and can construct objects (typically models) from
Assembler specifications. The constant Assembler.general is an
Assembler that knows how to construct some general patterns
of model specification.
How can I make a model according to a specification?
Suppose the Model M contains an Assembler specification whose
root - the Resource describing the whole Model to construct is
R (so R.getModel() == M). Invoke:
Assembler.general.openModel(R)
The result is the desired Model. Further details about the
Assembler interface, the special Assembler general, and the
details of specific Assemblers, are deferred to the
Assembler howto.
How can I specify …
In the remaining sections, the object we want to describe is given
the root resource my:root.
… a memory model?
my:root a ja:MemoryModel.
… an inference model?
my:root
    ja:reasoner [ja:reasonerURL theReasonerURL] ;
    ja:baseModel theBaseModelResource
    .
theReasonerURL is one of the reasoner (factory) URLs given in the
inference documentation and code; theBaseModelResource is another
resource in the same document describing the base model.
… some initialising content?
my:root
    ja:content [ja:externalContent <someContentURL>]
    ... rest of model specification ...
    .
The model will be pre-loaded with the contents of someContentURL.
… an ontology model?
my:root
    ja:ontModelSpec ja:OntModelSpecName ;
    ja:baseModel somebaseModel
    .
The OntModelSpecName can be any of the predefined Jena
OntModelSpec names, eg OWL_DL_MEM_RULE_INF. The baseModel is
another model description - it can be left out, in which case you
get an empty memory model. See
Assembler howto for construction of
non-predefined OntModelSpecs.\n\nOn this page
    
  
    What is an Assembler specification?
    What is an Assembler?
    How can I make a model according to a specification?
    How can I specify …
      
        … a memory model?
        … an inference model?
        … some initialising content?
        … an ontology model?
      
    
  

  
  
    Jena’s assembler provides a means of constructing Jena models
according to a recipe, where that recipe is itself stated in
RDF. This is the Assembler quickstart page. For more detailed
information, see the Assembler howto
or Inside assemblers.
What is an Assembler specification?
An Assembler specification is an RDF description of how to
construct a model and its associated resources, such as reasoners,
prefix mappings, and initial content. The Assembler vocabulary is
given in the Assembler schema,
and we’ll use the prefix ja for its identifiers.
What is an Assembler?
An Assembler is an object that implements the Assembler
interface and can construct objects (typically models) from
Assembler specifications. The constant Assembler.general is an
Assembler that knows how to construct some general patterns
of model specification.
How can I make a model according to a specification?
Suppose the Model M contains an Assembler specification whose
root - the Resource describing the whole Model to construct is
R (so R.getModel() == M). Invoke:
Assembler.general.openModel(R)
The result is the desired Model. Further details about the
Assembler interface, the special Assembler general, and the
details of specific Assemblers, are deferred to the
Assembler howto.
How can I specify …
In the remaining sections, the object we want to describe is given
the root resource my:root.
… a memory model?
my:root a ja:MemoryModel.
… an inference model?
my:root
    ja:reasoner [ja:reasonerURL theReasonerURL] ;
    ja:baseModel theBaseModelResource
    .
theReasonerURL is one of the reasoner (factory) URLs given in the
inference documentation and code; theBaseModelResource is another
resource in the same document describing the base model.
… some initialising content?
my:root
    ja:content [ja:externalContent <someContentURL>]
    ... rest of model specification ...
    .
The model will be pre-loaded with the contents of someContentURL.
… an ontology model?
my:root
    ja:ontModelSpec ja:OntModelSpecName ;
    ja:baseModel somebaseModel
    .
The OntModelSpecName can be any of the predefined Jena
OntModelSpec names, eg OWL_DL_MEM_RULE_INF. The baseModel is
another model description - it can be left out, in which case you
get an empty memory model. See
Assembler howto for construction of
non-predefined OntModelSpecs.

  
  
  
    On this page
    
  
    What is an Assembler specification?
    What is an Assembler?
    How can I make a model according to a specification?
    How can I specify …
      
        … a memory model?
        … an inference model?
        … some initialising content?
        … an ontology model?\n\n\n\nJena includes various command-line utilities which can help you with
a variety of tasks in developing Jena-based applications.
Index of tools

schemagen
using schemagen from maven

Setting up your Environment
An environment variable JENA_HOME is used by all the command line tools to configure the class path automatically for you.  You can set this up as follows:
On Linux / Mac

export JENA_HOME=the directory you downloaded Jena to
export PATH=$PATH:$JENA_HOME/bin

On Windows

SET JENA_HOME =the directory you downloaded Jena to
SET PATH=%PATH%;%JENA_HOME%\bat

Running the Tools
Once you’ve done the above you should now be able to run the tools from the command line like so:
On Linux / Mac

sparql --version

On Windows

sparql.bat --version

This command will simply print the versions of Jena and ARQ used in your distribution, all the tools support the --version  option.  To find out how to use a specific tool add the --help flag instead.
Note that many examples of using Jena tools typically use the Linux style invocation because most of the Jena developers work on Linux/Mac platforms.  When running on windows simply add .bat as an extension to the name of the command line tool to run it, on some versions of Windows this may not be required.
Common Issues with Running the Tools
If you receive errors stating that a class is not found then it is most likely that JENA_HOME is not set correctly.  As a quick check you can try the following to see if it is set appropriately:
On Linux / Mac

cd $JENA_HOME

On Windows

cd %JENA_HOME%

If this command fails then JENA_HOME is not correctly set, please ensure you have set it correctly and try again.
Windows users may experience problems if trying to run the tools when their JENA_HOME path contains spaces in it, there are two workarounds for this:

Move your Jena installation to a path without spaces
Grab the latest scripts from main where they have been fixed to safely handle this.  Future releases will include this fix and resolve this issue

Command Line Tools Quick Reference
riot and Related
See Reading and Writing RDF in Apache Jena for more information.


riot: parse RDF data, guessing the syntax from the file extension. Assumes that standard input is N-Quads/N-Triples unless
you tell it otherwise with the --syntax parameter. riot can also do RDFS inferencing, count triples, convert serializations,
validate syntax, concatenate datasets, and more.


turtle, ntriples, nquads, trig, rdfxml: specialized versions of riot that assume that the input is in the named serialization.


rdfparse: parse an RDF/XML document, for which you can usually just use riot, but this can also pull triples out of rdf:RDF elements
embedded at arbitrary places in an XML document if you need to deal with those.


SPARQL Queries on Local Files and Endpoints
See ARQ - Command Line Applications for more about these.


arq and sparql: run a query in a file named as a command line parameter on a dataset in one or more files named as command line parameters.


qparse: parse a query, report on any problems, and output a pretty-printed version of the query.


uparse: do the same thing as qparse but for update requests.


rsparql: send a local query to a SPARQL endpoint specified with a URL, giving you the same choice of output formats
that arq does.


rupdate: send a local update query to a SPARQL endpoint specified with a URL, assuming that is accepting updates from you.


Querying and Manipulating Fuseki Datasets
The following utilities let you work with data stored using a local
Fuseki triplestore. They can
be useful for automating queries and updates of data stored there. Each
requires an assembler file
pointing at a dataset as a parameter; Fuseki creates these for you.
For each pair of utilities shown, the first is used with data stored using the TDB format and the
second with data stored using the newer and more efficient TDB2 format.
The TDB and TDB2 - Command Line Tools
pages describe these further.


tdbquery, tdb2.tdbquery: query a dataset that has been stored with Fuseki.


tdbdump, tdb2.tdbdump: dump the contents of a Fuseki dataset to standard out.


tdbupdate, tdb2.tdbupdate: run an update request against a Fuseki dataset.


tdbloader, tdb2.tdbloader: load a data from a file into a Fuseki dataset.


tdbstats, tdb2.tdbstats: output a short report of information about a Fuseki dataset.


tdbbackup, tdb2.tdbbackup: create a gzipped copy of the Fuseki dataset’s triples.


not implemented for TDB1, tdb2.tdbcompact: reduce the size of the Fuseki dataset.


Other Handy Command Line Tools


shacl: validate a dataset against a set of shapes and constraints described in a
file that conforms to the W3C SHACL standard.
Jena’s SHACL page has more on this utility.


shex: validate data using ShEx from the
W3C Shape Expressions Community Group.
Jena’s ShEx page has more on this utility.


rdfdiff: compare the triples in two datasets, regardless of their serializations, and list
which are different between the two datasets. (Modeled on the UNIX diff utility.)


iri: Parse a IRI and tell you about it, with errors and warnings. Good for
checking for issues like proper escaping.\n\nOn this page
    
  
    
      
        Index of tools
        Setting up your Environment
        Running the Tools
        Common Issues with Running the Tools
        Command Line Tools Quick Reference
          
            riot and Related
            SPARQL Queries on Local Files and Endpoints
            Querying and Manipulating Fuseki Datasets
            Other Handy Command Line Tools
          
        
      
    
  

  
  
    Jena includes various command-line utilities which can help you with
a variety of tasks in developing Jena-based applications.
Index of tools

schemagen
using schemagen from maven

Setting up your Environment
An environment variable JENA_HOME is used by all the command line tools to configure the class path automatically for you.  You can set this up as follows:
On Linux / Mac

export JENA_HOME=the directory you downloaded Jena to
export PATH=$PATH:$JENA_HOME/bin

On Windows

SET JENA_HOME =the directory you downloaded Jena to
SET PATH=%PATH%;%JENA_HOME%\bat

Running the Tools
Once you’ve done the above you should now be able to run the tools from the command line like so:
On Linux / Mac

sparql --version

On Windows

sparql.bat --version

This command will simply print the versions of Jena and ARQ used in your distribution, all the tools support the --version  option.  To find out how to use a specific tool add the --help flag instead.
Note that many examples of using Jena tools typically use the Linux style invocation because most of the Jena developers work on Linux/Mac platforms.  When running on windows simply add .bat as an extension to the name of the command line tool to run it, on some versions of Windows this may not be required.
Common Issues with Running the Tools
If you receive errors stating that a class is not found then it is most likely that JENA_HOME is not set correctly.  As a quick check you can try the following to see if it is set appropriately:
On Linux / Mac

cd $JENA_HOME

On Windows

cd %JENA_HOME%

If this command fails then JENA_HOME is not correctly set, please ensure you have set it correctly and try again.
Windows users may experience problems if trying to run the tools when their JENA_HOME path contains spaces in it, there are two workarounds for this:

Move your Jena installation to a path without spaces
Grab the latest scripts from main where they have been fixed to safely handle this.  Future releases will include this fix and resolve this issue

Command Line Tools Quick Reference
riot and Related
See Reading and Writing RDF in Apache Jena for more information.


riot: parse RDF data, guessing the syntax from the file extension. Assumes that standard input is N-Quads/N-Triples unless
you tell it otherwise with the --syntax parameter. riot can also do RDFS inferencing, count triples, convert serializations,
validate syntax, concatenate datasets, and more.


turtle, ntriples, nquads, trig, rdfxml: specialized versions of riot that assume that the input is in the named serialization.


rdfparse: parse an RDF/XML document, for which you can usually just use riot, but this can also pull triples out of rdf:RDF elements
embedded at arbitrary places in an XML document if you need to deal with those.


SPARQL Queries on Local Files and Endpoints
See ARQ - Command Line Applications for more about these.


arq and sparql: run a query in a file named as a command line parameter on a dataset in one or more files named as command line parameters.


qparse: parse a query, report on any problems, and output a pretty-printed version of the query.


uparse: do the same thing as qparse but for update requests.


rsparql: send a local query to a SPARQL endpoint specified with a URL, giving you the same choice of output formats
that arq does.


rupdate: send a local update query to a SPARQL endpoint specified with a URL, assuming that is accepting updates from you.


Querying and Manipulating Fuseki Datasets
The following utilities let you work with data stored using a local
Fuseki triplestore. They can
be useful for automating queries and updates of data stored there. Each
requires an assembler file
pointing at a dataset as a parameter; Fuseki creates these for you.
For each pair of utilities shown, the first is used with data stored using the TDB format and the
second with data stored using the newer and more efficient TDB2 format.
The TDB and TDB2 - Command Line Tools
pages describe these further.


tdbquery, tdb2.tdbquery: query a dataset that has been stored with Fuseki.


tdbdump, tdb2.tdbdump: dump the contents of a Fuseki dataset to standard out.


tdbupdate, tdb2.tdbupdate: run an update request against a Fuseki dataset.


tdbloader, tdb2.tdbloader: load a data from a file into a Fuseki dataset.


tdbstats, tdb2.tdbstats: output a short report of information about a Fuseki dataset.


tdbbackup, tdb2.tdbbackup: create a gzipped copy of the Fuseki dataset’s triples.


not implemented for TDB1, tdb2.tdbcompact: reduce the size of the Fuseki dataset.


Other Handy Command Line Tools


shacl: validate a dataset against a set of shapes and constraints described in a
file that conforms to the W3C SHACL standard.
Jena’s SHACL page has more on this utility.


shex: validate data using ShEx from the
W3C Shape Expressions Community Group.
Jena’s ShEx page has more on this utility.


rdfdiff: compare the triples in two datasets, regardless of their serializations, and list
which are different between the two datasets. (Modeled on the UNIX diff utility.)


iri: Parse a IRI and tell you about it, with errors and warnings. Good for
checking for issues like proper escaping.



  
  
  
    On this page
    
  
    
      
        Index of tools
        Setting up your Environment
        Running the Tools
        Common Issues with Running the Tools
        Command Line Tools Quick Reference
          
            riot and Related
            SPARQL Queries on Local Files and Endpoints
            Querying and Manipulating Fuseki Datasets
            Other Handy Command Line Tools\n\n\n\nThis page describes support for accessing data with additional statements
derived using RDFS. It supports rdfs:subClassOf, rdfs:subPropertyOf, rdfs:domain and rdfs:range. It
does not provide RDF axioms. The RDFS vocabulary is not included in the data.
It does support use with RDF datasets, where each graph in the dataset has the
same RDFS vocabulary applied to it.
This is not a replacement for the Jena RDFS Reasoner support
which covers full RDFS inference.
The data is updateable, and graphs can be added and removed from the dataset.
The vocabulary can not be changed during the lifetime of the RDFS dataset.
API: RDFSFactory
The API provides operation to build RDF-enabled datasets from data storage and vocabularies:
Example:
DatasetGraph data = ...
// Load the vocabulary
Graph vocab = RDFDataMgr.loadGraph("vocabulary.ttl");
// Create a DatasetGraph with RDFS
DatasetGraph dsg = datasetRDFS(DatasetGraph data, Graph vocab );
// (Optional) Present as a Dataset.
Dataset dataset = DatasetFactory.wrap(dsg);
The vocabulary is processed to produce datastructure needed for processing the
data efficiently at run time. This is the SetupRDFS class that can be created
and shared; it is thread-safe.
SetupRDFS setup = setupRDFS(vocab);
Assembler: RDFS Dataset
Datasets with RDFS can be built with an assembler:
<#rdfsDS> rdf:type ja:DatasetRDFS ;
      ja:rdfsSchema <vocabulary.ttl>;
      ja:dataset <#baseDataset> ;
      .

<#baseDataset> rdf:type ...some dataset type ... ;
      ...
      .
where <#baseDataset> is the definition of the dataset to be enriched.
Assembler: RDFS Graph
It is possible to build a single Model:
<#rdfsGraph> rdf:type ja:GraphRDFS ;
    ja:rdfsSchema <vocabulary.ttl>;
    ja:graph <#baseGraph> ;
    .

<#baseGraph> rdf:type ja:MemoryModel;
    ...
More generally, inference models can be defined using the Jena Inference and Rule
engine:
jena-fuseki2/examples/config-inference-1.ttl.
Use with Fuseki
The files for this example are available at:
jena-fuseki2/examples/rdfs.
From the command line (here, loading data from a file into an in-memory dataset):
fuseki-server --data data.trig --rdfs vocabulary.ttl /dataset
or from a configuration file with an RDFS Dataset:
PREFIX :        <#>
PREFIX fuseki:  <http://jena.apache.org/fuseki#>
PREFIX rdf:     <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs:    <http://www.w3.org/2000/01/rdf-schema#>
PREFIX ja:      <http://jena.hpl.hp.com/2005/11/Assembler#>

[] rdf:type fuseki:Server ;
   fuseki:services (
     :service
   ) .

## Fuseki service /dataset with SPARQ query
## /dataset?query=
:service rdf:type fuseki:Service ;
    fuseki:name "dataset" ;
    fuseki:endpoint [ fuseki:operation fuseki:query ] ;
    fuseki:endpoint [ fuseki:operation fuseki:update ] ;
    fuseki:dataset :rdfsDataset ;
    .

## RDFS
:rdfsDataset rdf:type ja:DatasetRDFS ;
    ja:rdfsSchema <file:vocabulary.ttl>;
    ja:dataset :baseDataset;
    .

## Transactional in-memory dataset.
:baseDataset rdf:type ja:MemoryDataset ;
    ja:data <file:data.trig>;
    .
Querying the Fuseki server
With the SOH tools, a query (asking for plain
text output):
s-query --service http://localhost:3030/dataset --output=text --file query.rq 
or with curl:
curl --data @query.rq \
      --header 'Accept: text/plain' \
      --header 'Content-type: application/sparql-query' \
      http://localhost:3030/dataset
will return:
-------------------------
| s  | p        | o     |
=========================
| :s | ns:p     | :o    |
| :s | rdf:type | ns:D  |
| :o | rdf:type | ns:T1 |
| :o | rdf:type | ns:T3 |
| :o | rdf:type | ns:T2 |
-------------------------
Files
data.trig:
PREFIX :        <http://example/>
PREFIX ns:      <http://example/ns#>

:s ns:p :o .
vocabulary.ttl:
PREFIX xsd:     <http://www.w3.org/2001/XMLSchema#>
PREFIX rdf:     <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfs:    <http://www.w3.org/2000/01/rdf-schema#>
PREFIX skos:    <http://www.w3.org/2008/05/skos#>
PREFIX list:    <http://jena.hpl.hp.com/ARQ/list#>

PREFIX ns: <http://example/ns#>

ns:T1 rdfs:subClassOf ns:T2 .
ns:T2 rdfs:subClassOf ns:T3 .

ns:p rdfs:domain ns:D .
ns:p rdfs:range  ns:T1 .
query.rq:
PREFIX :      <http://example/>
PREFIX ns:    <http://example/ns#>
PREFIX rdf:   <http://www.w3.org/1999/02/22-rdf-syntax-ns#>

SELECT * { ?s ?p ?o }\n\nOn this page
    
  
    
      
        API: RDFSFactory
        Assembler: RDFS Dataset
        Assembler: RDFS Graph
      
    
    Use with Fuseki
      
        Querying the Fuseki server
        Files
      
    
  

  
  
    This page describes support for accessing data with additional statements
derived using RDFS. It supports rdfs:subClassOf, rdfs:subPropertyOf, rdfs:domain and rdfs:range. It
does not provide RDF axioms. The RDFS vocabulary is not included in the data.
It does support use with RDF datasets, where each graph in the dataset has the
same RDFS vocabulary applied to it.
This is not a replacement for the Jena RDFS Reasoner support
which covers full RDFS inference.
The data is updateable, and graphs can be added and removed from the dataset.
The vocabulary can not be changed during the lifetime of the RDFS dataset.
API: RDFSFactory
The API provides operation to build RDF-enabled datasets from data storage and vocabularies:
Example:
DatasetGraph data = ...
// Load the vocabulary
Graph vocab = RDFDataMgr.loadGraph("vocabulary.ttl");
// Create a DatasetGraph with RDFS
DatasetGraph dsg = datasetRDFS(DatasetGraph data, Graph vocab );
// (Optional) Present as a Dataset.
Dataset dataset = DatasetFactory.wrap(dsg);
The vocabulary is processed to produce datastructure needed for processing the
data efficiently at run time. This is the SetupRDFS class that can be created
and shared; it is thread-safe.
SetupRDFS setup = setupRDFS(vocab);
Assembler: RDFS Dataset
Datasets with RDFS can be built with an assembler:
<#rdfsDS> rdf:type ja:DatasetRDFS ;
      ja:rdfsSchema <vocabulary.ttl>;
      ja:dataset <#baseDataset> ;
      .

<#baseDataset> rdf:type ...some dataset type ... ;
      ...
      .
where <#baseDataset> is the definition of the dataset to be enriched.
Assembler: RDFS Graph
It is possible to build a single Model:
<#rdfsGraph> rdf:type ja:GraphRDFS ;
    ja:rdfsSchema <vocabulary.ttl>;
    ja:graph <#baseGraph> ;
    .

<#baseGraph> rdf:type ja:MemoryModel;
    ...
More generally, inference models can be defined using the Jena Inference and Rule
engine:
jena-fuseki2/examples/config-inference-1.ttl.
Use with Fuseki
The files for this example are available at:
jena-fuseki2/examples/rdfs.
From the command line (here, loading data from a file into an in-memory dataset):
fuseki-server --data data.trig --rdfs vocabulary.ttl /dataset
or from a configuration file with an RDFS Dataset:
PREFIX :        <#>
PREFIX fuseki:  <http://jena.apache.org/fuseki#>
PREFIX rdf:     <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs:    <http://www.w3.org/2000/01/rdf-schema#>
PREFIX ja:      <http://jena.hpl.hp.com/2005/11/Assembler#>

[] rdf:type fuseki:Server ;
   fuseki:services (
     :service
   ) .

## Fuseki service /dataset with SPARQ query
## /dataset?query=
:service rdf:type fuseki:Service ;
    fuseki:name "dataset" ;
    fuseki:endpoint [ fuseki:operation fuseki:query ] ;
    fuseki:endpoint [ fuseki:operation fuseki:update ] ;
    fuseki:dataset :rdfsDataset ;
    .

## RDFS
:rdfsDataset rdf:type ja:DatasetRDFS ;
    ja:rdfsSchema <file:vocabulary.ttl>;
    ja:dataset :baseDataset;
    .

## Transactional in-memory dataset.
:baseDataset rdf:type ja:MemoryDataset ;
    ja:data <file:data.trig>;
    .
Querying the Fuseki server
With the SOH tools, a query (asking for plain
text output):
s-query --service http://localhost:3030/dataset --output=text --file query.rq 
or with curl:
curl --data @query.rq \
      --header 'Accept: text/plain' \
      --header 'Content-type: application/sparql-query' \
      http://localhost:3030/dataset
will return:
-------------------------
| s  | p        | o     |
=========================
| :s | ns:p     | :o    |
| :s | rdf:type | ns:D  |
| :o | rdf:type | ns:T1 |
| :o | rdf:type | ns:T3 |
| :o | rdf:type | ns:T2 |
-------------------------
Files
data.trig:
PREFIX :        <http://example/>
PREFIX ns:      <http://example/ns#>

:s ns:p :o .
vocabulary.ttl:
PREFIX xsd:     <http://www.w3.org/2001/XMLSchema#>
PREFIX rdf:     <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfs:    <http://www.w3.org/2000/01/rdf-schema#>
PREFIX skos:    <http://www.w3.org/2008/05/skos#>
PREFIX list:    <http://jena.hpl.hp.com/ARQ/list#>

PREFIX ns: <http://example/ns#>

ns:T1 rdfs:subClassOf ns:T2 .
ns:T2 rdfs:subClassOf ns:T3 .

ns:p rdfs:domain ns:D .
ns:p rdfs:range  ns:T1 .
query.rq:
PREFIX :      <http://example/>
PREFIX ns:    <http://example/ns#>
PREFIX rdf:   <http://www.w3.org/1999/02/22-rdf-syntax-ns#>

SELECT * { ?s ?p ?o }

  
  
  
    On this page
    
  
    
      
        API: RDFSFactory
        Assembler: RDFS Dataset
        Assembler: RDFS Graph
      
    
    Use with Fuseki
      
        Querying the Fuseki server
        Files\n\n\n\nAn implementation of GeoSPARQL 1.0 standard for SPARQL query or API.
Integration with Fuseki is provided either by using the
GeoSPARQL assembler or using the self-contained original
jena-fuseki-geosparql.  In either case, this page
describes the GeoSPARQL supported features.
Getting Started
GeoSPARQL Jena can be accessed as a library using Maven etc. from Maven Central.
<dependency>
  <groupId>org.apache.jena</groupId>
  <artifactId>jena-geosparql</artifactId>
  <version>...</version>
</dependency>
Features
This implementation follows the 11-052r4 OGC GeoSPARQL standard
(https://www.ogc.org/standards/geosparql).  The implementation is pure Java
and does not require any set-up or configuration of any third party relational
databases and geospatial extensions.
It implements the six Conformance Classes described in the GeoSPARQL document:

Core
Topology Vocabulary
Geometry Extension
Geometry Topology
RDFS Entailment Extension
Query Rewrite Extension

The WKT (as described in 11-052r4) and GML 2.0 Simple Features Profile
(10-100r3) serialisations are supported.  Additional serialisations can be
implemented by extending the
org.apache.jena.geosparql.implementation.datatype.GeometryDatatype and
registering with Jena’s org.apache.jena.datatypes.TypeMapper.
All three spatial relation families are supported: Simple Feature, Egenhofer and RCC8.
Indexing and caching of spatial objects and relations is performed on-demand
during query execution.  Therefore, set-up delays should be minimal. Spatial
indexing is available based on the STRtree from the JTS library. The STRtree
is readonly once built and contributions of a QuadTree implementation are
welcome.
Benchmarking of the implementation against Strabon and Parliament has found it
to be comparable or quicker.  The benchmarking used was the Geographical query
and dataset (http://geographica.di.uoa.gr/).
Additional Features
The following additional features are also provided:

Geometry properties are automatically calculated and do not need to be asserted in the dataset.
Conversion between EPSG spatial/coordinate reference systems is applied
automatically. Therefore, mixed datasets or querying can be applied. This is
reliance upon local installation of Apache SIS EPSG dataset, see Key
Dependencies.
Units of measure are automatically converted to the appropriate units for the
coordinate reference system.
Geometry, transformation and spatial relation results are stored in persistent
and configurable time-limited caches to improve response times and reduce
recalculations.
Dataset conversion between serialisations and spatial/coordinate reference
systems. Tabular data can also be loaded, see RDF Tables project
(https://github.com/galbiston/rdf-tables).
Functions to test Geometry properties directly on Geometry Literals have been included for convenience.

SPARQL Query Configuration
Using the library for SPARQL querying requires one line of code.  All indexing
and caching is performed during query execution and so there should be minimal
delay during initialisation.  This will register the Property Functions with ARQ
query engine and configures the indexes used for time-limited caching.
There are three indexes which can be configured independently or switched off.
These indexes retain data that may be required again when a query is being
executed but may not be required between different queries.  Therefore, the
memory usage will grow during query execution and then recede as data is not
re-used.  All the indexes support concurrency and can be set to a maximum size
or allowed to increase capacity as required.

Geometry Literal: Geometry objects following de-serialisation from Geometry Literal.
Geometry Transform: Geometry objects resulting from coordinate transformations between spatial reference systems.
Query Rewrite: results of spatial relations between Feature and Geometry spatial objects.

Testing has found up to 20% improvement in query completion durations using the indexes.
The indexes can be configured by size, retention duration and frequency of clean up.


Basic setup with default values: GeoSPARQLConfig.setupMemoryIndex()


Indexes set to maximum sizes: GeoSPARQLConfig.setupMemoryIndexSize(50000, 50000, 50000)


Indexes set to remove objects not used after 5 seconds: GeoSPARQLConfig.setupMemoryIndexExpiry(5000, 5000, 5000)


No indexes setup (Query rewrite still performed but results not stored) : GeoSPARQLConfig.setupNoIndex()


No indexes and no query rewriting: GeoSPARQLConfig.setupNoIndex(false)


Reset indexes and other stored data: GeoSPARQLConfig.reset()


A variety of configuration methods are provided in
org.apache.jena.geosparql.configuration.GeoSPARQLConfig.  Caching of
frequently used but small quantity data is also applied in several registries,
e.g. coordinate reference systems and mathematical transformations.
Example GeoSPARQL query:
PREFIX geo: <http://www.opengis.net/ont/geosparql#>

SELECT ?obj
WHERE{
    ?subj geo:sfContains ?obj
} ORDER by ?obj
Querying Datasets & Models with SPARQL
The setup of GeoSPARQL Jena only needs to be performed once in an application.
After it is set up querying is performed using Jena’s standard query methods.
To query a Model with GeoSPARQL or standard SPARQL:
GeoSPARQLConfig.setupMemoryIndex();
Model model = .....;
String query = ....;

try (QueryExecution qe = QueryExecution.create(query, model)) {
    ResultSet rs = qe.execSelect();
    ResultSetFormatter.outputAsTSV(rs);
}
If your dataset needs to be separate from your application and accessed over
HTTP then you probably need the GeoSPARQL Assembler to
integrate with Fuseki.  The GeoSPARQL functionality needs to be setup in the
application or Fuseki server where the dataset is located.
It is recommended that hasDefaultGeometry properties are included in the
dataset to access all functionality.  It is necessary that SpatialObject
classes are asserted or inferred (i.e. a reasoner with the GeoSPARQL schema is
applied) in the dataset.  Methods to prepare a dataset can be found in
org.apache.jena.geosparql.configuration.GeoSPARQLOperations.
API The library can be used as an API in Java.  The main class to handle
geometries and their spatial relations is the GeometryWrapper.  This can be
obtained by parsing the string representation of a geometry using the
appropriate datatype (e.g. WKT or GML).  Alternatively, a Literal can be
extracted automatically using the GeometryWrapper.extract() method and
registered datatypes.  The GeometryWrapperFactory can be used to directly
construct a GeometryWrapper.  There is overlap between spatial relation
families so repeated methods are not specified.


Parse a Geometry Literal: GeometryWrapper geometryWrapper = WKTDatatype.INSTANCE.parse("POINT(1 1)");


Extract from a Jena Literal: GeometryWrapper geometryWrapper = GeometryWrapper.extract(geometryLiteral);


Create from a JTS Geometry: GeometryWrapper geometryWrapper = GeometryWrapperFactory.createGeometry(geometry, srsURI, geometryDatatypeURI);


Create from a JTS Point Geometry: GeometryWrapper geometryWrapper = GeometryWrapperFactory.createPoint(coordinate, srsURI, geometryDatatypeURI);


Convert CRS/SRS: GeometryWrapper otherGeometryWrapper = geometryWrapper.convertCRS("http://www.opengis.net/def/crs/EPSG/0/27700")


Spatial Relation: boolean isCrossing = geometryWrapper.crosses(otherGeometryWrapper);


DE-9IM Intersection Pattern: boolean isRelated = geometryWrapper.relate(otherGeometryWrapper, "TFFFTFFFT");


Geometry Property: boolean isEmpty = geometryWrapper.isEmpty();


The GeoSPARQL standard specifies that WKT Geometry Literals without an SRS URI are defaulted to CRS84 http://www.opengis.net/def/crs/OGC/1.3/CRS84.
Key Dependencies
GeoSPARQL
The OGC GeoSPARQL standard supports representing and querying geospatial data on
the Semantic Web.  GeoSPARQL defines a vocabulary for representing geospatial
data in RDF, and it defines an extension to the SPARQL query language for
processing geospatial data.  In addition, GeoSPARQL is designed to accommodate
systems based on qualitative spatial reasoning and systems based on quantitative
spatial computations.
The GeoSPARQL standard is based upon the OGC Simple Features standard
(http://www.opengeospatial.org/standards/sfa) used in relational databases.
Modifications and enhancements have been made for usage with RDF and SPARQL.
The Simple Features standard, and by extension GeoSPARQL, simplify calculations
to Euclidean planer geometry.  Therefore, datasets using a geographic
spatial/coordinate reference system, which are based on latitude and longitude
on an ellipsoid, e.g. WGS84, will have minor error introduced.  This error has
been deemed acceptable due to the simplification in calculation it offers.
Apache SIS/SIS_DATA Environment Variable
Apache Spatial Information System (SIS) is a free software, Java language
library for developing geospatial applications.  SIS provides data structures
for geographic features and associated meta-data along with methods to
manipulate those data structures.  The library is an implementation of GeoAPI
3.0 interfaces and can be used for desktop or server applications.
A subset of the EPSG spatial/coordinate reference systems are included by default.
The full EPSG dataset is not distributed due to the EPSG terms of use being incompatible with the Apache Licence.
Several options are available to include the EPSG dataset by setting the SIS_DATA environment variable (http://sis.apache.org/epsg.html).
An embedded EPSG dataset can be included in a Gradle application by adding the following dependency to build.gradle:
ext.sisVersion = "1.1"
implementation "org.apache.sis.non-free:sis-embedded-data:$sisVersion"

Java Topology Suite
The JTS Topology Suite is a Java library for creating and manipulating vector geometry.
Note
The following are implementation points that may be useful during usage.
GeoSPARQL Schema
An RDF/XML schema has been published for the GeoSPARQL v1.0 standard (v1.0.1 -
http://schemas.opengis.net/geosparql/1.0/geosparql_vocab_all.rdf).  This can
be applied to Jena Models (see the inference
documentation) to provide RDFS and OWL inferencing
on a GeoSPARQL conforming dataset.  However, the published schema does not
conform with the standard.
The property hasDefaultGeometry is missing from the schema and instead the
defaultGeometry property is stated.
This prevents RDFS inferencing being performed correctly and has been reported
to the OGC Standards Tracker.  A corrected version of the schema is available in
the Resources folder.
Spatial Relations
The GeoSPARQL and Simple Features standard both define the DE-9IM intersection
patterns for the three spatial relation families.  However, these patterns are
not always consistent with the patterns stated by the JTS library for certain
relations.
For example, GeoSPARQL/Simple Features use TFFFTFFFT equals relations in
Simple Feature, Egenhofer and RCC8.  However, this does not yield the
usually expected result when comparing a pair of point geometries.  The Simple
Features standard states that the boundary of a point is empty.  Therefore, the
boundary intersection of two points would also be empty so give a negative
comparison result.
JTS, and other libraries, use the alternative intersection pattern of
T*F**FFF*.  This is a combination of the within and contains relations and
yields the expected results for all geometry types.
The spatial relations utilised by JTS have been implemented as the extension
spatial:equals filter and property functions.  A user can also supply their
own DE-9IM intersection patterns by using the geof:relate filter function.
Spatial Relations and Geometry Shapes/Types
The spatial relations for the three spatial families do not apply to all
combinations of the geometry shapes (Point, LineString, Polygon) and their
collections (MultiPoint, MultiLineString, MultiPolygon).  Therefore, some
queries may not produce all the results that may initially be expected.
Some examples are:

In some relations there may only be results when a collection of shapes is being used, e.g. two multi-points can overlap but two points cannot.
A relation may only apply for one combination but not its reciprocal, e.g. a line may cross a polygon but a polygon may not cross a line.
The RCC8 family only applies to Polygon and MultiPolygon types.

Refer to pages 8-10 of 11-052r4 GeoSPARQL standard for more details.
Equals Relations
The three equals relations (sfEquals, ehEquals and rccEquals) use spatial
equality and not lexical equality.  Therefore, some comparisons using these
relations may not be as expected.
The JTS description of sfEquals is:

True if two geometries have at least one point in common and no point of either geometry lies in the exterior of the other geometry.

Therefore, two empty geometries will return false as they are not spatially equal.
Shapes which differ in the number of points but have the same geometry are equal and will return true.
e.g. LINESTRING (0 0, 0 10) and LINESTRING (0 0, 0 5, 0 10) are spatially equal.
Query Rewrite Extension
The Query Rewrite Extension provides for simpler querying syntax.  Feature and
Geometry can be used in spatial relations without needing the relations to be
asserted in the dataset.  This also means the Geometry Literal does not need
to be specified in the query.  In the case of Features this requires the
hasDefaultGeometry property to be used in the dataset.
This means the query:
?subj geo:hasDefaultGeometry ?subjGeom .
?subjGeom geo:hasSerialization ?subjLit .

?obj geo:hasDefaultGeometry ?objGeom .
?objGeom geo:hasSerialization ?objLit .

FILTER(geof:sfContains(?subjLit, ?objLit))
becomes:
?subj geo:sfContains ?obj .
Methods are available to apply the hasDefaultGeometry property to every
Geometry with a single hasGeometry property, see
org.apache.jena.geosparql.configuration.GeoSPARQLOperations.
Depending upon the spatial relation, queries may include the specified Feature
and Geometry in the results.  e.g. FeatureA is bound in a query on a dataset
only containing FeatureA and GeometryA. The results FeatureA and GeometryA are
returned rather than no results.  Therefore, filtering using
FILTER(!sameTerm(?subj, ?obj)) etc. may be needed in some cases.  The query
rewrite functionality can be switched off in the library configuration, see
org.apache.jena.geosparql.configuration.GeoSPARQLConfig.
Each dataset is assigned a Query Rewrite Index to store the results of previous
tests.  There is the potential that relations are tested multiple times in a
query (i.e. Feature-Feature, Feature-Geometry, Geometry-Geometry,
Geometry-Feature).  Therefore, it is useful to retain the results for at least
a short period of time.
Iterating through all combinations of spatial relations for a dataset containing
n Geometry Literals will produce 27n^2 true/false results (asserting the
true result statements in a dataset would be a subset).  Control is given on a
dataset basis to allow choice in when and how storage of rewrite results is
applied, e.g. store all found results on a small dataset but on demand for a
large dataset.
This index can be configured on a global and individual dataset basis for the
maximum size and duration until unused items are removed.  Query rewriting can
be switched on independently of the indexes, i.e. query rewriting can be
performed but an index is configured to not store the result.
As an extension to the standard, supplying a Geometry Literal is
also permitted. For example:
?subj geo:sfContains "POINT(0 0)"^^geo:wktLiteral .
Dataset Conversion
Methods to convert datasets between serialisations and spatial/coordinate
reference systems are available in:
org.apache.jena.geosparql..configuration.GeoSPARQLOperations
The following list shows some of the operations that can be performed.  Once
these operations have been performed they can be serialised to file or stored in
a Jena TDB to remove the need to reprocess.


Load a Jena Model from file: Model dataModel = RDFDataMgr.loadModel("data.ttl");


Convert Feature-GeometryLiteral to the GeoSPARQL Feature-Geometry-GeometryLiteral structure: Model geosparqlModel = GeoSPARQLOperations.convertGeometryStructure(dataModel);


Convert Feature-Lat, Feature-Lon Geo predicates to the GeoSPARQL  Feature-Geometry-GeometryLiteral structure, with option to remove Geo predicates:  Model geosparqlModel = GeoSPARQLOperations.convertGeoPredicates(dataModel, true);


Assert additional hasDefaultGeometry statements for single hasGeometry triples, used in Query Rewriting: GeoSPARQLOperations.applyDefaultGeometry(geosparqlModel);


Convert Geometry Literals to the WGS84 spatial reference system and WKT datatype: Model model = GeoSPARQLOperations.convert(geosparqlModel, "http://www.opengis.net/def/crs/EPSG/0/4326", "http://www.opengis.net/ont/geosparql#wktLiteral");


Apply GeoSPARQL schema with RDFS inferencing and assert additional statements in the Model: GeoSPARQLOperations.applyInferencing(model);


Apply commonly used GeoSPARQL prefixes for URIs to the model: GeoSPARQLOperations.applyPrefixes(model);


Create Spatial Index for a Model within a Dataset for spatial querying: Dataset dataset = SpatialIndex.wrapModel(model);


Other operations are available and can be applied to a Dataset containing
multiple Models and in some cases files and folders.  These operations do
not configure and set up the GeoSPARQL functions or indexes that are required
for querying.
Spatial Index
A Spatial Index can be created to improve searching of a dataset.  The Spatial
Index is expected to be unique to the dataset and should not be shared between
datasets.  Once built the Spatial Index cannot have additional items added to
it.
A Spatial Index is required for the jena-spatial property functions and is
optional for the GeoSPARQL spatial relations.  Only a single SRS can be used for
a Spatial Index, and it is recommended that datasets are converted to a single
SRS, see GeoSPARQLOperations.
Setting up a Spatial Index can be done through
org.apache.jena.geosparql.configuration.GeoSPARQLConfig.  Additional methods
for building, loading and saving Spatial Indexes are provided in
org.apache.jena.geosparql.spatial.SpatialIndex.
Units URI
Spatial/coordinate reference systems use a variety of measuring systems for
defining distances.  These can be specified using a URI identifier, as either
URL or URN, with conversion undertaken automatically as required.  It should be
noted that there is error inherent in spatial reference systems and some
variation in values may occur between different systems.
The following table gives some examples of units that are supported (additional
units can be added to the UnitsRegistry using the javax.measure.Unit API).
These URIs are all in the namespace http://www.opengis.net/def/uom/OGC/1.0/ and
here use the prefix units.

  
      
          URI
          Description
      
  
  
      
          units:kilometre or units:kilometer
          Kilometres
      
      
          units:metre or units:meter
          Metres
      
      
          units:mile or units:statuteMile
          Miles
      
      
          units:degree
          Degrees
      
      
          units:radian
          Radians
      
  

Full listing of default Units can be found in
org.apache.jena.geosparql.implementation.vocabulary.Unit_URI.
Geography Markup Language Support (GML)
The supported GML profile is GML 2.0 Simple Features Profile (10-100r3), which
is a profile of GML 3.2.1 (07-036r1).  The profile restricts the geometry shapes
permitted in GML 3.2.1 to a subset, see 10-100r3 page 22.  The profile supports
Points, LineString and Polygon shapes used in WKT. There are also additional
shape serialisations available in the profile that do not exist in WKT or JTS to
provide simplified representations which would otherwise use LineStrings or
Polygons.  Curves can be described by LineStringSegment, Arc, Circle and
CircleByCenterPoint. Surfaces can be formed similarly to Polygons or using
Curves.  These additional shapes can be read as part of a dataset or query but
will not be produced if the SRS of the shape is transformed, instead a
LineString or Polygon representation will be produced.
Details of the GML structure for these shapes can be found in the
geometryPrimitives.xsd,
geometryBasic0d1d.xsd,
geometryBasic2d.xsd
and
geometryAggregates.xsd
schemas.
The labelling of collections is as follows:

  
      
          Collection
          Geometry
      
  
  
      
          MultiPoint
          Point
      
      
          MultiCurve
          LineString, Curve
      
      
          MultiSurface
          Polygon, Surface
      
      
          MultiGeometry
          Point, LineString, Curve, Polygon, Surface
      
  

Apache Jena Spatial Functions/WGS84 Geo Predicates
The jena-spatial module contains several SPARQL functions for querying
datasets using the WGS84 Geo predicates for latitude
(http://www.w3.org/2003/01/geo/wgs84_pos#lat) and longitude
(http://www.w3.org/2003/01/geo/wgs84_pos#long).  These jena-spatial
functions are supported for both Geo predicates and Geometry Literals, i.e. a
GeoSPARQL dataset.  Additional SPARQL filter functions have been provided to
convert Geo predicate properties into WKT strings and calculate Great Circle and
Euclidean distances.  The jena-spatialfunctions require setting up a Spatial
Index for the target Dataset,
e.g. GeoSPARQLConfig.setupSpatialIndex(dataset);, see Spatial Index section.
Supported Features
The Geo predicate form of spatial representation is restricted to only ‘Point’
shapes in the WGS84 spatial/coordinate reference system.  The Geo predicates are
properties of the Feature and do not use the properties and structure of the
GeoSPARQL standard, including Geometry Literals.  Methods are available to
convert datasets from Geo predicates to GeoSPARQL structure, see:
org.apache.jena.geosparql.configuration.GeoSPARQLOperations
The spatial relations and query re-writing of GeoSPARQL outlined previously has been implemented for Geo predicates.
However, only certain spatial relations are valid for Point to Point relationships.
Refer to pages 8-10 of 11-052r4 GeoSPARQL standard for more details.
Geo predicates can be converted to Geometry Literals in query and then used with the GeoSPARQL filter functions.
?subj wgs:lat ?lat .
?subj wgs:long ?lon .
BIND(spatialF:convertLatLon(?lat, ?lon) as ?point) .
#Coordinate order is Lon/Lat without stated SRS URI.
BIND("POLYGON((...))"^^<http://www.opengis.net/ont/geosparql#wktLiteral> AS ?box) .
FILTER(geof:sfContains(?box, ?point))
Alternatively, utilising more shapes, relations and spatial reference systems
can be achieved by converting the dataset to the GeoSPARQL structure.
?subj geo:hasGeometry ?geom .
?geom geo:hasSerialization ?geomLit .
#Coordinate order is Lon/Lat without stated SRS URI.
BIND("POLYGON((...))"^^<http://www.opengis.net/ont/geosparql#wktLiteral> AS ?box) .
FILTER(geof:sfContains(?box, ?geomLit))
Datasets can contain both Geo predicates and Geometry Literals without
interference.  However, a dataset containing both types will only examine those
Features which have Geometry Literals for spatial relations, i.e. the check
for Geo predicates is a fallback when Geometry Literals aren’t found.
Therefore, it is not recommended to insert new Geo predicate properties
after a dataset has been converted to GeoSPARQL structure (unless corresponding
Geometry and Geometry Literals are included).
Filter Functions
These filter functions are available in the
http://jena.apache.org/function/spatial# namespace and here use the prefix
spatialF.

  
      
          Function Name
          Description
      
  
  
      
          ?wktString spatialF:convertLatLon(?lat, ?lon)
          Converts Lat and Lon double values into WKT string of a Point with WGS84 SRS.
      
      
          ?wktString spatialF:convertLatLonBox(?latMin, ?lonMin, ?latMax, ?lonMax)
          Converts Lat and Lon double values into WKT string of a Polygon forming a box with WGS84 SRS.
      
      
          ?boolean spatialF:equals(?geomLit1, ?geomLit2)
          True, if geomLit1 is spatially equal to geomLit2.
      
      
          ?boolean spatialF:nearby(?geomLit1, ?geomLit2, ?distance, ?unitsURI)
          True, if geomLit1 is within distance of geomLit2 using the distance units.
      
      
          ?boolean spatialF:withinCircle(?geomLit1, ?geomLit2, ?distance, ?unitsURI)
          True, if geomLit1 is within distance of geomLit2 using the distance units.
      
      
          ?radians spatialF:angle(?x1, ?y1, ?x2, ?y2)
          Angle clockwise from y-axis from Point(x1,y1) to Point (x2,y2) in 0 to 2π radians.
      
      
          ?degrees spatialF:angleDeg(?x, ?y1, ?x2, ?y2)
          Angle clockwise from y-axis from Point(x1,y1) to Point (x2,y2) in 0 to 360 degrees.
      
      
          ?distance spatialF:distance(?geomLit1, ?geomLit2, ?unitsURI)
          Distance between two Geometry Literals in distance units. Chooses distance measure based on SRS type. Great Circle distance for Geographic SRS and Euclidean otherwise.
      
      
          ?radians spatialF:azimuth(?lat1, ?lon1, ?lat2, ?lon2)
          Forward azimuth clockwise from North between two Lat/Lon Points in 0 to 2π radians.
      
      
          ?degrees spatialF:azimuthDeg(?lat1, ?lon1, ?lat2, ?lon2)
          Forward azimuth clockwise from North between two Lat/Lon Points in 0 to 360 degrees.
      
      
          ?distance spatialF:greatCircle(?lat1, ?lon1, ?lat2, ?lon2, ?unitsURI)
          Great Circle distance (Vincenty formula) between two Lat/Lon Points in distance units.
      
      
          ?distance spatialF:greatCircleGeom(?geomLit1, ?geomLit2, ?unitsURI)
          Great Circle distance (Vincenty formula) between two Geometry Literals in distance units. Use http://www.opengis.net/def/function/geosparql/distance from GeoSPARQL standard for Euclidean distance.
      
      
          ?geomLit2 spatialF:transform(?geomLit1, ?datatypeURI, ?srsURI)
          Transform Geometry Literal by Datatype and SRS.
      
      
          ?geomLit2 spatialF:transformDatatype(?geomLit1, ?datatypeURI)
          Transform Geometry Literal by Datatype.
      
      
          ?geomLit2 spatialF:transformSRS(?geomLit1, ?srsURI)
          Transform Geometry Literal by SRS.
      
  

Property Functions
These property functions are available in the http://jena.apache.org/spatial#
namespace and here use the prefix spatial.  This is the same namespace as the
jena-spatial functions utilise and these form direct replacements.  The
subject Feature may be bound, to test the pattern is true, or unbound, to find
all cases the pattern is true.  These property functions require a Spatial Index to be setup for the dataset.
The optional ?limit parameter restricts the number of results returned. The
default value is -1 which returns all results. No guarantee is given for
ordering of results.  The optional ?unitsURI parameter specifies the units of
a distance. The default value is kilometres through the string or resource
http://www.opengis.net/def/uom/OGC/1.0/kilometre.
The spatial:equals property function behaves the same way as the main
GeoSPARQL property functions. Either, both or neither of the subject and object
can be bound. A Spatial Index is not required for the dataset with the
spatial:equals property function.

  
      
          Function Name
          Description
      
  
  
      
          ?spatialObject1 spatial:equals ?spatialObject2
          Find spatialObjects (i.e. features or geometries) that are spatially equal.
      
      
          ?feature spatial:intersectBox(?latMin ?lonMin ?latMax ?lonMax [ ?limit])
          Find features that intersect the provided box, up to the limit.
      
      
          ?feature spatial:intersectBoxGeom(?geomLit1 ?geomLit2 [ ?limit])
          Find features that intersect the provided box, up to the limit.
      
      
          ?feature spatial:withinBox(?latMin ?lonMin ?latMax ?lonMax [ ?limit])
          Find features that intersect the provided box, up to the limit.
      
      
          ?feature spatial:withinBoxGeom(?geomLit1 ?geomLit2 [ ?limit])
          Find features that are within the provided box, up to the limit.
      
      
          ?feature spatial:nearby(?lat ?lon ?radius [ ?unitsURI [ ?limit]])
          Find features that are within radius of the distance units, up to the limit.
      
      
          ?feature spatial:nearbyGeom(?geomLit ?radius [ ?unitsURI [ ?limit]])
          Find features that are within radius of the distance units, up to the limit.
      
      
          ?feature spatial:withinCircle(?lat ?lon ?radius [ ?unitsURI [ ?limit]])
          Find features that are within radius of the distance units, up to the limit.
      
      
          ?feature spatial:withinCircleGeom(?geomLit ?radius [ ?unitsURI [ ?limit]])
          Find features that are within radius of the distance units, up to the limit.
      
  

The Cardinal Functions find all Features that are present in the specified
direction.  In Geographic spatial reference systems (SRS), e.g. WGS84 and CRS84,
the East/West directions wrap around.  Therefore, a search is made from the
shape’s edge for up to half the range of the SRS (i.e. 180 degrees in WGS84) and
will continue across the East/West boundary if necessary.  In other SRS,
e.g. Projected onto a flat plane, the East/West check is made from the shape’s
edge to the farthest limit of the SRS range, i.e. there is no wrap around.

  
      
          Cardinal Function Name
          Description
      
  
  
      
          ?feature spatial:north(?lat ?lon [ ?limit])
          Find features that are North of the Lat/Lon point (point to +90 degrees), up to the limit.
      
      
          ?feature spatial:northGeom(?geomLit [ ?limit])
          Find features that are North of the Geometry Literal, up to the limit.
      
      
          ?feature spatial:south(?lat ?lon [ ?limit])
          Find features that are South of the Lat/Lon point (point to -90 degrees), up to the limit.
      
      
          ?feature spatial:southGeom(?geomLit [ ?limit])
          Find features that are South of the Geometry Literal, up to the limit.
      
      
          ?feature spatial:east(?lat ?lon [ ?limit])
          Find features that are East of the Lat/Lon point (point plus 180 degrees longitude, wrapping round), up to the limit.
      
      
          ?feature spatial:eastGeom(?geomLit [ ?limit])
          Find features that are East of the Geometry Literal, up to the limit.
      
      
          ?feature spatial:west(?lat ?lon [ ?limit])
          Find features that are West of the Lat/Lon point (point minus 180 degrees longitude, wrapping round), up to the limit.
      
      
          ?feature spatial:westGeom(?geomLit [ ?limit])
          Find features that are West of the Geometry Literal, up to the limit.
      
  

Geometry Property Filter Functions
The GeoSPARQL standard provides a set of properties related to geometries, see
Section 8.4.  These are applied on the Geometry resource and are automatically
determined if not asserted in the data.  However, it may be necessary to
retrieve the properties of a Geometry Literal directly without an associated
Geometry resource.  Filter functions to do this have been included as part of
the http://www.opengis.net/def/function/geosparql/ namespace as a minor
variation to the GeoSPARQL standard.  The relevant functions using the geof
prefix are:

  
      
          Geometry Property Filter Function Name
          Description
      
  
  
      
          ?integer geof:dimension(?geometryLiteral)
          Topological dimension, e.g. 0 for Point, 1 for LineString and 2 for Polygon.
      
      
          ?integer geof:coordinateDimension(?geometryLiteral)
          Coordinate dimension, e.g. 2 for XY coordinates and 4 for XYZM coordinates.
      
      
          ?integer geof:spatialDimension(?geometryLiteral)
          Spatial dimension, e.g. 2 for XY coordinates and 3 for XYZM coordinates.
      
      
          ?boolean geof:isEmpty(?geometryLiteral)
          True, if geometry is empty.
      
      
          ?boolean geof:isSimple(?geometryLiteral)
          True, if geometry is simple.
      
      
          ?boolean geof:isValid(?geometryLiteral)
          True, if geometry is topologically valid.
      
  

A dataset that follows the GeoSPARQL Feature-Geometry-GeometryLiteral can have
simpler SPARQL queries without needing to use these functions by taking
advantage of the Query Rewriting functionality.  The geof:isValid filter
function and geo:isValid property for a Geometry resource are not part of the
GeoSPARQL standard but have been included as a minor variation.
Future Work

Implementing GeoJSON as a GeometryLiteral serialisation (https://tools.ietf.org/html/rfc7946).
Producing GeoJSON is already possible with geof:asGeoJSON(?geometryLiteral).

Contributors
The following individuals have made contributions to this project:

Greg Albiston
Haozhe Chen
Taha Osman

Why Use This Implementation?
There are several implementations of the GeoSPARQL standard.  The conformance
and completeness of these implementations is difficult to ascertain and varies
between features.
However, the following may be of interest when considering whether to use this
implementation based on reviewing several alternatives.

  
      
          This Implementation
          Other Implementations
      
  
  
      
          Implements all six components of the GeoSPARQL standard.
          Generally partially implement the Geometry Topology and Geometry Extensions. Do not implement the Query Rewrite Extension.
      
      
          Pure Java and does not require a supporting relational database. Configuration requires a single line of code (although Apache SIS may need some setting up, see above).
          Require setting up a database, configuring a geospatial extension and setting environment variables.
      
      
          Uses Jena, which conforms to the W3C standards for RDF and SPARQL. New versions of the standards will quickly feed through.
          Not fully RDF and SPARQL compliant, e.g. RDFS/OWL inferencing or SPARQL syntax. Adding your own schema may not produce inferences.
      
      
          Automatically determines geometry properties and handles mixed cases of units or coordinate reference systems. The GeoSPARQL standard suggests this approach but does not require it.
          Tend to produce errors or no results in these situations.
      
      
          Performs indexing and caching on-demand which reduces set-up time and only performs calculations that are required.
          Perform indexing in the data loading phase and initialisation phase, which can lead to lengthy delays (even on relatively small datasets).
      
      
          Uses JTS which does not truncate coordinate precision and applies spatial equality.
          May truncate coordinate precision and apply lexical equality, which is quicker but does not comply with the GeoSPARQL standard.\n\nOn this page
    
  
    Getting Started
    Features
    Additional Features
      
        SPARQL Query Configuration
        Querying Datasets & Models with SPARQL
        API The library can be used as an API in Java.  The main class to handle
      
    
    Key Dependencies
      
        GeoSPARQL
        Apache SIS/SIS_DATA Environment Variable
        Java Topology Suite
      
    
    Note
      
        GeoSPARQL Schema
        Spatial Relations
        Spatial Relations and Geometry Shapes/Types
        Equals Relations
        Query Rewrite Extension
        Dataset Conversion
        Spatial Index
        Units URI
      
    
    Geography Markup Language Support (GML)
    Apache Jena Spatial Functions/WGS84 Geo Predicates
      
        Supported Features
        Filter Functions
        Property Functions
      
    
    Geometry Property Filter Functions
    Future Work
    Contributors
    Why Use This Implementation?
  

  
  
    An implementation of GeoSPARQL 1.0 standard for SPARQL query or API.
Integration with Fuseki is provided either by using the
GeoSPARQL assembler or using the self-contained original
jena-fuseki-geosparql.  In either case, this page
describes the GeoSPARQL supported features.
Getting Started
GeoSPARQL Jena can be accessed as a library using Maven etc. from Maven Central.
<dependency>
  <groupId>org.apache.jena</groupId>
  <artifactId>jena-geosparql</artifactId>
  <version>...</version>
</dependency>
Features
This implementation follows the 11-052r4 OGC GeoSPARQL standard
(https://www.ogc.org/standards/geosparql).  The implementation is pure Java
and does not require any set-up or configuration of any third party relational
databases and geospatial extensions.
It implements the six Conformance Classes described in the GeoSPARQL document:

Core
Topology Vocabulary
Geometry Extension
Geometry Topology
RDFS Entailment Extension
Query Rewrite Extension

The WKT (as described in 11-052r4) and GML 2.0 Simple Features Profile
(10-100r3) serialisations are supported.  Additional serialisations can be
implemented by extending the
org.apache.jena.geosparql.implementation.datatype.GeometryDatatype and
registering with Jena’s org.apache.jena.datatypes.TypeMapper.
All three spatial relation families are supported: Simple Feature, Egenhofer and RCC8.
Indexing and caching of spatial objects and relations is performed on-demand
during query execution.  Therefore, set-up delays should be minimal. Spatial
indexing is available based on the STRtree from the JTS library. The STRtree
is readonly once built and contributions of a QuadTree implementation are
welcome.
Benchmarking of the implementation against Strabon and Parliament has found it
to be comparable or quicker.  The benchmarking used was the Geographical query
and dataset (http://geographica.di.uoa.gr/).
Additional Features
The following additional features are also provided:

Geometry properties are automatically calculated and do not need to be asserted in the dataset.
Conversion between EPSG spatial/coordinate reference systems is applied
automatically. Therefore, mixed datasets or querying can be applied. This is
reliance upon local installation of Apache SIS EPSG dataset, see Key
Dependencies.
Units of measure are automatically converted to the appropriate units for the
coordinate reference system.
Geometry, transformation and spatial relation results are stored in persistent
and configurable time-limited caches to improve response times and reduce
recalculations.
Dataset conversion between serialisations and spatial/coordinate reference
systems. Tabular data can also be loaded, see RDF Tables project
(https://github.com/galbiston/rdf-tables).
Functions to test Geometry properties directly on Geometry Literals have been included for convenience.

SPARQL Query Configuration
Using the library for SPARQL querying requires one line of code.  All indexing
and caching is performed during query execution and so there should be minimal
delay during initialisation.  This will register the Property Functions with ARQ
query engine and configures the indexes used for time-limited caching.
There are three indexes which can be configured independently or switched off.
These indexes retain data that may be required again when a query is being
executed but may not be required between different queries.  Therefore, the
memory usage will grow during query execution and then recede as data is not
re-used.  All the indexes support concurrency and can be set to a maximum size
or allowed to increase capacity as required.

Geometry Literal: Geometry objects following de-serialisation from Geometry Literal.
Geometry Transform: Geometry objects resulting from coordinate transformations between spatial reference systems.
Query Rewrite: results of spatial relations between Feature and Geometry spatial objects.

Testing has found up to 20% improvement in query completion durations using the indexes.
The indexes can be configured by size, retention duration and frequency of clean up.


Basic setup with default values: GeoSPARQLConfig.setupMemoryIndex()


Indexes set to maximum sizes: GeoSPARQLConfig.setupMemoryIndexSize(50000, 50000, 50000)


Indexes set to remove objects not used after 5 seconds: GeoSPARQLConfig.setupMemoryIndexExpiry(5000, 5000, 5000)


No indexes setup (Query rewrite still performed but results not stored) : GeoSPARQLConfig.setupNoIndex()


No indexes and no query rewriting: GeoSPARQLConfig.setupNoIndex(false)


Reset indexes and other stored data: GeoSPARQLConfig.reset()


A variety of configuration methods are provided in
org.apache.jena.geosparql.configuration.GeoSPARQLConfig.  Caching of
frequently used but small quantity data is also applied in several registries,
e.g. coordinate reference systems and mathematical transformations.
Example GeoSPARQL query:
PREFIX geo: <http://www.opengis.net/ont/geosparql#>

SELECT ?obj
WHERE{
    ?subj geo:sfContains ?obj
} ORDER by ?obj
Querying Datasets & Models with SPARQL
The setup of GeoSPARQL Jena only needs to be performed once in an application.
After it is set up querying is performed using Jena’s standard query methods.
To query a Model with GeoSPARQL or standard SPARQL:
GeoSPARQLConfig.setupMemoryIndex();
Model model = .....;
String query = ....;

try (QueryExecution qe = QueryExecution.create(query, model)) {
    ResultSet rs = qe.execSelect();
    ResultSetFormatter.outputAsTSV(rs);
}
If your dataset needs to be separate from your application and accessed over
HTTP then you probably need the GeoSPARQL Assembler to
integrate with Fuseki.  The GeoSPARQL functionality needs to be setup in the
application or Fuseki server where the dataset is located.
It is recommended that hasDefaultGeometry properties are included in the
dataset to access all functionality.  It is necessary that SpatialObject
classes are asserted or inferred (i.e. a reasoner with the GeoSPARQL schema is
applied) in the dataset.  Methods to prepare a dataset can be found in
org.apache.jena.geosparql.configuration.GeoSPARQLOperations.
API The library can be used as an API in Java.  The main class to handle
geometries and their spatial relations is the GeometryWrapper.  This can be
obtained by parsing the string representation of a geometry using the
appropriate datatype (e.g. WKT or GML).  Alternatively, a Literal can be
extracted automatically using the GeometryWrapper.extract() method and
registered datatypes.  The GeometryWrapperFactory can be used to directly
construct a GeometryWrapper.  There is overlap between spatial relation
families so repeated methods are not specified.


Parse a Geometry Literal: GeometryWrapper geometryWrapper = WKTDatatype.INSTANCE.parse("POINT(1 1)");


Extract from a Jena Literal: GeometryWrapper geometryWrapper = GeometryWrapper.extract(geometryLiteral);


Create from a JTS Geometry: GeometryWrapper geometryWrapper = GeometryWrapperFactory.createGeometry(geometry, srsURI, geometryDatatypeURI);


Create from a JTS Point Geometry: GeometryWrapper geometryWrapper = GeometryWrapperFactory.createPoint(coordinate, srsURI, geometryDatatypeURI);


Convert CRS/SRS: GeometryWrapper otherGeometryWrapper = geometryWrapper.convertCRS("http://www.opengis.net/def/crs/EPSG/0/27700")


Spatial Relation: boolean isCrossing = geometryWrapper.crosses(otherGeometryWrapper);


DE-9IM Intersection Pattern: boolean isRelated = geometryWrapper.relate(otherGeometryWrapper, "TFFFTFFFT");


Geometry Property: boolean isEmpty = geometryWrapper.isEmpty();


The GeoSPARQL standard specifies that WKT Geometry Literals without an SRS URI are defaulted to CRS84 http://www.opengis.net/def/crs/OGC/1.3/CRS84.
Key Dependencies
GeoSPARQL
The OGC GeoSPARQL standard supports representing and querying geospatial data on
the Semantic Web.  GeoSPARQL defines a vocabulary for representing geospatial
data in RDF, and it defines an extension to the SPARQL query language for
processing geospatial data.  In addition, GeoSPARQL is designed to accommodate
systems based on qualitative spatial reasoning and systems based on quantitative
spatial computations.
The GeoSPARQL standard is based upon the OGC Simple Features standard
(http://www.opengeospatial.org/standards/sfa) used in relational databases.
Modifications and enhancements have been made for usage with RDF and SPARQL.
The Simple Features standard, and by extension GeoSPARQL, simplify calculations
to Euclidean planer geometry.  Therefore, datasets using a geographic
spatial/coordinate reference system, which are based on latitude and longitude
on an ellipsoid, e.g. WGS84, will have minor error introduced.  This error has
been deemed acceptable due to the simplification in calculation it offers.
Apache SIS/SIS_DATA Environment Variable
Apache Spatial Information System (SIS) is a free software, Java language
library for developing geospatial applications.  SIS provides data structures
for geographic features and associated meta-data along with methods to
manipulate those data structures.  The library is an implementation of GeoAPI
3.0 interfaces and can be used for desktop or server applications.
A subset of the EPSG spatial/coordinate reference systems are included by default.
The full EPSG dataset is not distributed due to the EPSG terms of use being incompatible with the Apache Licence.
Several options are available to include the EPSG dataset by setting the SIS_DATA environment variable (http://sis.apache.org/epsg.html).
An embedded EPSG dataset can be included in a Gradle application by adding the following dependency to build.gradle:
ext.sisVersion = "1.1"
implementation "org.apache.sis.non-free:sis-embedded-data:$sisVersion"

Java Topology Suite
The JTS Topology Suite is a Java library for creating and manipulating vector geometry.
Note
The following are implementation points that may be useful during usage.
GeoSPARQL Schema
An RDF/XML schema has been published for the GeoSPARQL v1.0 standard (v1.0.1 -
http://schemas.opengis.net/geosparql/1.0/geosparql_vocab_all.rdf).  This can
be applied to Jena Models (see the inference
documentation) to provide RDFS and OWL inferencing
on a GeoSPARQL conforming dataset.  However, the published schema does not
conform with the standard.
The property hasDefaultGeometry is missing from the schema and instead the
defaultGeometry property is stated.
This prevents RDFS inferencing being performed correctly and has been reported
to the OGC Standards Tracker.  A corrected version of the schema is available in
the Resources folder.
Spatial Relations
The GeoSPARQL and Simple Features standard both define the DE-9IM intersection
patterns for the three spatial relation families.  However, these patterns are
not always consistent with the patterns stated by the JTS library for certain
relations.
For example, GeoSPARQL/Simple Features use TFFFTFFFT equals relations in
Simple Feature, Egenhofer and RCC8.  However, this does not yield the
usually expected result when comparing a pair of point geometries.  The Simple
Features standard states that the boundary of a point is empty.  Therefore, the
boundary intersection of two points would also be empty so give a negative
comparison result.
JTS, and other libraries, use the alternative intersection pattern of
T*F**FFF*.  This is a combination of the within and contains relations and
yields the expected results for all geometry types.
The spatial relations utilised by JTS have been implemented as the extension
spatial:equals filter and property functions.  A user can also supply their
own DE-9IM intersection patterns by using the geof:relate filter function.
Spatial Relations and Geometry Shapes/Types
The spatial relations for the three spatial families do not apply to all
combinations of the geometry shapes (Point, LineString, Polygon) and their
collections (MultiPoint, MultiLineString, MultiPolygon).  Therefore, some
queries may not produce all the results that may initially be expected.
Some examples are:

In some relations there may only be results when a collection of shapes is being used, e.g. two multi-points can overlap but two points cannot.
A relation may only apply for one combination but not its reciprocal, e.g. a line may cross a polygon but a polygon may not cross a line.
The RCC8 family only applies to Polygon and MultiPolygon types.

Refer to pages 8-10 of 11-052r4 GeoSPARQL standard for more details.
Equals Relations
The three equals relations (sfEquals, ehEquals and rccEquals) use spatial
equality and not lexical equality.  Therefore, some comparisons using these
relations may not be as expected.
The JTS description of sfEquals is:

True if two geometries have at least one point in common and no point of either geometry lies in the exterior of the other geometry.

Therefore, two empty geometries will return false as they are not spatially equal.
Shapes which differ in the number of points but have the same geometry are equal and will return true.
e.g. LINESTRING (0 0, 0 10) and LINESTRING (0 0, 0 5, 0 10) are spatially equal.
Query Rewrite Extension
The Query Rewrite Extension provides for simpler querying syntax.  Feature and
Geometry can be used in spatial relations without needing the relations to be
asserted in the dataset.  This also means the Geometry Literal does not need
to be specified in the query.  In the case of Features this requires the
hasDefaultGeometry property to be used in the dataset.
This means the query:
?subj geo:hasDefaultGeometry ?subjGeom .
?subjGeom geo:hasSerialization ?subjLit .

?obj geo:hasDefaultGeometry ?objGeom .
?objGeom geo:hasSerialization ?objLit .

FILTER(geof:sfContains(?subjLit, ?objLit))
becomes:
?subj geo:sfContains ?obj .
Methods are available to apply the hasDefaultGeometry property to every
Geometry with a single hasGeometry property, see
org.apache.jena.geosparql.configuration.GeoSPARQLOperations.
Depending upon the spatial relation, queries may include the specified Feature
and Geometry in the results.  e.g. FeatureA is bound in a query on a dataset
only containing FeatureA and GeometryA. The results FeatureA and GeometryA are
returned rather than no results.  Therefore, filtering using
FILTER(!sameTerm(?subj, ?obj)) etc. may be needed in some cases.  The query
rewrite functionality can be switched off in the library configuration, see
org.apache.jena.geosparql.configuration.GeoSPARQLConfig.
Each dataset is assigned a Query Rewrite Index to store the results of previous
tests.  There is the potential that relations are tested multiple times in a
query (i.e. Feature-Feature, Feature-Geometry, Geometry-Geometry,
Geometry-Feature).  Therefore, it is useful to retain the results for at least
a short period of time.
Iterating through all combinations of spatial relations for a dataset containing
n Geometry Literals will produce 27n^2 true/false results (asserting the
true result statements in a dataset would be a subset).  Control is given on a
dataset basis to allow choice in when and how storage of rewrite results is
applied, e.g. store all found results on a small dataset but on demand for a
large dataset.
This index can be configured on a global and individual dataset basis for the
maximum size and duration until unused items are removed.  Query rewriting can
be switched on independently of the indexes, i.e. query rewriting can be
performed but an index is configured to not store the result.
As an extension to the standard, supplying a Geometry Literal is
also permitted. For example:
?subj geo:sfContains "POINT(0 0)"^^geo:wktLiteral .
Dataset Conversion
Methods to convert datasets between serialisations and spatial/coordinate
reference systems are available in:
org.apache.jena.geosparql..configuration.GeoSPARQLOperations
The following list shows some of the operations that can be performed.  Once
these operations have been performed they can be serialised to file or stored in
a Jena TDB to remove the need to reprocess.


Load a Jena Model from file: Model dataModel = RDFDataMgr.loadModel("data.ttl");


Convert Feature-GeometryLiteral to the GeoSPARQL Feature-Geometry-GeometryLiteral structure: Model geosparqlModel = GeoSPARQLOperations.convertGeometryStructure(dataModel);


Convert Feature-Lat, Feature-Lon Geo predicates to the GeoSPARQL  Feature-Geometry-GeometryLiteral structure, with option to remove Geo predicates:  Model geosparqlModel = GeoSPARQLOperations.convertGeoPredicates(dataModel, true);


Assert additional hasDefaultGeometry statements for single hasGeometry triples, used in Query Rewriting: GeoSPARQLOperations.applyDefaultGeometry(geosparqlModel);


Convert Geometry Literals to the WGS84 spatial reference system and WKT datatype: Model model = GeoSPARQLOperations.convert(geosparqlModel, "http://www.opengis.net/def/crs/EPSG/0/4326", "http://www.opengis.net/ont/geosparql#wktLiteral");


Apply GeoSPARQL schema with RDFS inferencing and assert additional statements in the Model: GeoSPARQLOperations.applyInferencing(model);


Apply commonly used GeoSPARQL prefixes for URIs to the model: GeoSPARQLOperations.applyPrefixes(model);


Create Spatial Index for a Model within a Dataset for spatial querying: Dataset dataset = SpatialIndex.wrapModel(model);


Other operations are available and can be applied to a Dataset containing
multiple Models and in some cases files and folders.  These operations do
not configure and set up the GeoSPARQL functions or indexes that are required
for querying.
Spatial Index
A Spatial Index can be created to improve searching of a dataset.  The Spatial
Index is expected to be unique to the dataset and should not be shared between
datasets.  Once built the Spatial Index cannot have additional items added to
it.
A Spatial Index is required for the jena-spatial property functions and is
optional for the GeoSPARQL spatial relations.  Only a single SRS can be used for
a Spatial Index, and it is recommended that datasets are converted to a single
SRS, see GeoSPARQLOperations.
Setting up a Spatial Index can be done through
org.apache.jena.geosparql.configuration.GeoSPARQLConfig.  Additional methods
for building, loading and saving Spatial Indexes are provided in
org.apache.jena.geosparql.spatial.SpatialIndex.
Units URI
Spatial/coordinate reference systems use a variety of measuring systems for
defining distances.  These can be specified using a URI identifier, as either
URL or URN, with conversion undertaken automatically as required.  It should be
noted that there is error inherent in spatial reference systems and some
variation in values may occur between different systems.
The following table gives some examples of units that are supported (additional
units can be added to the UnitsRegistry using the javax.measure.Unit API).
These URIs are all in the namespace http://www.opengis.net/def/uom/OGC/1.0/ and
here use the prefix units.

  
      
          URI
          Description
      
  
  
      
          units:kilometre or units:kilometer
          Kilometres
      
      
          units:metre or units:meter
          Metres
      
      
          units:mile or units:statuteMile
          Miles
      
      
          units:degree
          Degrees
      
      
          units:radian
          Radians
      
  

Full listing of default Units can be found in
org.apache.jena.geosparql.implementation.vocabulary.Unit_URI.
Geography Markup Language Support (GML)
The supported GML profile is GML 2.0 Simple Features Profile (10-100r3), which
is a profile of GML 3.2.1 (07-036r1).  The profile restricts the geometry shapes
permitted in GML 3.2.1 to a subset, see 10-100r3 page 22.  The profile supports
Points, LineString and Polygon shapes used in WKT. There are also additional
shape serialisations available in the profile that do not exist in WKT or JTS to
provide simplified representations which would otherwise use LineStrings or
Polygons.  Curves can be described by LineStringSegment, Arc, Circle and
CircleByCenterPoint. Surfaces can be formed similarly to Polygons or using
Curves.  These additional shapes can be read as part of a dataset or query but
will not be produced if the SRS of the shape is transformed, instead a
LineString or Polygon representation will be produced.
Details of the GML structure for these shapes can be found in the
geometryPrimitives.xsd,
geometryBasic0d1d.xsd,
geometryBasic2d.xsd
and
geometryAggregates.xsd
schemas.
The labelling of collections is as follows:

  
      
          Collection
          Geometry
      
  
  
      
          MultiPoint
          Point
      
      
          MultiCurve
          LineString, Curve
      
      
          MultiSurface
          Polygon, Surface
      
      
          MultiGeometry
          Point, LineString, Curve, Polygon, Surface
      
  

Apache Jena Spatial Functions/WGS84 Geo Predicates
The jena-spatial module contains several SPARQL functions for querying
datasets using the WGS84 Geo predicates for latitude
(http://www.w3.org/2003/01/geo/wgs84_pos#lat) and longitude
(http://www.w3.org/2003/01/geo/wgs84_pos#long).  These jena-spatial
functions are supported for both Geo predicates and Geometry Literals, i.e. a
GeoSPARQL dataset.  Additional SPARQL filter functions have been provided to
convert Geo predicate properties into WKT strings and calculate Great Circle and
Euclidean distances.  The jena-spatialfunctions require setting up a Spatial
Index for the target Dataset,
e.g. GeoSPARQLConfig.setupSpatialIndex(dataset);, see Spatial Index section.
Supported Features
The Geo predicate form of spatial representation is restricted to only ‘Point’
shapes in the WGS84 spatial/coordinate reference system.  The Geo predicates are
properties of the Feature and do not use the properties and structure of the
GeoSPARQL standard, including Geometry Literals.  Methods are available to
convert datasets from Geo predicates to GeoSPARQL structure, see:
org.apache.jena.geosparql.configuration.GeoSPARQLOperations
The spatial relations and query re-writing of GeoSPARQL outlined previously has been implemented for Geo predicates.
However, only certain spatial relations are valid for Point to Point relationships.
Refer to pages 8-10 of 11-052r4 GeoSPARQL standard for more details.
Geo predicates can be converted to Geometry Literals in query and then used with the GeoSPARQL filter functions.
?subj wgs:lat ?lat .
?subj wgs:long ?lon .
BIND(spatialF:convertLatLon(?lat, ?lon) as ?point) .
#Coordinate order is Lon/Lat without stated SRS URI.
BIND("POLYGON((...))"^^<http://www.opengis.net/ont/geosparql#wktLiteral> AS ?box) .
FILTER(geof:sfContains(?box, ?point))
Alternatively, utilising more shapes, relations and spatial reference systems
can be achieved by converting the dataset to the GeoSPARQL structure.
?subj geo:hasGeometry ?geom .
?geom geo:hasSerialization ?geomLit .
#Coordinate order is Lon/Lat without stated SRS URI.
BIND("POLYGON((...))"^^<http://www.opengis.net/ont/geosparql#wktLiteral> AS ?box) .
FILTER(geof:sfContains(?box, ?geomLit))
Datasets can contain both Geo predicates and Geometry Literals without
interference.  However, a dataset containing both types will only examine those
Features which have Geometry Literals for spatial relations, i.e. the check
for Geo predicates is a fallback when Geometry Literals aren’t found.
Therefore, it is not recommended to insert new Geo predicate properties
after a dataset has been converted to GeoSPARQL structure (unless corresponding
Geometry and Geometry Literals are included).
Filter Functions
These filter functions are available in the
http://jena.apache.org/function/spatial# namespace and here use the prefix
spatialF.

  
      
          Function Name
          Description
      
  
  
      
          ?wktString spatialF:convertLatLon(?lat, ?lon)
          Converts Lat and Lon double values into WKT string of a Point with WGS84 SRS.
      
      
          ?wktString spatialF:convertLatLonBox(?latMin, ?lonMin, ?latMax, ?lonMax)
          Converts Lat and Lon double values into WKT string of a Polygon forming a box with WGS84 SRS.
      
      
          ?boolean spatialF:equals(?geomLit1, ?geomLit2)
          True, if geomLit1 is spatially equal to geomLit2.
      
      
          ?boolean spatialF:nearby(?geomLit1, ?geomLit2, ?distance, ?unitsURI)
          True, if geomLit1 is within distance of geomLit2 using the distance units.
      
      
          ?boolean spatialF:withinCircle(?geomLit1, ?geomLit2, ?distance, ?unitsURI)
          True, if geomLit1 is within distance of geomLit2 using the distance units.
      
      
          ?radians spatialF:angle(?x1, ?y1, ?x2, ?y2)
          Angle clockwise from y-axis from Point(x1,y1) to Point (x2,y2) in 0 to 2π radians.
      
      
          ?degrees spatialF:angleDeg(?x, ?y1, ?x2, ?y2)
          Angle clockwise from y-axis from Point(x1,y1) to Point (x2,y2) in 0 to 360 degrees.
      
      
          ?distance spatialF:distance(?geomLit1, ?geomLit2, ?unitsURI)
          Distance between two Geometry Literals in distance units. Chooses distance measure based on SRS type. Great Circle distance for Geographic SRS and Euclidean otherwise.
      
      
          ?radians spatialF:azimuth(?lat1, ?lon1, ?lat2, ?lon2)
          Forward azimuth clockwise from North between two Lat/Lon Points in 0 to 2π radians.
      
      
          ?degrees spatialF:azimuthDeg(?lat1, ?lon1, ?lat2, ?lon2)
          Forward azimuth clockwise from North between two Lat/Lon Points in 0 to 360 degrees.
      
      
          ?distance spatialF:greatCircle(?lat1, ?lon1, ?lat2, ?lon2, ?unitsURI)
          Great Circle distance (Vincenty formula) between two Lat/Lon Points in distance units.
      
      
          ?distance spatialF:greatCircleGeom(?geomLit1, ?geomLit2, ?unitsURI)
          Great Circle distance (Vincenty formula) between two Geometry Literals in distance units. Use http://www.opengis.net/def/function/geosparql/distance from GeoSPARQL standard for Euclidean distance.
      
      
          ?geomLit2 spatialF:transform(?geomLit1, ?datatypeURI, ?srsURI)
          Transform Geometry Literal by Datatype and SRS.
      
      
          ?geomLit2 spatialF:transformDatatype(?geomLit1, ?datatypeURI)
          Transform Geometry Literal by Datatype.
      
      
          ?geomLit2 spatialF:transformSRS(?geomLit1, ?srsURI)
          Transform Geometry Literal by SRS.
      
  

Property Functions
These property functions are available in the http://jena.apache.org/spatial#
namespace and here use the prefix spatial.  This is the same namespace as the
jena-spatial functions utilise and these form direct replacements.  The
subject Feature may be bound, to test the pattern is true, or unbound, to find
all cases the pattern is true.  These property functions require a Spatial Index to be setup for the dataset.
The optional ?limit parameter restricts the number of results returned. The
default value is -1 which returns all results. No guarantee is given for
ordering of results.  The optional ?unitsURI parameter specifies the units of
a distance. The default value is kilometres through the string or resource
http://www.opengis.net/def/uom/OGC/1.0/kilometre.
The spatial:equals property function behaves the same way as the main
GeoSPARQL property functions. Either, both or neither of the subject and object
can be bound. A Spatial Index is not required for the dataset with the
spatial:equals property function.

  
      
          Function Name
          Description
      
  
  
      
          ?spatialObject1 spatial:equals ?spatialObject2
          Find spatialObjects (i.e. features or geometries) that are spatially equal.
      
      
          ?feature spatial:intersectBox(?latMin ?lonMin ?latMax ?lonMax [ ?limit])
          Find features that intersect the provided box, up to the limit.
      
      
          ?feature spatial:intersectBoxGeom(?geomLit1 ?geomLit2 [ ?limit])
          Find features that intersect the provided box, up to the limit.
      
      
          ?feature spatial:withinBox(?latMin ?lonMin ?latMax ?lonMax [ ?limit])
          Find features that intersect the provided box, up to the limit.
      
      
          ?feature spatial:withinBoxGeom(?geomLit1 ?geomLit2 [ ?limit])
          Find features that are within the provided box, up to the limit.
      
      
          ?feature spatial:nearby(?lat ?lon ?radius [ ?unitsURI [ ?limit]])
          Find features that are within radius of the distance units, up to the limit.
      
      
          ?feature spatial:nearbyGeom(?geomLit ?radius [ ?unitsURI [ ?limit]])
          Find features that are within radius of the distance units, up to the limit.
      
      
          ?feature spatial:withinCircle(?lat ?lon ?radius [ ?unitsURI [ ?limit]])
          Find features that are within radius of the distance units, up to the limit.
      
      
          ?feature spatial:withinCircleGeom(?geomLit ?radius [ ?unitsURI [ ?limit]])
          Find features that are within radius of the distance units, up to the limit.
      
  

The Cardinal Functions find all Features that are present in the specified
direction.  In Geographic spatial reference systems (SRS), e.g. WGS84 and CRS84,
the East/West directions wrap around.  Therefore, a search is made from the
shape’s edge for up to half the range of the SRS (i.e. 180 degrees in WGS84) and
will continue across the East/West boundary if necessary.  In other SRS,
e.g. Projected onto a flat plane, the East/West check is made from the shape’s
edge to the farthest limit of the SRS range, i.e. there is no wrap around.

  
      
          Cardinal Function Name
          Description
      
  
  
      
          ?feature spatial:north(?lat ?lon [ ?limit])
          Find features that are North of the Lat/Lon point (point to +90 degrees), up to the limit.
      
      
          ?feature spatial:northGeom(?geomLit [ ?limit])
          Find features that are North of the Geometry Literal, up to the limit.
      
      
          ?feature spatial:south(?lat ?lon [ ?limit])
          Find features that are South of the Lat/Lon point (point to -90 degrees), up to the limit.
      
      
          ?feature spatial:southGeom(?geomLit [ ?limit])
          Find features that are South of the Geometry Literal, up to the limit.
      
      
          ?feature spatial:east(?lat ?lon [ ?limit])
          Find features that are East of the Lat/Lon point (point plus 180 degrees longitude, wrapping round), up to the limit.
      
      
          ?feature spatial:eastGeom(?geomLit [ ?limit])
          Find features that are East of the Geometry Literal, up to the limit.
      
      
          ?feature spatial:west(?lat ?lon [ ?limit])
          Find features that are West of the Lat/Lon point (point minus 180 degrees longitude, wrapping round), up to the limit.
      
      
          ?feature spatial:westGeom(?geomLit [ ?limit])
          Find features that are West of the Geometry Literal, up to the limit.
      
  

Geometry Property Filter Functions
The GeoSPARQL standard provides a set of properties related to geometries, see
Section 8.4.  These are applied on the Geometry resource and are automatically
determined if not asserted in the data.  However, it may be necessary to
retrieve the properties of a Geometry Literal directly without an associated
Geometry resource.  Filter functions to do this have been included as part of
the http://www.opengis.net/def/function/geosparql/ namespace as a minor
variation to the GeoSPARQL standard.  The relevant functions using the geof
prefix are:

  
      
          Geometry Property Filter Function Name
          Description
      
  
  
      
          ?integer geof:dimension(?geometryLiteral)
          Topological dimension, e.g. 0 for Point, 1 for LineString and 2 for Polygon.
      
      
          ?integer geof:coordinateDimension(?geometryLiteral)
          Coordinate dimension, e.g. 2 for XY coordinates and 4 for XYZM coordinates.
      
      
          ?integer geof:spatialDimension(?geometryLiteral)
          Spatial dimension, e.g. 2 for XY coordinates and 3 for XYZM coordinates.
      
      
          ?boolean geof:isEmpty(?geometryLiteral)
          True, if geometry is empty.
      
      
          ?boolean geof:isSimple(?geometryLiteral)
          True, if geometry is simple.
      
      
          ?boolean geof:isValid(?geometryLiteral)
          True, if geometry is topologically valid.
      
  

A dataset that follows the GeoSPARQL Feature-Geometry-GeometryLiteral can have
simpler SPARQL queries without needing to use these functions by taking
advantage of the Query Rewriting functionality.  The geof:isValid filter
function and geo:isValid property for a Geometry resource are not part of the
GeoSPARQL standard but have been included as a minor variation.
Future Work

Implementing GeoJSON as a GeometryLiteral serialisation (https://tools.ietf.org/html/rfc7946).
Producing GeoJSON is already possible with geof:asGeoJSON(?geometryLiteral).

Contributors
The following individuals have made contributions to this project:

Greg Albiston
Haozhe Chen
Taha Osman

Why Use This Implementation?
There are several implementations of the GeoSPARQL standard.  The conformance
and completeness of these implementations is difficult to ascertain and varies
between features.
However, the following may be of interest when considering whether to use this
implementation based on reviewing several alternatives.

  
      
          This Implementation
          Other Implementations
      
  
  
      
          Implements all six components of the GeoSPARQL standard.
          Generally partially implement the Geometry Topology and Geometry Extensions. Do not implement the Query Rewrite Extension.
      
      
          Pure Java and does not require a supporting relational database. Configuration requires a single line of code (although Apache SIS may need some setting up, see above).
          Require setting up a database, configuring a geospatial extension and setting environment variables.
      
      
          Uses Jena, which conforms to the W3C standards for RDF and SPARQL. New versions of the standards will quickly feed through.
          Not fully RDF and SPARQL compliant, e.g. RDFS/OWL inferencing or SPARQL syntax. Adding your own schema may not produce inferences.
      
      
          Automatically determines geometry properties and handles mixed cases of units or coordinate reference systems. The GeoSPARQL standard suggests this approach but does not require it.
          Tend to produce errors or no results in these situations.
      
      
          Performs indexing and caching on-demand which reduces set-up time and only performs calculations that are required.
          Perform indexing in the data loading phase and initialisation phase, which can lead to lengthy delays (even on relatively small datasets).
      
      
          Uses JTS which does not truncate coordinate precision and applies spatial equality.
          May truncate coordinate precision and apply lexical equality, which is quicker but does not comply with the GeoSPARQL standard.
      
  


  
  
  
    On this page
    
  
    Getting Started
    Features
    Additional Features
      
        SPARQL Query Configuration
        Querying Datasets & Models with SPARQL
        API The library can be used as an API in Java.  The main class to handle
      
    
    Key Dependencies
      
        GeoSPARQL
        Apache SIS/SIS_DATA Environment Variable
        Java Topology Suite
      
    
    Note
      
        GeoSPARQL Schema
        Spatial Relations
        Spatial Relations and Geometry Shapes/Types
        Equals Relations
        Query Rewrite Extension
        Dataset Conversion
        Spatial Index
        Units URI
      
    
    Geography Markup Language Support (GML)
    Apache Jena Spatial Functions/WGS84 Geo Predicates
      
        Supported Features
        Filter Functions
        Property Functions
      
    
    Geometry Property Filter Functions
    Future Work
    Contributors
    Why Use This Implementation?\n\n\n\nThis section of the documentation describes the current support for inference
  available within Jena. It includes an outline of the general inference API,
  together with details of the specific rule engines and configurations for RDFS
  and OWL inference supplied with Jena.
 Not all of the fine details of the API are covered here: refer to the Jena
  Javadoc to get the full details of the capabilities
  of the API. 
 Note that this is a preliminary version of this document, some errors or inconsistencies
  are possible, feedback to the 
  mailing lists
  is welcomed. 
Overview of inference support
The Jena inference subsystem is designed to allow a range of inference engines
  or reasoners to be plugged into Jena. Such engines are used to derive additional
  RDF assertions which are entailed from some base RDF together with any optional
  ontology information and the axioms and rules associated with the reasoner.
  The primary use of this mechanism is to support the use of languages such as
  RDFS and OWL which allow additional facts to be inferred from instance data
  and class descriptions. However, the machinery is designed to be quite general
  and, in particular, it includes a generic rule engine that can be used for many
  RDF processing or transformation tasks.
We will try to use the term inference to refer to the abstract process
  of deriving additional information and the term reasoner to refer to
  a specific code object that performs this task. Such usage is arbitrary and
  if we slip into using equivalent terms like reasoning and inference
  engine, please forgive us. 
The overall structure of the inference machinery is illustrated below. 

Applications normally access the inference machinery by using the ModelFactory
  to associate a data set with some reasoner to create a new Model. Queries to
  the created model will return not only those statements that were present in
  the original data but also additional statements than can be derived from the
  data using the rules or other inference mechanisms implemented by the reasoner.
As illustrated the inference machinery is actually implemented at the level
  of the Graph SPI, so that any of the different Model interfaces can be constructed
  around an inference Graph. In particular, the Ontology
  API provides convenient ways to link appropriate reasoners into the OntModels
  that it constructs. As part of the general RDF API we also provide an InfModel,
  this is an extension to the normal Model interface that provides
  additional control and access to an underlying inference graph. 
The reasoner API supports the notion of specializing a reasoner by binding
  it to a set of schema or ontology data using the bindSchema call.
  The specialized reasoner can then be attached to different sets of instance
  data using bind calls. In situations where the same schema information
  is to be used multiple times with different sets of instance data then this
  technique allows for some reuse of inferences across the different uses of the
  schema. In RDF there is no strong separation between schema (aka Ontology AKA
  tbox) data and instance (AKA abox) data and so any data, whether class or instance
  related, can be included in either the bind or bindSchema
  calls - the names are suggestive rather than restrictive.
To keep the design as open ended as possible Jena also includes a ReasonerRegistry.
  This is a static class though which the set of reasoners currently available
  can be examined. It is possible to register new reasoner types and to dynamically
  search for reasoners of a given type. The ReasonerRegistry also
  provides convenient access to prebuilt instances of the main supplied reasoners.
Available reasoners
Included in the Jena distribution are a number of predefined reasoners:

  Transitive reasoner: Provides support for storing and traversing class and property lattices.
    This implements just the transitive and reflexive properties
    of rdfs:subPropertyOf and rdfs:subClassOf.
  RDFS rule reasoner: Implements a configurable subset of the RDFS entailments.
  OWL, OWL Mini, OWL Micro Reasoners: 
  A set of useful but incomplete implementation of the OWL/Lite subset of the OWL/Full
    language. 
  Generic rule reasoner: A rule based reasoner that supports user defined rules. Forward chaining,
    tabled backward chaining and hybrid execution strategies are supported.

[Index]
The Inference API

  Generic reasoner API
  Small examples
  Operations on inference models
    - Validation
    - Extended list statements
    - Direct and indirect relations
    - Derivations
    - Accessing raw data and deductions
    - Processing control
    - Tracing 
  

Generic reasoner API
Finding a reasoner
For each type of reasoner there is a factory class (which conforms to the interface
  ReasonerFactory)
  an instance of which can be used to create instances of the associated
  Reasoner.
  The factory instances can be located by going directly to a known factory class
  and using the static theInstance() method or by retrieval from
  a global ReasonerRegistry
  which stores factory instances indexed by URI assigned to the reasoner. 
In addition, there are convenience methods on the ReasonerRegistry
  for locating a prebuilt instance of each of the main reasoners (getTransitiveReasoner,
  getRDFSReasoner, getRDFSSimpleReasoner, getOWLReasoner, getOWLMiniReasoner, getOWLMicroReasoner).
Note that the factory objects for constructing reasoners are just there to
  simplify the design and extension of the registry service. Once you have a reasoner
  instance, the same instance can reused multiple times by binding it to different
  datasets, without risk of interference - there is no need to create a new reasoner
  instance each time.
If working with the Ontology API it is
  not always necessary to explicitly locate a reasoner. The prebuilt instances
  of `OntSpecification` provide easy access to the appropriate reasoners to use for
  different Ontology configurations.
Similarly, if all you want is a plain RDF Model with RDFS inference included
  then the convenience methods ModelFactory.createRDFSModel can be
  used. 
Configuring a reasoner
The behaviour of many of the reasoners can be configured. To allow arbitrary
  configuration information to be passed to reasoners we use RDF to encode the
  configuration details. The ReasonerFactory.create method can be
  passed a Jena Resource object, the properties of that object will
  be used to configure the created reasoner.
To simplify the code required for simple cases we also provide a direct Java
  method to set a single configuration parameter, Reasoner.setParameter.
  The parameter being set is identified by the corresponding configuration property.
For the built in reasoners the available configuration parameters are described
  below and are predefined in the ReasonerVocabulary
  class.
The parameter value can normally be a String or a structured value. For example,
  to set a boolean value one can use the strings "true" or "false",
  or in Java use a Boolean object or in RDF use an instance of xsd:Boolean
Applying a reasoner to data
Once you have an instance of a reasoner it can then be attached to a set of
  RDF data to create an inference model. This can either be done by putting all
  the RDF data into one Model or by separating into two components - schema and
  instance data. For some external reasoners a hard separation may be required.
  For all of the built-in reasoners the separation is arbitrary. The prime value
  of this separation is to allow some deductions from one set of data (typically
  some schema definitions) to be efficiently applied to several subsidiary sets
  of data (typically sets of instance data).
If you want to specialize the reasoner this way, by partially-applying it to
  a set schema data, use the Reasoner.bindSchema method which returns
  a new, specialized, reasoner.
To bind the reasoner to the final data set to create an inference model see
  the ModelFactory
  methods, particularly ModelFactory.createInfModel. 
Accessing inferences
Finally, having created an inference model, any API operations which access
  RDF statements will be able to access additional statements which are entailed
  from the bound data by means of the reasoner. Depending on the reasoner these
  additional virtual statements may all be precomputed the first time the
  model is touched, may be dynamically recomputed each time or may be computed
  on-demand but cached.
Reasoner description
The reasoners can be described using RDF metadata which can be searched to
  locate reasoners with appropriate properties. The calls Reasoner.getCapabilities
  and Reasoner.supportsProperty are used to access this descriptive
  metadata.
[API Index] [Main Index]
Some small examples
These initial examples are not designed to illustrate the power of the reasoners
  but to illustrate the code required to set one up.
Let us first create a Jena model containing the statements that some property
  "p" is a subproperty of another property "q" and that we
  have a resource "a" with value "foo" for "p".
  This could be done by writing an RDF/XML or N3 file and reading that in but
  we have chosen to use the RDF API:
String NS = "urn:x-hp-jena:eg/";

// Build a trivial example data set
Model rdfsExample = ModelFactory.createDefaultModel();
Property p = rdfsExample.createProperty(NS, "p");
Property q = rdfsExample.createProperty(NS, "q");
rdfsExample.add(p, RDFS.subPropertyOf, q);
rdfsExample.createResource(NS+"a").addProperty(p, "foo");
Now we can create an inference model which performs RDFS inference over this
  data by using:
InfModel inf = ModelFactory.createRDFSModel(rdfsExample);  // [1]
We can then check that resulting model shows that "a" also has property
  "q" of value "foo" by virtue of the subPropertyOf entailment:
Resource a = inf.getResource(NS+"a");
System.out.println("Statement: " + a.getProperty(q));
Which prints the output:
    Statement: [urn:x-hp-jena:eg/a, urn:x-hp-jena:eg/q, Literal<foo>]

Alternatively we could have created an empty inference model and then added
  in the statements directly to that model.
If we wanted to use a different reasoner which is not available as a convenience
  method or wanted to configure one we would change line [1]. For example, to
  create the same setup manually we could replace [1] by:
Reasoner reasoner = ReasonerRegistry.getRDFSReasoner();
InfModel inf = ModelFactory.createInfModel(reasoner, rdfsExample);
or even more manually by
Reasoner reasoner = RDFSRuleReasonerFactory.theInstance().create(null);
InfModel inf = ModelFactory.createInfModel(reasoner, rdfsExample);
The purpose of creating a new reasoner instance like this variant would be
  to enable configuration parameters to be set. For example, if we were to listStatements
  on inf Model we would see that it also "includes" all the RDFS axioms,
  of which there are quite a lot. It is sometimes useful to suppress these and
  only see the "interesting" entailments. This can be done by setting
  the processing level parameter by creating a description of a new reasoner configuration
  and passing that to the factory method:
Resource config = ModelFactory.createDefaultModel()
                              .createResource()
                              .addProperty(ReasonerVocabulary.PROPsetRDFSLevel, "simple");
Reasoner reasoner = RDFSRuleReasonerFactory.theInstance().create(config);
InfModel inf = ModelFactory.createInfModel(reasoner, rdfsExample);
This is a rather long winded way of setting a single parameter, though it can
  be useful in the cases where you want to store this sort of configuration information
  in a separate (RDF) configuration file. For hardwired cases the following alternative
  is often simpler:
Reasoner reasoner = RDFSRuleReasonerFactory.theInstance()Create(null);
reasoner.setParameter(ReasonerVocabulary.PROPsetRDFSLevel,
                      ReasonerVocabulary.RDFS_SIMPLE);
InfModel inf = ModelFactory.createInfModel(reasoner, rdfsExample);
Finally, supposing you have a more complex set of schema information, defined
  in a Model called schema, and you want to apply this schema to several
  sets of instance data without redoing too many of the same intermediate deductions.
  This can be done by using the SPI level methods: 
Reasoner boundReasoner = reasoner.bindSchema(schema);
InfModel inf = ModelFactory.createInfModel(boundReasoner, data);
This creates a new reasoner, independent from the original, which contains
  the schema data. Any queries to an InfModel created using the boundReasoner
  will see the schema statements, the data statements and any statements entailed
  from the combination of the two. Any updates to the InfModel will be reflected
  in updates to the underlying data model - the schema model will not be affected.
[API Index] [Main Index]
Operations on inference models
For many applications one simply creates a model incorporating some inference
  step, using the ModelFactory methods, and then just works within
  the standard Jena Model API to access the entailed statements. However, sometimes
  it is necessary to gain more control over the processing or to access additional
  reasoner features not available as virtual triples.
Validation
The most common reasoner operation which can't be exposed through additional
  triples in the inference model is that of validation. Typically the ontology
  languages used with the semantic web allow constraints to be expressed, the
  validation interface is used to detect when such constraints are violated by
  some data set. 
A simple but typical example is that of datatype ranges in RDFS. RDFS allows
  us to specify the range of a property as lying within the value space of some
  datatype. If an RDF statement asserts an object value for that property which
  lies outside the given value space there is an inconsistency.
To test for inconsistencies with a data set using a reasoner we use the InfModel.validate()
  interface. This performs a global check across the schema and instance data
  looking for inconsistencies. The result is a ValidityReport object
  which comprises a simple pass/fail flag (ValidityReport.isValid())
  together with a list of specific reports (instances of the ValidityReport.Report
  interface) which detail any detected inconsistencies. At a minimum the individual
  reports should be printable descriptions of the problem but they can also contain
  an arbitrary reasoner-specific object which can be used to pass additional information
  which can be used for programmatic handling of the violations.
For example, to check a data set and list any problems one could do something
  like:
Model data = RDFDataMgr.loadModel(fname);
InfModel infmodel = ModelFactory.createRDFSModel(data);
ValidityReport validity = infmodel.validate();
if (validity.isValid()) {
    System.out.println("OK");
} else {
    System.out.println("Conflicts");
    for (Iterator i = validity.getReports(); i.hasNext(); ) {
        System.out.println(" - " + i.next());
    }
}
The file testing/reasoners/rdfs/dttest2.nt declares a property
  bar with range xsd:integer and attaches a bar
  value to some resource with the value "25.5"^^xsd:decimal.
  If we run the above sample code on this file we see:

  Conflicts 
    - Error (dtRange): Property http://www.hpl.hp.com/semweb/2003/eg#bar has a
    typed range Datatype[http://www.w3.org/2001/XMLSchema#integer -> class java.math.BigInteger]that
    is not compatible with 25.5:http://www.w3.org/2001/XMLSchema#decimal 

Whereas the file testing/reasoners/rdfs/dttest3.nt uses the value
  "25"^^xsd:decimal instead, which is a valid integer and so passes.

Note that the individual validation records can include warnings as well as
  errors. A warning does not affect the overall isValid() status
  but may indicate some issue the application may wish to be aware of. For example,
  it would be possible to develop a modification to the RDFS reasoner which warned
  about use of a property on a resource that is not explicitly declared to have
  the type of the domain of the property. 
A particular case of this arises in the case of OWL. In the Description Logic
  community a class which cannot have an instance is regarded as "inconsistent".
  That term is used because it generally arises from an error in the ontology.
  However, it is not a logical inconsistency - i.e. something giving rise to a
  contradiction. Having an instance of such a class is, clearly a logical error.
  In the Jena 2.2 release we clarified the semantics of isValid().
  An ontology which is logically consistent but contains empty classes is regarded
  as valid (that is isValid() is false only if there is a logical
  inconsistency). Class expressions which cannot be instantiated are treated as
  warnings rather than errors. To make it easier to test for this case there is
  an additional method Report.isClean() which returns true if the
  ontology is both valid (logically consistent) and generated no warnings (such
  as inconsistent classes).
Extended list statements
The default API supports accessing all entailed information at the level of
  individual triples. This is surprisingly flexible but there are queries which
  cannot be easily supported this way. The first such is when the query needs
  to make reference to an expression which is not already present in the data.
  For example, in description logic systems it is often possible to ask if there
  are any instances of some class expression. Whereas using the triple-based approach
  we can only ask if there are any instances of some class already defined (though
  it could be defined by a bNode rather than be explicitly named).
To overcome this limitation the InfModel API supports a notion
  of "posit", that is a set of assertions which can be used to temporarily
  declare new information such as the definition of some class expression. These
  temporary assertions can then be referenced by the other arguments to the listStatements
  command. With the current reasoners this is an expensive operation, involving
  the temporary creation of an entire new model with the additional posits added
  and all inference has to start again from scratch. Thus it is worth considering
  preloading your data with expressions you might need to query over. However,
  for some external reasoners, especially description logic reasoners, we anticipate
  restricted uses of this form of listStatement will be important.
Direct and indirect relationships
The second type of operation that is not obviously convenient at the triple
  level involves distinguishing between direct and indirect relationships. If
  a relation is transitive, for example rdfs:subClassOf, then we can define the
  notion of the minimal or direct form of the relationship from
  which all other values of the relation can be derived by transitive closure.



Normally, when an InfGraph is queried for a transitive relation the results
  returned show the inferred relations, i.e. the full transitive closure (all
  the links (ii) in the illustration). However, in some cases, such when as building
  a hierarchical UI widget to represent the graph, it is more convenient to only
  see the direct relations (iii). This is achieved by defining special direct
  aliases for those relations which can be queried this way. For the built in
  reasoners this functionality is available for rdfs:subClassOf and
  rdfs:subPropertyOf and the direct aliases for these are defined
  in ReasonerVocabulary.
Typically, the easiest way to work with such indirect and direct relations is
  to use the Ontology API which hides the
  grubby details of these property aliases.
Derivations
It is sometimes useful to be able to trace where an inferred statement was
  generated from. This is achieved using the InfModel.getDerivation(Statement)
  method. This returns a iterator over a set Derivation
  objects through which a brief description of the source of the derivation can
  be obtained. Typically understanding this involves tracing the sources for other
  statements which were used in this derivation and the Derivation.PrintTrace
  method is used to do this recursively.
The general form of the Derivation objects is quite abstract but in the case
  of the rule-based reasoners they have a more detailed internal structure that
  can be accessed - see RuleDerivation.
Derivation information is rather expensive to compute and store. For this reason,
  it is not recorded by default and InfModel.serDerivationLogging(true)
  must be used to enable derivations to be recorded. This should be called before
  any queries are made to the inference model.
As an illustration suppose that we have a raw data model which asserts three
  triples:
eg:A eg:p eg:B .
eg:B eg:p eg:C .
eg:C eg:p eg:D .
and suppose that we have a trivial rule set which computes the transitive closure
  over relation eg:p
String rules = "[rule1: (?a eg:p ?b) (?b eg:p ?c) -&gt; (?a eg:p ?c)]";
Reasoner reasoner = new GenericRuleReasoner(Rule.parseRules(rules));
reasoner.setDerivationLogging(true);
InfModel inf = ModelFactory.createInfModel(reasoner, rawData);
Then we can query whether eg:A is related through eg:p to eg:D and list the
  derivation route using the following code fragment: 
PrintWriter out = new PrintWriter(System.out);
for (StmtIterator i = inf.listStatements(A, p, D); i.hasNext(); ) {
    Statement s = i.nextStatement();
    System.out.println("Statement is " + s);
    for (Iterator id = inf.getDerivation(s); id.hasNext(); ) {
        Derivation deriv = (Derivation) id.next();
        deriv.printTrace(out, true);
    }
}
out.flush();
Which generates the output:
Statement is [urn:x-hp:eg/A, urn:x-hp:eg/p, urn:x-hp:eg/D]
    Rule rule1 concluded (eg:A eg:p eg:D) <-
        Fact (eg:A eg:p eg:B)
    Rule rule1 concluded (eg:B eg:p eg:D) <-
        Fact (eg:B eg:p eg:C)
        Fact (eg:C eg:p eg:D)

Accessing raw data and deductions
From an InfModel it is easy to retrieve the original, unchanged,
  data over which the model has been computed using the getRawModel()
  call. This returns a model equivalent to the one used in the initial bind
  call. It might not be the same Java object but it uses the same Java object
  to hold the underlying data graph. 
Some reasoners, notably the forward chaining rule engine, store the deduced
  statements in a concrete form and this set of deductions can be obtained separately
  by using the getDeductionsModel() call. 
Processing control
Having bound a Model into an InfModel by using a
  Reasoner its content can still be changed by the normal add
  and remove calls to the InfModel. Any such change
  the model will usually cause all current deductions and temporary rules to be
  discarded and inference will start again from scratch at the next query. Some
  reasoners, such as the RETE-based forward rule engine, can work incrementally.

In the non-incremental case then the processing will not be started until a
  query is made. In that way a sequence of add and removes can be undertaken without
  redundant work being performed at each change. In some applications it can be
  convenient to trigger the initial processing ahead of time to reduce the latency
  of the first query. This can be achieved using the InfModel.prepare()
  call. This call is not necessary in other cases, any query will automatically
  trigger an internal prepare phase if one is required.
There are times when the data in a model bound into an InfModel can is changed
  "behind the scenes" instead of through calls to the InfModel. If this
  occurs the result of future queries to the InfModel are unpredictable. To overcome
  this and force the InfModel to reconsult the raw data use the InfModel.rebind()
  call.
Finally, some reasoners can store both intermediate and final query results
  between calls. This can substantially reduce the cost of working with the inference
  services but at the expense of memory usage. It is possible to force an InfModel
  to discard all such cached state by using the InfModel.reset()
  call. It there are any outstanding queries (i.e. StmtIterators which have not
  been read to the end yet) then those will be aborted (the next hasNext() call
  will return false).
Tracing
When developing new reasoner configurations, especially new rule sets for the
  rule engines, it is sometimes useful to be able to trace the operations of the
  associated inference engine. Though, often this generates too much information
  to be of use and selective use of the print builtin can be more
  effective. 
Tracing is not supported by a convenience API call but, for those reasoners
  that support it, it can be enabled using:
reasoner.setParameter(ReasonerVocabulary.PROPtraceOn, Boolean.TRUE);
Dynamic tracing control is sometimes possible on the InfModel itself by retrieving
  its underlying InfGraph and calling setTraceOn() call. If you need
  to make use of this see the full javadoc for the relevant InfGraph implementation.
[API Index] [Main Index]
The RDFS reasoner

  RDFS reasoner - introduction and coverage
  RDFS Configuration
  RDFS Example
  RDFS implementation and performance notes

RDFS reasoner - intro and coverage
Jena includes an RDFS reasoner (RDFSRuleReasoner) which supports
  almost all of the RDFS entailments described by the RDF Core working group [RDF
  Semantics]. The only omissions are deliberate and are described below.
This reasoner is accessed using ModelFactory.createRDFSModel or
  manually via ReasonerRegistry.getRDFSReasoner().
During the preview phases of Jena experimental RDFS reasoners were released,
  some of which are still included in the code base for now but applications should
  not rely on their stability or continued existence.
When configured in full mode (see below for configuration information)
  then the RDFS reasoner implements all RDFS entailments except for the bNode
  closure rules. These closure rules imply, for example, that for all triples
  of the form:
eg:a eg:p nnn^^datatype .
we should introduce the corresponding blank nodes:
eg:a eg:p _:anon1 .
_:anon1 rdf:type datatype .
Whilst such rules are both correct and necessary to reduce RDF datatype entailment
  down to simple entailment they are not useful in implementation terms. In Jena
  simple entailment can be implemented by translating a graph containing bNodes
  to an equivalent query containing variables in place of the bNodes. Such a query
  is can directly match the literal node and the RDF API can be used to extract
  the datatype of the literal. The value to applications of directly seeing the
  additional bNode triples, even in virtual triple form, is negligible
  and so this has been deliberately omitted from the reasoner. 
[RDFS Index] [Main Index]
RDFS configuration
The RDFSRuleReasoner can be configured to work at three different compliance
  levels: 

  Full
  This implements all of the RDFS axioms and closure rules with the exception
    of bNode entailments and datatypes (rdfD 1). See above for comments on these.
    This is an expensive mode because all statements in the data graph need to
    be checked for possible use of container membership properties. It also generates
    type assertions for all resources and properties mentioned in the data (rdf1,
    rdfs4a, rdfs4b).
  Default
  This omits the expensive checks for container membership properties and
    the "everything is a resource" and "everything used as a property
    is one" rules (rdf1, rdfs4a, rdfs4b). The latter information is available
    through the Jena API and creating virtual triples to this effect has little
    practical value.
    This mode does include all the axiomatic rules. Thus, for example, even querying
    an "empty" RDFS InfModel will return triples such as [rdf:type
    rdfs:range rdfs:Class].
  Simple
  This implements just the transitive closure of subPropertyOf and subClassOf
    relations, the domain and range entailments and the implications of subPropertyOf
    and subClassOf. It omits all of the axioms. This is probably the most useful
    mode but is not the default because it is a less complete implementation of
    the standard. 

The level can be set using the setParameter call, e.g.
reasoner.setParameter(ReasonerVocabulary.PROPsetRDFSLevel,
                      ReasonerVocabulary.RDFS_SIMPLE);
or by constructing an RDF configuration description and passing that to the
  RDFSRuleReasonerFactory e.g.
Resource config = ModelFactory.createDefaultModel()
                  .createResource()
                  .addProperty(ReasonerVocabulary.PROPsetRDFSLevel, "simple");
Reasoner reasoner = RDFSRuleReasonerFactory.theInstance()Create(config);
Summary of parameters

  
    Parameter
    Values
    Description
  
  
    
      PROPsetRDFSLevel
    
    "full", "default", "simple"
    
      Sets the RDFS processing level as described above.
    
  
  
    
      PROPenableCMPScan
    
    Boolean
    
      If true forces a preprocessing pass which finds all usages
        of rdf:_n properties and declares them as ContainerMembershipProperties.
        This is implied by setting the level parameter to "full" and
        is not normally used directly.
    
  
  
    
      PROPtraceOn
    
    Boolean
    
      If true switches on exhaustive tracing of rule executions
        at the INFO level.
    
  
  
    
      PROPderivationLogging
    
    Boolean
    
      If true causes derivation routes to be recorded internally
        so that future getDerivation calls can return useful information.
    
  

[RDFS Index] [Main Index]
RDFS Example
As a complete worked example let us create a simple RDFS schema, some instance
  data and use an instance of the RDFS reasoner to query the two.
We shall use a trivial schema:
<rdf:Description rdf:about="eg:mum">
  <rdfs:subPropertyOf rdf:resource="eg:parent"/>
</rdf:Description>

<rdf:Description rdf:about="eg:parent">
  <rdfs:range  rdf:resource="eg:Person"/>
  <rdfs:domain rdf:resource="eg:Person"/>
</rdf:Description>

<rdf:Description rdf:about="eg:age">
  <rdfs:range rdf:resource="xsd:integer" />
</rdf:Description>
This defines a property parent from Person to Person,
  a sub-property mum of parent and an integer-valued
  property age.
We shall also use the even simpler instance file:
<Teenager rdf:about="eg:colin">
    <mum rdf:resource="eg:rosy" />
    <age>13</age>
</Teenager>

  Which defines a Teenager called colin who has a mum
  rosy and an age of 13.
Then the following code fragment can be used to read files containing these
  definitions, create an inference model and query it for information on the rdf:type
  of colin and the rdf:type of Person:
Model schema = RDFDataMgr.loadModel("file:data/rdfsDemoSchema.rdf");
Model data = RDFDataMgr.loadModel("file:data/rdfsDemoData.rdf");
InfModel infmodel = ModelFactory.createRDFSModel(schema, data);

Resource colin = infmodel.getResource("urn:x-hp:eg/colin");
System.out.println("colin has types:");
printStatements(infmodel, colin, RDF.type, null);

Resource Person = infmodel.getResource("urn:x-hp:eg/Person");
System.out.println("\nPerson has types:");
printStatements(infmodel, Person, RDF.type, null);
This produces the output:
colin has types:
 - (eg:colin rdf:type eg:Teenager)
 - (eg:colin rdf:type rdfs:Resource)
 - (eg:colin rdf:type eg:Person)

Person has types:
 - (eg:Person rdf:type rdfs:Class)
 - (eg:Person rdf:type rdfs:Resource)
This says that colin is both a Teenager (by direct
  definition), a Person (because he has a mum which
  means he has a parent and the domain of parent is
  Person) and an rdfs:Resource. It also says that Person
  is an rdfs:Class, even though that wasn't explicitly in the schema,
  because it is used as object of range and domain statements.
If we add the additional code:
ValidityReport validity = infmodel.validate();
if (validity.isValid()) {
    System.out.println("\nOK");
} else {
    System.out.println("\nConflicts");
    for (Iterator i = validity.getReports(); i.hasNext(); ) {
        ValidityReport.Report report = (ValidityReport.Report)i.next();
        System.out.println(" - " + report);
    }
}

  Then we get the additional output:
Conflicts
 - Error (dtRange): Property urn:x-hp:eg/age has a typed range
Datatype[http://www.w3.org/2001/XMLSchema#integer -> class java.math.BigInteger]
that is not compatible with 13
because the age was given using an RDF plain literal where as the schema requires
  it to be a datatyped literal which is compatible with xsd:integer.
[RDFS Index] [Main Index]
RDFS implementation and performance notes
The RDFSRuleReasoner is a hybrid implementation. The subproperty and subclass
  lattices are eagerly computed and stored in a compact in-memory form using the
  TransitiveReasoner (see below). The identification of which container membership
  properties (properties like rdf:_1) are present is implemented using a preprocessing
  hook. The rest of the RDFS operations are implemented by explicit rule sets
  executed by the general hybrid rule reasoner. The three different processing
  levels correspond to different rule sets. These rule sets are located by looking
  for files "`etc/*.rules`" on the classpath and so could,
  in principle, be overridden by applications wishing to modify the rules. 
Performance for in-memory queries appears to be good. Using a synthetic dataset
  we obtain the following times to determine the extension of a class from a class
  hierarchy:

  
    Set
    #concepts
    total instances
    #instances of concept
    JenaRDFS
    XSB*
  
  
    1
    155
    1550
    310
    0.07
    0.16
  
  
    2
    780
    7800
    1560
    0.25
    0.47
  
  
    3
    3905
    39050
    7810
    1.16
    2.11
  

The times are in seconds, normalized to a 1.1GHz Pentium processor. The XSB*
  figures are taken from a pre-published paper and may not be directly comparable
  (for example they do not include any rule compilation time) - they are just
  offered to illustrate that the RDFSRuleReasoner has broadly similar scaling
  and performance to other rule-based implementations.
The Jena RDFS implementation has not been tested and evaluated over database
  models. The Jena architecture makes it easy to construct such models but in
  the absence of caching we would expect the performance to be poor. Future work
  on adapting the rule engines to exploit the capabilities of the more sophisticated
  database backends will be considered.
[RDFS Index] [Main Index]
The OWL reasoner

  OWL reasoner introduction
  OWL coverage
  OWL configuration
  OWL example
  OWL notes and limitations

The second major set of reasoners supplied with Jena is a rule-based
  implementation of the OWL/lite subset of OWL/full.
The current release includes a default OWL reasoner and two small/faster configurations.
Each of the configurations is intended to be a sound implementation of a subset of OWL/full semantics
but none of them is complete (in the technical sense). For complete OWL DL reasoning use
an external DL reasoner such as Pellet, Racer or FaCT. Performance (especially memory use) of the fuller reasoner
configuration still leaves something to be desired and will the subject of future work - time permitting.
See also subsection 5 for notes on more specific limitations
  of the current implementation. 
OWL coverage
The Jena OWL reasoners could be described as instance-based reasoners. That
  is, they work by using rules to propagate the if- and only-if- implications of
  the OWL constructs on instance data. Reasoning about classes is done indirectly
  - for each declared class a prototypical instance is created and elaborated.
  If the prototype for a class A can be deduced as being a member of class B then
  we conclude that A is a subClassOf B. This approach is in contrast to more sophisticated
  Description Logic reasoners which work with class expressions and can be less
  efficient when handling instance data but more efficient with complex class expressions
  and able to provide complete reasoning. 
We thus anticipate that the OWL rule reasoner will be most suited to applications
  involving primarily instance reasoning with relatively simple, regular ontologies
  and least suited to applications involving large rich ontologies. A better characterisation
  of the tradeoffs involved would be useful and will be sought.
We intend that the OWL reasoners should be smooth extensions of the RDFS reasoner
  described above. That is all RDFS entailments found by the RDFS reasoner will
  also be found by the OWL reasoners and scaling on RDFS schemas should be similar
  (though there are some costs, see later). The instance-based implementation
  technique is in keeping with this "RDFS plus a bit" approach.
Another reason for choosing this inference approach is that it makes it possible
  to experiment with support for different constructs, including constructs that
  go beyond OWL, by modification of the rule set. In particular, some applications
  of interest to ourselves involve ontology transformation which very often implies
  the need to support property composition. This is something straightforward
  to express in rule-based form and harder to express in standard Description
  Logics.
Since RDFS is not a subset of the OWL/Lite or OWL/DL languages the Jena implementation
  is an incomplete implementation of OWL/full. We provide three implementations
  a default ("full" one), a slightly cut down "mini" and a
  rather smaller/faster "micro". The default OWL rule reasoner (ReasonerRegistry.getOWLReasoner())
  supports the constructs as listed below. The OWLMini reasoner is nearly the
  same but omits the forward entailments from minCardinality/someValuesFrom restrictions
  - that is it avoids introducing bNodes which avoids some infinite expansions
  and enables it to meet the Jena API contract more precisely. The OWLMicro reasoner
  just supports RDFS plus the various property axioms, intersectionOf, unionOf
  (partial) and hasValue. It omits the cardinality restrictions and equality axioms,
  which enables it to achieve much higher performance. 

  
    Constructs
    Supported by
    Notes
  
  
    
      rdfs:subClassOf, rdfs:subPropertyOf, rdf:type
    
    all
    
      Normal RDFS semantics supported including meta use (e.g.
        taking the subPropertyOf subClassOf).
    
  
  
    
      rdfs:domain, rdfs:range
    
    all
    
      Stronger if-and-only-if semantics supported
    
  
  
    
      owl:intersectionOf
    
    all
     
  
  
    
      owl:unionOf
    
    all
    
      Partial support. If C=unionOf(A,B) then will infer that
        A,B are subclasses of C, and thus that instances of A or B are instances
        of C. Does not handle the reverse (that an instance of C must be either
        an instance of A or an instance of B).
    
  
  
    
      owl:equivalentClass
    
    all
    
      
    
  
  
    
      owl:disjointWith
    
    full, mini
    
      
    
  
  
    
      owl:sameAs, owl:differentFrom, owl:distinctMembers
    
    full, mini
    
      owl:distinctMembers is currently translated into a quadratic
        set of owl:differentFrom assertions.
    
  
  
    
      Owl:Thing
    
    all
     
  
  
    
      owl:equivalentProperty, owl:inverseOf 
    
    all
    
      
    
  
  
    
      owl:FunctionalProperty, owl:InverseFunctionalProperty
    
    all
    
      
    
  
  
    
      owl:SymmetricProperty, owl:TransitiveProperty
    
    all
    
      
    
  
  
    
      owl:someValuesFrom
    
    full, (mini)
    
      
        Full supports both directions (existence of a value implies membership
          of someValuesFrom restriction, membership of someValuesFrom implies
          the existence of a bNode representing the value).
          Mini omits the latter "bNode introduction" which avoids some
          infinite closures.
      
    
  
  
    
      owl:allValuesFrom
    
    full, mini
    
      Partial support, forward direction only (member of a allValuesFrom(p,
        C) implies that all p values are of type C). Does handle cases where the
        reverse direction is trivially true (e.g. by virtue of a global rdfs:range
        axiom). 
    
  
  
    
      owl:minCardinality, owl:maxCardinality, owl:cardinality
    
    full, (mini)
    
      
        Restricted to cardinalities of 0 or 1, though higher cardinalities
          are partially supported in validation for the case of literal-valued
          properties.
          Mini omits the bNodes introduction in the minCardinality(1) case, see
          someValuesFrom above.
      
    
  
  
    
      owl:hasValue
    
    all
    
      
    
  

The critical constructs which go beyond OWL/lite and are not supported in the
  Jena OWL reasoner are complementOf and oneOf. As noted above the support for
  unionOf is partial (due to limitations of the rule based approach) but is useful
  for traversing class hierarchies.
Even within these constructs rule based implementations are limited in the
  extent to which they can handle equality reasoning - propositions provable by
  reasoning over concrete and introduced instances are covered but reasoning by
  cases is not supported.
Nevertheless, the full reasoner passes the normative OWL working group positive
  and negative entailment tests for the supported constructs, though some tests
  need modification for the comprehension axioms (see below).
The OWL rule set does include incomplete support for validation of datasets
  using the above constructs. Specifically, it tests for:

  Illegal existence of a property restricted by a maxCardinality(0) restriction.
  Two individuals both sameAs and differentFrom each other.
  Two classes declared as disjoint but where one subsumes the other (currently
    reported as a violation concerning the class prototypes, error message to
    be improved).
  Range or a allValuesFrom violations for DatatypeProperties.
  Too many literal-values for a DatatypeProperty restricted by a maxCardinality(N)
    restriction.

[OWL Index] [Main Index]
OWL Configuration
This reasoner is accessed using ModelFactory.createOntologyModel
  with the prebuilt OntSpecification
  OWL*_MEM_RULES_INF or manually via ReasonerRegistry.getOWLReasoner().
There are no OWL-specific configuration parameters though the reasoner supports
  the standard control parameters:

  
    Parameter
    Values
    Description
  
  
    
      PROPtraceOn
    
    boolean
    
      If true switches on exhaustive tracing of rule executions
        at the INFO level.
    
  
  
    
      PROPderivationLogging
    
    Boolean
    
      If true causes derivation routes to be recorded internally
        so that future getDerivation calls can return useful information.
    
  

As we gain experience with the ways in which OWL is used and the capabilities
  of the rule-based approach we imagine useful subsets of functionality emerging
  - like that supported by the RDFS reasoner in the form of the level settings.
[OWL Index] [Main Index]
OWL Example
As an example of using the OWL inference support, consider the sample schema
  and data file in the data directory - owlDemoSchema.rdf
  and owlDemoData.rdf. 
The schema file shows a simple, artificial ontology concerning computers which
  defines a GamingComputer as a Computer which includes at least one bundle of
  type GameBundle and a component with the value gamingGraphics. 
The data file shows information on several hypothetical computer configurations
  including two different descriptions of the configurations "whiteBoxZX"
  and "bigName42".
We can create an instance of the OWL reasoner, specialized to the demo schema
  and then apply that to the demo data to obtain an inference model, as follows:
Model schema = RDFDataMgr.loadModel("file:data/owlDemoSchema.rdf");
Model data = RDFDataMgr.loadModel("file:data/owlDemoData.rdf");
Reasoner reasoner = ReasonerRegistry.getOWLReasoner();
reasoner = reasoner.bindSchema(schema);
InfModel infmodel = ModelFactory.createInfModel(reasoner, data);
A typical example operation on such a model would be to find out all we know
  about a specific instance, for example the nForce mother board.
  This can be done using:
Resource nForce = infmodel.getResource("urn:x-hp:eg/nForce");
System.out.println("nForce *:");
printStatements(infmodel, nForce, null, null);
 where printStatements is defined by: 
public void printStatements(Model m, Resource s, Property p, Resource o) {
    for (StmtIterator i = m.listStatements(s,p,o); i.hasNext(); ) {
        Statement stmt = i.nextStatement();
        System.out.println(" - " + PrintUtil.print(stmt));
    }
}
This produces the output:
nForce *:
 - (eg:nForce rdf:type owl:Thing)
 - (eg:nForce owl:sameAs eg:unknownMB)
 - (eg:nForce owl:sameAs eg:nForce)
 - (eg:nForce rdf:type eg:MotherBoard)
 - (eg:nForce rdf:type rdfs:Resource)
 - (eg:nForce rdf:type a3b24:f7822755ad:-7ffd)
 - (eg:nForce eg:hasGraphics eg:gamingGraphics)
 - (eg:nForce eg:hasComponent eg:gamingGraphics)
Note that this includes inferences based on subClass inheritance (being an
  eg:MotherBoard implies it is an owl:Thing and an rdfs:Resource),
  property inheritance (eg:hasComponent eg:gameGraphics derives from
  hasGraphics being a subProperty of hasComponent) and
  cardinality reasoning (it is the sameAs eg:unknownMB because computers
  are defined to have only one motherboard and the two different descriptions
  of whileBoxZX use these two different terms for the mother board).
  The anonymous rdf:type statement references the "hasValue(eg:hasComponent,
  eg:gamingGraphics)" restriction mentioned in the definition of GamingComputer.
A second, typical operation is instance recognition. Testing if an individual
  is an instance of a class expression. In this case the whileBoxZX
  is identifiable as a GamingComputer because it is a Computer,
  is explicitly declared as having an appropriate bundle and can be inferred to
  have a gamingGraphics component from the combination of the nForce
  inferences we've already seen and the transitivity of hasComponent.
  We can test this using:
Resource gamingComputer = infmodel.getResource("urn:x-hp:eg/GamingComputer");
Resource whiteBox = infmodel.getResource("urn:x-hp:eg/whiteBoxZX");
if (infmodel.contains(whiteBox, RDF.type, gamingComputer)) {
    System.out.println("White box recognized as gaming computer");
} else {
    System.out.println("Failed to recognize white box correctly");
}
 Which generates the output:
  White box recognized as gaming computer
Finally, we can check for inconsistencies within the data by using the validation
  interface:
ValidityReport validity = infmodel.validate();
if (validity.isValid()) {
    System.out.println("OK");
} else {
    System.out.println("Conflicts");
    for (Iterator i = validity.getReports(); i.hasNext(); ) {
        ValidityReport.Report report = (ValidityReport.Report)i.next();
        System.out.println(" - " + report);
    }
}
Which generates the output:
Conflicts
 - Error (conflict): Two individuals both same and different, may be
   due to disjoint classes or functional properties
Culprit = eg:nForce2
Implicated node: eg:bigNameSpecialMB
… + 3 other similar reports

This is due to the two records for the bigName42 configuration
  referencing two motherboards which are explicitly defined to be different resources
  and thus violate the FunctionProperty nature of hasMotherBoard.
[OWL Index] [Main Index]
OWL notes and limitations
Comprehension axioms
A critical implication of our variant of the instance-based approach is that
  the reasoner does not directly answer queries relating to dynamically introduced
  class expressions.
For example, given a model containing the RDF assertions corresponding to the
  two OWL axioms:
class A = intersectionOf (minCardinality(P, 1), maxCardinality(P,1))
class B = cardinality(P,1)
Then the reasoner can demonstrate that classes A and B are equivalent, in particular
  that any instance of A is an instance of B and vice versa. However, given a
  model just containing the first set of assertions you cannot directly query
  the inference model for the individual triples that make up cardinality(P,1).
  If the relevant class expressions are not already present in your model then
  you need to use the list-with-posits mechanism described above,
  though be warned that such posits start inference afresh each time and can be
  expensive. 
Actually, it would be possible to introduce comprehension axioms for simple
  cases like this example. We have, so far, chosen not to do so. First, since
  the OWL/full closure is generally infinite, some limitation on comprehension
  inferences seems to be useful. Secondly, the typical queries that Jena applications
  expect to be able to issue would suddenly jump in size and cost - causing a
  support nightmare. For example, queries such as (a, rdf:type, *) would become
  near-unusable.
Approximately, 10 of the OWL working group tests for the supported OWL subset
  currently rely on such comprehension inferences. The shipping version of the
  Jena rule reasoner passes these tests only after they have been rewritten to
  avoid the comprehension requirements.

Prototypes
As noted above the current OWL rule set introduces prototypical instances for
  each defined class. These prototypical instances used to be visible to queries.
  From release 2.1 they are used internally but should not longer be visible.

Direct/indirect
We noted above that the Jena reasoners support
  a separation of direct and indirect relations for transitive properties such
  as subClassOf. The current implementation of the full and mini OWL reasoner
  fails to do this and the direct forms of the queries will fail. The OWL Micro
  reasoner, which is but a small extension of RDFS, does support the direct queries.
This does not affect querying though the Ontology API, which works around this
  limitation. It only affects direct RDF accesses to the inference model.
Performance
The OWL reasoners use the rule engines for all inference. The full and mini
  configurations omit some of the performance tricks employed by the RDFS reasoner
  (notably the use of the custom transitive reasoner) making those OWL reasoner
  configurations slower than the RDFS reasoner on pure RDFS data (typically around
  x3-4 slow down). The OWL Micro reasoner is intended to be as close to RDFS performance
  while also supporting the core OWL constructs as described earlier.
Once the owl constructs are used then substantial reasoning can be required.
  The most expensive aspect of the supported constructs is the equality reasoning
  implied by use of cardinality restrictions and FunctionalProperties. The current
  rule set implements equality reasoning by identifying all sameAs deductions
  during the initial forward "prepare" phase. This may require the entire
  instance dataset to be touched several times searching for occurrences of FunctionalProperties.
Beyond this the rules implementing the OWL constructs can interact in complex
  ways leading to serious performance overheads for complex ontologies. Characterising
  the sorts of ontologies and inference problems that are well tackled by this
  sort of implementation and those best handled by plugging a Description Logic
  engine, or a saturation theorem prover, into Jena is a topic for future work.
One random hint: explicitly importing the owl.owl definitions causes much duplication
  of rule use and a substantial slow down - the OWL axioms that the reasoner can
  handle are already built in and don't need to be redeclared.
Incompleteness
The rule based approach cannot offer a complete solution for OWL/Lite, let
  alone the OWL/Full fragment corresponding to the OWL/Lite constructs. In addition
  the current implementation is still under development and may well have omissions
  and oversights. We intend that the reasoner should be sound (all inferred triples
  should be valid) but not complete. 
[OWL Index] [Main Index]
The transitive reasoner
The TransitiveReasoner provides support for storing and traversing class and
  property lattices. This implements just the transitive and symmetric
  properties of rdfs:subPropertyOf and rdfs:subClassOf.
  It is not all that exciting on its own but is one of the building blocks used
  for the more complex reasoners. It is a hardwired Java implementation that stores
  the class and property lattices as graph structures. It is slightly higher performance,
  and somewhat more space efficient, than the alternative of using the pure rule
  engines to performance transitive closure but its main advantage is that it
  implements the direct/minimal version of those relations as well as the transitively
  closed version.
The GenericRuleReasoner (see below) can optionally use an instance
  of the transitive reasoner for handling these two properties. This is the approach
  used in the default RDFS reasoner.
It has no configuration options.
[Index]
The general purpose rule engine

  Overview of the rule engine(s)
  Rule syntax and structure
  Forward chaining engine
  Backward chaining engine
  Hybrid engine
  GenericRuleReasoner configuration
  Builtin primitives
  Example
  Combining RDFS/OWL with custom rules
  Notes
  Extensions

Overview of the rule engine(s)
Jena includes a general purpose rule-based reasoner which is used to implement
  both the RDFS and OWL reasoners but is also available for general use. This
  reasoner supports rule-based inference over RDF graphs and provides forward
  chaining, backward chaining and a hybrid execution model. To be more exact,
  there are two internal rule engines one forward chaining RETE engine and one
  tabled datalog engine - they can be run separately or the forward engine can
  be used to prime the backward engine which in turn will be used to answer queries.
The various engine configurations are all accessible through a single parameterized
  reasoner GenericRuleReasoner.
  At a minimum a GenericRuleReasoner requires a ruleset to define
  its behaviour. A GenericRuleReasoner instance with a ruleset can
  be used like any of the other reasoners described above - that is it can be
  bound to a data model and used to answer queries to the resulting inference
  model. 
The rule reasoner can also be extended by registering new procedural primitives.
  The current release includes a starting set of primitives which are sufficient
  for the RDFS and OWL implementations but is easily extensible.
[Rule Index] [Main Index]
Rule syntax and structure
A rule for the rule-based reasoner is defined by a Java Rule
  object with a list of body terms (premises), a list of head terms (conclusions)
  and an optional name and optional direction. Each term or ClauseEntry
  is either a triple pattern, an extended triple pattern or a call to a builtin
  primitive. A rule set is simply a List of Rules.
For convenience a rather simple parser is included with Rule which allows rules
  to be specified in reasonably compact form in text source files. However, it
  would be perfectly possible to define alternative parsers which handle rules
  encoded using, say, XML or RDF and generate Rule objects as output. It would
  also be possible to build a real parser for the current text file syntax which
  offered better error recovery and diagnostics.
An informal description of the simplified text rule syntax is:
Rule      :=   bare-rule .
          or   [ bare-rule ]       or   [ ruleName : bare-rule ]
bare-rule :=   term, … term -> hterm, … hterm    // forward rule
or   bhterm <- term, … term    // backward rule
hterm     :=   term
or   [ bare-rule ]
term      :=   (node, node, node)           // triple pattern
or   (node, node, functor)        // extended triple pattern
or   builtin(node, … node)      // invoke procedural primitive
bhterm      :=   (node, node, node)           // triple pattern
functor   :=   functorName(node, … node)  // structured literal
node      :=   uri-ref                   // e.g. http://foo.com/eg
or   prefix:localname          // e.g. rdf:type
or   <uri-ref>          // e.g. <myscheme:myuri>
or   ?varname                    // variable
or   ‘a literal’                 // a plain string literal
or   ’lex’^^typeURI              // a typed literal, xsd:* type names supported
or   number                      // e.g. 42 or 25.5
The "," separators are optional.
The difference between the forward and backward rule syntax is only relevant
  for the hybrid execution strategy, see below.
The functor in an extended triple pattern is used to create and access
  structured literal values. The functorName can be any simple identifier and
  is not related to the execution of builtin procedural primitives, it is just
  a datastructure. It is useful when a single semantic structure is defined across
  multiple triples and allows a rule to collect those triples together in one
  place.
To keep rules readable qname syntax is supported for URI refs. The set of known
  prefixes is those registered with the PrintUtil
  object. This initially knows about rdf, rdfs, owl, xsd and a test namespace
  eg, but more mappings can be registered in java code. In addition it is possible to
  define additional prefix mappings in the rule file, see below. 
Here are some example rules which illustrate most of these constructs:
[allID: (?C rdf:type owl:Restriction), (?C owl:onProperty ?P),
     (?C owl:allValuesFrom ?D)  -> (?C owl:equivalentClass all(?P, ?D)) ]
[all2: (?C rdfs:subClassOf all(?P, ?D)) -> print(‘Rule for ‘, ?C)
[all1b: (?Y rdf:type ?D) <- (?X ?P ?Y), (?X rdf:type ?C) ] ]
[max1: (?A rdf:type max(?P, 1)), (?A ?P ?B), (?A ?P ?C)
-> (?B owl:sameAs ?C) ]

Rule allID illustrates the functor use for collecting the components
  of an OWL restriction into a single datastructure which can then fire further
  rules. Rule all2 illustrates a forward rule which creates a new
  backward rule and also calls the print procedural primitive. Rule
  max1 illustrates use of numeric literals.


Rule files may be loaded and parsed using:
List rules = Rule.rulesFromURL("file:myfile.rules");
or
BufferedReader br = /* open reader */ ;
List rules = Rule.parseRules( Rule.rulesParserFromReader(br) );
or
String ruleSrc = /* list of rules in line */
List rules = Rule.parseRules( rulesSrc );
In the first two cases (reading from a URL or a BufferedReader) the rule file is
preprocessed by a simple processor which strips comments and supports some additional
macro commands:

# ...
A comment line.
// ...
A comment line.
@prefix pre: <http://domain/url#>.
Defines a prefix pre which can be used in the rules.
The prefix is local to the rule file.
@include <urlToRuleFile>.
Includes the rules defined in the given file in this file. The included rules
will appear before the user defined rules, irrespective of where in the file
the @include directive appears. A set of special cases is supported to allow
a rule file to include the predefined rules for RDFS and OWL - in place of a real
URL for a rule file use one of the keywords
RDFS
OWL
OWLMicro
OWLMini (case insensitive).


So an example complete rule file which includes the RDFS rules and defines
a single extra rule is:
# Example rule file
@prefix pre: <http://jena.hpl.hp.com/prefix#>.
@include <RDFS>.

[rule1: (?f pre:father ?a) (?u pre:brother ?f) -> (?u pre:uncle ?a)]
[Rule Index] [Main Index]
Forward chaining engine
If the rule reasoner is configured to run in forward mode then only the forward
  chaining engine will be used. The first time the inference Model is queried
  (or when an explicit prepare() call is made, see above)
  then all of the relevant data in the model will be submitted to the rule engine.
  Any rules which fire that create additional triples do so in an internal deductions
  graph and can in turn trigger additional rules. There is a remove primitive
  that can be used to remove triples and such removals can also trigger rules
  to fire in removal mode. This cascade of rule firings continues until no more
  rules can fire. It is perfectly possible, though not a good idea, to write rules
  that will loop infinitely at this point.
Once the preparation phase is complete the inference graph will act as if it
  were the union of all the statements in the original model together with all
  the statements in the internal deductions graph generated by the rule firings.
  All queries will see all of these statements and will be of similar speed to
  normal model accesses. It is possible to separately access the original raw
  data and the set of deduced statements if required, see above.
If the inference model is changed by adding or removing statements through
  the normal API then this will trigger further rule firings. The forward rules
  work incrementally and only the consequences of the added or removed triples
  will be explored. The default rule engine is based on the standard RETE algorithm
  (C.L Forgy, RETE: A fast algorithm for the many pattern/many object pattern
  match problem, Artificial Intelligence 1982) which is optimized for such
  incremental changes. 
When run in forward mode all rules are treated as forward even if they were
  written in backward ("<-") syntax. This allows the same rule set
  to be used in different modes to explore the performance tradeoffs.
There is no guarantee of the order in which matching rules will fire or the
  order in which body terms will be tested, however once a rule fires its head-terms
  will be executed in left-to-right sequence.
In forward mode then head-terms which assert backward rules (such as all1b
  above) are ignored.
There are in fact two forward engines included within the Jena code base,
  an earlier non-RETE implementation is retained for now because it can be more
  efficient in some circumstances but has identical external semantics. This alternative
  engine is likely to be eliminated in a future release once more tuning has been
  done to the default RETE engine.
[Rule Index] [Main Index]
Backward chaining engine
If the rule reasoner is run in backward chaining mode it uses a logic programming
  (LP) engine with a similar execution strategy to Prolog engines. When the inference
  Model is queried then the query is translated into a goal and the engine attempts
  to satisfy that goal by matching to any stored triples and by goal resolution
  against the backward chaining rules.
Except as noted below rules will be executed in top-to-bottom, left-to-right
  order with backtracking, as in SLD resolution. In fact, the rule language is
  essentially datalog rather than full prolog, whilst the functor syntax within
  rules does allow some creation of nested data structures they are flat (not
  recursive) and so can be regarded a syntactic sugar for datalog.
As a datalog language the rule syntax is a little surprising because it restricts
  all properties to be binary (as in RDF) and allows variables in any position
  including the property position. In effect, rules of the form:
(s, p, o), (s1, p1, o1) ... <- (sb1, pb1, ob1), .... 
Can be thought of as being translated to datalog rules of the form:
triple(s, p, o)    :- triple(sb1, pb1, ob1), ...
triple(s1, p1, o1) :- triple(sb1, pb1, ob1), ...
...
where "triple/3" is a hidden implicit predicate. Internally, this
  transformation is not actually used, instead the rules are implemented directly.
In addition, all the data in the raw model supplied to the engine is treated
  as if it were a set of triple(s,p,o) facts which are prepended to
  the front of the rule set. Again, the implementation does not actually work
  that way but consults the source graph, with all its storage and indexing capabilities,
  directly.
Because the order of triples in a Model is not defined then this is one violation
  to strict top-to-bottom execution. Essentially all ground facts are consulted
  before all rule clauses but the ordering of ground facts is arbitrary.
Tabling
The LP engine supports tabling. When a goal is tabled then all previously computed
  matches to that goal are recorded (memoized) and used when satisfying future
  similar goals. When such a tabled goal is called and all known answers have
  been consumed then the goal will suspend until some other execution branch has
  generated new results and then be resumed. This allows one to successfully run
  recursive rules such as transitive closure which would be infinite loops in
  normal SLD prolog. This execution strategy, SLG, is essentially the same as
  that used in the well known XSB system.
In the Jena rule engine the goals to be tabled are identified by the property
  field of the triple. One can request that all goals be tabled by calling the
  tableAll() primitive or that all goals involving a given property
  P be tabled by calling table(P). Note that if any
  property is tabled then goals such as (A, ?P, ?X) will all be tabled
  because the property variable might match one of the tabled properties.
Thus the rule set:
-> table(rdfs:subClassOf).
[r1: (?A rdfs:subClassOf ?C) <- (?A rdfs:subClassOf ?B) (?B rdfs:subClassOf ?C)]
will successfully compute the transitive closure of the subClassOf relation.
  Any query of the form (*, rdfs:subClassOf, *) will be satisfied by a mixture
  of ground facts and resolution of rule r1. Without the first line this rule
  would be an infinite loop. 
The tabled results of each query are kept indefinitely. This means that queries
  can exploit all of the results of the subgoals involved in previous queries.
  In essence we build up a closure of the data set in response to successive queries.
  The reset() operation on the inference model will force these tabled
  results to be discarded, thus saving memory at the expense of response time
  for future queries.
When the inference Model is updated by adding or removing statements all tabled
  results are discarded by an internal reset() and the next query
  will rebuild the tabled results from scratch. 

Note that backward rules can only have one consequent so that if writing rules that
might be run in either backward or forward mode then they should be limited to a single consequent each.

[Rule Index] [Main Index]
Hybrid rule engine
The rule reasoner has the option of employing both of the individual rule engines
  in conjunction. When run in this hybrid mode the data flows look something
  like this: 

The forward engine runs, as described above, and maintains a set
  of inferred statements in the deductions store. Any forward rules which
  assert new backward rules will instantiate those rules according to the forward
  variable bindings and pass the instantiated rules on to the backward engine.
Queries are answered by using the backward chaining LP engine,
  employing the merge of the supplied and generated rules applied to the merge
  of the raw and deduced data.
This split allows the ruleset developer to achieve greater performance
  by only including backward rules which are relevant to the dataset at hand.
  In particular, we can use the forward rules to compile a set of backward rules
  from the ontology information in the dataset. As a simple example consider trying
  to implement the RDFS subPropertyOf entailments using a rule engine. A simple
  approach would involve rules like:
 (?a ?q ?b) <- (?p rdfs:subPropertyOf ?q), (?a ?p ?b) .

Such a rule would work but every goal would match the head of
  this rule and so every query would invoke a dynamic test for whether there was
  a subProperty of the property being queried for. Instead the hybrid rule:
(?p rdfs:subPropertyOf ?q), notEqual(?p,?q) -> [ (?a ?q ?b) <- (?a ?p ?b) ] .
would precompile all the declared subPropertyOf relationships
  into simple chain rules which would only fire if the query goal references a
  property which actually has a sub property. If there are no subPropertyOf relationships
  then there will be no overhead at query time for such a rule.
Note that there are no loops in the above data flows. The backward
  rules are not employed when searching for matches to forward rule terms. This
  two-phase execution is simple to understand and keeps the semantics of the rule
  engines straightforward. However, it does mean that care needs to be take when
  formulating rules. If in the above example there were ways that the subPropertyOf
  relation could be derived from some other relations then that derivation would
  have to be accessible to the forward rules for the above to be complete.
Updates to an inference Model working in hybrid mode will discard
  all the tabled LP results, as they do in the pure backward case. However, the
  forward rules still work incrementally, including incrementally asserting or
  removing backward rules in response to the data changes.
[Rule Index] [Main Index]
GenericRuleReasoner configuration
As with the other reasoners there are a set of parameters, identified by RDF
  properties, to control behaviour of the GenericRuleReasoner. These
  parameters can be set using the Reasoner.setParameter call or passed
  into the Reasoner factory in an RDF Model.
The primary parameter required to instantiate a useful GenericRuleReasoner
  is a rule set which can be passed into the constructor, for example:
String ruleSrc = "[rule1: (?a eg:p ?b) (?b eg:p ?c) -&gt; (?a eg:p ?c)]";
List rules = Rule.parseRules(ruleSrc);
...
Reasoner reasoner = new GenericRuleReasoner(rules);</pre>
A short cut, useful when the rules are defined in local text files using the
  syntax described earlier, is the ruleSet parameter which gives
  a file name which should be loadable from either the classpath or relative to
  the current working directory.

Summary of parameters

  
    Parameter
    Values
    Description
  
  
    
      PROPruleMode
    
    "forward", "forwardRETE", "backward",
      "hybrid" 
    
      Sets the rule direction mode as discussed above. Default
        is "hybrid".
    
  
  
    
      PROPruleSet
    
    filename-string
    
      The name of a rule text file which can be found on the
        classpath or from the current directory. 
    
  
  
    
      PROPenableTGCCaching
    
    Boolean
    
      If true, causes an instance of the TransitiveReasoner
        to be inserted in the forward dataflow to cache the transitive closure
        of the subProperty and subClass lattices.
    
  
  
    
      PROPenableFunctorFiltering
    
    Boolean
    
      If set to true, this causes the structured literals (functors)
        generated by rules to be filtered out of any final queries. This allows
        them to be used for storing intermediate results hidden from the view
        of the InfModel's clients.
    
  
  
    
      PROPenableOWLTranslation
    
    Boolean
    
      If set to true this causes a procedural preprocessing
        step to be inserted in the dataflow which supports the OWL reasoner (it
        translates intersectionOf clauses into groups of backward rules in a way
        that is clumsy to express in pure rule form).
    
  
  
    
      PROPtraceOn
    
    Boolean
    
      If true, switches on exhaustive tracing of rule executions
        at the INFO level.
    
  
  
    
      PROPderivationLogging
    
    Boolean
    
      If true, causes derivation routes to be recorded internally
        so that future getDerivation calls can return useful information.
    
  

[Rule Index] [Main Index]
Builtin primitives
The procedural primitives which can be called by the rules are each implemented
  by a Java object stored in a registry. Additional primitives can be created
  and registered - see below for more details.
Each primitive can optionally be used in either the rule body, the rule head
  or both. If used in the rule body then as well as binding variables (and any
  procedural side-effects like printing) the primitive can act as a test - if
  it returns false the rule will not match. Primitives used in the rule head
  are only used for their side effects.
The set of builtin primitives available at the time writing are:

  
    Builtin
    Operations
  
  
    
      isLiteral(?x) notLiteral(?x)
        isFunctor(?x) notFunctor(?x)
        isBNode(?x) notBNode(?x)
    
    
      Test whether the single argument is or is not a literal,
        a functor-valued literal or a blank-node, respectively.
    
  
  
    bound(?x...) unbound(?x..)
    
      Test if all of the arguments are bound (not bound) variables
    
  
  
    equal(?x,?y) notEqual(?x,?y)
    
      Test if x=y (or x != y). The equality test is semantic
        equality so that, for example, the xsd:int 1 and the xsd:decimal 1 would
        test equal.
    
  
  
    
      lessThan(?x, ?y), greaterThan(?x, ?y)
        le(?x, ?y), ge(?x, ?y)
    
    
      Test if x is <, >, <= or >= y. Only passes
        if both x and y are numbers or time instants (can be integer or
floating point or XSDDateTime).
    
  
  
    
      sum(?a, ?b, ?c)
        addOne(?a, ?c)
        difference(?a, ?b, ?c)
        min(?a, ?b, ?c)
        max(?a, ?b, ?c)
        product(?a, ?b, ?c)
        quotient(?a, ?b, ?c)
      
    
      Sets c to be (a+b), (a+1) (a-b), min(a,b), max(a,b), (a*b), (a/b). Note that these
        do not run backwards, if in sum a and c are bound and b is
        unbound then the test will fail rather than bind b to (c-a). This could
        be fixed.
    
  
  
    
      strConcat(?a1, .. ?an, ?t)
        uriConcat(?a1, .. ?an, ?t)
      
    
      Concatenates the lexical form of all the arguments except
      the last, then binds the last argument to a plain literal (strConcat) or a
      URI node (uriConcat) with that lexical form. In both cases if an argument
      node is a URI node the URI will be used as the lexical form.
    
  
  
    
      regex(?t, ?p)regex(?t, ?p, ?m1, .. ?mn)
      
    
      Matches the lexical form of a literal (?t) against
      a regular expression pattern given by another literal (?p).
      If the match succeeds, and if there are any additional arguments then
      it will bind the first n capture groups to the arguments ?m1 to ?mn.
      The regular expression pattern syntax is that provided by java.util.regex.
      Note that the capture groups are numbered from 1 and the first capture group
      will be bound to ?m1, we ignore the implicit capture group 0 which corresponds to
      the entire matched string. So for example
      regexp('foo bar', '(.*) (.*)', ?m1, ?m2)
      will bind m1 to "foo" and m2 to "bar".
    
  
  
    now(?x)
    
      Binds ?x to an xsd:dateTime value corresponding to the current time.
    
  
  
    makeTemp(?x)
    
      Binds ?x to a newly created blank node.
    
  
  
    makeInstance(?x, ?p, ?v)
      makeInstance(?x, ?p, ?t, ?v)
    
      Binds ?v to be a blank node which is asserted as the value
        of the ?p property on resource ?x and optionally has type ?t. Multiple
        calls with the same arguments will return the same blank node each time
        - thus allowing this call to be used in backward rules.
    
  
  
    makeSkolem(?x, ?v1, ... ?vn)
    
      Binds ?x to be a blank node. The blank node is generated
      based on the values of the remain ?vi arguments, so the same combination of
      arguments will generate the same bNode.
    
  
  
    noValue(?x, ?p)
      noValue(?x ?p ?v)
    
      True if there is no known triple (x, p, *) or (x, p, v)
        in the model or the explicit forward deductions so far. 
    
  
  
    remove(n, ...)drop(n, ...)
    
      Remove the statement (triple) which caused the n'th body
        term of this (forward-only) rule to match. Remove will propagate the
        change to other consequent rules including the firing rule (which must
        thus be guarded by some other clauses).
        In particular, if the removed statement (triple) appears in the body of
        a rule that has already fired, the consequences of such rule are
        retracted from the deducted model.
         Drop will silently remove the
        triple(s) from the graph but not fire any rules as a consequence.
        These are clearly non-monotonic operations and, in particular, the
        behaviour of a rule set in which different rules both drop and create the
        same triple(s) is undefined.
    
  
  
    isDType(?l, ?t) notDType(?l, ?t)
    
      Tests if literal ?l is (or is not) an instance of the
        datatype defined by resource ?t.
    
  
  
    print(?x, ...)
    
      Print (to standard out) a representation of each argument.
        This is useful for debugging rather than serious IO work.
    
  
  
    listContains(?l, ?x) listNotContains(?l, ?x)
    
      Passes if ?l is a list which contains (does not contain) the element ?x,
      both arguments must be ground, can not be used as a generator.
    
  
  
    listEntry(?list, ?index, ?val)
    
      Binds ?val to the ?index'th entry
in the RDF list ?list. If there is no such entry the variable will be unbound
and the call will fail. Only usable in rule bodies.
    
  
  
    listLength(?l, ?len)
    
      Binds ?len to the length of the list ?l.
    
  
  
    listEqual(?la, ?lb) listNotEqual(?la, ?lb)
    
      listEqual tests if the two arguments are both lists and contain
      the same elements. The equality test is semantic equality on literals (sameValueAs) but
      will not take into account owl:sameAs aliases. listNotEqual is the negation of this (passes if listEqual fails).
    
  
  
    listMapAsObject(?s, ?p ?l)  listMapAsSubject(?l, ?p, ?o)
    
      These can only be used as actions in the head of a rule.
      They deduce a set of triples derived from the list argument ?l : listMapAsObject asserts
      triples (?s ?p ?x) for each ?x in the list ?l, listMapAsSubject asserts triples (?x ?p ?o). 
    
  
  
    table(?p) tableAll()
    
      Declare that all goals involving property ?p (or all goals)
        should be tabled by the backward engine.
    
  
  
    hide(p)
    
      Declares that statements involving the predicate p should be hidden.
Queries to the model will not report such statements. This is useful to enable non-monotonic
forward rules to define flag predicates which are only used for inference control and
do not "pollute" the inference results.
    
  

[Rule Index] [Main Index]
Example
As a simple illustration suppose we wish to create a simple ontology language
  in which we can declare one property as being the concatenation of two others
  and to build a rule reasoner to implement this.
As a simple design we define two properties eg:concatFirst, eg:concatSecond
  which declare the first and second properties in a concatenation. Thus the triples:
eg:r eg:concatFirst  eg:p .
eg:r eg:concatSecond eg:q .
mean that the property r = p o q.
Suppose we have a Jena Model rawModel which contains the above assertions together
  with the additional facts:
eg:A eg:p eg:B .
eg:B eg:q eg:C .
Then we want to be able to conclude that A is related to C through the composite
  relation r. The following code fragment constructs and runs a rule reasoner
  instance to implement this:
String rules =
    "[r1: (?c eg:concatFirst ?p), (?c eg:concatSecond ?q) -&gt; " +
    "     [r1b: (?x ?c ?y) &lt;- (?x ?p ?z) (?z ?q ?y)] ]";
Reasoner reasoner = new GenericRuleReasoner(Rule.parseRules(rules));
InfModel inf = ModelFactory.createInfModel(reasoner, rawData);
System.out.println("A * * =&gt;");
Iterator list = inf.listStatements(A, null, (RDFNode)null);
while (list.hasNext()) {
    System.out.println(" - " + list.next());
}
When run on a rawData model contain the above four triples this generates the
  (correct) output:
A * * =>
 - [urn:x-hp:eg/A, urn:x-hp:eg/p, urn:x-hp:eg/B]
 - [urn:x-hp:eg/A, urn:x-hp:eg/r, urn:x-hp:eg/C]
Example 2
As a second example, we'll look at ways to define a property as being both
  symmetric and transitive. Of course, this can be done directly in OWL but there
  are times when one might wish to do this outside of the full OWL rule set and,
  in any case, it makes for a compact illustration.
This time we'll put the rules in a separate file to simplify editing them and
  we'll use the machinery for configuring a reasoner using an RDF specification.
  The code then looks something like this:
// Register a namespace for use in the demo
String demoURI = "http://jena.hpl.hp.com/demo#";
PrintUtil.registerPrefix("demo", demoURI);

// Create an (RDF) specification of a hybrid reasoner which
// loads its data from an external file.
Model m = ModelFactory.createDefaultModel();
Resource configuration =  m.createResource();
configuration.addProperty(ReasonerVocabulary.PROPruleMode, "hybrid");
configuration.addProperty(ReasonerVocabulary.PROPruleSet,  "data/demo.rules");

// Create an instance of such a reasoner
Reasoner reasoner = GenericRuleReasonerFactory.theInstance().create(configuration);

// Load test data
Model data = RDFDataMgr.loadModel("file:data/demoData.rdf");
InfModel infmodel = ModelFactory.createInfModel(reasoner, data);

// Query for all things related to "a" by "p"
Property p = data.getProperty(demoURI, "p");
Resource a = data.getResource(demoURI + "a");
StmtIterator i = infmodel.listStatements(a, p, (RDFNode)null);
while (i.hasNext()) {
    System.out.println(" - " + PrintUtil.print(i.nextStatement()));
}
Here is file data/demo.rules which defines property demo:p
  as being both symmetric and transitive using pure forward rules:
[transitiveRule: (?A demo:p ?B), (?B demo:p ?C) -> (?A > demo:p ?C) ]
[symmetricRule: (?Y demo:p ?X) -> (?X demo:p ?Y) ] 
 Running this on data/demoData.rdf gives the
  correct output:
- (demo:a demo:p demo:c)
- (demo:a demo:p demo:a)
- (demo:a demo:p demo:d)
- (demo:a demo:p demo:b)
However, those example rules are overly specialized. It would be better to
  define a new class of property to indicate symmetric-transitive properties and
  and make demo:p a member of that class. We can generalize the rules
  to support this:
[transitiveRule: (?P rdf:type demo:TransProp)(?A ?P ?B), (?B ?P ?C)
                     -> (?A ?P ?C) ]
[symmetricRule: (?P rdf:type demo:TransProp)(?Y ?P ?X)
                     -> (?X ?P ?Y) ]
 These rules work but they compute the complete symmetric-transitive closure
  of p when the graph is first prepared. Suppose we have a lot of p values but
  only want to query some of them it would be better to compute the closure on
  demand using backward rules. We could do this using the same rules run in pure
  backward mode but then the rules would fire lots of times as they checked every
  property at query time to see if it has been declared as a demo:TransProp.
  The hybrid rule system allows us to get round this by using forward rules to
  recognize any demo:TransProp declarations once and to generate
  the appropriate backward rules:
-> tableAll().
[rule1: (?P rdf:type demo:TransProp) ->
[ (?X ?P ?Y) <- (?Y ?P ?X) ]
[ (?A ?P ?C) <- (?A ?P ?B), (?B ?P ?C) ]
] 
[Rule Index] [Main Index]
Combining RDFS/OWL with custom rules
Sometimes one wishes to write generic inference rules but combine them
 with some RDFS or OWL inference. With the current Jena architecture limited forms of this
 is possible but you need to be aware of the limitations.
There are two ways of achieving this sort of configuration within Jena (not
 counting using an external engine that already supports such a combination).
Firstly, it is possible to cascade reasoners, i.e. to construct one InfModel
 using another InfModel as the base data. The strength of this approach is that
 the two inference processes are separate and so can be of different sorts. For
 example one could create a GenericRuleReasoner whose base model is an external
 OWL reasoner. The chief weakness of the approach is that it is "layered" - the
 outer InfModel can see the results of the inner InfModel but not vice versa.
 For some applications that layering is fine and it is clear which way the
 inference should be layered, for some it is not. A second possible weakness
 is performance. A query to an InfModel is generally expensive and involves lots
 of queries to the data. The outer InfModel in our layered case will
 typically issue a lot of queries to the inner model, each of which may
 trigger more inference. If the inner model caches all of its inferences
 (e.g. a forward rule engine) then there may not be very much redundancy there but
 if not then performance can suffer dramatically. 
Secondly, one can create a single GenericRuleReasoner whose rules combine
 rules for RDFS or OWL and custom rules. At first glance this looks like it
 gets round the layering limitation. However, the default Jena RDFS and OWL
 rulesets use the Hybrid rule engine. The hybrid engine is itself layered, forward rules
 do not see the results of any backward rules. Thus layering is still present though you
 have finer grain control - all your inferences you want the RDFS/OWL rules to see
 should be forward, all the inferences which need all of the results of the RDFS/OWL rules
 should be backward. Note that the RDFS and OWL rulesets assume certain settings
 for the GenericRuleReasoner so a typical configuration is:
Model data = RDFDataMgr.loadModel("file:data.n3");
List rules = Rule.rulesFromURL("myrules.rules");

GenericRuleReasoner reasoner = new GenericRuleReasoner(rules);
reasoner.setOWLTranslation(true);               // not needed in RDFS case
reasoner.setTransitiveClosureCaching(true);

InfModel inf = ModelFactory.createInfModel(reasoner, data);
Where the myrules.rules file will use @include to include
 one of the RDFS or OWL rule sets.
One  useful variant on this option, at least in simple cases, is
 to manually include a pure (non-hybrid) ruleset for the RDFS/OWL fragment
 you want so that there is no layering problem. [The reason the default
 rulesets use the hybrid mode is a performance tradeoff - trying to
 balance the better performance of forward reasoning with the cost of
 computing all possible answers when an application might only want a few.]

 A simple example of this is that the interesting bits of RDFS
 can be captured by enabling TransitiveClosureCaching and including just the
 four core rules:
[rdfs2:  (?x ?p ?y), (?p rdfs:domain ?c) -> (?x rdf:type ?c)]
[rdfs3:  (?x ?p ?y), (?p rdfs:range ?c) -> (?y rdf:type ?c)]
[rdfs6:  (?a ?p ?b), (?p rdfs:subPropertyOf ?q) -> (?a ?q ?b)]
[rdfs9:  (?x rdfs:subClassOf ?y), (?a rdf:type ?x) -> (?a rdf:type ?y)]

[Rule Index] [Main Index]
Notes
One final aspect of the general rule engine to mention is that of validation
  rules. We described earlier how reasoners can implement a validate
  call which returns a set of error reports and warnings about inconsistencies
  in a dataset. Some reasoners (e.g. the RDFS reasoner) implement this feature
  through procedural code. Others (e.g. the OWL reasoner) does so using yet more
  rules.
Validation rules take the general form:
(?v rb:validation on()) ...  ->
    [ (?X rb:violation error('summary', 'description', args)) <- ...) ] .
The validation calls can be "switched on" by inserting an
  additional triple into the graph of the form:
_:anon rb:validation on() .
This makes it possible to build rules, such as the template above, which are
  ignored unless validation has been switched on - thus avoiding potential overhead
  in normal operation. This is optional and the "validation on()" guard
  can be omitted.
Then the validate call queries the inference graph for all triples of the form:
?x rb:violation f(summary, description, args) .
The subject resource is the "prime suspect" implicated in the inconsistency,
  the relation rb:violation is a reserved property used to communicate
  validation reports from the rules to the reasoner, the object is a structured
  (functor-valued) literal. The name of the functor indicates the type of violation
  and is normally error or warning, the first argument
  is a short form summary of the type of problem, the second is a descriptive
  text and the remaining arguments are other resources involved in the inconsistency.

Future extensions will improve the formatting capabilities and flexibility
  of this mechanism. 
[Rule Index] [Main Index]
Extensions
There are several places at which the rule system can be extended by application
  code.
Rule syntax
First, as mentioned earlier, the rule engines themselves only see rules in
  terms of the Rule Java object. Thus applications are free to define an alternative
  rule syntax so long as it can be compiled into Rule objects.
Builtins
Second, the set of procedural builtins can be extended. A builtin should implement
  the Builtin
  interface. The easiest way to achieve this is by subclassing BaseBuiltin
  and defining a name (getName), the number of arguments expected
  (getArgLength) and one or both of bodyCall and headAction.
  The bodyCall method is used when the builtin is invoked in the
  body of a rule clause and should return true or false according to whether the
  test passes. In both cases the arguments may be variables or bound values and
  the supplied RuleContext
  object can be used to dereference bound variables and to bind new variables.

Once the Builtin has been defined then an instance of it needs to be registered
  with BuiltinRegistry
  for it to be seen by the rule parser and interpreters.
The easiest way to experiment with this is to look at the examples in the builtins
  directory. 
Preprocessing hooks
The rule reasoner can optionally run a sequence of procedural preprocessing
  hooks over the data at the time the inference graph is prepared. These
  procedural hooks can be used to perform tests or translations which are slow
  or inconvenient to express in rule form. See GenericRuleReasoner.addPreprocessingHook
  and the RulePreprocessHook
  class for more details.
[Index]
Extending the inference support
Apart from the extension points in the rule reasoner discussed above, the intention
  is that it should be possible to plug external inference engines into Jena.
  The core interfaces of InfGraph and Reasoner are kept
  as simple and generic as we can to make this possible and the ReasonerRegistry
  provides a means for mapping from reasoner ids (URIs) to reasoner instances
  at run time.
In a future Jena release we plan to provide at least one adapter to an example,
  freely available, reasoner to both validate the machinery and to provide an
  example of how this extension can be done.
[Index]
Futures
Contributions for the following areas would be very welcome:

  Develop a custom equality reasoner which can handle the "owl:sameAs"
    and related processing more efficiently that the plain rules engine.
  Tune the RETE engine to perform better with highly non-ground patterns.
  Tune the LP engine to further reduce memory usage (in particular explore
    subsumption tabling rather than the current variant tabling).
  Investigate routes to better integrating the rule reasoner with underlying
    database engines. This is a rather larger and longer term task than the others
    above and is the least likely to happen in the near future.

[Index]\n\nOn this page
    
  
    Overview of inference support
      
        Available reasoners
      
    
    The Inference API
      
        Generic reasoner API
          
            Finding a reasoner
            Configuring a reasoner
            Applying a reasoner to data
            Accessing inferences
            Reasoner description
          
        
        Some small examples
        Operations on inference models
          
            Validation
            Extended list statements
            Direct and indirect relationships
            Derivations
            Accessing raw data and deductions
            Processing control
            Tracing
          
        
      
    
    The RDFS reasoner
      
        RDFS reasoner - intro and coverage
        RDFS configuration
          
            Summary of parameters
          
        
        RDFS Example
        RDFS implementation and performance notes
      
    
    The OWL reasoner
      
        OWL coverage
        OWL Configuration
        OWL Example
        OWL notes and limitations
          
            Comprehension axioms
            Prototypes
            Direct/indirect
            Performance
            Incompleteness
          
        
      
    
    The transitive reasoner
    The general purpose rule engine
      
        Overview of the rule engine(s)
        Rule syntax and structure
        Forward chaining engine
        Backward chaining engine
          
            Tabling
          
        
        Hybrid rule engine
        GenericRuleReasoner configuration
          
            Summary of parameters
          
        
        Builtin primitives
        Example
          
            Example 2
          
        
        Combining RDFS/OWL with custom rules
        Notes
        Extensions
          
            Rule syntax
            Builtins
            Preprocessing hooks
          
        
      
    
    Extending the inference support
    Futures
  

  
  
    This section of the documentation describes the current support for inference
  available within Jena. It includes an outline of the general inference API,
  together with details of the specific rule engines and configurations for RDFS
  and OWL inference supplied with Jena.
 Not all of the fine details of the API are covered here: refer to the Jena
  Javadoc to get the full details of the capabilities
  of the API. 
 Note that this is a preliminary version of this document, some errors or inconsistencies
  are possible, feedback to the 
  mailing lists
  is welcomed. 
Overview of inference support
The Jena inference subsystem is designed to allow a range of inference engines
  or reasoners to be plugged into Jena. Such engines are used to derive additional
  RDF assertions which are entailed from some base RDF together with any optional
  ontology information and the axioms and rules associated with the reasoner.
  The primary use of this mechanism is to support the use of languages such as
  RDFS and OWL which allow additional facts to be inferred from instance data
  and class descriptions. However, the machinery is designed to be quite general
  and, in particular, it includes a generic rule engine that can be used for many
  RDF processing or transformation tasks.
We will try to use the term inference to refer to the abstract process
  of deriving additional information and the term reasoner to refer to
  a specific code object that performs this task. Such usage is arbitrary and
  if we slip into using equivalent terms like reasoning and inference
  engine, please forgive us. 
The overall structure of the inference machinery is illustrated below. 

Applications normally access the inference machinery by using the ModelFactory
  to associate a data set with some reasoner to create a new Model. Queries to
  the created model will return not only those statements that were present in
  the original data but also additional statements than can be derived from the
  data using the rules or other inference mechanisms implemented by the reasoner.
As illustrated the inference machinery is actually implemented at the level
  of the Graph SPI, so that any of the different Model interfaces can be constructed
  around an inference Graph. In particular, the Ontology
  API provides convenient ways to link appropriate reasoners into the OntModels
  that it constructs. As part of the general RDF API we also provide an InfModel,
  this is an extension to the normal Model interface that provides
  additional control and access to an underlying inference graph. 
The reasoner API supports the notion of specializing a reasoner by binding
  it to a set of schema or ontology data using the bindSchema call.
  The specialized reasoner can then be attached to different sets of instance
  data using bind calls. In situations where the same schema information
  is to be used multiple times with different sets of instance data then this
  technique allows for some reuse of inferences across the different uses of the
  schema. In RDF there is no strong separation between schema (aka Ontology AKA
  tbox) data and instance (AKA abox) data and so any data, whether class or instance
  related, can be included in either the bind or bindSchema
  calls - the names are suggestive rather than restrictive.
To keep the design as open ended as possible Jena also includes a ReasonerRegistry.
  This is a static class though which the set of reasoners currently available
  can be examined. It is possible to register new reasoner types and to dynamically
  search for reasoners of a given type. The ReasonerRegistry also
  provides convenient access to prebuilt instances of the main supplied reasoners.
Available reasoners
Included in the Jena distribution are a number of predefined reasoners:

  Transitive reasoner: Provides support for storing and traversing class and property lattices.
    This implements just the transitive and reflexive properties
    of rdfs:subPropertyOf and rdfs:subClassOf.
  RDFS rule reasoner: Implements a configurable subset of the RDFS entailments.
  OWL, OWL Mini, OWL Micro Reasoners: 
  A set of useful but incomplete implementation of the OWL/Lite subset of the OWL/Full
    language. 
  Generic rule reasoner: A rule based reasoner that supports user defined rules. Forward chaining,
    tabled backward chaining and hybrid execution strategies are supported.

[Index]
The Inference API

  Generic reasoner API
  Small examples
  Operations on inference models
    - Validation
    - Extended list statements
    - Direct and indirect relations
    - Derivations
    - Accessing raw data and deductions
    - Processing control
    - Tracing 
  

Generic reasoner API
Finding a reasoner
For each type of reasoner there is a factory class (which conforms to the interface
  ReasonerFactory)
  an instance of which can be used to create instances of the associated
  Reasoner.
  The factory instances can be located by going directly to a known factory class
  and using the static theInstance() method or by retrieval from
  a global ReasonerRegistry
  which stores factory instances indexed by URI assigned to the reasoner. 
In addition, there are convenience methods on the ReasonerRegistry
  for locating a prebuilt instance of each of the main reasoners (getTransitiveReasoner,
  getRDFSReasoner, getRDFSSimpleReasoner, getOWLReasoner, getOWLMiniReasoner, getOWLMicroReasoner).
Note that the factory objects for constructing reasoners are just there to
  simplify the design and extension of the registry service. Once you have a reasoner
  instance, the same instance can reused multiple times by binding it to different
  datasets, without risk of interference - there is no need to create a new reasoner
  instance each time.
If working with the Ontology API it is
  not always necessary to explicitly locate a reasoner. The prebuilt instances
  of `OntSpecification` provide easy access to the appropriate reasoners to use for
  different Ontology configurations.
Similarly, if all you want is a plain RDF Model with RDFS inference included
  then the convenience methods ModelFactory.createRDFSModel can be
  used. 
Configuring a reasoner
The behaviour of many of the reasoners can be configured. To allow arbitrary
  configuration information to be passed to reasoners we use RDF to encode the
  configuration details. The ReasonerFactory.create method can be
  passed a Jena Resource object, the properties of that object will
  be used to configure the created reasoner.
To simplify the code required for simple cases we also provide a direct Java
  method to set a single configuration parameter, Reasoner.setParameter.
  The parameter being set is identified by the corresponding configuration property.
For the built in reasoners the available configuration parameters are described
  below and are predefined in the ReasonerVocabulary
  class.
The parameter value can normally be a String or a structured value. For example,
  to set a boolean value one can use the strings "true" or "false",
  or in Java use a Boolean object or in RDF use an instance of xsd:Boolean
Applying a reasoner to data
Once you have an instance of a reasoner it can then be attached to a set of
  RDF data to create an inference model. This can either be done by putting all
  the RDF data into one Model or by separating into two components - schema and
  instance data. For some external reasoners a hard separation may be required.
  For all of the built-in reasoners the separation is arbitrary. The prime value
  of this separation is to allow some deductions from one set of data (typically
  some schema definitions) to be efficiently applied to several subsidiary sets
  of data (typically sets of instance data).
If you want to specialize the reasoner this way, by partially-applying it to
  a set schema data, use the Reasoner.bindSchema method which returns
  a new, specialized, reasoner.
To bind the reasoner to the final data set to create an inference model see
  the ModelFactory
  methods, particularly ModelFactory.createInfModel. 
Accessing inferences
Finally, having created an inference model, any API operations which access
  RDF statements will be able to access additional statements which are entailed
  from the bound data by means of the reasoner. Depending on the reasoner these
  additional virtual statements may all be precomputed the first time the
  model is touched, may be dynamically recomputed each time or may be computed
  on-demand but cached.
Reasoner description
The reasoners can be described using RDF metadata which can be searched to
  locate reasoners with appropriate properties. The calls Reasoner.getCapabilities
  and Reasoner.supportsProperty are used to access this descriptive
  metadata.
[API Index] [Main Index]
Some small examples
These initial examples are not designed to illustrate the power of the reasoners
  but to illustrate the code required to set one up.
Let us first create a Jena model containing the statements that some property
  "p" is a subproperty of another property "q" and that we
  have a resource "a" with value "foo" for "p".
  This could be done by writing an RDF/XML or N3 file and reading that in but
  we have chosen to use the RDF API:
String NS = "urn:x-hp-jena:eg/";

// Build a trivial example data set
Model rdfsExample = ModelFactory.createDefaultModel();
Property p = rdfsExample.createProperty(NS, "p");
Property q = rdfsExample.createProperty(NS, "q");
rdfsExample.add(p, RDFS.subPropertyOf, q);
rdfsExample.createResource(NS+"a").addProperty(p, "foo");
Now we can create an inference model which performs RDFS inference over this
  data by using:
InfModel inf = ModelFactory.createRDFSModel(rdfsExample);  // [1]
We can then check that resulting model shows that "a" also has property
  "q" of value "foo" by virtue of the subPropertyOf entailment:
Resource a = inf.getResource(NS+"a");
System.out.println("Statement: " + a.getProperty(q));
Which prints the output:
    Statement: [urn:x-hp-jena:eg/a, urn:x-hp-jena:eg/q, Literal<foo>]

Alternatively we could have created an empty inference model and then added
  in the statements directly to that model.
If we wanted to use a different reasoner which is not available as a convenience
  method or wanted to configure one we would change line [1]. For example, to
  create the same setup manually we could replace [1] by:
Reasoner reasoner = ReasonerRegistry.getRDFSReasoner();
InfModel inf = ModelFactory.createInfModel(reasoner, rdfsExample);
or even more manually by
Reasoner reasoner = RDFSRuleReasonerFactory.theInstance().create(null);
InfModel inf = ModelFactory.createInfModel(reasoner, rdfsExample);
The purpose of creating a new reasoner instance like this variant would be
  to enable configuration parameters to be set. For example, if we were to listStatements
  on inf Model we would see that it also "includes" all the RDFS axioms,
  of which there are quite a lot. It is sometimes useful to suppress these and
  only see the "interesting" entailments. This can be done by setting
  the processing level parameter by creating a description of a new reasoner configuration
  and passing that to the factory method:
Resource config = ModelFactory.createDefaultModel()
                              .createResource()
                              .addProperty(ReasonerVocabulary.PROPsetRDFSLevel, "simple");
Reasoner reasoner = RDFSRuleReasonerFactory.theInstance().create(config);
InfModel inf = ModelFactory.createInfModel(reasoner, rdfsExample);
This is a rather long winded way of setting a single parameter, though it can
  be useful in the cases where you want to store this sort of configuration information
  in a separate (RDF) configuration file. For hardwired cases the following alternative
  is often simpler:
Reasoner reasoner = RDFSRuleReasonerFactory.theInstance()Create(null);
reasoner.setParameter(ReasonerVocabulary.PROPsetRDFSLevel,
                      ReasonerVocabulary.RDFS_SIMPLE);
InfModel inf = ModelFactory.createInfModel(reasoner, rdfsExample);
Finally, supposing you have a more complex set of schema information, defined
  in a Model called schema, and you want to apply this schema to several
  sets of instance data without redoing too many of the same intermediate deductions.
  This can be done by using the SPI level methods: 
Reasoner boundReasoner = reasoner.bindSchema(schema);
InfModel inf = ModelFactory.createInfModel(boundReasoner, data);
This creates a new reasoner, independent from the original, which contains
  the schema data. Any queries to an InfModel created using the boundReasoner
  will see the schema statements, the data statements and any statements entailed
  from the combination of the two. Any updates to the InfModel will be reflected
  in updates to the underlying data model - the schema model will not be affected.
[API Index] [Main Index]
Operations on inference models
For many applications one simply creates a model incorporating some inference
  step, using the ModelFactory methods, and then just works within
  the standard Jena Model API to access the entailed statements. However, sometimes
  it is necessary to gain more control over the processing or to access additional
  reasoner features not available as virtual triples.
Validation
The most common reasoner operation which can't be exposed through additional
  triples in the inference model is that of validation. Typically the ontology
  languages used with the semantic web allow constraints to be expressed, the
  validation interface is used to detect when such constraints are violated by
  some data set. 
A simple but typical example is that of datatype ranges in RDFS. RDFS allows
  us to specify the range of a property as lying within the value space of some
  datatype. If an RDF statement asserts an object value for that property which
  lies outside the given value space there is an inconsistency.
To test for inconsistencies with a data set using a reasoner we use the InfModel.validate()
  interface. This performs a global check across the schema and instance data
  looking for inconsistencies. The result is a ValidityReport object
  which comprises a simple pass/fail flag (ValidityReport.isValid())
  together with a list of specific reports (instances of the ValidityReport.Report
  interface) which detail any detected inconsistencies. At a minimum the individual
  reports should be printable descriptions of the problem but they can also contain
  an arbitrary reasoner-specific object which can be used to pass additional information
  which can be used for programmatic handling of the violations.
For example, to check a data set and list any problems one could do something
  like:
Model data = RDFDataMgr.loadModel(fname);
InfModel infmodel = ModelFactory.createRDFSModel(data);
ValidityReport validity = infmodel.validate();
if (validity.isValid()) {
    System.out.println("OK");
} else {
    System.out.println("Conflicts");
    for (Iterator i = validity.getReports(); i.hasNext(); ) {
        System.out.println(" - " + i.next());
    }
}
The file testing/reasoners/rdfs/dttest2.nt declares a property
  bar with range xsd:integer and attaches a bar
  value to some resource with the value "25.5"^^xsd:decimal.
  If we run the above sample code on this file we see:

  Conflicts 
    - Error (dtRange): Property http://www.hpl.hp.com/semweb/2003/eg#bar has a
    typed range Datatype[http://www.w3.org/2001/XMLSchema#integer -> class java.math.BigInteger]that
    is not compatible with 25.5:http://www.w3.org/2001/XMLSchema#decimal 

Whereas the file testing/reasoners/rdfs/dttest3.nt uses the value
  "25"^^xsd:decimal instead, which is a valid integer and so passes.

Note that the individual validation records can include warnings as well as
  errors. A warning does not affect the overall isValid() status
  but may indicate some issue the application may wish to be aware of. For example,
  it would be possible to develop a modification to the RDFS reasoner which warned
  about use of a property on a resource that is not explicitly declared to have
  the type of the domain of the property. 
A particular case of this arises in the case of OWL. In the Description Logic
  community a class which cannot have an instance is regarded as "inconsistent".
  That term is used because it generally arises from an error in the ontology.
  However, it is not a logical inconsistency - i.e. something giving rise to a
  contradiction. Having an instance of such a class is, clearly a logical error.
  In the Jena 2.2 release we clarified the semantics of isValid().
  An ontology which is logically consistent but contains empty classes is regarded
  as valid (that is isValid() is false only if there is a logical
  inconsistency). Class expressions which cannot be instantiated are treated as
  warnings rather than errors. To make it easier to test for this case there is
  an additional method Report.isClean() which returns true if the
  ontology is both valid (logically consistent) and generated no warnings (such
  as inconsistent classes).
Extended list statements
The default API supports accessing all entailed information at the level of
  individual triples. This is surprisingly flexible but there are queries which
  cannot be easily supported this way. The first such is when the query needs
  to make reference to an expression which is not already present in the data.
  For example, in description logic systems it is often possible to ask if there
  are any instances of some class expression. Whereas using the triple-based approach
  we can only ask if there are any instances of some class already defined (though
  it could be defined by a bNode rather than be explicitly named).
To overcome this limitation the InfModel API supports a notion
  of "posit", that is a set of assertions which can be used to temporarily
  declare new information such as the definition of some class expression. These
  temporary assertions can then be referenced by the other arguments to the listStatements
  command. With the current reasoners this is an expensive operation, involving
  the temporary creation of an entire new model with the additional posits added
  and all inference has to start again from scratch. Thus it is worth considering
  preloading your data with expressions you might need to query over. However,
  for some external reasoners, especially description logic reasoners, we anticipate
  restricted uses of this form of listStatement will be important.
Direct and indirect relationships
The second type of operation that is not obviously convenient at the triple
  level involves distinguishing between direct and indirect relationships. If
  a relation is transitive, for example rdfs:subClassOf, then we can define the
  notion of the minimal or direct form of the relationship from
  which all other values of the relation can be derived by transitive closure.



Normally, when an InfGraph is queried for a transitive relation the results
  returned show the inferred relations, i.e. the full transitive closure (all
  the links (ii) in the illustration). However, in some cases, such when as building
  a hierarchical UI widget to represent the graph, it is more convenient to only
  see the direct relations (iii). This is achieved by defining special direct
  aliases for those relations which can be queried this way. For the built in
  reasoners this functionality is available for rdfs:subClassOf and
  rdfs:subPropertyOf and the direct aliases for these are defined
  in ReasonerVocabulary.
Typically, the easiest way to work with such indirect and direct relations is
  to use the Ontology API which hides the
  grubby details of these property aliases.
Derivations
It is sometimes useful to be able to trace where an inferred statement was
  generated from. This is achieved using the InfModel.getDerivation(Statement)
  method. This returns a iterator over a set Derivation
  objects through which a brief description of the source of the derivation can
  be obtained. Typically understanding this involves tracing the sources for other
  statements which were used in this derivation and the Derivation.PrintTrace
  method is used to do this recursively.
The general form of the Derivation objects is quite abstract but in the case
  of the rule-based reasoners they have a more detailed internal structure that
  can be accessed - see RuleDerivation.
Derivation information is rather expensive to compute and store. For this reason,
  it is not recorded by default and InfModel.serDerivationLogging(true)
  must be used to enable derivations to be recorded. This should be called before
  any queries are made to the inference model.
As an illustration suppose that we have a raw data model which asserts three
  triples:
eg:A eg:p eg:B .
eg:B eg:p eg:C .
eg:C eg:p eg:D .
and suppose that we have a trivial rule set which computes the transitive closure
  over relation eg:p
String rules = "[rule1: (?a eg:p ?b) (?b eg:p ?c) -&gt; (?a eg:p ?c)]";
Reasoner reasoner = new GenericRuleReasoner(Rule.parseRules(rules));
reasoner.setDerivationLogging(true);
InfModel inf = ModelFactory.createInfModel(reasoner, rawData);
Then we can query whether eg:A is related through eg:p to eg:D and list the
  derivation route using the following code fragment: 
PrintWriter out = new PrintWriter(System.out);
for (StmtIterator i = inf.listStatements(A, p, D); i.hasNext(); ) {
    Statement s = i.nextStatement();
    System.out.println("Statement is " + s);
    for (Iterator id = inf.getDerivation(s); id.hasNext(); ) {
        Derivation deriv = (Derivation) id.next();
        deriv.printTrace(out, true);
    }
}
out.flush();
Which generates the output:
Statement is [urn:x-hp:eg/A, urn:x-hp:eg/p, urn:x-hp:eg/D]
    Rule rule1 concluded (eg:A eg:p eg:D) <-
        Fact (eg:A eg:p eg:B)
    Rule rule1 concluded (eg:B eg:p eg:D) <-
        Fact (eg:B eg:p eg:C)
        Fact (eg:C eg:p eg:D)

Accessing raw data and deductions
From an InfModel it is easy to retrieve the original, unchanged,
  data over which the model has been computed using the getRawModel()
  call. This returns a model equivalent to the one used in the initial bind
  call. It might not be the same Java object but it uses the same Java object
  to hold the underlying data graph. 
Some reasoners, notably the forward chaining rule engine, store the deduced
  statements in a concrete form and this set of deductions can be obtained separately
  by using the getDeductionsModel() call. 
Processing control
Having bound a Model into an InfModel by using a
  Reasoner its content can still be changed by the normal add
  and remove calls to the InfModel. Any such change
  the model will usually cause all current deductions and temporary rules to be
  discarded and inference will start again from scratch at the next query. Some
  reasoners, such as the RETE-based forward rule engine, can work incrementally.

In the non-incremental case then the processing will not be started until a
  query is made. In that way a sequence of add and removes can be undertaken without
  redundant work being performed at each change. In some applications it can be
  convenient to trigger the initial processing ahead of time to reduce the latency
  of the first query. This can be achieved using the InfModel.prepare()
  call. This call is not necessary in other cases, any query will automatically
  trigger an internal prepare phase if one is required.
There are times when the data in a model bound into an InfModel can is changed
  "behind the scenes" instead of through calls to the InfModel. If this
  occurs the result of future queries to the InfModel are unpredictable. To overcome
  this and force the InfModel to reconsult the raw data use the InfModel.rebind()
  call.
Finally, some reasoners can store both intermediate and final query results
  between calls. This can substantially reduce the cost of working with the inference
  services but at the expense of memory usage. It is possible to force an InfModel
  to discard all such cached state by using the InfModel.reset()
  call. It there are any outstanding queries (i.e. StmtIterators which have not
  been read to the end yet) then those will be aborted (the next hasNext() call
  will return false).
Tracing
When developing new reasoner configurations, especially new rule sets for the
  rule engines, it is sometimes useful to be able to trace the operations of the
  associated inference engine. Though, often this generates too much information
  to be of use and selective use of the print builtin can be more
  effective. 
Tracing is not supported by a convenience API call but, for those reasoners
  that support it, it can be enabled using:
reasoner.setParameter(ReasonerVocabulary.PROPtraceOn, Boolean.TRUE);
Dynamic tracing control is sometimes possible on the InfModel itself by retrieving
  its underlying InfGraph and calling setTraceOn() call. If you need
  to make use of this see the full javadoc for the relevant InfGraph implementation.
[API Index] [Main Index]
The RDFS reasoner

  RDFS reasoner - introduction and coverage
  RDFS Configuration
  RDFS Example
  RDFS implementation and performance notes

RDFS reasoner - intro and coverage
Jena includes an RDFS reasoner (RDFSRuleReasoner) which supports
  almost all of the RDFS entailments described by the RDF Core working group [RDF
  Semantics]. The only omissions are deliberate and are described below.
This reasoner is accessed using ModelFactory.createRDFSModel or
  manually via ReasonerRegistry.getRDFSReasoner().
During the preview phases of Jena experimental RDFS reasoners were released,
  some of which are still included in the code base for now but applications should
  not rely on their stability or continued existence.
When configured in full mode (see below for configuration information)
  then the RDFS reasoner implements all RDFS entailments except for the bNode
  closure rules. These closure rules imply, for example, that for all triples
  of the form:
eg:a eg:p nnn^^datatype .
we should introduce the corresponding blank nodes:
eg:a eg:p _:anon1 .
_:anon1 rdf:type datatype .
Whilst such rules are both correct and necessary to reduce RDF datatype entailment
  down to simple entailment they are not useful in implementation terms. In Jena
  simple entailment can be implemented by translating a graph containing bNodes
  to an equivalent query containing variables in place of the bNodes. Such a query
  is can directly match the literal node and the RDF API can be used to extract
  the datatype of the literal. The value to applications of directly seeing the
  additional bNode triples, even in virtual triple form, is negligible
  and so this has been deliberately omitted from the reasoner. 
[RDFS Index] [Main Index]
RDFS configuration
The RDFSRuleReasoner can be configured to work at three different compliance
  levels: 

  Full
  This implements all of the RDFS axioms and closure rules with the exception
    of bNode entailments and datatypes (rdfD 1). See above for comments on these.
    This is an expensive mode because all statements in the data graph need to
    be checked for possible use of container membership properties. It also generates
    type assertions for all resources and properties mentioned in the data (rdf1,
    rdfs4a, rdfs4b).
  Default
  This omits the expensive checks for container membership properties and
    the "everything is a resource" and "everything used as a property
    is one" rules (rdf1, rdfs4a, rdfs4b). The latter information is available
    through the Jena API and creating virtual triples to this effect has little
    practical value.
    This mode does include all the axiomatic rules. Thus, for example, even querying
    an "empty" RDFS InfModel will return triples such as [rdf:type
    rdfs:range rdfs:Class].
  Simple
  This implements just the transitive closure of subPropertyOf and subClassOf
    relations, the domain and range entailments and the implications of subPropertyOf
    and subClassOf. It omits all of the axioms. This is probably the most useful
    mode but is not the default because it is a less complete implementation of
    the standard. 

The level can be set using the setParameter call, e.g.
reasoner.setParameter(ReasonerVocabulary.PROPsetRDFSLevel,
                      ReasonerVocabulary.RDFS_SIMPLE);
or by constructing an RDF configuration description and passing that to the
  RDFSRuleReasonerFactory e.g.
Resource config = ModelFactory.createDefaultModel()
                  .createResource()
                  .addProperty(ReasonerVocabulary.PROPsetRDFSLevel, "simple");
Reasoner reasoner = RDFSRuleReasonerFactory.theInstance()Create(config);
Summary of parameters

  
    Parameter
    Values
    Description
  
  
    
      PROPsetRDFSLevel
    
    "full", "default", "simple"
    
      Sets the RDFS processing level as described above.
    
  
  
    
      PROPenableCMPScan
    
    Boolean
    
      If true forces a preprocessing pass which finds all usages
        of rdf:_n properties and declares them as ContainerMembershipProperties.
        This is implied by setting the level parameter to "full" and
        is not normally used directly.
    
  
  
    
      PROPtraceOn
    
    Boolean
    
      If true switches on exhaustive tracing of rule executions
        at the INFO level.
    
  
  
    
      PROPderivationLogging
    
    Boolean
    
      If true causes derivation routes to be recorded internally
        so that future getDerivation calls can return useful information.
    
  

[RDFS Index] [Main Index]
RDFS Example
As a complete worked example let us create a simple RDFS schema, some instance
  data and use an instance of the RDFS reasoner to query the two.
We shall use a trivial schema:
<rdf:Description rdf:about="eg:mum">
  <rdfs:subPropertyOf rdf:resource="eg:parent"/>
</rdf:Description>

<rdf:Description rdf:about="eg:parent">
  <rdfs:range  rdf:resource="eg:Person"/>
  <rdfs:domain rdf:resource="eg:Person"/>
</rdf:Description>

<rdf:Description rdf:about="eg:age">
  <rdfs:range rdf:resource="xsd:integer" />
</rdf:Description>
This defines a property parent from Person to Person,
  a sub-property mum of parent and an integer-valued
  property age.
We shall also use the even simpler instance file:
<Teenager rdf:about="eg:colin">
    <mum rdf:resource="eg:rosy" />
    <age>13</age>
</Teenager>

  Which defines a Teenager called colin who has a mum
  rosy and an age of 13.
Then the following code fragment can be used to read files containing these
  definitions, create an inference model and query it for information on the rdf:type
  of colin and the rdf:type of Person:
Model schema = RDFDataMgr.loadModel("file:data/rdfsDemoSchema.rdf");
Model data = RDFDataMgr.loadModel("file:data/rdfsDemoData.rdf");
InfModel infmodel = ModelFactory.createRDFSModel(schema, data);

Resource colin = infmodel.getResource("urn:x-hp:eg/colin");
System.out.println("colin has types:");
printStatements(infmodel, colin, RDF.type, null);

Resource Person = infmodel.getResource("urn:x-hp:eg/Person");
System.out.println("\nPerson has types:");
printStatements(infmodel, Person, RDF.type, null);
This produces the output:
colin has types:
 - (eg:colin rdf:type eg:Teenager)
 - (eg:colin rdf:type rdfs:Resource)
 - (eg:colin rdf:type eg:Person)

Person has types:
 - (eg:Person rdf:type rdfs:Class)
 - (eg:Person rdf:type rdfs:Resource)
This says that colin is both a Teenager (by direct
  definition), a Person (because he has a mum which
  means he has a parent and the domain of parent is
  Person) and an rdfs:Resource. It also says that Person
  is an rdfs:Class, even though that wasn't explicitly in the schema,
  because it is used as object of range and domain statements.
If we add the additional code:
ValidityReport validity = infmodel.validate();
if (validity.isValid()) {
    System.out.println("\nOK");
} else {
    System.out.println("\nConflicts");
    for (Iterator i = validity.getReports(); i.hasNext(); ) {
        ValidityReport.Report report = (ValidityReport.Report)i.next();
        System.out.println(" - " + report);
    }
}

  Then we get the additional output:
Conflicts
 - Error (dtRange): Property urn:x-hp:eg/age has a typed range
Datatype[http://www.w3.org/2001/XMLSchema#integer -> class java.math.BigInteger]
that is not compatible with 13
because the age was given using an RDF plain literal where as the schema requires
  it to be a datatyped literal which is compatible with xsd:integer.
[RDFS Index] [Main Index]
RDFS implementation and performance notes
The RDFSRuleReasoner is a hybrid implementation. The subproperty and subclass
  lattices are eagerly computed and stored in a compact in-memory form using the
  TransitiveReasoner (see below). The identification of which container membership
  properties (properties like rdf:_1) are present is implemented using a preprocessing
  hook. The rest of the RDFS operations are implemented by explicit rule sets
  executed by the general hybrid rule reasoner. The three different processing
  levels correspond to different rule sets. These rule sets are located by looking
  for files "`etc/*.rules`" on the classpath and so could,
  in principle, be overridden by applications wishing to modify the rules. 
Performance for in-memory queries appears to be good. Using a synthetic dataset
  we obtain the following times to determine the extension of a class from a class
  hierarchy:

  
    Set
    #concepts
    total instances
    #instances of concept
    JenaRDFS
    XSB*
  
  
    1
    155
    1550
    310
    0.07
    0.16
  
  
    2
    780
    7800
    1560
    0.25
    0.47
  
  
    3
    3905
    39050
    7810
    1.16
    2.11
  

The times are in seconds, normalized to a 1.1GHz Pentium processor. The XSB*
  figures are taken from a pre-published paper and may not be directly comparable
  (for example they do not include any rule compilation time) - they are just
  offered to illustrate that the RDFSRuleReasoner has broadly similar scaling
  and performance to other rule-based implementations.
The Jena RDFS implementation has not been tested and evaluated over database
  models. The Jena architecture makes it easy to construct such models but in
  the absence of caching we would expect the performance to be poor. Future work
  on adapting the rule engines to exploit the capabilities of the more sophisticated
  database backends will be considered.
[RDFS Index] [Main Index]
The OWL reasoner

  OWL reasoner introduction
  OWL coverage
  OWL configuration
  OWL example
  OWL notes and limitations

The second major set of reasoners supplied with Jena is a rule-based
  implementation of the OWL/lite subset of OWL/full.
The current release includes a default OWL reasoner and two small/faster configurations.
Each of the configurations is intended to be a sound implementation of a subset of OWL/full semantics
but none of them is complete (in the technical sense). For complete OWL DL reasoning use
an external DL reasoner such as Pellet, Racer or FaCT. Performance (especially memory use) of the fuller reasoner
configuration still leaves something to be desired and will the subject of future work - time permitting.
See also subsection 5 for notes on more specific limitations
  of the current implementation. 
OWL coverage
The Jena OWL reasoners could be described as instance-based reasoners. That
  is, they work by using rules to propagate the if- and only-if- implications of
  the OWL constructs on instance data. Reasoning about classes is done indirectly
  - for each declared class a prototypical instance is created and elaborated.
  If the prototype for a class A can be deduced as being a member of class B then
  we conclude that A is a subClassOf B. This approach is in contrast to more sophisticated
  Description Logic reasoners which work with class expressions and can be less
  efficient when handling instance data but more efficient with complex class expressions
  and able to provide complete reasoning. 
We thus anticipate that the OWL rule reasoner will be most suited to applications
  involving primarily instance reasoning with relatively simple, regular ontologies
  and least suited to applications involving large rich ontologies. A better characterisation
  of the tradeoffs involved would be useful and will be sought.
We intend that the OWL reasoners should be smooth extensions of the RDFS reasoner
  described above. That is all RDFS entailments found by the RDFS reasoner will
  also be found by the OWL reasoners and scaling on RDFS schemas should be similar
  (though there are some costs, see later). The instance-based implementation
  technique is in keeping with this "RDFS plus a bit" approach.
Another reason for choosing this inference approach is that it makes it possible
  to experiment with support for different constructs, including constructs that
  go beyond OWL, by modification of the rule set. In particular, some applications
  of interest to ourselves involve ontology transformation which very often implies
  the need to support property composition. This is something straightforward
  to express in rule-based form and harder to express in standard Description
  Logics.
Since RDFS is not a subset of the OWL/Lite or OWL/DL languages the Jena implementation
  is an incomplete implementation of OWL/full. We provide three implementations
  a default ("full" one), a slightly cut down "mini" and a
  rather smaller/faster "micro". The default OWL rule reasoner (ReasonerRegistry.getOWLReasoner())
  supports the constructs as listed below. The OWLMini reasoner is nearly the
  same but omits the forward entailments from minCardinality/someValuesFrom restrictions
  - that is it avoids introducing bNodes which avoids some infinite expansions
  and enables it to meet the Jena API contract more precisely. The OWLMicro reasoner
  just supports RDFS plus the various property axioms, intersectionOf, unionOf
  (partial) and hasValue. It omits the cardinality restrictions and equality axioms,
  which enables it to achieve much higher performance. 

  
    Constructs
    Supported by
    Notes
  
  
    
      rdfs:subClassOf, rdfs:subPropertyOf, rdf:type
    
    all
    
      Normal RDFS semantics supported including meta use (e.g.
        taking the subPropertyOf subClassOf).
    
  
  
    
      rdfs:domain, rdfs:range
    
    all
    
      Stronger if-and-only-if semantics supported
    
  
  
    
      owl:intersectionOf
    
    all
     
  
  
    
      owl:unionOf
    
    all
    
      Partial support. If C=unionOf(A,B) then will infer that
        A,B are subclasses of C, and thus that instances of A or B are instances
        of C. Does not handle the reverse (that an instance of C must be either
        an instance of A or an instance of B).
    
  
  
    
      owl:equivalentClass
    
    all
    
      
    
  
  
    
      owl:disjointWith
    
    full, mini
    
      
    
  
  
    
      owl:sameAs, owl:differentFrom, owl:distinctMembers
    
    full, mini
    
      owl:distinctMembers is currently translated into a quadratic
        set of owl:differentFrom assertions.
    
  
  
    
      Owl:Thing
    
    all
     
  
  
    
      owl:equivalentProperty, owl:inverseOf 
    
    all
    
      
    
  
  
    
      owl:FunctionalProperty, owl:InverseFunctionalProperty
    
    all
    
      
    
  
  
    
      owl:SymmetricProperty, owl:TransitiveProperty
    
    all
    
      
    
  
  
    
      owl:someValuesFrom
    
    full, (mini)
    
      
        Full supports both directions (existence of a value implies membership
          of someValuesFrom restriction, membership of someValuesFrom implies
          the existence of a bNode representing the value).
          Mini omits the latter "bNode introduction" which avoids some
          infinite closures.
      
    
  
  
    
      owl:allValuesFrom
    
    full, mini
    
      Partial support, forward direction only (member of a allValuesFrom(p,
        C) implies that all p values are of type C). Does handle cases where the
        reverse direction is trivially true (e.g. by virtue of a global rdfs:range
        axiom). 
    
  
  
    
      owl:minCardinality, owl:maxCardinality, owl:cardinality
    
    full, (mini)
    
      
        Restricted to cardinalities of 0 or 1, though higher cardinalities
          are partially supported in validation for the case of literal-valued
          properties.
          Mini omits the bNodes introduction in the minCardinality(1) case, see
          someValuesFrom above.
      
    
  
  
    
      owl:hasValue
    
    all
    
      
    
  

The critical constructs which go beyond OWL/lite and are not supported in the
  Jena OWL reasoner are complementOf and oneOf. As noted above the support for
  unionOf is partial (due to limitations of the rule based approach) but is useful
  for traversing class hierarchies.
Even within these constructs rule based implementations are limited in the
  extent to which they can handle equality reasoning - propositions provable by
  reasoning over concrete and introduced instances are covered but reasoning by
  cases is not supported.
Nevertheless, the full reasoner passes the normative OWL working group positive
  and negative entailment tests for the supported constructs, though some tests
  need modification for the comprehension axioms (see below).
The OWL rule set does include incomplete support for validation of datasets
  using the above constructs. Specifically, it tests for:

  Illegal existence of a property restricted by a maxCardinality(0) restriction.
  Two individuals both sameAs and differentFrom each other.
  Two classes declared as disjoint but where one subsumes the other (currently
    reported as a violation concerning the class prototypes, error message to
    be improved).
  Range or a allValuesFrom violations for DatatypeProperties.
  Too many literal-values for a DatatypeProperty restricted by a maxCardinality(N)
    restriction.

[OWL Index] [Main Index]
OWL Configuration
This reasoner is accessed using ModelFactory.createOntologyModel
  with the prebuilt OntSpecification
  OWL*_MEM_RULES_INF or manually via ReasonerRegistry.getOWLReasoner().
There are no OWL-specific configuration parameters though the reasoner supports
  the standard control parameters:

  
    Parameter
    Values
    Description
  
  
    
      PROPtraceOn
    
    boolean
    
      If true switches on exhaustive tracing of rule executions
        at the INFO level.
    
  
  
    
      PROPderivationLogging
    
    Boolean
    
      If true causes derivation routes to be recorded internally
        so that future getDerivation calls can return useful information.
    
  

As we gain experience with the ways in which OWL is used and the capabilities
  of the rule-based approach we imagine useful subsets of functionality emerging
  - like that supported by the RDFS reasoner in the form of the level settings.
[OWL Index] [Main Index]
OWL Example
As an example of using the OWL inference support, consider the sample schema
  and data file in the data directory - owlDemoSchema.rdf
  and owlDemoData.rdf. 
The schema file shows a simple, artificial ontology concerning computers which
  defines a GamingComputer as a Computer which includes at least one bundle of
  type GameBundle and a component with the value gamingGraphics. 
The data file shows information on several hypothetical computer configurations
  including two different descriptions of the configurations "whiteBoxZX"
  and "bigName42".
We can create an instance of the OWL reasoner, specialized to the demo schema
  and then apply that to the demo data to obtain an inference model, as follows:
Model schema = RDFDataMgr.loadModel("file:data/owlDemoSchema.rdf");
Model data = RDFDataMgr.loadModel("file:data/owlDemoData.rdf");
Reasoner reasoner = ReasonerRegistry.getOWLReasoner();
reasoner = reasoner.bindSchema(schema);
InfModel infmodel = ModelFactory.createInfModel(reasoner, data);
A typical example operation on such a model would be to find out all we know
  about a specific instance, for example the nForce mother board.
  This can be done using:
Resource nForce = infmodel.getResource("urn:x-hp:eg/nForce");
System.out.println("nForce *:");
printStatements(infmodel, nForce, null, null);
 where printStatements is defined by: 
public void printStatements(Model m, Resource s, Property p, Resource o) {
    for (StmtIterator i = m.listStatements(s,p,o); i.hasNext(); ) {
        Statement stmt = i.nextStatement();
        System.out.println(" - " + PrintUtil.print(stmt));
    }
}
This produces the output:
nForce *:
 - (eg:nForce rdf:type owl:Thing)
 - (eg:nForce owl:sameAs eg:unknownMB)
 - (eg:nForce owl:sameAs eg:nForce)
 - (eg:nForce rdf:type eg:MotherBoard)
 - (eg:nForce rdf:type rdfs:Resource)
 - (eg:nForce rdf:type a3b24:f7822755ad:-7ffd)
 - (eg:nForce eg:hasGraphics eg:gamingGraphics)
 - (eg:nForce eg:hasComponent eg:gamingGraphics)
Note that this includes inferences based on subClass inheritance (being an
  eg:MotherBoard implies it is an owl:Thing and an rdfs:Resource),
  property inheritance (eg:hasComponent eg:gameGraphics derives from
  hasGraphics being a subProperty of hasComponent) and
  cardinality reasoning (it is the sameAs eg:unknownMB because computers
  are defined to have only one motherboard and the two different descriptions
  of whileBoxZX use these two different terms for the mother board).
  The anonymous rdf:type statement references the "hasValue(eg:hasComponent,
  eg:gamingGraphics)" restriction mentioned in the definition of GamingComputer.
A second, typical operation is instance recognition. Testing if an individual
  is an instance of a class expression. In this case the whileBoxZX
  is identifiable as a GamingComputer because it is a Computer,
  is explicitly declared as having an appropriate bundle and can be inferred to
  have a gamingGraphics component from the combination of the nForce
  inferences we've already seen and the transitivity of hasComponent.
  We can test this using:
Resource gamingComputer = infmodel.getResource("urn:x-hp:eg/GamingComputer");
Resource whiteBox = infmodel.getResource("urn:x-hp:eg/whiteBoxZX");
if (infmodel.contains(whiteBox, RDF.type, gamingComputer)) {
    System.out.println("White box recognized as gaming computer");
} else {
    System.out.println("Failed to recognize white box correctly");
}
 Which generates the output:
  White box recognized as gaming computer
Finally, we can check for inconsistencies within the data by using the validation
  interface:
ValidityReport validity = infmodel.validate();
if (validity.isValid()) {
    System.out.println("OK");
} else {
    System.out.println("Conflicts");
    for (Iterator i = validity.getReports(); i.hasNext(); ) {
        ValidityReport.Report report = (ValidityReport.Report)i.next();
        System.out.println(" - " + report);
    }
}
Which generates the output:
Conflicts
 - Error (conflict): Two individuals both same and different, may be
   due to disjoint classes or functional properties
Culprit = eg:nForce2
Implicated node: eg:bigNameSpecialMB
… + 3 other similar reports

This is due to the two records for the bigName42 configuration
  referencing two motherboards which are explicitly defined to be different resources
  and thus violate the FunctionProperty nature of hasMotherBoard.
[OWL Index] [Main Index]
OWL notes and limitations
Comprehension axioms
A critical implication of our variant of the instance-based approach is that
  the reasoner does not directly answer queries relating to dynamically introduced
  class expressions.
For example, given a model containing the RDF assertions corresponding to the
  two OWL axioms:
class A = intersectionOf (minCardinality(P, 1), maxCardinality(P,1))
class B = cardinality(P,1)
Then the reasoner can demonstrate that classes A and B are equivalent, in particular
  that any instance of A is an instance of B and vice versa. However, given a
  model just containing the first set of assertions you cannot directly query
  the inference model for the individual triples that make up cardinality(P,1).
  If the relevant class expressions are not already present in your model then
  you need to use the list-with-posits mechanism described above,
  though be warned that such posits start inference afresh each time and can be
  expensive. 
Actually, it would be possible to introduce comprehension axioms for simple
  cases like this example. We have, so far, chosen not to do so. First, since
  the OWL/full closure is generally infinite, some limitation on comprehension
  inferences seems to be useful. Secondly, the typical queries that Jena applications
  expect to be able to issue would suddenly jump in size and cost - causing a
  support nightmare. For example, queries such as (a, rdf:type, *) would become
  near-unusable.
Approximately, 10 of the OWL working group tests for the supported OWL subset
  currently rely on such comprehension inferences. The shipping version of the
  Jena rule reasoner passes these tests only after they have been rewritten to
  avoid the comprehension requirements.

Prototypes
As noted above the current OWL rule set introduces prototypical instances for
  each defined class. These prototypical instances used to be visible to queries.
  From release 2.1 they are used internally but should not longer be visible.

Direct/indirect
We noted above that the Jena reasoners support
  a separation of direct and indirect relations for transitive properties such
  as subClassOf. The current implementation of the full and mini OWL reasoner
  fails to do this and the direct forms of the queries will fail. The OWL Micro
  reasoner, which is but a small extension of RDFS, does support the direct queries.
This does not affect querying though the Ontology API, which works around this
  limitation. It only affects direct RDF accesses to the inference model.
Performance
The OWL reasoners use the rule engines for all inference. The full and mini
  configurations omit some of the performance tricks employed by the RDFS reasoner
  (notably the use of the custom transitive reasoner) making those OWL reasoner
  configurations slower than the RDFS reasoner on pure RDFS data (typically around
  x3-4 slow down). The OWL Micro reasoner is intended to be as close to RDFS performance
  while also supporting the core OWL constructs as described earlier.
Once the owl constructs are used then substantial reasoning can be required.
  The most expensive aspect of the supported constructs is the equality reasoning
  implied by use of cardinality restrictions and FunctionalProperties. The current
  rule set implements equality reasoning by identifying all sameAs deductions
  during the initial forward "prepare" phase. This may require the entire
  instance dataset to be touched several times searching for occurrences of FunctionalProperties.
Beyond this the rules implementing the OWL constructs can interact in complex
  ways leading to serious performance overheads for complex ontologies. Characterising
  the sorts of ontologies and inference problems that are well tackled by this
  sort of implementation and those best handled by plugging a Description Logic
  engine, or a saturation theorem prover, into Jena is a topic for future work.
One random hint: explicitly importing the owl.owl definitions causes much duplication
  of rule use and a substantial slow down - the OWL axioms that the reasoner can
  handle are already built in and don't need to be redeclared.
Incompleteness
The rule based approach cannot offer a complete solution for OWL/Lite, let
  alone the OWL/Full fragment corresponding to the OWL/Lite constructs. In addition
  the current implementation is still under development and may well have omissions
  and oversights. We intend that the reasoner should be sound (all inferred triples
  should be valid) but not complete. 
[OWL Index] [Main Index]
The transitive reasoner
The TransitiveReasoner provides support for storing and traversing class and
  property lattices. This implements just the transitive and symmetric
  properties of rdfs:subPropertyOf and rdfs:subClassOf.
  It is not all that exciting on its own but is one of the building blocks used
  for the more complex reasoners. It is a hardwired Java implementation that stores
  the class and property lattices as graph structures. It is slightly higher performance,
  and somewhat more space efficient, than the alternative of using the pure rule
  engines to performance transitive closure but its main advantage is that it
  implements the direct/minimal version of those relations as well as the transitively
  closed version.
The GenericRuleReasoner (see below) can optionally use an instance
  of the transitive reasoner for handling these two properties. This is the approach
  used in the default RDFS reasoner.
It has no configuration options.
[Index]
The general purpose rule engine

  Overview of the rule engine(s)
  Rule syntax and structure
  Forward chaining engine
  Backward chaining engine
  Hybrid engine
  GenericRuleReasoner configuration
  Builtin primitives
  Example
  Combining RDFS/OWL with custom rules
  Notes
  Extensions

Overview of the rule engine(s)
Jena includes a general purpose rule-based reasoner which is used to implement
  both the RDFS and OWL reasoners but is also available for general use. This
  reasoner supports rule-based inference over RDF graphs and provides forward
  chaining, backward chaining and a hybrid execution model. To be more exact,
  there are two internal rule engines one forward chaining RETE engine and one
  tabled datalog engine - they can be run separately or the forward engine can
  be used to prime the backward engine which in turn will be used to answer queries.
The various engine configurations are all accessible through a single parameterized
  reasoner GenericRuleReasoner.
  At a minimum a GenericRuleReasoner requires a ruleset to define
  its behaviour. A GenericRuleReasoner instance with a ruleset can
  be used like any of the other reasoners described above - that is it can be
  bound to a data model and used to answer queries to the resulting inference
  model. 
The rule reasoner can also be extended by registering new procedural primitives.
  The current release includes a starting set of primitives which are sufficient
  for the RDFS and OWL implementations but is easily extensible.
[Rule Index] [Main Index]
Rule syntax and structure
A rule for the rule-based reasoner is defined by a Java Rule
  object with a list of body terms (premises), a list of head terms (conclusions)
  and an optional name and optional direction. Each term or ClauseEntry
  is either a triple pattern, an extended triple pattern or a call to a builtin
  primitive. A rule set is simply a List of Rules.
For convenience a rather simple parser is included with Rule which allows rules
  to be specified in reasonably compact form in text source files. However, it
  would be perfectly possible to define alternative parsers which handle rules
  encoded using, say, XML or RDF and generate Rule objects as output. It would
  also be possible to build a real parser for the current text file syntax which
  offered better error recovery and diagnostics.
An informal description of the simplified text rule syntax is:
Rule      :=   bare-rule .
          or   [ bare-rule ]       or   [ ruleName : bare-rule ]
bare-rule :=   term, … term -> hterm, … hterm    // forward rule
or   bhterm <- term, … term    // backward rule
hterm     :=   term
or   [ bare-rule ]
term      :=   (node, node, node)           // triple pattern
or   (node, node, functor)        // extended triple pattern
or   builtin(node, … node)      // invoke procedural primitive
bhterm      :=   (node, node, node)           // triple pattern
functor   :=   functorName(node, … node)  // structured literal
node      :=   uri-ref                   // e.g. http://foo.com/eg
or   prefix:localname          // e.g. rdf:type
or   <uri-ref>          // e.g. <myscheme:myuri>
or   ?varname                    // variable
or   ‘a literal’                 // a plain string literal
or   ’lex’^^typeURI              // a typed literal, xsd:* type names supported
or   number                      // e.g. 42 or 25.5
The "," separators are optional.
The difference between the forward and backward rule syntax is only relevant
  for the hybrid execution strategy, see below.
The functor in an extended triple pattern is used to create and access
  structured literal values. The functorName can be any simple identifier and
  is not related to the execution of builtin procedural primitives, it is just
  a datastructure. It is useful when a single semantic structure is defined across
  multiple triples and allows a rule to collect those triples together in one
  place.
To keep rules readable qname syntax is supported for URI refs. The set of known
  prefixes is those registered with the PrintUtil
  object. This initially knows about rdf, rdfs, owl, xsd and a test namespace
  eg, but more mappings can be registered in java code. In addition it is possible to
  define additional prefix mappings in the rule file, see below. 
Here are some example rules which illustrate most of these constructs:
[allID: (?C rdf:type owl:Restriction), (?C owl:onProperty ?P),
     (?C owl:allValuesFrom ?D)  -> (?C owl:equivalentClass all(?P, ?D)) ]
[all2: (?C rdfs:subClassOf all(?P, ?D)) -> print(‘Rule for ‘, ?C)
[all1b: (?Y rdf:type ?D) <- (?X ?P ?Y), (?X rdf:type ?C) ] ]
[max1: (?A rdf:type max(?P, 1)), (?A ?P ?B), (?A ?P ?C)
-> (?B owl:sameAs ?C) ]

Rule allID illustrates the functor use for collecting the components
  of an OWL restriction into a single datastructure which can then fire further
  rules. Rule all2 illustrates a forward rule which creates a new
  backward rule and also calls the print procedural primitive. Rule
  max1 illustrates use of numeric literals.


Rule files may be loaded and parsed using:
List rules = Rule.rulesFromURL("file:myfile.rules");
or
BufferedReader br = /* open reader */ ;
List rules = Rule.parseRules( Rule.rulesParserFromReader(br) );
or
String ruleSrc = /* list of rules in line */
List rules = Rule.parseRules( rulesSrc );
In the first two cases (reading from a URL or a BufferedReader) the rule file is
preprocessed by a simple processor which strips comments and supports some additional
macro commands:

# ...
A comment line.
// ...
A comment line.
@prefix pre: <http://domain/url#>.
Defines a prefix pre which can be used in the rules.
The prefix is local to the rule file.
@include <urlToRuleFile>.
Includes the rules defined in the given file in this file. The included rules
will appear before the user defined rules, irrespective of where in the file
the @include directive appears. A set of special cases is supported to allow
a rule file to include the predefined rules for RDFS and OWL - in place of a real
URL for a rule file use one of the keywords
RDFS
OWL
OWLMicro
OWLMini (case insensitive).


So an example complete rule file which includes the RDFS rules and defines
a single extra rule is:
# Example rule file
@prefix pre: <http://jena.hpl.hp.com/prefix#>.
@include <RDFS>.

[rule1: (?f pre:father ?a) (?u pre:brother ?f) -> (?u pre:uncle ?a)]
[Rule Index] [Main Index]
Forward chaining engine
If the rule reasoner is configured to run in forward mode then only the forward
  chaining engine will be used. The first time the inference Model is queried
  (or when an explicit prepare() call is made, see above)
  then all of the relevant data in the model will be submitted to the rule engine.
  Any rules which fire that create additional triples do so in an internal deductions
  graph and can in turn trigger additional rules. There is a remove primitive
  that can be used to remove triples and such removals can also trigger rules
  to fire in removal mode. This cascade of rule firings continues until no more
  rules can fire. It is perfectly possible, though not a good idea, to write rules
  that will loop infinitely at this point.
Once the preparation phase is complete the inference graph will act as if it
  were the union of all the statements in the original model together with all
  the statements in the internal deductions graph generated by the rule firings.
  All queries will see all of these statements and will be of similar speed to
  normal model accesses. It is possible to separately access the original raw
  data and the set of deduced statements if required, see above.
If the inference model is changed by adding or removing statements through
  the normal API then this will trigger further rule firings. The forward rules
  work incrementally and only the consequences of the added or removed triples
  will be explored. The default rule engine is based on the standard RETE algorithm
  (C.L Forgy, RETE: A fast algorithm for the many pattern/many object pattern
  match problem, Artificial Intelligence 1982) which is optimized for such
  incremental changes. 
When run in forward mode all rules are treated as forward even if they were
  written in backward ("<-") syntax. This allows the same rule set
  to be used in different modes to explore the performance tradeoffs.
There is no guarantee of the order in which matching rules will fire or the
  order in which body terms will be tested, however once a rule fires its head-terms
  will be executed in left-to-right sequence.
In forward mode then head-terms which assert backward rules (such as all1b
  above) are ignored.
There are in fact two forward engines included within the Jena code base,
  an earlier non-RETE implementation is retained for now because it can be more
  efficient in some circumstances but has identical external semantics. This alternative
  engine is likely to be eliminated in a future release once more tuning has been
  done to the default RETE engine.
[Rule Index] [Main Index]
Backward chaining engine
If the rule reasoner is run in backward chaining mode it uses a logic programming
  (LP) engine with a similar execution strategy to Prolog engines. When the inference
  Model is queried then the query is translated into a goal and the engine attempts
  to satisfy that goal by matching to any stored triples and by goal resolution
  against the backward chaining rules.
Except as noted below rules will be executed in top-to-bottom, left-to-right
  order with backtracking, as in SLD resolution. In fact, the rule language is
  essentially datalog rather than full prolog, whilst the functor syntax within
  rules does allow some creation of nested data structures they are flat (not
  recursive) and so can be regarded a syntactic sugar for datalog.
As a datalog language the rule syntax is a little surprising because it restricts
  all properties to be binary (as in RDF) and allows variables in any position
  including the property position. In effect, rules of the form:
(s, p, o), (s1, p1, o1) ... <- (sb1, pb1, ob1), .... 
Can be thought of as being translated to datalog rules of the form:
triple(s, p, o)    :- triple(sb1, pb1, ob1), ...
triple(s1, p1, o1) :- triple(sb1, pb1, ob1), ...
...
where "triple/3" is a hidden implicit predicate. Internally, this
  transformation is not actually used, instead the rules are implemented directly.
In addition, all the data in the raw model supplied to the engine is treated
  as if it were a set of triple(s,p,o) facts which are prepended to
  the front of the rule set. Again, the implementation does not actually work
  that way but consults the source graph, with all its storage and indexing capabilities,
  directly.
Because the order of triples in a Model is not defined then this is one violation
  to strict top-to-bottom execution. Essentially all ground facts are consulted
  before all rule clauses but the ordering of ground facts is arbitrary.
Tabling
The LP engine supports tabling. When a goal is tabled then all previously computed
  matches to that goal are recorded (memoized) and used when satisfying future
  similar goals. When such a tabled goal is called and all known answers have
  been consumed then the goal will suspend until some other execution branch has
  generated new results and then be resumed. This allows one to successfully run
  recursive rules such as transitive closure which would be infinite loops in
  normal SLD prolog. This execution strategy, SLG, is essentially the same as
  that used in the well known XSB system.
In the Jena rule engine the goals to be tabled are identified by the property
  field of the triple. One can request that all goals be tabled by calling the
  tableAll() primitive or that all goals involving a given property
  P be tabled by calling table(P). Note that if any
  property is tabled then goals such as (A, ?P, ?X) will all be tabled
  because the property variable might match one of the tabled properties.
Thus the rule set:
-> table(rdfs:subClassOf).
[r1: (?A rdfs:subClassOf ?C) <- (?A rdfs:subClassOf ?B) (?B rdfs:subClassOf ?C)]
will successfully compute the transitive closure of the subClassOf relation.
  Any query of the form (*, rdfs:subClassOf, *) will be satisfied by a mixture
  of ground facts and resolution of rule r1. Without the first line this rule
  would be an infinite loop. 
The tabled results of each query are kept indefinitely. This means that queries
  can exploit all of the results of the subgoals involved in previous queries.
  In essence we build up a closure of the data set in response to successive queries.
  The reset() operation on the inference model will force these tabled
  results to be discarded, thus saving memory at the expense of response time
  for future queries.
When the inference Model is updated by adding or removing statements all tabled
  results are discarded by an internal reset() and the next query
  will rebuild the tabled results from scratch. 

Note that backward rules can only have one consequent so that if writing rules that
might be run in either backward or forward mode then they should be limited to a single consequent each.

[Rule Index] [Main Index]
Hybrid rule engine
The rule reasoner has the option of employing both of the individual rule engines
  in conjunction. When run in this hybrid mode the data flows look something
  like this: 

The forward engine runs, as described above, and maintains a set
  of inferred statements in the deductions store. Any forward rules which
  assert new backward rules will instantiate those rules according to the forward
  variable bindings and pass the instantiated rules on to the backward engine.
Queries are answered by using the backward chaining LP engine,
  employing the merge of the supplied and generated rules applied to the merge
  of the raw and deduced data.
This split allows the ruleset developer to achieve greater performance
  by only including backward rules which are relevant to the dataset at hand.
  In particular, we can use the forward rules to compile a set of backward rules
  from the ontology information in the dataset. As a simple example consider trying
  to implement the RDFS subPropertyOf entailments using a rule engine. A simple
  approach would involve rules like:
 (?a ?q ?b) <- (?p rdfs:subPropertyOf ?q), (?a ?p ?b) .

Such a rule would work but every goal would match the head of
  this rule and so every query would invoke a dynamic test for whether there was
  a subProperty of the property being queried for. Instead the hybrid rule:
(?p rdfs:subPropertyOf ?q), notEqual(?p,?q) -> [ (?a ?q ?b) <- (?a ?p ?b) ] .
would precompile all the declared subPropertyOf relationships
  into simple chain rules which would only fire if the query goal references a
  property which actually has a sub property. If there are no subPropertyOf relationships
  then there will be no overhead at query time for such a rule.
Note that there are no loops in the above data flows. The backward
  rules are not employed when searching for matches to forward rule terms. This
  two-phase execution is simple to understand and keeps the semantics of the rule
  engines straightforward. However, it does mean that care needs to be take when
  formulating rules. If in the above example there were ways that the subPropertyOf
  relation could be derived from some other relations then that derivation would
  have to be accessible to the forward rules for the above to be complete.
Updates to an inference Model working in hybrid mode will discard
  all the tabled LP results, as they do in the pure backward case. However, the
  forward rules still work incrementally, including incrementally asserting or
  removing backward rules in response to the data changes.
[Rule Index] [Main Index]
GenericRuleReasoner configuration
As with the other reasoners there are a set of parameters, identified by RDF
  properties, to control behaviour of the GenericRuleReasoner. These
  parameters can be set using the Reasoner.setParameter call or passed
  into the Reasoner factory in an RDF Model.
The primary parameter required to instantiate a useful GenericRuleReasoner
  is a rule set which can be passed into the constructor, for example:
String ruleSrc = "[rule1: (?a eg:p ?b) (?b eg:p ?c) -&gt; (?a eg:p ?c)]";
List rules = Rule.parseRules(ruleSrc);
...
Reasoner reasoner = new GenericRuleReasoner(rules);</pre>
A short cut, useful when the rules are defined in local text files using the
  syntax described earlier, is the ruleSet parameter which gives
  a file name which should be loadable from either the classpath or relative to
  the current working directory.

Summary of parameters

  
    Parameter
    Values
    Description
  
  
    
      PROPruleMode
    
    "forward", "forwardRETE", "backward",
      "hybrid" 
    
      Sets the rule direction mode as discussed above. Default
        is "hybrid".
    
  
  
    
      PROPruleSet
    
    filename-string
    
      The name of a rule text file which can be found on the
        classpath or from the current directory. 
    
  
  
    
      PROPenableTGCCaching
    
    Boolean
    
      If true, causes an instance of the TransitiveReasoner
        to be inserted in the forward dataflow to cache the transitive closure
        of the subProperty and subClass lattices.
    
  
  
    
      PROPenableFunctorFiltering
    
    Boolean
    
      If set to true, this causes the structured literals (functors)
        generated by rules to be filtered out of any final queries. This allows
        them to be used for storing intermediate results hidden from the view
        of the InfModel's clients.
    
  
  
    
      PROPenableOWLTranslation
    
    Boolean
    
      If set to true this causes a procedural preprocessing
        step to be inserted in the dataflow which supports the OWL reasoner (it
        translates intersectionOf clauses into groups of backward rules in a way
        that is clumsy to express in pure rule form).
    
  
  
    
      PROPtraceOn
    
    Boolean
    
      If true, switches on exhaustive tracing of rule executions
        at the INFO level.
    
  
  
    
      PROPderivationLogging
    
    Boolean
    
      If true, causes derivation routes to be recorded internally
        so that future getDerivation calls can return useful information.
    
  

[Rule Index] [Main Index]
Builtin primitives
The procedural primitives which can be called by the rules are each implemented
  by a Java object stored in a registry. Additional primitives can be created
  and registered - see below for more details.
Each primitive can optionally be used in either the rule body, the rule head
  or both. If used in the rule body then as well as binding variables (and any
  procedural side-effects like printing) the primitive can act as a test - if
  it returns false the rule will not match. Primitives used in the rule head
  are only used for their side effects.
The set of builtin primitives available at the time writing are:

  
    Builtin
    Operations
  
  
    
      isLiteral(?x) notLiteral(?x)
        isFunctor(?x) notFunctor(?x)
        isBNode(?x) notBNode(?x)
    
    
      Test whether the single argument is or is not a literal,
        a functor-valued literal or a blank-node, respectively.
    
  
  
    bound(?x...) unbound(?x..)
    
      Test if all of the arguments are bound (not bound) variables
    
  
  
    equal(?x,?y) notEqual(?x,?y)
    
      Test if x=y (or x != y). The equality test is semantic
        equality so that, for example, the xsd:int 1 and the xsd:decimal 1 would
        test equal.
    
  
  
    
      lessThan(?x, ?y), greaterThan(?x, ?y)
        le(?x, ?y), ge(?x, ?y)
    
    
      Test if x is <, >, <= or >= y. Only passes
        if both x and y are numbers or time instants (can be integer or
floating point or XSDDateTime).
    
  
  
    
      sum(?a, ?b, ?c)
        addOne(?a, ?c)
        difference(?a, ?b, ?c)
        min(?a, ?b, ?c)
        max(?a, ?b, ?c)
        product(?a, ?b, ?c)
        quotient(?a, ?b, ?c)
      
    
      Sets c to be (a+b), (a+1) (a-b), min(a,b), max(a,b), (a*b), (a/b). Note that these
        do not run backwards, if in sum a and c are bound and b is
        unbound then the test will fail rather than bind b to (c-a). This could
        be fixed.
    
  
  
    
      strConcat(?a1, .. ?an, ?t)
        uriConcat(?a1, .. ?an, ?t)
      
    
      Concatenates the lexical form of all the arguments except
      the last, then binds the last argument to a plain literal (strConcat) or a
      URI node (uriConcat) with that lexical form. In both cases if an argument
      node is a URI node the URI will be used as the lexical form.
    
  
  
    
      regex(?t, ?p)regex(?t, ?p, ?m1, .. ?mn)
      
    
      Matches the lexical form of a literal (?t) against
      a regular expression pattern given by another literal (?p).
      If the match succeeds, and if there are any additional arguments then
      it will bind the first n capture groups to the arguments ?m1 to ?mn.
      The regular expression pattern syntax is that provided by java.util.regex.
      Note that the capture groups are numbered from 1 and the first capture group
      will be bound to ?m1, we ignore the implicit capture group 0 which corresponds to
      the entire matched string. So for example
      regexp('foo bar', '(.*) (.*)', ?m1, ?m2)
      will bind m1 to "foo" and m2 to "bar".
    
  
  
    now(?x)
    
      Binds ?x to an xsd:dateTime value corresponding to the current time.
    
  
  
    makeTemp(?x)
    
      Binds ?x to a newly created blank node.
    
  
  
    makeInstance(?x, ?p, ?v)
      makeInstance(?x, ?p, ?t, ?v)
    
      Binds ?v to be a blank node which is asserted as the value
        of the ?p property on resource ?x and optionally has type ?t. Multiple
        calls with the same arguments will return the same blank node each time
        - thus allowing this call to be used in backward rules.
    
  
  
    makeSkolem(?x, ?v1, ... ?vn)
    
      Binds ?x to be a blank node. The blank node is generated
      based on the values of the remain ?vi arguments, so the same combination of
      arguments will generate the same bNode.
    
  
  
    noValue(?x, ?p)
      noValue(?x ?p ?v)
    
      True if there is no known triple (x, p, *) or (x, p, v)
        in the model or the explicit forward deductions so far. 
    
  
  
    remove(n, ...)drop(n, ...)
    
      Remove the statement (triple) which caused the n'th body
        term of this (forward-only) rule to match. Remove will propagate the
        change to other consequent rules including the firing rule (which must
        thus be guarded by some other clauses).
        In particular, if the removed statement (triple) appears in the body of
        a rule that has already fired, the consequences of such rule are
        retracted from the deducted model.
         Drop will silently remove the
        triple(s) from the graph but not fire any rules as a consequence.
        These are clearly non-monotonic operations and, in particular, the
        behaviour of a rule set in which different rules both drop and create the
        same triple(s) is undefined.
    
  
  
    isDType(?l, ?t) notDType(?l, ?t)
    
      Tests if literal ?l is (or is not) an instance of the
        datatype defined by resource ?t.
    
  
  
    print(?x, ...)
    
      Print (to standard out) a representation of each argument.
        This is useful for debugging rather than serious IO work.
    
  
  
    listContains(?l, ?x) listNotContains(?l, ?x)
    
      Passes if ?l is a list which contains (does not contain) the element ?x,
      both arguments must be ground, can not be used as a generator.
    
  
  
    listEntry(?list, ?index, ?val)
    
      Binds ?val to the ?index'th entry
in the RDF list ?list. If there is no such entry the variable will be unbound
and the call will fail. Only usable in rule bodies.
    
  
  
    listLength(?l, ?len)
    
      Binds ?len to the length of the list ?l.
    
  
  
    listEqual(?la, ?lb) listNotEqual(?la, ?lb)
    
      listEqual tests if the two arguments are both lists and contain
      the same elements. The equality test is semantic equality on literals (sameValueAs) but
      will not take into account owl:sameAs aliases. listNotEqual is the negation of this (passes if listEqual fails).
    
  
  
    listMapAsObject(?s, ?p ?l)  listMapAsSubject(?l, ?p, ?o)
    
      These can only be used as actions in the head of a rule.
      They deduce a set of triples derived from the list argument ?l : listMapAsObject asserts
      triples (?s ?p ?x) for each ?x in the list ?l, listMapAsSubject asserts triples (?x ?p ?o). 
    
  
  
    table(?p) tableAll()
    
      Declare that all goals involving property ?p (or all goals)
        should be tabled by the backward engine.
    
  
  
    hide(p)
    
      Declares that statements involving the predicate p should be hidden.
Queries to the model will not report such statements. This is useful to enable non-monotonic
forward rules to define flag predicates which are only used for inference control and
do not "pollute" the inference results.
    
  

[Rule Index] [Main Index]
Example
As a simple illustration suppose we wish to create a simple ontology language
  in which we can declare one property as being the concatenation of two others
  and to build a rule reasoner to implement this.
As a simple design we define two properties eg:concatFirst, eg:concatSecond
  which declare the first and second properties in a concatenation. Thus the triples:
eg:r eg:concatFirst  eg:p .
eg:r eg:concatSecond eg:q .
mean that the property r = p o q.
Suppose we have a Jena Model rawModel which contains the above assertions together
  with the additional facts:
eg:A eg:p eg:B .
eg:B eg:q eg:C .
Then we want to be able to conclude that A is related to C through the composite
  relation r. The following code fragment constructs and runs a rule reasoner
  instance to implement this:
String rules =
    "[r1: (?c eg:concatFirst ?p), (?c eg:concatSecond ?q) -&gt; " +
    "     [r1b: (?x ?c ?y) &lt;- (?x ?p ?z) (?z ?q ?y)] ]";
Reasoner reasoner = new GenericRuleReasoner(Rule.parseRules(rules));
InfModel inf = ModelFactory.createInfModel(reasoner, rawData);
System.out.println("A * * =&gt;");
Iterator list = inf.listStatements(A, null, (RDFNode)null);
while (list.hasNext()) {
    System.out.println(" - " + list.next());
}
When run on a rawData model contain the above four triples this generates the
  (correct) output:
A * * =>
 - [urn:x-hp:eg/A, urn:x-hp:eg/p, urn:x-hp:eg/B]
 - [urn:x-hp:eg/A, urn:x-hp:eg/r, urn:x-hp:eg/C]
Example 2
As a second example, we'll look at ways to define a property as being both
  symmetric and transitive. Of course, this can be done directly in OWL but there
  are times when one might wish to do this outside of the full OWL rule set and,
  in any case, it makes for a compact illustration.
This time we'll put the rules in a separate file to simplify editing them and
  we'll use the machinery for configuring a reasoner using an RDF specification.
  The code then looks something like this:
// Register a namespace for use in the demo
String demoURI = "http://jena.hpl.hp.com/demo#";
PrintUtil.registerPrefix("demo", demoURI);

// Create an (RDF) specification of a hybrid reasoner which
// loads its data from an external file.
Model m = ModelFactory.createDefaultModel();
Resource configuration =  m.createResource();
configuration.addProperty(ReasonerVocabulary.PROPruleMode, "hybrid");
configuration.addProperty(ReasonerVocabulary.PROPruleSet,  "data/demo.rules");

// Create an instance of such a reasoner
Reasoner reasoner = GenericRuleReasonerFactory.theInstance().create(configuration);

// Load test data
Model data = RDFDataMgr.loadModel("file:data/demoData.rdf");
InfModel infmodel = ModelFactory.createInfModel(reasoner, data);

// Query for all things related to "a" by "p"
Property p = data.getProperty(demoURI, "p");
Resource a = data.getResource(demoURI + "a");
StmtIterator i = infmodel.listStatements(a, p, (RDFNode)null);
while (i.hasNext()) {
    System.out.println(" - " + PrintUtil.print(i.nextStatement()));
}
Here is file data/demo.rules which defines property demo:p
  as being both symmetric and transitive using pure forward rules:
[transitiveRule: (?A demo:p ?B), (?B demo:p ?C) -> (?A > demo:p ?C) ]
[symmetricRule: (?Y demo:p ?X) -> (?X demo:p ?Y) ] 
 Running this on data/demoData.rdf gives the
  correct output:
- (demo:a demo:p demo:c)
- (demo:a demo:p demo:a)
- (demo:a demo:p demo:d)
- (demo:a demo:p demo:b)
However, those example rules are overly specialized. It would be better to
  define a new class of property to indicate symmetric-transitive properties and
  and make demo:p a member of that class. We can generalize the rules
  to support this:
[transitiveRule: (?P rdf:type demo:TransProp)(?A ?P ?B), (?B ?P ?C)
                     -> (?A ?P ?C) ]
[symmetricRule: (?P rdf:type demo:TransProp)(?Y ?P ?X)
                     -> (?X ?P ?Y) ]
 These rules work but they compute the complete symmetric-transitive closure
  of p when the graph is first prepared. Suppose we have a lot of p values but
  only want to query some of them it would be better to compute the closure on
  demand using backward rules. We could do this using the same rules run in pure
  backward mode but then the rules would fire lots of times as they checked every
  property at query time to see if it has been declared as a demo:TransProp.
  The hybrid rule system allows us to get round this by using forward rules to
  recognize any demo:TransProp declarations once and to generate
  the appropriate backward rules:
-> tableAll().
[rule1: (?P rdf:type demo:TransProp) ->
[ (?X ?P ?Y) <- (?Y ?P ?X) ]
[ (?A ?P ?C) <- (?A ?P ?B), (?B ?P ?C) ]
] 
[Rule Index] [Main Index]
Combining RDFS/OWL with custom rules
Sometimes one wishes to write generic inference rules but combine them
 with some RDFS or OWL inference. With the current Jena architecture limited forms of this
 is possible but you need to be aware of the limitations.
There are two ways of achieving this sort of configuration within Jena (not
 counting using an external engine that already supports such a combination).
Firstly, it is possible to cascade reasoners, i.e. to construct one InfModel
 using another InfModel as the base data. The strength of this approach is that
 the two inference processes are separate and so can be of different sorts. For
 example one could create a GenericRuleReasoner whose base model is an external
 OWL reasoner. The chief weakness of the approach is that it is "layered" - the
 outer InfModel can see the results of the inner InfModel but not vice versa.
 For some applications that layering is fine and it is clear which way the
 inference should be layered, for some it is not. A second possible weakness
 is performance. A query to an InfModel is generally expensive and involves lots
 of queries to the data. The outer InfModel in our layered case will
 typically issue a lot of queries to the inner model, each of which may
 trigger more inference. If the inner model caches all of its inferences
 (e.g. a forward rule engine) then there may not be very much redundancy there but
 if not then performance can suffer dramatically. 
Secondly, one can create a single GenericRuleReasoner whose rules combine
 rules for RDFS or OWL and custom rules. At first glance this looks like it
 gets round the layering limitation. However, the default Jena RDFS and OWL
 rulesets use the Hybrid rule engine. The hybrid engine is itself layered, forward rules
 do not see the results of any backward rules. Thus layering is still present though you
 have finer grain control - all your inferences you want the RDFS/OWL rules to see
 should be forward, all the inferences which need all of the results of the RDFS/OWL rules
 should be backward. Note that the RDFS and OWL rulesets assume certain settings
 for the GenericRuleReasoner so a typical configuration is:
Model data = RDFDataMgr.loadModel("file:data.n3");
List rules = Rule.rulesFromURL("myrules.rules");

GenericRuleReasoner reasoner = new GenericRuleReasoner(rules);
reasoner.setOWLTranslation(true);               // not needed in RDFS case
reasoner.setTransitiveClosureCaching(true);

InfModel inf = ModelFactory.createInfModel(reasoner, data);
Where the myrules.rules file will use @include to include
 one of the RDFS or OWL rule sets.
One  useful variant on this option, at least in simple cases, is
 to manually include a pure (non-hybrid) ruleset for the RDFS/OWL fragment
 you want so that there is no layering problem. [The reason the default
 rulesets use the hybrid mode is a performance tradeoff - trying to
 balance the better performance of forward reasoning with the cost of
 computing all possible answers when an application might only want a few.]

 A simple example of this is that the interesting bits of RDFS
 can be captured by enabling TransitiveClosureCaching and including just the
 four core rules:
[rdfs2:  (?x ?p ?y), (?p rdfs:domain ?c) -> (?x rdf:type ?c)]
[rdfs3:  (?x ?p ?y), (?p rdfs:range ?c) -> (?y rdf:type ?c)]
[rdfs6:  (?a ?p ?b), (?p rdfs:subPropertyOf ?q) -> (?a ?q ?b)]
[rdfs9:  (?x rdfs:subClassOf ?y), (?a rdf:type ?x) -> (?a rdf:type ?y)]

[Rule Index] [Main Index]
Notes
One final aspect of the general rule engine to mention is that of validation
  rules. We described earlier how reasoners can implement a validate
  call which returns a set of error reports and warnings about inconsistencies
  in a dataset. Some reasoners (e.g. the RDFS reasoner) implement this feature
  through procedural code. Others (e.g. the OWL reasoner) does so using yet more
  rules.
Validation rules take the general form:
(?v rb:validation on()) ...  ->
    [ (?X rb:violation error('summary', 'description', args)) <- ...) ] .
The validation calls can be "switched on" by inserting an
  additional triple into the graph of the form:
_:anon rb:validation on() .
This makes it possible to build rules, such as the template above, which are
  ignored unless validation has been switched on - thus avoiding potential overhead
  in normal operation. This is optional and the "validation on()" guard
  can be omitted.
Then the validate call queries the inference graph for all triples of the form:
?x rb:violation f(summary, description, args) .
The subject resource is the "prime suspect" implicated in the inconsistency,
  the relation rb:violation is a reserved property used to communicate
  validation reports from the rules to the reasoner, the object is a structured
  (functor-valued) literal. The name of the functor indicates the type of violation
  and is normally error or warning, the first argument
  is a short form summary of the type of problem, the second is a descriptive
  text and the remaining arguments are other resources involved in the inconsistency.

Future extensions will improve the formatting capabilities and flexibility
  of this mechanism. 
[Rule Index] [Main Index]
Extensions
There are several places at which the rule system can be extended by application
  code.
Rule syntax
First, as mentioned earlier, the rule engines themselves only see rules in
  terms of the Rule Java object. Thus applications are free to define an alternative
  rule syntax so long as it can be compiled into Rule objects.
Builtins
Second, the set of procedural builtins can be extended. A builtin should implement
  the Builtin
  interface. The easiest way to achieve this is by subclassing BaseBuiltin
  and defining a name (getName), the number of arguments expected
  (getArgLength) and one or both of bodyCall and headAction.
  The bodyCall method is used when the builtin is invoked in the
  body of a rule clause and should return true or false according to whether the
  test passes. In both cases the arguments may be variables or bound values and
  the supplied RuleContext
  object can be used to dereference bound variables and to bind new variables.

Once the Builtin has been defined then an instance of it needs to be registered
  with BuiltinRegistry
  for it to be seen by the rule parser and interpreters.
The easiest way to experiment with this is to look at the examples in the builtins
  directory. 
Preprocessing hooks
The rule reasoner can optionally run a sequence of procedural preprocessing
  hooks over the data at the time the inference graph is prepared. These
  procedural hooks can be used to perform tests or translations which are slow
  or inconvenient to express in rule form. See GenericRuleReasoner.addPreprocessingHook
  and the RulePreprocessHook
  class for more details.
[Index]
Extending the inference support
Apart from the extension points in the rule reasoner discussed above, the intention
  is that it should be possible to plug external inference engines into Jena.
  The core interfaces of InfGraph and Reasoner are kept
  as simple and generic as we can to make this possible and the ReasonerRegistry
  provides a means for mapping from reasoner ids (URIs) to reasoner instances
  at run time.
In a future Jena release we plan to provide at least one adapter to an example,
  freely available, reasoner to both validate the machinery and to provide an
  example of how this extension can be done.
[Index]
Futures
Contributions for the following areas would be very welcome:

  Develop a custom equality reasoner which can handle the "owl:sameAs"
    and related processing more efficiently that the plain rules engine.
  Tune the RETE engine to perform better with highly non-ground patterns.
  Tune the LP engine to further reduce memory usage (in particular explore
    subsumption tabling rather than the current variant tabling).
  Investigate routes to better integrating the rule reasoner with underlying
    database engines. This is a rather larger and longer term task than the others
    above and is the least likely to happen in the near future.

[Index]

  
  
  
    On this page
    
  
    Overview of inference support
      
        Available reasoners
      
    
    The Inference API
      
        Generic reasoner API
          
            Finding a reasoner
            Configuring a reasoner
            Applying a reasoner to data
            Accessing inferences
            Reasoner description
          
        
        Some small examples
        Operations on inference models
          
            Validation
            Extended list statements
            Direct and indirect relationships
            Derivations
            Accessing raw data and deductions
            Processing control
            Tracing
          
        
      
    
    The RDFS reasoner
      
        RDFS reasoner - intro and coverage
        RDFS configuration
          
            Summary of parameters
          
        
        RDFS Example
        RDFS implementation and performance notes
      
    
    The OWL reasoner
      
        OWL coverage
        OWL Configuration
        OWL Example
        OWL notes and limitations
          
            Comprehension axioms
            Prototypes
            Direct/indirect
            Performance
            Incompleteness
          
        
      
    
    The transitive reasoner
    The general purpose rule engine
      
        Overview of the rule engine(s)
        Rule syntax and structure
        Forward chaining engine
        Backward chaining engine
          
            Tabling
          
        
        Hybrid rule engine
        GenericRuleReasoner configuration
          
            Summary of parameters
          
        
        Builtin primitives
        Example
          
            Example 2
          
        
        Combining RDFS/OWL with custom rules
        Notes
        Extensions
          
            Rule syntax
            Builtins
            Preprocessing hooks
          
        
      
    
    Extending the inference support
    Futures\n\n\n\nThis section is a general introduction to the Jena
ontology API, including some of the common tasks you may need
to perform. We
won’t go into all of the many details of the API here: you should
expect to refer to the Javadoc to
get full details of the capabilities of the API.
Please note that this section covers the new Jena ontology API, which has been introduced since Jena 5.1.0.
The legacy Jena Ontology API documentation can be found here.
Prerequisites
We’ll assume that you have a basic familiarity with RDF and with
Jena. If not, there are other
Jena help documents you can read for background
on these topics, and a collection of tutorials.
Jena is a programming toolkit, using the Java programming language.
While there are a few command-line tools to help you perform some
key tasks using Jena, mostly you use Jena by writing Java programs.
The examples in this document will be primarily code samples.
We also won’t be explaining the OWL or RDFS ontology languages in
much detail in this document. You should refer to
supporting documentation for details on those languages, for
example the W3C OWL document index.
Overview
The section of the manual is broken into a number of sections. You
do not need to read them in sequence, though later sections may
refer to concepts and techniques introduced in earlier sections.
The sections are:

General concepts
Running example: the ESWC ontology
Creating ontology models
Compound ontology documents and imports processing
GraphRepository
GraphMaker
OntModel triple representation: OntStatement
The generic ontology type: OntObject
Ontology entities
Ontology classes
Ontology dataranges
Ontology properties
Instances or individuals
Ontology meta-data
Ontology inference: overview
Working with persistent ontologies
Utilities

Further assistance
Hopefully, this document will be sufficient to help most readers
to get started using the Jena ontology API. For further support,
please post questions to the Jena support list,
or file a bug report.
Please note that we ask that you use the support list or the bug-tracker
to communicate with the Jena team, rather than send email to the team
members directly. This helps us manage Jena support more effectively,
and facilitates contributions from other Jena community members.
General concepts
In a widely-quoted definition, an ontology is

“a specification of a conceptualization”
[Gruber, T. 1993]

Let’s unpack that brief characterisation a bit. An
ontology allows a programmer to specify, in an open, meaningful,
way, the concepts and relationships that collectively characterise
some domain of interest. Examples might be the concepts of red and white wine,
grape varieties, vintage years, wineries and so forth that
characterise the domain of ‘wine’, and relationships such as
‘wineries produce wines’, ‘wines have a year of production’. This
wine ontology might be developed initially for a particular
application, such as a stock-control system at a wine warehouse. As
such, it may be considered similar to a well-defined database
schema. The advantage to an ontology is that it is an explicit,
first-class description. So having been developed for one purpose,
it can be published and reused for other purposes. For example, a
given winery may use the wine ontology to link its production
schedule to the stock system at the wine warehouse. Alternatively,
a wine recommendation program may use the wine ontology, and a
description (ontology) of different dishes to recommend wines for a
given menu.
There are many ways of writing down an ontology, and a variety of
opinions as to what kinds of definition should go in one. In
practice, the contents of an ontology are largely driven by the
kinds of application it will be used to support. In Jena, we do not
take a particular view on the minimal or necessary components of an
ontology. Rather, we try to support a variety of common techniques.
In this section, we try to explain what is – and to some extent what
isn’t – possible using Jena’s ontology support.
Since Jena is fundamentally an RDF platform, Jena’s ontology
support is limited to ontology formalisms built on top of RDF.
Specifically this means RDFS,
the varieties of
OWL.
We will provide a very brief introduction to these languages here,
but please refer to the extensive on-line documentation for these
formalisms for complete and authoritative details.
RDFS
RDFS is the weakest ontology language supported by Jena. RDFS
allows the ontologist to build a simple hierarchy of concepts, and
a hierarchy of properties. Consider the following trivial
characterisation (with apologies to biology-trained readers!):

Table 1: A simple concept hierarchy
Using RDFS, we can say that my ontology has five classes, and that
Plant is a sub-class of Organism and so on. So every animal
is also an organism. A good way to think of these classes is as
describing sets of individuals: organism is intended to describe
a set of living things, some of which are animals (i.e. a sub-set
of the set of organisms is the set of animals), and some animals
are fish (a subset of the set of all animals is the set of all
fish).
To describe the attributes of these classes, we can associate
properties with the classes. For example, animals have sensory
organs (noses, eyes, etc.). A general property of an animal might
be senseOrgan, to denote any given sensory organs a particular
animal has. In general, fish have eyes, so a fish might have a
eyes property to refer to a description of the particular eye
structure of some species. Since eyes are a type of sensory organ,
we can capture this relationship between these properties by saying
that eye is a sub-property-of senseOrgan. Thus if a given fish
has two eyes, it also has two sense organs. (It may have more, but
we know that it must have two).
We can describe this simple hierarchy with RDFS. In general, the
class hierarchy is a graph rather than a tree (i.e. not like Java
class inheritance). The
slime mold is popularly,
though perhaps not accurately, thought of as an organism that has
characteristics of both plants and animals. We might model a slime
mold in our ontology as a class that has both plant and animal
classes among its super-classes. RDFS is too weak a language to
express the constraint that a thing cannot be both a plant and an animal (which is
perhaps lucky for the slime molds). In RDFS, we can only name the
classes, we cannot construct expressions to describe interesting
classes. However, for many applications it is sufficient to state
the basic vocabulary, and RDFS is perfectly well suited to this.
Note also that we can both describe classes, in general terms, and we
can describe particular instances of those classes. So there may
be a particular individual Fred who is a Fish (i.e. has
rdf:type Fish), and who has two eyes. Their companion Freda, a
Mexican Tetra, or
blind cave fish, has no eyes. One use of an ontology is to allow us
to fill-in missing information about individuals. Thus, though it
is not stated directly, we can deduce that Fred is also an Animal
and an Organism. Assume that there was no rdf:type asserting that
Freda is a Fish. We may still infer Freda’s rdf:type since Freda
has lateral lines as
sense organs, and these only occur in fish. In RDFS, we state that
the domain of the lateralLines property is the Fish class, so
an RDFS reasoner can infer that Freda must be a fish.
OWL
In general, OWL allows us to say everything that RDFS allows, and
much more besides. A key part of OWL is the ability to describe
classes in more interesting and complex ways. For example, in OWL
we can say that Plant and Animal are disjoint classes: no
individual can be both a plant and an animal (which would have the
unfortunate consequence of making SlimeMold an empty class).
SaltwaterFish might be the intersection of Fish and the class
SeaDwellers (which also includes, for example, cetaceans and sea
plants).
Suppose we have a property covering, intended to represent the
scales of a fish or the fur of a mammal. We can now refine the
mammal class to be ‘animals that have a covering that is hair’,
using a property restriction to express the condition that
property covering has a value from the class Hair. Similarly
TropicalFish might be the intersection of the class of Fish and
the class of things that have TropicalOcean as their habitat.
Finally (for this brief overview), we can say more about properties
in OWL. In RDFS, properties can be related via a property
hierarchy. OWL extends this by allowing properties to be denoted as
transitive, symmetric or functional, and allow one property
to be declared to be the inverse of another. OWL also makes a
distinction between properties that have individuals (RDF resources)
as their range and properties that have data-values (known as
literals in RDF terminology) as their range.
Respectively these are object properties and datatype properties.
One consequence of the RDF lineage of OWL is
that OWL ontologies cannot make statements about literal values. We
cannot say in RDF that seven has the property of being a prime number.
We can, of course, say that the class of primes includes seven, doing so
doesn’t require a number to be the subject of an RDF statement. In
OWL, this distinction is important: only object properties can
be transitive or symmetric.
The OWL language is sub-divided into several syntax classes:
OWL2 Full, OWL2 DL, OWL2 RL, OWL2 EL, OWL2 QL,
and also OWL1 Lite, OWL1 DL and OWL1 Full.
The last three are deprecated now.
OWL2 EL, OWL2 QL and OWL2 RL do not permit some constructions allowed in OWL2 Full and OWL2 DL.
Although OWL1 is deprecated, Jena Ontology API still supports it.
The intent for OWL2 RL, EL, QL, and also OWL1 Lite and OWL1 DL,
is to make the task of reasoning with expressions in that subset more tractable.
Specifically, OWL (1 & 2) DL is intended to be able to be processed efficiently by a
description logic
reasoner. OWL1 Lite is intended to be amenable to processing by a
variety of reasonably simple inference algorithms, though experts
in the field have challenged how successfully this has been
achieved.
OWL 2 EL is particularly useful in
applications employing ontologies that contain very large numbers of properties and/or classes.
The EL acronym reflects the profile’s basis in the EL family of description logics,
logics that provide only Existential quantification.
OWL 2 QL is aimed at applications that
use very large volumes of instance data, and where query answering is
the most important reasoning task.
The QL acronym reflects the fact that query answering in this profile
can be implemented by rewriting queries into a standard relational Query Language.
OWL 2 RL is aimed at applications that
require scalable reasoning without sacrificing too much expressive power.
The RL acronym reflects the fact that reasoning in this profile can be implemented using a standard Rule Language.
While the OWL standards documents note that OWL builds on top of
the (revised) RDF specifications, it is possible to treat OWL as a
separate language in its own right, and not something that is built
on an RDF foundation. This view uses RDF as a serialisation syntax;
the RDF-centric view treats RDF triples as the core of the OWL
formalism. While both views are valid, in Jena we take the
RDF-centric view.
Ontology languages and the Jena Ontology API
As we outlined above, there are various different ontology languages
available for representing ontology information on the semantic
web. They range from the most expressive, OWL Full, through to the
weakest, RDFS. Through the Ontology API, Jena aims to provide a
consistent programming interface for ontology application
development, independent of which ontology language you are using
in your programs.
The Jena Ontology API is language-neutral: the Java class names are not
specific to the underlying language. For example, the OntClass
Java class can represent an OWL class or RDFS class.
To represent the differences between the various representations,
each of the ontology languages has a specification, which lists the
permitted constructs and the names of the classes and properties.
Thus in the OWL profile is it owl:ObjectProperty (short for
http://www.w3.org/2002/07/owl#ObjectProperty) and in the RDFS
attempt to get an object property will cause an error
and search for all object properties will return empty java Stream.
The specification is bound to an ontology model, which is an extended
version of Jena’s
Model class.
The base Model allows access to the statements in a collection of
RDF data.
OntModel
extends this by adding support for the kinds of constructs expected to
be in an ontology: classes (in a class hierarchy), properties (in a
property hierarchy) and individuals.
When you’re working with an
ontology in Jena, all of the state information remains encoded as
RDF triples (accessed as Jena
Statements) stored in the RDF
model. The ontology API
doesn’t change the RDF representation of ontologies. What it does
do is add a set of convenience classes and methods that make it
easier for you to write programs that manipulate the underlying RDF
triples.
The predicate names defined in the ontology language correspond to
the accessor methods on the Java classes in the API. For example,
an OntClass has a method to list its super-classes, which
corresponds to the values of the subClassOf property in the RDF
representation. This point is worth re-emphasising: no information
is stored in the OntClass object itself. When you call the
OntClass superClasses() method, Jena will retrieve the
information from the underlying RDF triples. Similarly, adding a
subclass to an OntClass asserts an additional RDF triple, typically
with predicate rdfs:subClassOf into
the model.
Ontologies and reasoning
One of the key benefits of building an ontology-based application
is using a reasoner to derive additional truths about the concepts
you are modelling. We saw a simple instance of this above: the
assertion “Fred is a Fish” entails the deduction “Fred is an
Animal”. There are many different styles of automated reasoner, and
very many different reasoning algorithms. Jena includes support for
a variety of reasoners through the
inference API.
A common feature of Jena
reasoners is that they create a new RDF model which appears to
contain the triples that are derived from reasoning as well as the
triples that were asserted in the base model. This extended model
nevertheless still conforms to the contract for Jena models.
It can be used wherever a non-inference model can be used. The ontology
API exploits this feature: the convenience methods provide by the ontology API
can query an extended inference model in just the same way
that they can a plain RDF model. In fact, this is such a common pattern that
we provide simple recipes for constructing ontology models whose
language, storage model and reasoning engine can all be simply
specified when an OntModel is created. We’ll show examples shortly.
Figure 2 shows one way of visualising this:

Graph is an internal Jena interface that supports the composition
of sets of RDF triples. The asserted statements, which may have
been read in from an ontology document, are held in the base graph.
The reasoner, or inference engine, can use the contents of the base
graph and the semantic rules of the language to show a more
complete set of base and entailed triples. This is also presented via a Graph
interface, so the OntModel works only with the outermost interface.
This regularity allows us to very easily build ontology models with
or without a reasoner. It also means that the base graph can be an
in-memory store, a database-backed persistent store, or some other
storage structure altogether – e.g. an LDAP directory – again without
affecting the operation of the ontology model (but noting that these
different approaches may have very different efficiency profiles).
RDF-level polymorphism and Java
Deciding which Java abstract class to use to represent a given RDF
resource can be surprisingly subtle. Consider the following RDF
sample:
<owl:Class rdf:ID="DigitalCamera">
</owl:Class>

This declares that the resource with the relative URI
#DigitalCamera is an OWL ontology class. It suggests that it
would be appropriate to model that declaration in Java with an
instance of an OntClass. Now suppose we add a triple to the RDF
model to augment the class declaration with some more information:
<owl:Class rdf:ID="DigitalCamera">
  <rdf:type owl:NamedIndividual />
</owl:Class>

Now we are stating that #DigitalCamera is an OWL Named Individual.
This is valid in OWL2, but, for example, in OWL1 DL,
such a punning is not allowed.
The problem we then have is that Java does not
allow us to dynamically change the Java class of the object
representing this resource. The resource has not changed: it still
has URI #DigitalCamera. But the appropriate Java class Jena might
choose to encapsulate it has changed from OntClass to OntIndividual.
Conversely, if we subsequently remove the rdf:type owl:NamedIndividual
from the model, using the OntIndividual Java class is no longer
appropriate.
Even worse, OWL2 and OWL1 Full allow us to state the following (rather
counter-intuitive) construction:
<owl:Class rdf:ID="DigitalCamera">
  <rdf:type owl:ObjectProperty />
</owl:Class>

That is, #DigitalCamera is both a class and a property. While
this may not be a very useful claim, it illustrates a basic
point: we cannot rely on a consistent or unique mapping between an
RDF resource and the appropriate Java abstraction.
Jena accepts this basic characteristic of polymorphism at the RDF
level by considering that the Java abstraction (OntClass,
OntClass.Restriction, OntDataProperty, etc.) is just a view or facet
of the resource. That is, there is a one-to-many mapping from a
resource to the facets that the resource can present. If the
resource is typed as an owl:Class, it can present the OntClass
facet; given other types, it can present other facets. Jena
provides the .as() method to efficiently map from an RDF object
to one of its allowable facets. Given a RDF object (i.e. an
instance of org.apache.jena.rdf.model.RDFNode or one of its
sub-types), you can get a facet by invoking as() with an argument
that denotes the facet required. Specifically, the facet is
identified by the Java class object of the desired facet. For
example, to get the OntClass facet of a resource, we can write:
Resource r = myModel.getResource( myNS + "DigitalCamera" );
OntClass cls = r.as( OntClass.class );

This pattern allows our code to defer decisions about the correct Java
abstraction to use until run-time. The choice can depend on the
properties of the resource itself. If a given RDFNode will not
support the conversion to a given facet, it will raise a
OntJenaException.Conversion. We can test whether .as() will succeed for a
given facet with canAs(). This RDF-level polymorphism is used
extensively in the Jena ontology API to allow maximum flexibility
in handling ontology data.
Running example: the ESWC ontology
To illustrate the principles of using the ontology API, we will use
examples drawn from the
ESWC ontology
This ontology presents a simple model for describing the concepts
and activities associated with a typical academic conference. A
copy of the ontology serialized in RDF/XML is included with the
Jena download, see:
[eswc-2006-09-21.rdf]
(note that you may need to view the page source in some browsers to
see the XML code).
A subset of the classes and properties from the ontology are shown
in Figure 3:

Figure 3: Classes and properties from ESWC ontology
We will use elements from this ontology to illustrate the ontology
API throughout the rest of this document.
Creating ontology models
An ontology model is an extension of the Jena RDF model,
providing extra capabilities for handling ontologies. Ontology
models are created through the Jena
OntModelFactory.
The simplest way to create an ontology model is as follows:
OntModel m = OntModelFactory.createModel();

This will create an ontology model with the default settings,
which are set for maximum compatibility with the previous version
of Jena. These defaults are:

OWL2-DL language
in-memory triples graph
builtin RDFS inference, which principally produces entailments from the
sub-class and sub-property hierarchies.

The builtin RDFS inference is a cut down inference
which is done by model itself without any attached reasoner.
To have complete RDFS inference use, e.g., OWL2_DL_MEM_RDFS_INF specification.
In many applications, such as driving a GUI, RDFS inference is too
strong. For example, every class is inferred to be an immediate sub-class of
owl:Thing. In other applications, stronger reasoning is needed.
In general, to create an OntModel with a particular reasoner or
language profile, you should pass a model specification to the
createModel call.
For example, an OWL model that performs no reasoning at all can be created with:
OntModel m = OntModelFactory.createModel( OntSpecification.OWL2_DL_MEM );

Beyond these basic choices, the complexities of configuring an
ontology model are wrapped up in a recipe object called
OntSpecification.
This specification allows complete control over the configuration
choices for the ontology model, including the language profile in
use and the reasoner.
A number of common recipes are pre-declared as constants in
OntSpecification, and listed below.

  
      
          OntSpecification
          Language profile
          Storage model
          Reasoner
      
  
  
      
          OWL2_DL_MEM_BILTIN_RDFS_INF
          OWL2 DL
          in-memory
          builtin reasoner with RDFS-level entailment-rules
      
      
          OWL2_DL_MEM
          OWL2 DL
          in-memory
          none
      
      
          OWL2_DL_MEM_TRANS_INF
          OWL2 DL
          in-memory
          transitive class-hierarchy inference
      
      
          OWL2_DL_MEM_RULES_INF
          OWL2 DL
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL2_DL_MEM_RDFS_INF
          OWL2 DL
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          OWL2_FULL_MEM
          OWL2 Full
          in-memory
          none
      
      
          OWL2_FULL_MEM_TRANS_INF
          OWL2 Full
          in-memory
          transitive class-hierarchy inference
      
      
          OWL2_FULL_MEM_RULES_INF
          OWL2 Full
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL2_FULL_MEM_RDFS_INF
          OWL2 Full
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          OWL2_FULL_MEM_MICRO_RULES_INF
          OWL2 Full
          in-memory
          optimised rule-based reasoner with OWL rules
      
      
          OWL2_FULL_MEM_MINI_RULES_INF
          OWL2 Full
          in-memory
          rule-based reasoner with subset of OWL rules
      
      
          OWL2_EL_MEM
          OWL2 EL
          in-memory
          none
      
      
          OWL2_EL_MEM_TRANS_INF
          OWL2 EL
          in-memory
          transitive class-hierarchy inference
      
      
          OWL2_EL_MEM_RULES_INF
          OWL2 EL
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL2_EL_MEM_RDFS_INF
          OWL2 EL
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          OWL2_QL_MEM
          OWL2 QL
          in-memory
          none
      
      
          OWL2_QL_MEM_TRANS_INF
          OWL2 QL
          in-memory
          transitive class-hierarchy inference
      
      
          OWL2_QL_MEM_RULES_INF
          OWL2 QL
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL2_QL_MEM_RDFS_INF
          OWL2 QL
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          OWL2_RL_MEM
          OWL2 RL
          in-memory
          none
      
      
          OWL2_RL_MEM_TRANS_INF
          OWL2 RL
          in-memory
          transitive class-hierarchy inference
      
      
          OWL2_RL_MEM_RULES_INF
          OWL2 RL
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL2_RL_MEM_RDFS_INF
          OWL2 RL
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          OWL1_DL_MEM
          OWL1 DL
          in-memory
          none
      
      
          OWL1_DL_MEM_TRANS_INF
          OWL1 DL
          in-memory
          transitive class-hierarchy inference
      
      
          OWL1_DL_MEM_RULES_INF
          OWL1 DL
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL1_DL_MEM_RDFS_INF
          OWL1 DL
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          OWL1_FULL_MEM
          OWL1 Full
          in-memory
          none
      
      
          OWL1_FULL_MEM_TRANS_INF
          OWL1 Full
          in-memory
          transitive class-hierarchy inference
      
      
          OWL1_FULL_MEM_RULES_INF
          OWL1 Full
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL1_FULL_MEM_RDFS_INF
          OWL1 Full
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          OWL1_FULL_MEM_MICRO_RULES_INF
          OWL1 Full
          in-memory
          optimised rule-based reasoner with OWL rules
      
      
          OWL1_FULL_MEM_MINI_RULES_INF
          OWL1 Full
          in-memory
          rule-based reasoner with subset of OWL rules
      
      
          OWL1_LITE_MEM
          OWL1 Lite
          in-memory
          none
      
      
          OWL1_LITE_MEM_TRANS_INF
          OWL1 Lite
          in-memory
          transitive class-hierarchy inference
      
      
          OWL1_LITE_MEM_RULES_INF
          OWL1 Lite
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL1_LITE_MEM_RDFS_INF
          OWL1 Lite
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          RDFS_MEM
          RDFS
          in-memory
          none
      
      
          RDFS_MEM_TRANS_INF
          RDFS
          in-memory
          transitive class-hierarchy inference
      
      
          RDFS_MEM_RDFS_INF
          RDFS
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
  

For details of reasoner capabilities, please see the
inference documentation and the Javadoc
for
OntSpecification.
See also further discussion below.
To create a custom model specification,
you can create OntPersonality object
and create a new OntSpecification from its constructor:
OntPersonality OWL2_FULL_PERSONALITY = OntPersonalities.OWL2_ONT_PERSONALITY()
                .setBuiltins(OntPersonalities.OWL2_FULL_BUILTINS)
                .setReserved(OntPersonalities.OWL2_RESERVED)
                .setPunnings(OntPersonalities.OWL_NO_PUNNINGS)
                .setConfig(OntConfigs.OWL2_CONFIG)
                .build();
OntSpecification OWL2_FULL_MEM_RDFS_INF = new OntSpecification(
    OWL2_FULL_PERSONALITY, RDFSRuleReasonerFactory.theInstance()
);

The first parameter in the builder above is the vocabulary
(see OntPersonality.Builtins)
that contains a set of OWL entities’ IRIs that do not require an explicit declaration (e.g., owl:Thing).
The second parameter is the vocabulary
(see OntPersonality.Reserved),
which is for system resources and properties that cannot represent any OWL object.
The third vocabulary
(see OntPersonality.Punnings)
contains description of OWL punnings.
The last parameter in the builder is the
OntConfig
that allows fine-tuning the behavior.
There are the following configuration settings
(see OntModelControls):

  
      
          Setting
          Description
      
  
  
      
          ALLOW_GENERIC_CLASS_EXPRESSIONS
          If this key is set to true, there is a special type of class expressions, which includes any structure declared as owl:Class or owl:Restriction that cannot be classified as a specific type. This option is for compatibility with legacy OntModel.
      
      
          ALLOW_NAMED_CLASS_EXPRESSIONS
          If this key is set to true, all class expressions are allowed to be named (can have URI). This option is for compatibility with legacy OntModel.
      
      
          USE_BUILTIN_HIERARCHY_SUPPORT
          If this key is set to true, then the class/property hierarchies (e.g., see OntClass.subClasses()) are to be inferred by the naked model itself using builtin algorithms.
      
      
          USE_CHOOSE_MOST_SUITABLE_ONTOLOGY_HEADER_STRATEGY
          If true, a multiple ontology header is allowed.
      
      
          USE_GENERATE_ONTOLOGY_HEADER_IF_ABSENT_STRATEGY
          If true, OntID will be generated automatically if it is absent (as a b-node). OWL2 requires one and only one ontology header.
      
      
          USE_LEGACY_COMPATIBLE_NAMED_CLASS_FACTORY
          If true, named class testing is compatible with the legacy Jena OntModel, otherwise, a strict check against the specification for the class declaration is performed (owl:Class for OWL & rdfs:Class for RDFS types are required).
      
      
          USE_OWL_CLASS_DISJOINT_WITH_FEATURE
          Controls owl:disjointWith functionality.
      
      
          USE_OWL_CLASS_EQUIVALENT_FEATURE
          Controls owl:equivalentClass functionality.
      
      
          USE_OWL_DATA_PROPERTY_FUNCTIONAL_FEATURE
          Controls data owl:FunctionalProperty functionality.
      
      
          USE_OWL_INDIVIDUAL_DIFFERENT_FROM_FEATURE
          Controls owl:differentFrom functionality.
      
      
          USE_OWL_INDIVIDUAL_SAME_AS_FEATURE
          Controls owl:sameAs functionality.
      
      
          USE_OWL_INVERSE_OBJECT_PROPERTIES_FEATURE
          Controls owl:inverseOf functionality (InverseObjectProperty axiom).
      
      
          USE_OWL_OBJECT_PROPERTY_FUNCTIONAL_FEATURE
          Controls object owl:FunctionalProperty functionality.
      
      
          USE_OWL_PROPERTY_ASYMMETRIC_FEATURE
          Controls owl:AsymmetricProperty functionality.
      
      
          USE_OWL_PROPERTY_CHAIN_AXIOM_FEATURE
          Controls owl:propertyChainAxiom functionality.
      
      
          USE_OWL_PROPERTY_EQUIVALENT_FEATURE
          Controls owl:equivalentProperty functionality.
      
      
          USE_OWL_PROPERTY_INVERSE_FUNCTIONAL_FEATURE
          Controls owl:InverseFunctionalProperty functionality.
      
      
          USE_OWL_PROPERTY_IRREFLEXIVE_FEATURE
          Controls owl:IrreflexiveProperty functionality.
      
      
          USE_OWL_PROPERTY_REFLEXIVE_FEATURE
          Controls owl:ReflexiveProperty functionality.
      
      
          USE_OWL_PROPERTY_SYMMETRIC_FEATURE
          Controls owl:SymmetricProperty functionality.
      
      
          USE_OWL_PROPERTY_TRANSITIVE_FEATURE
          Controls owl:TransitiveProperty functionality.
      
      
          USE_OWL1_DATARANGE_DECLARATION_FEATURE
          If this key is set to true, then owl:DataRange (OWL1) is used instead of rdfs:Datatype (OWL2).
      
      
          USE_OWL1_DISTINCT_MEMBERS_PREDICATE_FEATURE
          If this key is set to true, then owl:distinctMembers (OWL1) is used instead of owl:members (OWL2).
      
      
          USE_OWL2_CLASS_HAS_KEY_FEATURE
          Controls owl:hasKey functionality.
      
      
          USE_OWL2_DEPRECATED_VOCABULARY_FEATURE
          If this key is set to true, then owl:DataRange and owl:distinctMembers will also be considered, although in OWL2 they are deprecated.
      
      
          USE_OWL2_NAMED_CLASS_DISJOINT_UNION_FEATURE
          Controls owl:disjointUnionOf functionality.
      
      
          USE_OWL2_NAMED_INDIVIDUAL_DECLARATION_FEATURE
          If this key is set to true, then owl:NamedIndividual declaration is used for creating individuals (method OntModel#createIndividual(String iri)).
      
      
          USE_OWL2_PROPERTY_DISJOINT_WITH_FEATURE
          Controls owl:propertyDisjointWith functionality.
      
      
          USE_OWL2_QUALIFIED_CARDINALITY_RESTRICTION_FEATURE
          If this key is set to true, then owl:qualifiedCardinality, owl:maxQualifiedCardinality, owl:minQualifiedCardinality predicates are allowed for Cardinality restrictions.
      
      
          USE_SIMPLIFIED_TYPE_CHECKING_WHILE_LIST_INDIVIDUALS
          Used while listing individuals (OntModel.individuals()).
      
  

Compound ontology documents and imports processing
The OWL ontology language includes some facilities for
creating modular ontologies that can be re-used in a similar manner
to software modules. In particular, one ontology can import
another. Jena helps ontology developers to work with modular
ontologies by automatically handling the imports statements in
ontology models.
The key idea is that the base model of an ontology model is
actually a collection of models, one per imported model. This means
we have to modify figure 2 a bit. Figure 4 shows how the ontology
model builds a collection of import models:


Figure 4: ontology model compound document structure for imports
We will use the term document to describe an ontology serialized
in some transport syntax, such as RDF/XML or N3. This terminology
isn’t used by the OWL or RDFS standards, but it is a convenient way
to refer to the written artifacts. However, from a broad view of
the interlinked semantic web, a document view imposes artificial
boundaries between regions of the global web of data and isn’t necessarily
a useful way of thinking about ontologies.
We will load an ontology document into an ontology model in the
same way as a normal Jena model, using the read method. There are
several variants on read, that handle differences in the source of
the document (to be read from a resolvable URL or directly from an
input stream or reader), the base URI that will resolve any
relative URI’s in the source document, and the serialisation
language. In summary, these variants are:
read( String url )
read( Reader reader, String base )
read( InputStream reader, String base )
read( String url, String lang )
read( Reader reader, String base, String lang )
read( InputStream reader, String base, String lang )

You can use any of these methods to load an ontology document. Note
that we advise that you avoid the read() variants that accept
a java.io.Reader argument when loading XML documents containing
internationalised character sets, since the handling of character
encoding by the Reader and by XML parsers is not compatible.
By default, when an ontology model reads an ontology document, it
will not locate and load the document’s imports.
To automatically handle all documents from imports closure, a specialized method from OntModelFactory should be used:
GraphRepository repository = GraphRepository.createGraphDocumentRepositoryMem();
OntModel m = OntModelFactory.createModel(graph, OntSpecification.OWL2_DL_MEM_BUILTIN_INF, repository);

An OWL document may contain an individual owl:Ontology, which
contains meta-data about that document itself. For example:
<owl:Ontology rdf:about="">
  <dc:creator rdf:value="Ian Dickinson" />
  <owl:imports rdf:resource="http://jena.apache.org/examples/imported-ontology-iri" />
  <owl:versionIRI rdf:resource="http://jena.apache.org/examples/this-ontology-iri" />
</owl:Ontology>

In OWL2 this section is mandatory and there must be one and only one per document.
It corresponds
OntID object.
In the example above, the construct rdf:about="" is a relative URI.
It will resolve to the document’s base URI.
In OWL2 the identifier of ontology is either version IRI, ontology IRI or document IRI
(see OWL 2 Web Ontology Language Structural Specification: Imports).
The owl:imports line states
that this ontology is constructed using classes, properties and
individuals from the referenced ontology,
which identifier in the example above is http://jena.apache.org/examples/imported-ontology-iri.
When an OntModel, created with GraphRepository, reads
this document, it will notice the owl:imports line and attempt to
load the imported ontology into a sub-model of the ontology model
being constructed.
The definitions from both the base ontology and all the imports will be visible to the reasoner.
Each imported ontology document is held in a separate graph
structure. This is important: we want to keep the original source
ontology separate from the imports. When we write the model out
again, normally only the base model is written (the alternative is
that all you see is a confusing union of everything). And when we
update the model, only the base model changes. To get the base
model or base graph from an OntModel, use:
Model base = thisOntModel.getBaseModel();

Imports are processed recursively, so if our base document imports
ontology A, and A imports B, we will end up with the structure shown
in Figure 4. Note that the imports have been flattened out. A cycle
check is used to prevent the document handler getting stuck if, for
example, A imports B which imports A!
To dynamically control imports, the methods OntModel#addImport,
OntModel#removeImport, OntModel#hasImport and OntModel#imports can be used.
E.g.:
thisOntModel.addImport(otherOntModel);

If the ontology is created with GraphRepository,
adding a statement <this-ont-id> owl:imports <other-ont-id> will import the corresponding ontology.
More convenient way to add the import, is to use OntID object:
thisOntModel.getID().addImport("other-ontology-iri");

GraphRepository
GraphRepository
is an abstraction that provides access to graphs.
The method GraphRepository#createGraphDocumentRepositoryMem() creates an implementation DocumentGraphRepository
that stores graphs in memory.
The method DocumentGraphRepository#get returns graphs by reference id,
which can be a URL or a path to a file.
If the graph is not in the repository, it will be downloaded from the provided link.
Using the DocumentGraphRepository#addMapping method,
you can match the graph ID to the actual location of the document:
DocumentGraphRepository repo = GraphRepository.createGraphDocumentRepositoryMem();
repo.addMapping("http://this-ontology", "file://example.ttl");
Graph graph = repo.get("http://this-ontology");

If the GraphRepository is passed as a parameter to the corresponding OntModelFactory#createModel method,
it will contain
UnionGraph graphs
that provide connectivity between ontologies.
GraphMaker
GraphMaker
is another abstraction that provides access to graphs.
It is primary intended to be a facade for persistent storage.
The method GraphRepository#createPersistentGraphRepository(GraphMaker) allows
to manage persistent ontologies backed by GraphMaker.
See also Working with persistent ontologies.
OntModel triple representation: OntStatement
OntStatement is an extended org.apache.jena.rdf.model.Statement.
It has additional methods to support OWL2 annotations.
For example, the following snippet
OntModel m = OntModelFactory.createModel( OntSpecification.OWL2_DL_MEM );
OntStatement st1 = m.createOntClass("X").getMainStatement();
OntStatement st2 = st1.addAnnotation(m.getRDFSComment(), "comment#1");
OntStatement st3 = st2.addAnnotation(m.getRDFSLabel(), "label#1");
OntStatement st4 = st3.addAnnotation(m.getRDFSLabel(), "label#2");

will produce the following RDF:
PREFIX owl:  <http://www.w3.org/2002/07/owl#>
PREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX xsd:  <http://www.w3.org/2001/XMLSchema#>

<X>     rdf:type      owl:Class;
        rdfs:comment  "comment#1" .

[ rdf:type               owl:Annotation;
  rdfs:label             "label#2";
  owl:annotatedProperty  rdfs:label;
  owl:annotatedSource    [ rdf:type               owl:Axiom;
                           rdfs:label             "label#1";
                           owl:annotatedProperty  rdfs:comment;
                           owl:annotatedSource    <X>;
                           owl:annotatedTarget    "comment#1"
                         ];
  owl:annotatedTarget    "label#1"
] .

The generic ontology type: OntObject
All of the classes in the ontology API that represent ontology
values have
OntObject
as a common super-class.
This makes OntObject a good place to
put shared functionality for all such classes, and makes a handy
common return value for general methods. The Java interface
OntObject extends more general OntResource
which in turns extends Jena’s RDF Resource
interface, so any general method that accepts a resource or an
RDFNode
will also accept an OntObject, and consequently, any other
ontology value.
Some of the common attributes of an ontology object that are
expressed through methods on OntObject are shown below:

  
      
          Attribute
          Meaning
      
  
  
      
          objectType
          A concret java Class-type of this OntObject
      
      
          mainStatement
          The main OntStatement, which determines the nature of this ontological resource. In most cases it is a declaration and wraps a triple with predicate rdf:type
      
      
          spec
          All characteristic statements of the ontology resource, i. e., all those statements which completely determine this object nature according to the OWL2 specification; mainStatement is a part of spec
      
      
          content
          spec plus all additional statements in which this object is the subject, minus those of them whose predicate is an annotation property (i.e. annotations are not included)
      
      
          annotations
          All top-level annotations attached to the mainStatement of this object
      
      
          statements
          Model’s statements for which this object is a subject
      
      
          objects
          Lists typed Resources for which this object is a subject
      
      
          types
          Equivalent to objects(RDF.type, Resource.class)
      
      
          isLocal
          Determines if this Ontology Resource is locally defined, which means mainStatement belongs to a base graph
      
  

The generic way to list OntObjects of a particular type is the method <T extends OntObject> OntModel#ontObject(Class<T>)
Ontology entities
In OWL2, there are six kinds of named (IRI) resources,
called OWL entities.
The common supertype is OntEntity,
which has following sub-types:

OntClass.Named - a named class expression.
OntDataRange.Named - a named data range expression.
OntIndividual.Named - a named individual
OntObjectProperty.Named - a non-inverse object property
OntDataProperty - a datatype property
OntAnnotationProperty - an annotation property

OntEntity can be ontology defined or builtin, e.g. owl:Thing is a builtin OntClass.Named
Ontology classes
Classes are the basic building blocks of an ontology.
A class is represented in Jena by an
OntClass
object. As mentioned above, an ontology class
is a facet of an RDF resource. One way, therefore, to get an
ontology class is to convert a plain RDF resource into
its class facet. Assume that m is a
suitably defined OntModel, into which the ESWC ontology has
already been read, and that NS is a variable denoting the
ontology namespace:
Resource r = m.getResource( NS + "Paper" );
OntClass paper = r.as( OntClass.class );

This can be shortened by calling getOntClass() on the ontology
model:
OntClass paper = m.getOntClass( NS + "Paper" );

The getOntClass method will retrieve the resource with the given
URI, and attempt to obtain the OntClass facet. If either of these
operations fail, getOntClass() will return null. Compare this
with the createOntClass method, which will reuse an existing
resource if possible, or create a new class resource if not:
OntClass paper     = m.createOntClass( NS + "Paper" );
OntClass bestPaper = m.createOntClass( NS + "BestPaper" );

In OWL2 OntClass can be either named class (URI resource) or anonymous class expression.
OWL1 OntSpecifications also allow named class expressions.
An anonymous class expression is
a class description with no associated URI, which have structure determined by the specification.
Anonymous classes are
often used when building more complex ontologies in OWL.
They are less useful in RDFS.
OntClass anonClass = m.createObjectUnionOf(classes);

Once you have the ontology class object, you can begin processing
it through the methods defined on OntClass. The attributes of a
class are handled in a similar way to the attributes of
OntObject, above, with a collection of methods to set, add, get,
test, list and remove values. Properties of classes that are
handled in this way are:

  
      
          Attribute
          Meaning
      
  
  
      
          subClasses
          A subclass of this class, i.e. those classes that are declared rdfs:subClassOf this class.
      
      
          superClasses
          A super-class of this class, i.e. a class that this class is a rdfs:subClassOf.
      
      
          equivalentClasses
          A class that represents the same concept as this class. This is not just having the same class extension: the class ‘British Prime Minister in 2003’ contains the same individual as the class ’the husband of Cherie Blair’, but they represent different concepts.
      
      
          disjointWith
          Denotes a class with which this class has no instances in common.
      
      
          hasKey
          OWL2 Language feature Keys
      
      
          disjointUnions
          OWL2 language feature Disjoint Union, which only applicable to named classes
      
  

Thus, in our example ontology, we can print a list the subclasses
of an Artefact as follows:
OntClass artefact = m.getOntClass( NS + "Artefact" );
artefact.subClasses().forEach( it -> System.out.println( it.getURI() ) );

Note that, under RDFS and OWL semantics, each class is a sub-class
of itself (in other words, rdfs:subClassOf is reflexive). While
this is true in the semantics, Jena users have reported finding
it inconvenient. Therefore, the subClasses and
superClasses convenience methods remove the reflexive from the list of
results returned by the iterator. However, if you use the plain
Model API to query for rdfs:subClassOf triples, assuming that a
reasoner is in use, the reflexive triple will appear among the deduced
triples.
Given an OntClass object, you can create or remove members of the
class extension – individuals that are instances of the class –
using the following methods:

  
      
          Method
          Meaning
      
  
  
      
          individuals()individuals(boolean direct)
          Returns a Stream over those instances that include this class among their rdf:type values. The direct flag can be used to select individuals that are direct members of the class, rather than indirectly through the class hierarchy. Thus if p1 has rdf:type :Paper, it will appear in the Stream returned by individuals on :Artefact, but not in the Stream returned by individuals(false) on :Artefact.
      
      
          createIndividual()createIndividual(String uri)
          Adds a resource to the model, whose asserted rdf:type is this ontology class. If no URI is given, the individual is an anonymous resource.
      
      
          removeIndividual(Resource individual)
          Removes the association between the given individual and this ontology class. Effectively, this removes the rdf:type link between this class and the resource. Note that this is not the same as removing the individual altogether, unless the only thing that is known about the resource is that it is a member of the class.
      
  

To test whether a class is a root of the class hierarchy in this
model (i.e. it has no known super-classes), call
isHierarchyRoot().
The domain of a property is intended to allow entailments about the
class of an individual, given that it appears as a statement
subject. It is not a constraint that can be used to validate a
document, in the way that XML schema can do. Nevertheless, many
developers find it convenient to use the domain of a property to
document the design intent that the property only applies to known
instances of the domain class. Given this observation, it can be a
useful debugging or display aide to show the properties that have
this class among their domain classes. The method
declaredProperties() attempts to identify the properties that
are intended to apply to instances of this class. Using
declaredProperties is explained in detail in the
RDF frames how-to.
The following class expressions are supported:

  
      
          Java Class
          OWL2 construct
      
  
  
      
          OntClass.Named
          Class Entity
      
      
          OntClass.IntersectionOf
          Intersection of Class Expressions
      
      
          OntClass.UnionOf
          Union of Class Expressions
      
      
          OntClass.ComplementOf
          Complement of Class Expressions
      
      
          OntClass.OneOf
          Enumeration of Individuals
      
      
          OntClass.ObjectAllValuesFrom
          Universal Quantification
      
      
          OntClass.ObjectSomeValuesFrom
          Existential Quantification
      
      
          OntClass.ObjectHasValue
          Individual Value Restriction
      
      
          OntClass.HasSelf
          Self Restriction
      
      
          OntClass.ObjectCardinality
          Exact Cardinality
      
      
          OntClass.ObjectMaxCardinality
          Maximum Cardinality
      
      
          OntClass.ObjectMinCardinality
          Minimum Cardinaloty
      
      
          OntClass.DataAllValuesFrom
          Universal Qualification
      
      
          OntClass.DataSomeValuesFrom
          Existential Quantification
      
      
          OntClass.DataHasValue
          Literal Value Restriction
      
      
          OntClass.DataCardinality
          Exact Cardinality
      
      
          OntClass.DataMaxCardinality
          Maximum Cardinality
      
      
          OntClass.DataMinCardinality
          Minimum Cardinality
      
      
          OntClass.NaryDataAllValuesFrom
          Universal Qualification
      
      
          OntClass.NaryDataSomeValuesFrom
          Existential Quantification
      
  

Complex class expressions
We introduced the handling of basic, named classes above. These are
the only kind of class descriptions available in RDFS. In OWL,
however, there are a number of additional types of class
expression, which allow richer and more expressive descriptions of
concepts.
In OWL2, all class expressions (with except of named classes) must be anonymous resources.
In OWL1, for compatibility reasons, they are allowed to be named.
There are two main categories of additional class
expression: restrictions and logical expressions
We’ll examine each in turn.
Restriction class expressions
A
restriction
defines a class by reference to one of the properties of the
individuals that comprise the members of the class, and then
placing some constraint on that property. For example, in a simple
view of animal taxonomy, we might say that mammals are covered in
fur, and birds in feathers. Thus the property hasCovering is in
one case restricted to have the value fur, in the other to have
the value feathers. This is a has value restriction. Six
restriction types are currently defined by OWL:

  
      
          Restriction type
          Meaning
      
  
  
      
          has value
          The restricted property has exactly the given value.
      
      
          all values from
          All values of the restricted property, if it has any, are members of the given class.
      
      
          some values from
          The property has at least one value which is a member of the given class.
      
      
          cardinality
          The property has exactly n values, for some positive integer n.
      
      
          min cardinality
          The property has at least n values, for some positive integer n.
      
      
          max cardinality
          The property has at most n values, for some positive integer n.
      
      
          object has self
          A self-restriction consists of an object property expression p, and it contains all those individuals that are connected by p to themselves.
      
  

Jena provides a number of ways of creating restrictions, or
retrieving them from a model.
// list restriction with a given 
OntRestriction r = m.ontObjects(OntClass.ObjectSomeValuesFrom.class);

You can create a new restriction created by nominating the property
that the restriction applies to:
// anonymous restriction on property p
OntObjectProperty p = m.createObjectProperty( NS + "p" );
OntClass c = m.createOntClass( NS + "c" );
OntClass.Restriction r = m.createObjectMaxCardinality( p, 42, c );

A common case is that we want the restrictions on some property
p. In this case, from an object denoting p we can list the
restrictions that mention that property:
OntObjectProperty p = m.getObjectProperty( NS + "p" );
Stream<OntClass.Restriction> i = p.referringRestrictions();

A general restriction can be converted to a specific type of
restriction via as... methods (if the information is already in the
model), or, if the information is not in the model, via
convertTo... methods. For example, to convert the example
restriction r from the example above to an all values from
restriction, we can do the following:
OntClass c = m.createClass( NS + "SomeClass" );
AllValuesFromRestriction avf = r.convertToAllValuesFromRestriction( c );

To create a particular restriction ab initio, we can use the
creation methods defined on OntModel. For example:
OntClass c = m.createOntClass( NS + "SomeClass" );
OntObjectProperty p = m.createObjectProperty( NS + "p" );
OntClass.ObjectAllValuesFrom avf = m.createObjectAllValuesFrom( p, c );

Assuming that the above code fragment was using a model m which
was created with the OWL language profile, it creates a instance of
an OWL restriction that would have the following definition in
RDF/XML:
<owl:Restriction>
  <owl:onProperty rdf:resource="#p"/>
  <owl:allValuesFrom rdf:resource="#SomeClass"/>
</owl:Restriction>

Once we have a particular restriction object, there are methods
following the standard add, get, set and test naming pattern to
access the aspects of the restriction. For example, in a camera
ontology, we might find this definition of a class describing
Large-Format cameras:
<owl:Class rdf:ID="Large-Format">
  <rdfs:subClassOf rdf:resource="#Camera"/>
  <rdfs:subClassOf>
    <owl:Restriction>
      <owl:onProperty rdf:resource="#body"/>
      <owl:allValuesFrom rdf:resource="#BodyWithNonAdjustableShutterSpeed"/>
   </owl:Restriction>
  </rdfs:subClassOf>
</owl:Class>

Here’s one way to access the components of the all values from
restriction. Assume m contains a suitable camera ontology:
OntClass LargeFormat = m.getOntClass(ns + "Large-Format");
LargeFormat.superClasses()
        .filter(it -> it.canAs(OntClass.ObjectAllValuesFrom.class))
        .map(it -> it.as(OntClass.ObjectAllValuesFrom.class))
        .forEach(av ->
                System.out.println("AllValuesFrom class " + 
                        av.getValue().getURI() +
                        " on property " + 
                        av.getProperty().getURI())
        );

Boolean Connectives and Enumeration of Individuals
Most developers are familiar with the use of Boolean operators to
construct propositional expressions: conjunction (and), disjunction
(or) and negation (not). OWL provides a means for constructing
expressions describing classes with analogous operators, by
considering class descriptions in terms of the set of individuals
that comprise the members of the class.
Suppose we wish to say that an instance x has rdf:type A and
rdf:type B. This means that x is both a member of the set of
individuals in A, and in the set of individuals in B. Thus, x lies
in the intersection of classes A and B. If, on the other hand, A
is either has rdf:type A or B, then x must lie in the union
of A and B. Finally, to say that x does not have rdf:type A,
it must lie in the complement of A. These operations, union,
intersection and complement are the Boolean operators for
constructing class expressions. While complement takes only a
single argument, union and intersection must necessarily take more
than one argument. Before continuing with constructing and using
In additional to these three class expressions, OWL2 also offers
Enumeration of Individuals.
An enumeration of individuals ObjectOneOf( a1 ... an ) contains exactly the individuals ai with 1 ≤ i ≤ n.
Intersection, union and complement class expressions
Given Jena’s ability to construct lists, building intersection and
union class expressions is straightforward. The create methods on
OntModel allow us to construct an intersection or union directly.
For example, we can define the class of UK
industry-related conferences as the intersection of conferences
with a UK location and conferences with an industrial track. Here’s
the XML declaration:
<owl:Class rdf:ID="UKIndustrialConference">
  <owl:intersectionOf rdf:parseType="Collection">
    <owl:Restriction>
      <owl:onProperty rdf:resource="#hasLocation"/>
      <owl:hasValue rdf:resource="#united_kingdom"/>
    </owl:Restriction>
    <owl:Restriction>
      <owl:onProperty rdf:resource="#hasPart"/>
      <owl:someValuesFrom rdf:resource="#IndustryTrack"/>
    </owl:Restriction>
  </owl:intersectionOf>
</owl:Class>

Or, more compactly in N3/Turtle:
:UKIndustrialConference a owl:Class ;
    owl:intersectionOf (
       [a owl:Restriction ;
          owl:onProperty :hasLocation ;
          owl:hasValue :united_kingdom]
       [a owl:Restriction ;
          owl:onProperty :hasPart ;
          owl:someValuesFrom :IndustryTrack]
      )

Here is code to create this class declaration using Jena, assuming
that m is a model into which the ESWC ontology has been read:
// get the class references
OntClass place = m.getOntClass( ns + "Place" );
OntClass indTrack = m.getOntClass( ns + "IndustryTrack" );

// get the property references
OntObjectProperty hasPart = m.getObjectProperty( ns + "hasPart" );
OntObjectProperty hasLoc = m.getObjectProperty( ns + "hasLocation" );

// create the UK instance
OntIndividual uk = place.createIndividual( ns + "united_kingdom" );

// now the anonymous restrictions
OntClass.ObjectHasValue ukLocation =
        m.createObjectHasValue( hasLoc, uk );
OntClass.ObjectSomeValuesFrom hasIndTrack =
        m.createObjectSomeValuesFrom(  hasPart, indTrack );

// finally, create the intersection class
OntClass.IntersectionOf ukIndustrialConf =
        m.createObjectIntersectionOf( ukLocation, hasIndTrack );

Enumeration of Individuals
The final type class expression allowed by OWL is the enumerated
class. Recall that a class is a set of individuals. Often, we want
to define the members of the class implicitly: for example, “the class
of UK conferences”. Sometimes it is convenient to define a class
explicitly, by stating the individuals the class contains. An
OntClass.OneOf
is exactly the class whose members are the given individuals. For
example, we know that the class of PrimaryColours contains exactly
red, green and blue, and no others.
In Jena, an enumerated class is created in a similar way to other
classes. The set of values that comprise the enumeration is
described by an RDFList. For example, here’s a class defining the
countries that comprise the United Kingdom:
<owl:Class rdf:ID="UKCountries">
  <owl:oneOf rdf:parseType="Collection">
    <eswc:Place rdf:about="#england"/>
    <eswc:Place rdf:about="#scotland"/>
    <eswc:Place rdf:about="#wales"/>
    <eswc:Place rdf:about="#northern_ireland"/>
  </owl:oneOf>
</owl:Class>

To list the contents of this enumeration, we could do the
following:
OntClass place = m.getOntClass( ns + "Place" );

OntClass.OneOf ukCountries = m.createObjectOneOf(
        place.createIndividual( ns + "england" ),
        place.createIndividual( ns + "scotland" ),
        place.createIndividual( ns + "wales" ),
        place.createIndividual( ns + "northern_ireland" )
);

ukCountries.getList().members().forEach( System.out::println );

Listing classes
In many applications, we need to inspect the set of classes
in an ontology.
The primary method to list any OntObject’s, including OntClasses,
is <T extends OntObject> OntModel#ontObjects(Class<T>), which returns java Stream.
In additional to that, there are more specialized methods:
public Stream<OntClass.Named> classes();
public Stream<OntClass> hierarchyRoots();

In OWL, class
expressions are typically not named, but are denoted by anonymous
resources (aka bNodes). In many applications, such as displaying
an ontology in a user interface, we want to pick out the named
classes only, ignoring those denoted by bNodes. This is what
classes() does. The method hierarchyRoots()
identifies the classes that are uppermost in the class hierarchy
contained in the given model. These are the classes that have no
super-classes. The iteration returned by
hierarchyRoots() may contain anonymous classes.
You should also note that it is important to close the Stream
returned from the list methods, particularly when the underlying
store is a database. This is necessary so that any state (e.g., the
database connection resources) can be released. Closing happens
automatically when the hasNext() method on the underlying iterator returns
false. If your code does not iterate all the way to the end of the
iterator, you should call the Stream#close() method explicitly. Note
also that the values returned by these streams will depend on the
asserted data and the reasoner being used.
Ontology DataRanges
The concept of OWL DataRange is similar to class expressions.
There is also named data range, called datatype
(OntDataRange.Named),
and five kinds of anonymous data range expressions:
data ComplementOf, data IntersectionOf, data UnionOf, data OneOf and datatype restriction (see table below).
See the
OntDataRange javadoc
for more details.
Example:
m.createDataRestriction(
    XSD.integer.inModel(m).as(OntDataRange.Named.class),
    m.createFacetRestriction(OntFacetRestriction.FractionDigits.class, m.createTypedLiteral(42))
);

The following data range expressions are supported:

  
      
          Java Class
          OWL2 construct
      
  
  
      
          OntDataRange.Named
          Datatype Entity
      
      
          OntDataRange.ComplementOf
          Complement of Data Ranges,
      
      
          OntDataRange.IntersectionOf
          Intersection of Data Ranges,
      
      
          OntDataRange.UnionOf
          Union of Data Ranges,
      
      
          OntDataRange.OneOf
          Enumeration of Literals
      
      
          OntDataRange.Restriction
          Datatype Restrictions.
      
  

Ontology properties
In an ontology, a property denotes the name of a relationship
between resources, or between a resource and a data value.
Usually it corresponds to a predicate in logic representations, with one exception:
in OWL2 there is also Inverse Object Property Expression.
One interesting aspect of RDFS and OWL is that
properties are not defined as aspects of some enclosing class, but
are first-class objects in their own right. This means that
ontologies and ontology-applications can store, retrieve and make
assertions about properties directly. Consequently, Jena has a set
of Java classes that allow you to conveniently manipulate the
properties represented in an ontology model.
A named property in an ontology model is an extension of the core Jena
API class
Property
and allows access to the additional information that can be
asserted about properties in an ontology language. The common API
super-class for representing named and anonymous ontology properties in Java is
OntProperty.
There is also OntNamedProperty supertype,
which extends standard RDF Property, and OntRelationalProperty, which is supertype for OntDataProperty and OntObjectProperty.
Again, using the pattern of add, set, get, list, has, and remove
methods, we can access the following attributes of an
OntProperty:

  
      
          Attribute
          Meaning
      
  
  
      
          subProperty
          A sub property of this property; i.e. a property which is declared to be a rdfs:subPropertyOf this property. If p is a sub property of q, and we know that A p B is true, we can infer that A q B is also true. For OntObjectProperty there is also ObjectPropertyChain.
      
      
          superProperty
          A super property of this property, i.e. a property that this property is a rdfs:subPropertyOf
      
      
          domain
          Denotes the class or classes that form the domain of this property. Multiple domain values are interpreted as a conjunction. The domain denotes the class of value the property maps from.
      
      
          range
          Denotes the class or classes (for object properties) or datarange or dataranges (for datatype properties) that form the range of this property. Multiple range values are interpreted as a conjunction. The range denotes the class of values the property maps to.
      
      
          equivalentProperty
          Denotes a property that is the same as this property. This attribute is only for OntRealProperty.
      
      
          disjointProperty
          A disjoint object properties axiom states that all of the object property expressions OPEi, 1 ≤ i ≤ n, are pairwise disjoint; that is, no individual x can be connected to an individual y by both OPEi and OPEj for i ≠ j. Applicable only for OntRealPropery
      
      
          inverse
          Denotes a property that is the inverse of this property. Thus if q is the inverse of p, and we know that A q B, then we can infer that B p A. This attribute is only for OntObjectProperty.
      
  

In the example ontology, the property hasProgramme has a domain
of OrganizedEvent, a range of Programme and the human-readable label “has programme”.
We can reconstruct this definition in an
empty ontology model as follows:
OntModel m = OntModelFactory.createModel( OntSpecification.OWL2_FULL_MEM );
OntClass programme = m.createOntClass( NS + "Programme" );
OntClass orgEvent = m.createOntClass( NS + "OrganizedEvent" );

OntObjectProperty hasProgramme = m.createObjectProperty( NS + "hasProgramme" );

hasProgramme.addDomain( orgEvent );
hasProgramme.addRange( programme );
hasProgramme.addLabel( "has programme", "en" );

As a further example, we can alternatively add information to an
existing ontology. To add a super-property hasDeadline, to
generalise the separate properties denoting the submission
deadline, notification deadline and camera-ready deadline, do:
String ns = "http://www.eswc2006.org/technologies/ontology#";
OntModel m = OntModelFactory.createModel( OntSpecification.OWL2_FULL_MEM );
m.read( "https://raw.githubusercontent.com/apache/jena/main/jena-core/src-examples/data/eswc-2006-09-21.rdf" );

OntDataProperty subDeadline = m.getDataProperty( ns + "hasSubmissionDeadline" );
OntDataProperty notifyDeadline = m.getDataProperty( ns + "hasNotificationDeadline" );
OntDataProperty cameraDeadline = m.getDataProperty( ns + "hasCameraReadyDeadline" );

OntDataProperty deadline = m.createDataProperty( ns + "deadline" );
deadline.addDomain( m.getOntClass( ns + "Call" ) );
deadline.addRange( m.getDatatype(XSD.dateTime) );

deadline.addSubProperty( subDeadline );
deadline.addSubProperty( notifyDeadline );
deadline.addSubProperty( cameraDeadline );

Note that, although we called the addSubProperty method on the
object representing the new super-property, the serialized form of
the ontology will contain rdfs:subPropertyOf axioms on each of
the sub-property resources, since this is what the language
defines. Jena will, in general, try to allow symmetric access to
sub-properties and sub-classes from either direction.
Object and Datatype properties
OWL refines the basic property type from RDF into two
sub-types: object properties and datatype properties. The
difference between them is that an object property can have only
individuals in its range, while a datatype property has concrete
data literals (only) in its range. Some OWL reasoners are able to
exploit the differences between object and datatype properties to
perform more efficient reasoning over ontologies. OWL also adds an
annotation property, which is defined to have no semantic
entailments, and so is useful when annotating ontology documents,
for example.
Functional properties
OWL permits object and datatype properties to be functional –
that is, for a given individual in the domain, the range value will
always be the same. In particular, if father is a functional
property, and individual :jane has father :jim and
father :james, a reasoner is entitled to conclude that :jim and
:james denote the same individual. A functional property is
equivalent to stating that the property has a maximum cardinality
of one.
To declare a functional property, expression property.setFunctional(true) can be used.
Other property types
There are several additional characteristics of ObjectProperty that
represent additional capabilities of ontology properties:
transitive,
symmetric,
asymmetric,
inverse-functional,
reflexive,
irreflexive.
Transitive property means that if p is transitive, and we know :a p :b and also
b p :c, we can infer that :a p :c. A
Symmetric property means that if p is symmetric, and we know :a p :b, we can infer
:b p :a.
An inverse functional property
means that for any given range element, the domain value is unique.
An object property asymmetry axiom states
that the object property expression p is asymmetric — that is,
if an individual x is connected by p to an individual y, then y cannot be connected by p to x.
An object property reflexivity axiom states
that the object property expression p is reflexive — that is,
each individual is connected by p to itself.
An object property irreflexivity axiom states
that the object property expression p is irreflexive — that is,
no individual is connected by p to itself.
Instances or individuals
The Individual (or Instance in terms of legacy OntModel) is present
by the class OntIndividual.
The definition of individual is a class-assertion a rdf:type C., where C is OntClass and a is IRI or Blank Node.
Thus, unlike legacy Jena OntModel, in general not every resource can be represented as an OntIndividual,
although this is true in some specifications, such as OntSpecification.OWL1_FULL_MEM_RDFS_INF.
There are several ways to create individuals.
OntClass c = m.createOntClass( NS + "SomeClass" );

// first way: use a call on OntModel
OntIndividual ind0 = m.createOntIndividual( NS + "ind0", c );
OntIndividual ind1 = m.createOntIndividual( null, c );

// second way: create a named (uri) individual; this way works for OWL2 ontologies
OntIndividual ind2 = m.createOntIndividual( NS + "ind0" );

// third way: use a call on OntClass
OntIndividual ind3 = c.createIndividual( NS + "ind1" );
OntIndividual ind4 = c.createIndividual();

There is a wide range of methods for listing and manipulating related individuals, classes and properties.
For listing methods see the table:

  
      
          Method
          Effect
      
  
  
      
          sameIndividuals
          Lists all same individuals. The pattern to search for is ai owl:sameAs aj, where ai is this individual.
      
      
          disjoints
          Lists all OntDisjoint sections where this individual is a member.
      
      
          differentIndividuals
          Lists all different individuals. The pattern to search for is ai owl:differentFrom aj, where ai is this individual.
      
      
          positiveAssertions
          Lists all positive assertions for this individual (ai PN aj, a R v, where PN is named object property, R is a data property, v is a literal).
      
      
          negativeAssertions
          Lists all negative property assertions for this individual.
      
      
          classes
          Returns all class types
      
  

The most important method here is classes.
The interface OntIndividual provides a set of methods for testing and manipulating
the ontology classes to which an individual belongs. This is a
convenience: OWL and RDFS denote class membership through the
rdf:type property.
There are methods OntIndividual#classes(boolean direct), #classes(), addClassAssertion, hasOntClass, ontClass,
attachClass, dettachClass for listing,
getting and setting the rdf:type of an individual,
which denotes a class to which the resource belongs (noting that, in RDF and OWL, a resource can belong to many classes at once).
The rdf:type property is one for which many entailment rules are defined in the semantic models of the various ontology languages.
Therefore, the values that classes() returns is more than usually dependent on the reasoner bound to the ontology model.
For example, suppose we have class A, class B which is a subclass of A, and resource x whose asserted rdf:type is B.
With no reasoner, listing x’s RDF types will return only B.
If the reasoner is able to calculate the closure of the subclass hierarchy (and most can),
x’s RDF types would also include A.
A complete OWL reasoner would also infer that x has rdf:type owl:Thing and rdf:Resource.
For some tasks, getting a complete list of the RDF types of a resource is exactly what is needed.
For other tasks, this is not the case.
If you are developing an ontology editor, for example,
you may want to distinguish in its display between inferred and asserted types.
In the above example, only x rdf:type B is asserted, everything else is inferred.
One way to make this distinction is to make use of the base model (see Figure 4).
Getting the resource from the base model and listing the type properties
there would return only the asserted values.
For example:
// create the base model
String source = "https://www.w3.org/TR/2003/PR-owl-guide-20031215/wine";
String ns = "http://www.w3.org/TR/2003/PR-owl-guide-20031209/wine#";
OntModel base = OntModelFactory.createModel( OntSpecification.OWL2_DL_MEM );
base.read( source, "RDF/XML" );

// create the reasoning model using the base
OntModel inf = OntModelFactory.createModel( base.getGraph(), OntSpecification.OWL2_DL_MEM_RDFS_INF );

// create a country for this example
OntIndividual p1 = base.getIndividual( ns + "CorbansPrivateBinSauvignonBlanc");

// list the asserted types
p1.classes().forEach(clazz -> System.out.println( p1.getURI() + " is asserted in class " + clazz ));

// list the inferred types
OntIndividual p2 = inf.getIndividual( ns + "CorbansPrivateBinSauvignonBlanc");
p2.classes().forEach(clazz -> System.out.println( p2.getURI() + " is inferred to be in class " + clazz ));

For other user interface or presentation tasks,
we may want something between the complete list of types and the base list of only the asserted values.
Consider the class hierarchy in figure 5 (i):

Figure 5: asserted and inferred relationships
Figure 5 (i) shows a base model, containing a class hierarchy and an instance x.
Figure 5 (ii) shows the full set of relationships that might be inferred from this base model.
In Figure 5 (iii), we see only direct or maximally specific relationships.
For example, in 5 (iii) x does not have rdf:type A,
since this is a relationship covered by the fact that x has rdf:type D,
and D is a subclass of A.
Notice also that the rdf:type B link is also removed from the direct graph, for a similar reason.
Thus, the direct graph hides relationships from both the inferred and asserted graphs.
When displaying instance x in a user interface, particularly in a tree view of some kind,
the direct graph is often the most useful as it contains the useful information in the most compact form.
Ontology meta-data
In OWL, but not RDFS, meta-data about the ontology
itself is encoded as properties on a resource of type
owl:Ontology. By convention,
the URI of this individual is the URL, or web address, of the ontology document
itself. In the XML serialisation, this is typically shown as:
<owl:Ontology rdf:about="">
</owl:Ontology>

Note that the construct rdf:about="" does not indicate a
resource with no URI; it is in fact a shorthand way of referencing
the base URI of the document containing the ontology. The base
URI may be stated in the document through an xml:base declaration
in the XML preamble. The base URI can also be specified when
reading the document via Jena’s Model API (see the read() methods
on OntModel
for reference).
We can attach various meta-data statements to this object to
indicate attributes of the ontology as a whole, using the Java object
OntID:
m.getID()
        .annotate(m.getAnnotationProperty(OWL2.backwardCompatibleWith), m.createResource("http://example.com/v1"))
        .annotate(m.getRDFSSeeAlso(), m.createResource("http://example.com/v2"))
        .addComment("xxx");

In the Jena API, the ontology’s metadata properties can be accessed
through the
OntID
interface. Suppose we wish to know the list of URI’s that the
ontology imports. First, we must obtain the resource representing the
ontology itself:
OntModel m = ...;  
OntID id = m.getID();
id.imports().forEach( System.out::println );

Note that in OWL2 ontology document should contain one and only one ontology header (i.e. OntID).
The OntModel#getID method will generate the ontology header if it is missing.
A common practice is also to use the Ontology element to attach
Dublin Core metadata
to the ontology document. Jena provides a copy
of the Dublin Core vocabulary, in org.apache.jena.vocabulary.DCTerms.
To attach a statement saying that the ontology was authored by John
Smith, we can say:
OntID ont = m.getID();
ont.addProperty( DCTerms.creator, "John Smith" );

It is also possible to programmatically add imports and other
meta-data to a model, for example:
String base = ...; // the base URI of the ontology
OntModel m = ...;

OntID ont = m.setID( base );
ont.addImport( "http://example.com/import1" );
ont.addImport( "http://example.com/import2" );

Note that under default conditions, simply adding (or removing) an
owl:imports statement to a model will not cause the corresponding
document to be imported (or removed).
However, if model created with GraphRepository attached, it will start noticing
the addition or removal of owl:imports statements.
Ontology inference: overview
You have the choice of whether to use the Ontology API with Jena’s
reasoning capability turned on, and, if so, which of the various
reasoners to use. Sometimes a reasoner will add information to the
ontology model that it is not useful for your application to see. A
good example is an ontology editor. Here, you may wish to present
your users with the information they have entered in to their
ontology; the addition of the entailed information into the
editor’s display would be very confusing. Since Jena does not have
a means for distinguishing inferred statements from those
statements asserted into the base model, a common choice for
ontology editors and similar applications is to run with no
reasoner.
In many other cases, however, it is the addition of the reasoner
that makes the ontology useful. For example, if we know that John
is the father of Mary, we would expect a ‘yes’ if we query whether
John is the parent of Mary. The parent relationship is not
asserted, but we know from our ontology that fatherOf is a
sub-property of parentOf. If ‘John fatherOf Mary’ is true, then
‘John parentOf Mary’ is also true. The integrated reasoning
capability in Jena exists to allow just such entailments to be seen
and used.
For a complete and thorough description of Jena’s inference
capabilities, please see the
reasoner documentation. This section of
of the ontology API documentation is intended to serve as only a
brief guide and overview.
Recall from the introduction that the reasoners in Jena operate by
making it appear that triples entailed by the inference engine
are part of the model in just the same way as the asserted triples
(see Figure 2). The underlying architecture allows the reasoner to
be part of the same Java virtual machine (as is the case with the
built-in rule-based reasoners), or in a separate process on the
local computer, or even a remote computer. Of course, each of these
choices will have different characteristics of what reasoning
capabilities are supported, and what the implications for
performance are.
The reasoner attached to an ontology model, if any, is specified
through the
OntSpecification.
The Java object OntSpecification has two parameters: OntPersonality and ReasonerFactory.
The ReasonerRegistry provides a collection of pre-built reasoners –
see the reasoner documentation for more details. However, it is
also possible for you to define your own reasoner that conforms to
the appropriate interface. For example, there is an in-process
interface to the open-source
Pellet reasoner.
To facilitate the choice of reasoners for a given model, some
common choices have been included in the pre-built ontology model
specifications available as static fields on OntSpecification. The
available choices are described in the section on
ont model specifications, above.
Depending on which of these choices is made, the statements
returned from queries to a given ontology model may vary
considerably.
Additional notes
Jena’s inference machinery defines some specialised services that
are not exposed through the addition of extra triples to the model.
These are exposed by the
InfModel
interface; for convenience there is the method OntModel#asInferenceModel() to make
these services directly available to the user. Please note that
calling this method on an ontology model that does
not contain a reasoner will cause an error.
In general, inference models will add many additional
statements to a given model, including the axioms appropriate to
the ontology language. This is typically not something you will
want in the output when the model is serialized, so
write() on an ontology model will only write the statements from the base model.
This is typically the desired behaviour, but there are occasions
(e.g. during debugging) when you may want to write the entire
model, virtual triples included. The easiest way to achieve this is
to call the writeAll() method on OntModel. An alternative
technique, which can sometimes be useful for a variety of
use-cases, including debugging, is to snapshot the model by
constructing a temporary plain model and adding to it: the contents
of the ontology model:
OntModel m = ...

// snapshot the contents of ont model om
Model snapshot = ModelFactory.createDefaultModel();
snapshot.add( om );

Working with persistent ontologies
A common way to work with ontology data is to load the ontology
axioms and instances at run-time from a set of source documents.
This is a very flexible approach, but has limitations. In
particular, your application must parse the source documents each
time it is run. For large ontologies, this can be a source of
significant overhead. Jena provides an implementation of the RDF
model interface that stores the triples persistently in a database.
This saves the overhead of loading the model each time, and means
that you can store RDF models significantly larger than the
computer’s main memory, but at the expense of a higher overhead (a
database interaction) to retrieve and update RDF data from the
model. In this section we briefly discuss using the ontology API with
Jena’s persistent database models.
For information on setting-up and accessing the persistent models
themselves, see the TDB
reference sections.
There are two somewhat separate requirements for persistently
storing ontology data. The first is making the main or base model
itself persistent. The second is re-using or creating persistent
models for the imports of an ontology. These two requirements are
handled slightly differently.
To retrieve a Jena model from the database API, we have to know its
name. Fortunately, common practice for ontologies on the Semantic
Web is that each is named with a URI. We use this URI to name the
model that is stored in the database. Note carefully what is
actually happening here: we are exploiting a feature of the
database sub-system to make persistently stored ontologies easy to
retrieve, but we are not in any sense resolving the URI of the
model. Once placed into the database, the name of the model is
treated as an opaque string.
To create a persistent model for the ontology
http://example.org/Customers, we create a graph maker that will
access our underlying database, and use the ontology URI as the
database name. We then take the resulting persistent model, and use
it as the base model when constructing an ontology model:
Graph base = getMaker().createGraph( "http://example.org/Customers" );
OntModel m = OntModelFactory.createModel( base, OntSpecification.OWL2_DL_MEM );

Here we assume that the getMaker() method returns a suitably
initialized GraphMaker that will open the connection to the
database. This step only creates a persistent model named with the
ontology URI. To initialize the content, we must either add
statements to the model using the OntModel API, or do a one-time
read from a document:
m.read( "http://example.org/Customers" );

Once this step is completed, the model contents may be accessed in
future without needing to read again.
A GraphMaker may be wrapped
by PersistentGraphRepository using the method GraphRepository#createPersistentGraphRepository(GraphMaker).
This allows managing ontology relationships automatically, without interacting with GraphMaker directly.
Note on performance The built-in Jena reasoners, including the
rule reasoners, make many small queries into the model in order to
propagate the effects of rules firing. When using a persistent
database model, each of these small queries creates an SQL
interaction with the database engine. This is a very inefficient
way to interact with a database system, and performance suffers as
a result. Efficient reasoning over large, persistent databases is
currently an open research challenge. Our best suggested
work-around is, where possible, to snapshot the contents of the
database-backed model into RAM for the duration of processing by
the inference engine. An alternative solution, that may be
applicable if your application does not write to the datastore
often, is to precompute the inference closure of the ontology and
data in-memory, then store that into a database model to be queried
by the run-time application. Such an off-line processing
architecture will clearly not be applicable to every application
problem.
Utilities
There are several utilities, which can be used for various purposes.
Graphs
is a collection of methods for working with various types of graphs, including UnionGraph.
StdModels is for working with general-purpose Models,
and OntModels is
for working with OntModels.
Some of the useful methods are:

OntModels#getLCA( OntClass u, OntClass v ) - determine the lowest common ancestor for classes u and v.
This is the class that is lowest in the class hierarchy, and which includes both u and v among its sub-classes.
StdModels#findShortestPath( Model m, Resource start, RDFNode end, Filter onPath ) - breadth-first search, including a cycle check,
to locate the shortest path from start to end, in which every triple on the path returns true to the onPath predicate.
OntModels#namedHierarchyRoots( OntModel m ) - compute a list containing the uppermost fringe of the class hierarchy
in the given model which consists only of named classes.\n\nOn this page
    
  
    
      
        Prerequisites
      
    
    Overview
      
        Further assistance
      
    
    General concepts
      
        RDFS
        OWL
        Ontology languages and the Jena Ontology API
        Ontologies and reasoning
        RDF-level polymorphism and Java
      
    
    Running example: the ESWC ontology
    Creating ontology models
    Compound ontology documents and imports processing
    GraphRepository
    GraphMaker
    OntModel triple representation: OntStatement
    The generic ontology type: OntObject
    Ontology entities
    Ontology classes
      
        Complex class expressions
        Restriction class expressions
        Boolean Connectives and Enumeration of Individuals
          
            Intersection, union and complement class expressions
            Enumeration of Individuals
          
        
        Listing classes
      
    
    Ontology DataRanges
    Ontology properties
      
        Object and Datatype properties
        Functional properties
        Other property types
      
    
    Instances or individuals
    Ontology meta-data
    Ontology inference: overview
      
        Additional notes
      
    
    Working with persistent ontologies
    Utilities
  

  
  
    This section is a general introduction to the Jena
ontology API, including some of the common tasks you may need
to perform. We
won’t go into all of the many details of the API here: you should
expect to refer to the Javadoc to
get full details of the capabilities of the API.
Please note that this section covers the new Jena ontology API, which has been introduced since Jena 5.1.0.
The legacy Jena Ontology API documentation can be found here.
Prerequisites
We’ll assume that you have a basic familiarity with RDF and with
Jena. If not, there are other
Jena help documents you can read for background
on these topics, and a collection of tutorials.
Jena is a programming toolkit, using the Java programming language.
While there are a few command-line tools to help you perform some
key tasks using Jena, mostly you use Jena by writing Java programs.
The examples in this document will be primarily code samples.
We also won’t be explaining the OWL or RDFS ontology languages in
much detail in this document. You should refer to
supporting documentation for details on those languages, for
example the W3C OWL document index.
Overview
The section of the manual is broken into a number of sections. You
do not need to read them in sequence, though later sections may
refer to concepts and techniques introduced in earlier sections.
The sections are:

General concepts
Running example: the ESWC ontology
Creating ontology models
Compound ontology documents and imports processing
GraphRepository
GraphMaker
OntModel triple representation: OntStatement
The generic ontology type: OntObject
Ontology entities
Ontology classes
Ontology dataranges
Ontology properties
Instances or individuals
Ontology meta-data
Ontology inference: overview
Working with persistent ontologies
Utilities

Further assistance
Hopefully, this document will be sufficient to help most readers
to get started using the Jena ontology API. For further support,
please post questions to the Jena support list,
or file a bug report.
Please note that we ask that you use the support list or the bug-tracker
to communicate with the Jena team, rather than send email to the team
members directly. This helps us manage Jena support more effectively,
and facilitates contributions from other Jena community members.
General concepts
In a widely-quoted definition, an ontology is

“a specification of a conceptualization”
[Gruber, T. 1993]

Let’s unpack that brief characterisation a bit. An
ontology allows a programmer to specify, in an open, meaningful,
way, the concepts and relationships that collectively characterise
some domain of interest. Examples might be the concepts of red and white wine,
grape varieties, vintage years, wineries and so forth that
characterise the domain of ‘wine’, and relationships such as
‘wineries produce wines’, ‘wines have a year of production’. This
wine ontology might be developed initially for a particular
application, such as a stock-control system at a wine warehouse. As
such, it may be considered similar to a well-defined database
schema. The advantage to an ontology is that it is an explicit,
first-class description. So having been developed for one purpose,
it can be published and reused for other purposes. For example, a
given winery may use the wine ontology to link its production
schedule to the stock system at the wine warehouse. Alternatively,
a wine recommendation program may use the wine ontology, and a
description (ontology) of different dishes to recommend wines for a
given menu.
There are many ways of writing down an ontology, and a variety of
opinions as to what kinds of definition should go in one. In
practice, the contents of an ontology are largely driven by the
kinds of application it will be used to support. In Jena, we do not
take a particular view on the minimal or necessary components of an
ontology. Rather, we try to support a variety of common techniques.
In this section, we try to explain what is – and to some extent what
isn’t – possible using Jena’s ontology support.
Since Jena is fundamentally an RDF platform, Jena’s ontology
support is limited to ontology formalisms built on top of RDF.
Specifically this means RDFS,
the varieties of
OWL.
We will provide a very brief introduction to these languages here,
but please refer to the extensive on-line documentation for these
formalisms for complete and authoritative details.
RDFS
RDFS is the weakest ontology language supported by Jena. RDFS
allows the ontologist to build a simple hierarchy of concepts, and
a hierarchy of properties. Consider the following trivial
characterisation (with apologies to biology-trained readers!):

Table 1: A simple concept hierarchy
Using RDFS, we can say that my ontology has five classes, and that
Plant is a sub-class of Organism and so on. So every animal
is also an organism. A good way to think of these classes is as
describing sets of individuals: organism is intended to describe
a set of living things, some of which are animals (i.e. a sub-set
of the set of organisms is the set of animals), and some animals
are fish (a subset of the set of all animals is the set of all
fish).
To describe the attributes of these classes, we can associate
properties with the classes. For example, animals have sensory
organs (noses, eyes, etc.). A general property of an animal might
be senseOrgan, to denote any given sensory organs a particular
animal has. In general, fish have eyes, so a fish might have a
eyes property to refer to a description of the particular eye
structure of some species. Since eyes are a type of sensory organ,
we can capture this relationship between these properties by saying
that eye is a sub-property-of senseOrgan. Thus if a given fish
has two eyes, it also has two sense organs. (It may have more, but
we know that it must have two).
We can describe this simple hierarchy with RDFS. In general, the
class hierarchy is a graph rather than a tree (i.e. not like Java
class inheritance). The
slime mold is popularly,
though perhaps not accurately, thought of as an organism that has
characteristics of both plants and animals. We might model a slime
mold in our ontology as a class that has both plant and animal
classes among its super-classes. RDFS is too weak a language to
express the constraint that a thing cannot be both a plant and an animal (which is
perhaps lucky for the slime molds). In RDFS, we can only name the
classes, we cannot construct expressions to describe interesting
classes. However, for many applications it is sufficient to state
the basic vocabulary, and RDFS is perfectly well suited to this.
Note also that we can both describe classes, in general terms, and we
can describe particular instances of those classes. So there may
be a particular individual Fred who is a Fish (i.e. has
rdf:type Fish), and who has two eyes. Their companion Freda, a
Mexican Tetra, or
blind cave fish, has no eyes. One use of an ontology is to allow us
to fill-in missing information about individuals. Thus, though it
is not stated directly, we can deduce that Fred is also an Animal
and an Organism. Assume that there was no rdf:type asserting that
Freda is a Fish. We may still infer Freda’s rdf:type since Freda
has lateral lines as
sense organs, and these only occur in fish. In RDFS, we state that
the domain of the lateralLines property is the Fish class, so
an RDFS reasoner can infer that Freda must be a fish.
OWL
In general, OWL allows us to say everything that RDFS allows, and
much more besides. A key part of OWL is the ability to describe
classes in more interesting and complex ways. For example, in OWL
we can say that Plant and Animal are disjoint classes: no
individual can be both a plant and an animal (which would have the
unfortunate consequence of making SlimeMold an empty class).
SaltwaterFish might be the intersection of Fish and the class
SeaDwellers (which also includes, for example, cetaceans and sea
plants).
Suppose we have a property covering, intended to represent the
scales of a fish or the fur of a mammal. We can now refine the
mammal class to be ‘animals that have a covering that is hair’,
using a property restriction to express the condition that
property covering has a value from the class Hair. Similarly
TropicalFish might be the intersection of the class of Fish and
the class of things that have TropicalOcean as their habitat.
Finally (for this brief overview), we can say more about properties
in OWL. In RDFS, properties can be related via a property
hierarchy. OWL extends this by allowing properties to be denoted as
transitive, symmetric or functional, and allow one property
to be declared to be the inverse of another. OWL also makes a
distinction between properties that have individuals (RDF resources)
as their range and properties that have data-values (known as
literals in RDF terminology) as their range.
Respectively these are object properties and datatype properties.
One consequence of the RDF lineage of OWL is
that OWL ontologies cannot make statements about literal values. We
cannot say in RDF that seven has the property of being a prime number.
We can, of course, say that the class of primes includes seven, doing so
doesn’t require a number to be the subject of an RDF statement. In
OWL, this distinction is important: only object properties can
be transitive or symmetric.
The OWL language is sub-divided into several syntax classes:
OWL2 Full, OWL2 DL, OWL2 RL, OWL2 EL, OWL2 QL,
and also OWL1 Lite, OWL1 DL and OWL1 Full.
The last three are deprecated now.
OWL2 EL, OWL2 QL and OWL2 RL do not permit some constructions allowed in OWL2 Full and OWL2 DL.
Although OWL1 is deprecated, Jena Ontology API still supports it.
The intent for OWL2 RL, EL, QL, and also OWL1 Lite and OWL1 DL,
is to make the task of reasoning with expressions in that subset more tractable.
Specifically, OWL (1 & 2) DL is intended to be able to be processed efficiently by a
description logic
reasoner. OWL1 Lite is intended to be amenable to processing by a
variety of reasonably simple inference algorithms, though experts
in the field have challenged how successfully this has been
achieved.
OWL 2 EL is particularly useful in
applications employing ontologies that contain very large numbers of properties and/or classes.
The EL acronym reflects the profile’s basis in the EL family of description logics,
logics that provide only Existential quantification.
OWL 2 QL is aimed at applications that
use very large volumes of instance data, and where query answering is
the most important reasoning task.
The QL acronym reflects the fact that query answering in this profile
can be implemented by rewriting queries into a standard relational Query Language.
OWL 2 RL is aimed at applications that
require scalable reasoning without sacrificing too much expressive power.
The RL acronym reflects the fact that reasoning in this profile can be implemented using a standard Rule Language.
While the OWL standards documents note that OWL builds on top of
the (revised) RDF specifications, it is possible to treat OWL as a
separate language in its own right, and not something that is built
on an RDF foundation. This view uses RDF as a serialisation syntax;
the RDF-centric view treats RDF triples as the core of the OWL
formalism. While both views are valid, in Jena we take the
RDF-centric view.
Ontology languages and the Jena Ontology API
As we outlined above, there are various different ontology languages
available for representing ontology information on the semantic
web. They range from the most expressive, OWL Full, through to the
weakest, RDFS. Through the Ontology API, Jena aims to provide a
consistent programming interface for ontology application
development, independent of which ontology language you are using
in your programs.
The Jena Ontology API is language-neutral: the Java class names are not
specific to the underlying language. For example, the OntClass
Java class can represent an OWL class or RDFS class.
To represent the differences between the various representations,
each of the ontology languages has a specification, which lists the
permitted constructs and the names of the classes and properties.
Thus in the OWL profile is it owl:ObjectProperty (short for
http://www.w3.org/2002/07/owl#ObjectProperty) and in the RDFS
attempt to get an object property will cause an error
and search for all object properties will return empty java Stream.
The specification is bound to an ontology model, which is an extended
version of Jena’s
Model class.
The base Model allows access to the statements in a collection of
RDF data.
OntModel
extends this by adding support for the kinds of constructs expected to
be in an ontology: classes (in a class hierarchy), properties (in a
property hierarchy) and individuals.
When you’re working with an
ontology in Jena, all of the state information remains encoded as
RDF triples (accessed as Jena
Statements) stored in the RDF
model. The ontology API
doesn’t change the RDF representation of ontologies. What it does
do is add a set of convenience classes and methods that make it
easier for you to write programs that manipulate the underlying RDF
triples.
The predicate names defined in the ontology language correspond to
the accessor methods on the Java classes in the API. For example,
an OntClass has a method to list its super-classes, which
corresponds to the values of the subClassOf property in the RDF
representation. This point is worth re-emphasising: no information
is stored in the OntClass object itself. When you call the
OntClass superClasses() method, Jena will retrieve the
information from the underlying RDF triples. Similarly, adding a
subclass to an OntClass asserts an additional RDF triple, typically
with predicate rdfs:subClassOf into
the model.
Ontologies and reasoning
One of the key benefits of building an ontology-based application
is using a reasoner to derive additional truths about the concepts
you are modelling. We saw a simple instance of this above: the
assertion “Fred is a Fish” entails the deduction “Fred is an
Animal”. There are many different styles of automated reasoner, and
very many different reasoning algorithms. Jena includes support for
a variety of reasoners through the
inference API.
A common feature of Jena
reasoners is that they create a new RDF model which appears to
contain the triples that are derived from reasoning as well as the
triples that were asserted in the base model. This extended model
nevertheless still conforms to the contract for Jena models.
It can be used wherever a non-inference model can be used. The ontology
API exploits this feature: the convenience methods provide by the ontology API
can query an extended inference model in just the same way
that they can a plain RDF model. In fact, this is such a common pattern that
we provide simple recipes for constructing ontology models whose
language, storage model and reasoning engine can all be simply
specified when an OntModel is created. We’ll show examples shortly.
Figure 2 shows one way of visualising this:

Graph is an internal Jena interface that supports the composition
of sets of RDF triples. The asserted statements, which may have
been read in from an ontology document, are held in the base graph.
The reasoner, or inference engine, can use the contents of the base
graph and the semantic rules of the language to show a more
complete set of base and entailed triples. This is also presented via a Graph
interface, so the OntModel works only with the outermost interface.
This regularity allows us to very easily build ontology models with
or without a reasoner. It also means that the base graph can be an
in-memory store, a database-backed persistent store, or some other
storage structure altogether – e.g. an LDAP directory – again without
affecting the operation of the ontology model (but noting that these
different approaches may have very different efficiency profiles).
RDF-level polymorphism and Java
Deciding which Java abstract class to use to represent a given RDF
resource can be surprisingly subtle. Consider the following RDF
sample:
<owl:Class rdf:ID="DigitalCamera">
</owl:Class>

This declares that the resource with the relative URI
#DigitalCamera is an OWL ontology class. It suggests that it
would be appropriate to model that declaration in Java with an
instance of an OntClass. Now suppose we add a triple to the RDF
model to augment the class declaration with some more information:
<owl:Class rdf:ID="DigitalCamera">
  <rdf:type owl:NamedIndividual />
</owl:Class>

Now we are stating that #DigitalCamera is an OWL Named Individual.
This is valid in OWL2, but, for example, in OWL1 DL,
such a punning is not allowed.
The problem we then have is that Java does not
allow us to dynamically change the Java class of the object
representing this resource. The resource has not changed: it still
has URI #DigitalCamera. But the appropriate Java class Jena might
choose to encapsulate it has changed from OntClass to OntIndividual.
Conversely, if we subsequently remove the rdf:type owl:NamedIndividual
from the model, using the OntIndividual Java class is no longer
appropriate.
Even worse, OWL2 and OWL1 Full allow us to state the following (rather
counter-intuitive) construction:
<owl:Class rdf:ID="DigitalCamera">
  <rdf:type owl:ObjectProperty />
</owl:Class>

That is, #DigitalCamera is both a class and a property. While
this may not be a very useful claim, it illustrates a basic
point: we cannot rely on a consistent or unique mapping between an
RDF resource and the appropriate Java abstraction.
Jena accepts this basic characteristic of polymorphism at the RDF
level by considering that the Java abstraction (OntClass,
OntClass.Restriction, OntDataProperty, etc.) is just a view or facet
of the resource. That is, there is a one-to-many mapping from a
resource to the facets that the resource can present. If the
resource is typed as an owl:Class, it can present the OntClass
facet; given other types, it can present other facets. Jena
provides the .as() method to efficiently map from an RDF object
to one of its allowable facets. Given a RDF object (i.e. an
instance of org.apache.jena.rdf.model.RDFNode or one of its
sub-types), you can get a facet by invoking as() with an argument
that denotes the facet required. Specifically, the facet is
identified by the Java class object of the desired facet. For
example, to get the OntClass facet of a resource, we can write:
Resource r = myModel.getResource( myNS + "DigitalCamera" );
OntClass cls = r.as( OntClass.class );

This pattern allows our code to defer decisions about the correct Java
abstraction to use until run-time. The choice can depend on the
properties of the resource itself. If a given RDFNode will not
support the conversion to a given facet, it will raise a
OntJenaException.Conversion. We can test whether .as() will succeed for a
given facet with canAs(). This RDF-level polymorphism is used
extensively in the Jena ontology API to allow maximum flexibility
in handling ontology data.
Running example: the ESWC ontology
To illustrate the principles of using the ontology API, we will use
examples drawn from the
ESWC ontology
This ontology presents a simple model for describing the concepts
and activities associated with a typical academic conference. A
copy of the ontology serialized in RDF/XML is included with the
Jena download, see:
[eswc-2006-09-21.rdf]
(note that you may need to view the page source in some browsers to
see the XML code).
A subset of the classes and properties from the ontology are shown
in Figure 3:

Figure 3: Classes and properties from ESWC ontology
We will use elements from this ontology to illustrate the ontology
API throughout the rest of this document.
Creating ontology models
An ontology model is an extension of the Jena RDF model,
providing extra capabilities for handling ontologies. Ontology
models are created through the Jena
OntModelFactory.
The simplest way to create an ontology model is as follows:
OntModel m = OntModelFactory.createModel();

This will create an ontology model with the default settings,
which are set for maximum compatibility with the previous version
of Jena. These defaults are:

OWL2-DL language
in-memory triples graph
builtin RDFS inference, which principally produces entailments from the
sub-class and sub-property hierarchies.

The builtin RDFS inference is a cut down inference
which is done by model itself without any attached reasoner.
To have complete RDFS inference use, e.g., OWL2_DL_MEM_RDFS_INF specification.
In many applications, such as driving a GUI, RDFS inference is too
strong. For example, every class is inferred to be an immediate sub-class of
owl:Thing. In other applications, stronger reasoning is needed.
In general, to create an OntModel with a particular reasoner or
language profile, you should pass a model specification to the
createModel call.
For example, an OWL model that performs no reasoning at all can be created with:
OntModel m = OntModelFactory.createModel( OntSpecification.OWL2_DL_MEM );

Beyond these basic choices, the complexities of configuring an
ontology model are wrapped up in a recipe object called
OntSpecification.
This specification allows complete control over the configuration
choices for the ontology model, including the language profile in
use and the reasoner.
A number of common recipes are pre-declared as constants in
OntSpecification, and listed below.

  
      
          OntSpecification
          Language profile
          Storage model
          Reasoner
      
  
  
      
          OWL2_DL_MEM_BILTIN_RDFS_INF
          OWL2 DL
          in-memory
          builtin reasoner with RDFS-level entailment-rules
      
      
          OWL2_DL_MEM
          OWL2 DL
          in-memory
          none
      
      
          OWL2_DL_MEM_TRANS_INF
          OWL2 DL
          in-memory
          transitive class-hierarchy inference
      
      
          OWL2_DL_MEM_RULES_INF
          OWL2 DL
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL2_DL_MEM_RDFS_INF
          OWL2 DL
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          OWL2_FULL_MEM
          OWL2 Full
          in-memory
          none
      
      
          OWL2_FULL_MEM_TRANS_INF
          OWL2 Full
          in-memory
          transitive class-hierarchy inference
      
      
          OWL2_FULL_MEM_RULES_INF
          OWL2 Full
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL2_FULL_MEM_RDFS_INF
          OWL2 Full
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          OWL2_FULL_MEM_MICRO_RULES_INF
          OWL2 Full
          in-memory
          optimised rule-based reasoner with OWL rules
      
      
          OWL2_FULL_MEM_MINI_RULES_INF
          OWL2 Full
          in-memory
          rule-based reasoner with subset of OWL rules
      
      
          OWL2_EL_MEM
          OWL2 EL
          in-memory
          none
      
      
          OWL2_EL_MEM_TRANS_INF
          OWL2 EL
          in-memory
          transitive class-hierarchy inference
      
      
          OWL2_EL_MEM_RULES_INF
          OWL2 EL
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL2_EL_MEM_RDFS_INF
          OWL2 EL
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          OWL2_QL_MEM
          OWL2 QL
          in-memory
          none
      
      
          OWL2_QL_MEM_TRANS_INF
          OWL2 QL
          in-memory
          transitive class-hierarchy inference
      
      
          OWL2_QL_MEM_RULES_INF
          OWL2 QL
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL2_QL_MEM_RDFS_INF
          OWL2 QL
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          OWL2_RL_MEM
          OWL2 RL
          in-memory
          none
      
      
          OWL2_RL_MEM_TRANS_INF
          OWL2 RL
          in-memory
          transitive class-hierarchy inference
      
      
          OWL2_RL_MEM_RULES_INF
          OWL2 RL
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL2_RL_MEM_RDFS_INF
          OWL2 RL
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          OWL1_DL_MEM
          OWL1 DL
          in-memory
          none
      
      
          OWL1_DL_MEM_TRANS_INF
          OWL1 DL
          in-memory
          transitive class-hierarchy inference
      
      
          OWL1_DL_MEM_RULES_INF
          OWL1 DL
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL1_DL_MEM_RDFS_INF
          OWL1 DL
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          OWL1_FULL_MEM
          OWL1 Full
          in-memory
          none
      
      
          OWL1_FULL_MEM_TRANS_INF
          OWL1 Full
          in-memory
          transitive class-hierarchy inference
      
      
          OWL1_FULL_MEM_RULES_INF
          OWL1 Full
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL1_FULL_MEM_RDFS_INF
          OWL1 Full
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          OWL1_FULL_MEM_MICRO_RULES_INF
          OWL1 Full
          in-memory
          optimised rule-based reasoner with OWL rules
      
      
          OWL1_FULL_MEM_MINI_RULES_INF
          OWL1 Full
          in-memory
          rule-based reasoner with subset of OWL rules
      
      
          OWL1_LITE_MEM
          OWL1 Lite
          in-memory
          none
      
      
          OWL1_LITE_MEM_TRANS_INF
          OWL1 Lite
          in-memory
          transitive class-hierarchy inference
      
      
          OWL1_LITE_MEM_RULES_INF
          OWL1 Lite
          in-memory
          rule-based reasoner with OWL rules
      
      
          OWL1_LITE_MEM_RDFS_INF
          OWL1 Lite
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
      
          RDFS_MEM
          RDFS
          in-memory
          none
      
      
          RDFS_MEM_TRANS_INF
          RDFS
          in-memory
          transitive class-hierarchy inference
      
      
          RDFS_MEM_RDFS_INF
          RDFS
          in-memory
          rule reasoner with RDFS-level entailment-rules
      
  

For details of reasoner capabilities, please see the
inference documentation and the Javadoc
for
OntSpecification.
See also further discussion below.
To create a custom model specification,
you can create OntPersonality object
and create a new OntSpecification from its constructor:
OntPersonality OWL2_FULL_PERSONALITY = OntPersonalities.OWL2_ONT_PERSONALITY()
                .setBuiltins(OntPersonalities.OWL2_FULL_BUILTINS)
                .setReserved(OntPersonalities.OWL2_RESERVED)
                .setPunnings(OntPersonalities.OWL_NO_PUNNINGS)
                .setConfig(OntConfigs.OWL2_CONFIG)
                .build();
OntSpecification OWL2_FULL_MEM_RDFS_INF = new OntSpecification(
    OWL2_FULL_PERSONALITY, RDFSRuleReasonerFactory.theInstance()
);

The first parameter in the builder above is the vocabulary
(see OntPersonality.Builtins)
that contains a set of OWL entities’ IRIs that do not require an explicit declaration (e.g., owl:Thing).
The second parameter is the vocabulary
(see OntPersonality.Reserved),
which is for system resources and properties that cannot represent any OWL object.
The third vocabulary
(see OntPersonality.Punnings)
contains description of OWL punnings.
The last parameter in the builder is the
OntConfig
that allows fine-tuning the behavior.
There are the following configuration settings
(see OntModelControls):

  
      
          Setting
          Description
      
  
  
      
          ALLOW_GENERIC_CLASS_EXPRESSIONS
          If this key is set to true, there is a special type of class expressions, which includes any structure declared as owl:Class or owl:Restriction that cannot be classified as a specific type. This option is for compatibility with legacy OntModel.
      
      
          ALLOW_NAMED_CLASS_EXPRESSIONS
          If this key is set to true, all class expressions are allowed to be named (can have URI). This option is for compatibility with legacy OntModel.
      
      
          USE_BUILTIN_HIERARCHY_SUPPORT
          If this key is set to true, then the class/property hierarchies (e.g., see OntClass.subClasses()) are to be inferred by the naked model itself using builtin algorithms.
      
      
          USE_CHOOSE_MOST_SUITABLE_ONTOLOGY_HEADER_STRATEGY
          If true, a multiple ontology header is allowed.
      
      
          USE_GENERATE_ONTOLOGY_HEADER_IF_ABSENT_STRATEGY
          If true, OntID will be generated automatically if it is absent (as a b-node). OWL2 requires one and only one ontology header.
      
      
          USE_LEGACY_COMPATIBLE_NAMED_CLASS_FACTORY
          If true, named class testing is compatible with the legacy Jena OntModel, otherwise, a strict check against the specification for the class declaration is performed (owl:Class for OWL & rdfs:Class for RDFS types are required).
      
      
          USE_OWL_CLASS_DISJOINT_WITH_FEATURE
          Controls owl:disjointWith functionality.
      
      
          USE_OWL_CLASS_EQUIVALENT_FEATURE
          Controls owl:equivalentClass functionality.
      
      
          USE_OWL_DATA_PROPERTY_FUNCTIONAL_FEATURE
          Controls data owl:FunctionalProperty functionality.
      
      
          USE_OWL_INDIVIDUAL_DIFFERENT_FROM_FEATURE
          Controls owl:differentFrom functionality.
      
      
          USE_OWL_INDIVIDUAL_SAME_AS_FEATURE
          Controls owl:sameAs functionality.
      
      
          USE_OWL_INVERSE_OBJECT_PROPERTIES_FEATURE
          Controls owl:inverseOf functionality (InverseObjectProperty axiom).
      
      
          USE_OWL_OBJECT_PROPERTY_FUNCTIONAL_FEATURE
          Controls object owl:FunctionalProperty functionality.
      
      
          USE_OWL_PROPERTY_ASYMMETRIC_FEATURE
          Controls owl:AsymmetricProperty functionality.
      
      
          USE_OWL_PROPERTY_CHAIN_AXIOM_FEATURE
          Controls owl:propertyChainAxiom functionality.
      
      
          USE_OWL_PROPERTY_EQUIVALENT_FEATURE
          Controls owl:equivalentProperty functionality.
      
      
          USE_OWL_PROPERTY_INVERSE_FUNCTIONAL_FEATURE
          Controls owl:InverseFunctionalProperty functionality.
      
      
          USE_OWL_PROPERTY_IRREFLEXIVE_FEATURE
          Controls owl:IrreflexiveProperty functionality.
      
      
          USE_OWL_PROPERTY_REFLEXIVE_FEATURE
          Controls owl:ReflexiveProperty functionality.
      
      
          USE_OWL_PROPERTY_SYMMETRIC_FEATURE
          Controls owl:SymmetricProperty functionality.
      
      
          USE_OWL_PROPERTY_TRANSITIVE_FEATURE
          Controls owl:TransitiveProperty functionality.
      
      
          USE_OWL1_DATARANGE_DECLARATION_FEATURE
          If this key is set to true, then owl:DataRange (OWL1) is used instead of rdfs:Datatype (OWL2).
      
      
          USE_OWL1_DISTINCT_MEMBERS_PREDICATE_FEATURE
          If this key is set to true, then owl:distinctMembers (OWL1) is used instead of owl:members (OWL2).
      
      
          USE_OWL2_CLASS_HAS_KEY_FEATURE
          Controls owl:hasKey functionality.
      
      
          USE_OWL2_DEPRECATED_VOCABULARY_FEATURE
          If this key is set to true, then owl:DataRange and owl:distinctMembers will also be considered, although in OWL2 they are deprecated.
      
      
          USE_OWL2_NAMED_CLASS_DISJOINT_UNION_FEATURE
          Controls owl:disjointUnionOf functionality.
      
      
          USE_OWL2_NAMED_INDIVIDUAL_DECLARATION_FEATURE
          If this key is set to true, then owl:NamedIndividual declaration is used for creating individuals (method OntModel#createIndividual(String iri)).
      
      
          USE_OWL2_PROPERTY_DISJOINT_WITH_FEATURE
          Controls owl:propertyDisjointWith functionality.
      
      
          USE_OWL2_QUALIFIED_CARDINALITY_RESTRICTION_FEATURE
          If this key is set to true, then owl:qualifiedCardinality, owl:maxQualifiedCardinality, owl:minQualifiedCardinality predicates are allowed for Cardinality restrictions.
      
      
          USE_SIMPLIFIED_TYPE_CHECKING_WHILE_LIST_INDIVIDUALS
          Used while listing individuals (OntModel.individuals()).
      
  

Compound ontology documents and imports processing
The OWL ontology language includes some facilities for
creating modular ontologies that can be re-used in a similar manner
to software modules. In particular, one ontology can import
another. Jena helps ontology developers to work with modular
ontologies by automatically handling the imports statements in
ontology models.
The key idea is that the base model of an ontology model is
actually a collection of models, one per imported model. This means
we have to modify figure 2 a bit. Figure 4 shows how the ontology
model builds a collection of import models:


Figure 4: ontology model compound document structure for imports
We will use the term document to describe an ontology serialized
in some transport syntax, such as RDF/XML or N3. This terminology
isn’t used by the OWL or RDFS standards, but it is a convenient way
to refer to the written artifacts. However, from a broad view of
the interlinked semantic web, a document view imposes artificial
boundaries between regions of the global web of data and isn’t necessarily
a useful way of thinking about ontologies.
We will load an ontology document into an ontology model in the
same way as a normal Jena model, using the read method. There are
several variants on read, that handle differences in the source of
the document (to be read from a resolvable URL or directly from an
input stream or reader), the base URI that will resolve any
relative URI’s in the source document, and the serialisation
language. In summary, these variants are:
read( String url )
read( Reader reader, String base )
read( InputStream reader, String base )
read( String url, String lang )
read( Reader reader, String base, String lang )
read( InputStream reader, String base, String lang )

You can use any of these methods to load an ontology document. Note
that we advise that you avoid the read() variants that accept
a java.io.Reader argument when loading XML documents containing
internationalised character sets, since the handling of character
encoding by the Reader and by XML parsers is not compatible.
By default, when an ontology model reads an ontology document, it
will not locate and load the document’s imports.
To automatically handle all documents from imports closure, a specialized method from OntModelFactory should be used:
GraphRepository repository = GraphRepository.createGraphDocumentRepositoryMem();
OntModel m = OntModelFactory.createModel(graph, OntSpecification.OWL2_DL_MEM_BUILTIN_INF, repository);

An OWL document may contain an individual owl:Ontology, which
contains meta-data about that document itself. For example:
<owl:Ontology rdf:about="">
  <dc:creator rdf:value="Ian Dickinson" />
  <owl:imports rdf:resource="http://jena.apache.org/examples/imported-ontology-iri" />
  <owl:versionIRI rdf:resource="http://jena.apache.org/examples/this-ontology-iri" />
</owl:Ontology>

In OWL2 this section is mandatory and there must be one and only one per document.
It corresponds
OntID object.
In the example above, the construct rdf:about="" is a relative URI.
It will resolve to the document’s base URI.
In OWL2 the identifier of ontology is either version IRI, ontology IRI or document IRI
(see OWL 2 Web Ontology Language Structural Specification: Imports).
The owl:imports line states
that this ontology is constructed using classes, properties and
individuals from the referenced ontology,
which identifier in the example above is http://jena.apache.org/examples/imported-ontology-iri.
When an OntModel, created with GraphRepository, reads
this document, it will notice the owl:imports line and attempt to
load the imported ontology into a sub-model of the ontology model
being constructed.
The definitions from both the base ontology and all the imports will be visible to the reasoner.
Each imported ontology document is held in a separate graph
structure. This is important: we want to keep the original source
ontology separate from the imports. When we write the model out
again, normally only the base model is written (the alternative is
that all you see is a confusing union of everything). And when we
update the model, only the base model changes. To get the base
model or base graph from an OntModel, use:
Model base = thisOntModel.getBaseModel();

Imports are processed recursively, so if our base document imports
ontology A, and A imports B, we will end up with the structure shown
in Figure 4. Note that the imports have been flattened out. A cycle
check is used to prevent the document handler getting stuck if, for
example, A imports B which imports A!
To dynamically control imports, the methods OntModel#addImport,
OntModel#removeImport, OntModel#hasImport and OntModel#imports can be used.
E.g.:
thisOntModel.addImport(otherOntModel);

If the ontology is created with GraphRepository,
adding a statement <this-ont-id> owl:imports <other-ont-id> will import the corresponding ontology.
More convenient way to add the import, is to use OntID object:
thisOntModel.getID().addImport("other-ontology-iri");

GraphRepository
GraphRepository
is an abstraction that provides access to graphs.
The method GraphRepository#createGraphDocumentRepositoryMem() creates an implementation DocumentGraphRepository
that stores graphs in memory.
The method DocumentGraphRepository#get returns graphs by reference id,
which can be a URL or a path to a file.
If the graph is not in the repository, it will be downloaded from the provided link.
Using the DocumentGraphRepository#addMapping method,
you can match the graph ID to the actual location of the document:
DocumentGraphRepository repo = GraphRepository.createGraphDocumentRepositoryMem();
repo.addMapping("http://this-ontology", "file://example.ttl");
Graph graph = repo.get("http://this-ontology");

If the GraphRepository is passed as a parameter to the corresponding OntModelFactory#createModel method,
it will contain
UnionGraph graphs
that provide connectivity between ontologies.
GraphMaker
GraphMaker
is another abstraction that provides access to graphs.
It is primary intended to be a facade for persistent storage.
The method GraphRepository#createPersistentGraphRepository(GraphMaker) allows
to manage persistent ontologies backed by GraphMaker.
See also Working with persistent ontologies.
OntModel triple representation: OntStatement
OntStatement is an extended org.apache.jena.rdf.model.Statement.
It has additional methods to support OWL2 annotations.
For example, the following snippet
OntModel m = OntModelFactory.createModel( OntSpecification.OWL2_DL_MEM );
OntStatement st1 = m.createOntClass("X").getMainStatement();
OntStatement st2 = st1.addAnnotation(m.getRDFSComment(), "comment#1");
OntStatement st3 = st2.addAnnotation(m.getRDFSLabel(), "label#1");
OntStatement st4 = st3.addAnnotation(m.getRDFSLabel(), "label#2");

will produce the following RDF:
PREFIX owl:  <http://www.w3.org/2002/07/owl#>
PREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX xsd:  <http://www.w3.org/2001/XMLSchema#>

<X>     rdf:type      owl:Class;
        rdfs:comment  "comment#1" .

[ rdf:type               owl:Annotation;
  rdfs:label             "label#2";
  owl:annotatedProperty  rdfs:label;
  owl:annotatedSource    [ rdf:type               owl:Axiom;
                           rdfs:label             "label#1";
                           owl:annotatedProperty  rdfs:comment;
                           owl:annotatedSource    <X>;
                           owl:annotatedTarget    "comment#1"
                         ];
  owl:annotatedTarget    "label#1"
] .

The generic ontology type: OntObject
All of the classes in the ontology API that represent ontology
values have
OntObject
as a common super-class.
This makes OntObject a good place to
put shared functionality for all such classes, and makes a handy
common return value for general methods. The Java interface
OntObject extends more general OntResource
which in turns extends Jena’s RDF Resource
interface, so any general method that accepts a resource or an
RDFNode
will also accept an OntObject, and consequently, any other
ontology value.
Some of the common attributes of an ontology object that are
expressed through methods on OntObject are shown below:

  
      
          Attribute
          Meaning
      
  
  
      
          objectType
          A concret java Class-type of this OntObject
      
      
          mainStatement
          The main OntStatement, which determines the nature of this ontological resource. In most cases it is a declaration and wraps a triple with predicate rdf:type
      
      
          spec
          All characteristic statements of the ontology resource, i. e., all those statements which completely determine this object nature according to the OWL2 specification; mainStatement is a part of spec
      
      
          content
          spec plus all additional statements in which this object is the subject, minus those of them whose predicate is an annotation property (i.e. annotations are not included)
      
      
          annotations
          All top-level annotations attached to the mainStatement of this object
      
      
          statements
          Model’s statements for which this object is a subject
      
      
          objects
          Lists typed Resources for which this object is a subject
      
      
          types
          Equivalent to objects(RDF.type, Resource.class)
      
      
          isLocal
          Determines if this Ontology Resource is locally defined, which means mainStatement belongs to a base graph
      
  

The generic way to list OntObjects of a particular type is the method <T extends OntObject> OntModel#ontObject(Class<T>)
Ontology entities
In OWL2, there are six kinds of named (IRI) resources,
called OWL entities.
The common supertype is OntEntity,
which has following sub-types:

OntClass.Named - a named class expression.
OntDataRange.Named - a named data range expression.
OntIndividual.Named - a named individual
OntObjectProperty.Named - a non-inverse object property
OntDataProperty - a datatype property
OntAnnotationProperty - an annotation property

OntEntity can be ontology defined or builtin, e.g. owl:Thing is a builtin OntClass.Named
Ontology classes
Classes are the basic building blocks of an ontology.
A class is represented in Jena by an
OntClass
object. As mentioned above, an ontology class
is a facet of an RDF resource. One way, therefore, to get an
ontology class is to convert a plain RDF resource into
its class facet. Assume that m is a
suitably defined OntModel, into which the ESWC ontology has
already been read, and that NS is a variable denoting the
ontology namespace:
Resource r = m.getResource( NS + "Paper" );
OntClass paper = r.as( OntClass.class );

This can be shortened by calling getOntClass() on the ontology
model:
OntClass paper = m.getOntClass( NS + "Paper" );

The getOntClass method will retrieve the resource with the given
URI, and attempt to obtain the OntClass facet. If either of these
operations fail, getOntClass() will return null. Compare this
with the createOntClass method, which will reuse an existing
resource if possible, or create a new class resource if not:
OntClass paper     = m.createOntClass( NS + "Paper" );
OntClass bestPaper = m.createOntClass( NS + "BestPaper" );

In OWL2 OntClass can be either named class (URI resource) or anonymous class expression.
OWL1 OntSpecifications also allow named class expressions.
An anonymous class expression is
a class description with no associated URI, which have structure determined by the specification.
Anonymous classes are
often used when building more complex ontologies in OWL.
They are less useful in RDFS.
OntClass anonClass = m.createObjectUnionOf(classes);

Once you have the ontology class object, you can begin processing
it through the methods defined on OntClass. The attributes of a
class are handled in a similar way to the attributes of
OntObject, above, with a collection of methods to set, add, get,
test, list and remove values. Properties of classes that are
handled in this way are:

  
      
          Attribute
          Meaning
      
  
  
      
          subClasses
          A subclass of this class, i.e. those classes that are declared rdfs:subClassOf this class.
      
      
          superClasses
          A super-class of this class, i.e. a class that this class is a rdfs:subClassOf.
      
      
          equivalentClasses
          A class that represents the same concept as this class. This is not just having the same class extension: the class ‘British Prime Minister in 2003’ contains the same individual as the class ’the husband of Cherie Blair’, but they represent different concepts.
      
      
          disjointWith
          Denotes a class with which this class has no instances in common.
      
      
          hasKey
          OWL2 Language feature Keys
      
      
          disjointUnions
          OWL2 language feature Disjoint Union, which only applicable to named classes
      
  

Thus, in our example ontology, we can print a list the subclasses
of an Artefact as follows:
OntClass artefact = m.getOntClass( NS + "Artefact" );
artefact.subClasses().forEach( it -> System.out.println( it.getURI() ) );

Note that, under RDFS and OWL semantics, each class is a sub-class
of itself (in other words, rdfs:subClassOf is reflexive). While
this is true in the semantics, Jena users have reported finding
it inconvenient. Therefore, the subClasses and
superClasses convenience methods remove the reflexive from the list of
results returned by the iterator. However, if you use the plain
Model API to query for rdfs:subClassOf triples, assuming that a
reasoner is in use, the reflexive triple will appear among the deduced
triples.
Given an OntClass object, you can create or remove members of the
class extension – individuals that are instances of the class –
using the following methods:

  
      
          Method
          Meaning
      
  
  
      
          individuals()individuals(boolean direct)
          Returns a Stream over those instances that include this class among their rdf:type values. The direct flag can be used to select individuals that are direct members of the class, rather than indirectly through the class hierarchy. Thus if p1 has rdf:type :Paper, it will appear in the Stream returned by individuals on :Artefact, but not in the Stream returned by individuals(false) on :Artefact.
      
      
          createIndividual()createIndividual(String uri)
          Adds a resource to the model, whose asserted rdf:type is this ontology class. If no URI is given, the individual is an anonymous resource.
      
      
          removeIndividual(Resource individual)
          Removes the association between the given individual and this ontology class. Effectively, this removes the rdf:type link between this class and the resource. Note that this is not the same as removing the individual altogether, unless the only thing that is known about the resource is that it is a member of the class.
      
  

To test whether a class is a root of the class hierarchy in this
model (i.e. it has no known super-classes), call
isHierarchyRoot().
The domain of a property is intended to allow entailments about the
class of an individual, given that it appears as a statement
subject. It is not a constraint that can be used to validate a
document, in the way that XML schema can do. Nevertheless, many
developers find it convenient to use the domain of a property to
document the design intent that the property only applies to known
instances of the domain class. Given this observation, it can be a
useful debugging or display aide to show the properties that have
this class among their domain classes. The method
declaredProperties() attempts to identify the properties that
are intended to apply to instances of this class. Using
declaredProperties is explained in detail in the
RDF frames how-to.
The following class expressions are supported:

  
      
          Java Class
          OWL2 construct
      
  
  
      
          OntClass.Named
          Class Entity
      
      
          OntClass.IntersectionOf
          Intersection of Class Expressions
      
      
          OntClass.UnionOf
          Union of Class Expressions
      
      
          OntClass.ComplementOf
          Complement of Class Expressions
      
      
          OntClass.OneOf
          Enumeration of Individuals
      
      
          OntClass.ObjectAllValuesFrom
          Universal Quantification
      
      
          OntClass.ObjectSomeValuesFrom
          Existential Quantification
      
      
          OntClass.ObjectHasValue
          Individual Value Restriction
      
      
          OntClass.HasSelf
          Self Restriction
      
      
          OntClass.ObjectCardinality
          Exact Cardinality
      
      
          OntClass.ObjectMaxCardinality
          Maximum Cardinality
      
      
          OntClass.ObjectMinCardinality
          Minimum Cardinaloty
      
      
          OntClass.DataAllValuesFrom
          Universal Qualification
      
      
          OntClass.DataSomeValuesFrom
          Existential Quantification
      
      
          OntClass.DataHasValue
          Literal Value Restriction
      
      
          OntClass.DataCardinality
          Exact Cardinality
      
      
          OntClass.DataMaxCardinality
          Maximum Cardinality
      
      
          OntClass.DataMinCardinality
          Minimum Cardinality
      
      
          OntClass.NaryDataAllValuesFrom
          Universal Qualification
      
      
          OntClass.NaryDataSomeValuesFrom
          Existential Quantification
      
  

Complex class expressions
We introduced the handling of basic, named classes above. These are
the only kind of class descriptions available in RDFS. In OWL,
however, there are a number of additional types of class
expression, which allow richer and more expressive descriptions of
concepts.
In OWL2, all class expressions (with except of named classes) must be anonymous resources.
In OWL1, for compatibility reasons, they are allowed to be named.
There are two main categories of additional class
expression: restrictions and logical expressions
We’ll examine each in turn.
Restriction class expressions
A
restriction
defines a class by reference to one of the properties of the
individuals that comprise the members of the class, and then
placing some constraint on that property. For example, in a simple
view of animal taxonomy, we might say that mammals are covered in
fur, and birds in feathers. Thus the property hasCovering is in
one case restricted to have the value fur, in the other to have
the value feathers. This is a has value restriction. Six
restriction types are currently defined by OWL:

  
      
          Restriction type
          Meaning
      
  
  
      
          has value
          The restricted property has exactly the given value.
      
      
          all values from
          All values of the restricted property, if it has any, are members of the given class.
      
      
          some values from
          The property has at least one value which is a member of the given class.
      
      
          cardinality
          The property has exactly n values, for some positive integer n.
      
      
          min cardinality
          The property has at least n values, for some positive integer n.
      
      
          max cardinality
          The property has at most n values, for some positive integer n.
      
      
          object has self
          A self-restriction consists of an object property expression p, and it contains all those individuals that are connected by p to themselves.
      
  

Jena provides a number of ways of creating restrictions, or
retrieving them from a model.
// list restriction with a given 
OntRestriction r = m.ontObjects(OntClass.ObjectSomeValuesFrom.class);

You can create a new restriction created by nominating the property
that the restriction applies to:
// anonymous restriction on property p
OntObjectProperty p = m.createObjectProperty( NS + "p" );
OntClass c = m.createOntClass( NS + "c" );
OntClass.Restriction r = m.createObjectMaxCardinality( p, 42, c );

A common case is that we want the restrictions on some property
p. In this case, from an object denoting p we can list the
restrictions that mention that property:
OntObjectProperty p = m.getObjectProperty( NS + "p" );
Stream<OntClass.Restriction> i = p.referringRestrictions();

A general restriction can be converted to a specific type of
restriction via as... methods (if the information is already in the
model), or, if the information is not in the model, via
convertTo... methods. For example, to convert the example
restriction r from the example above to an all values from
restriction, we can do the following:
OntClass c = m.createClass( NS + "SomeClass" );
AllValuesFromRestriction avf = r.convertToAllValuesFromRestriction( c );

To create a particular restriction ab initio, we can use the
creation methods defined on OntModel. For example:
OntClass c = m.createOntClass( NS + "SomeClass" );
OntObjectProperty p = m.createObjectProperty( NS + "p" );
OntClass.ObjectAllValuesFrom avf = m.createObjectAllValuesFrom( p, c );

Assuming that the above code fragment was using a model m which
was created with the OWL language profile, it creates a instance of
an OWL restriction that would have the following definition in
RDF/XML:
<owl:Restriction>
  <owl:onProperty rdf:resource="#p"/>
  <owl:allValuesFrom rdf:resource="#SomeClass"/>
</owl:Restriction>

Once we have a particular restriction object, there are methods
following the standard add, get, set and test naming pattern to
access the aspects of the restriction. For example, in a camera
ontology, we might find this definition of a class describing
Large-Format cameras:
<owl:Class rdf:ID="Large-Format">
  <rdfs:subClassOf rdf:resource="#Camera"/>
  <rdfs:subClassOf>
    <owl:Restriction>
      <owl:onProperty rdf:resource="#body"/>
      <owl:allValuesFrom rdf:resource="#BodyWithNonAdjustableShutterSpeed"/>
   </owl:Restriction>
  </rdfs:subClassOf>
</owl:Class>

Here’s one way to access the components of the all values from
restriction. Assume m contains a suitable camera ontology:
OntClass LargeFormat = m.getOntClass(ns + "Large-Format");
LargeFormat.superClasses()
        .filter(it -> it.canAs(OntClass.ObjectAllValuesFrom.class))
        .map(it -> it.as(OntClass.ObjectAllValuesFrom.class))
        .forEach(av ->
                System.out.println("AllValuesFrom class " + 
                        av.getValue().getURI() +
                        " on property " + 
                        av.getProperty().getURI())
        );

Boolean Connectives and Enumeration of Individuals
Most developers are familiar with the use of Boolean operators to
construct propositional expressions: conjunction (and), disjunction
(or) and negation (not). OWL provides a means for constructing
expressions describing classes with analogous operators, by
considering class descriptions in terms of the set of individuals
that comprise the members of the class.
Suppose we wish to say that an instance x has rdf:type A and
rdf:type B. This means that x is both a member of the set of
individuals in A, and in the set of individuals in B. Thus, x lies
in the intersection of classes A and B. If, on the other hand, A
is either has rdf:type A or B, then x must lie in the union
of A and B. Finally, to say that x does not have rdf:type A,
it must lie in the complement of A. These operations, union,
intersection and complement are the Boolean operators for
constructing class expressions. While complement takes only a
single argument, union and intersection must necessarily take more
than one argument. Before continuing with constructing and using
In additional to these three class expressions, OWL2 also offers
Enumeration of Individuals.
An enumeration of individuals ObjectOneOf( a1 ... an ) contains exactly the individuals ai with 1 ≤ i ≤ n.
Intersection, union and complement class expressions
Given Jena’s ability to construct lists, building intersection and
union class expressions is straightforward. The create methods on
OntModel allow us to construct an intersection or union directly.
For example, we can define the class of UK
industry-related conferences as the intersection of conferences
with a UK location and conferences with an industrial track. Here’s
the XML declaration:
<owl:Class rdf:ID="UKIndustrialConference">
  <owl:intersectionOf rdf:parseType="Collection">
    <owl:Restriction>
      <owl:onProperty rdf:resource="#hasLocation"/>
      <owl:hasValue rdf:resource="#united_kingdom"/>
    </owl:Restriction>
    <owl:Restriction>
      <owl:onProperty rdf:resource="#hasPart"/>
      <owl:someValuesFrom rdf:resource="#IndustryTrack"/>
    </owl:Restriction>
  </owl:intersectionOf>
</owl:Class>

Or, more compactly in N3/Turtle:
:UKIndustrialConference a owl:Class ;
    owl:intersectionOf (
       [a owl:Restriction ;
          owl:onProperty :hasLocation ;
          owl:hasValue :united_kingdom]
       [a owl:Restriction ;
          owl:onProperty :hasPart ;
          owl:someValuesFrom :IndustryTrack]
      )

Here is code to create this class declaration using Jena, assuming
that m is a model into which the ESWC ontology has been read:
// get the class references
OntClass place = m.getOntClass( ns + "Place" );
OntClass indTrack = m.getOntClass( ns + "IndustryTrack" );

// get the property references
OntObjectProperty hasPart = m.getObjectProperty( ns + "hasPart" );
OntObjectProperty hasLoc = m.getObjectProperty( ns + "hasLocation" );

// create the UK instance
OntIndividual uk = place.createIndividual( ns + "united_kingdom" );

// now the anonymous restrictions
OntClass.ObjectHasValue ukLocation =
        m.createObjectHasValue( hasLoc, uk );
OntClass.ObjectSomeValuesFrom hasIndTrack =
        m.createObjectSomeValuesFrom(  hasPart, indTrack );

// finally, create the intersection class
OntClass.IntersectionOf ukIndustrialConf =
        m.createObjectIntersectionOf( ukLocation, hasIndTrack );

Enumeration of Individuals
The final type class expression allowed by OWL is the enumerated
class. Recall that a class is a set of individuals. Often, we want
to define the members of the class implicitly: for example, “the class
of UK conferences”. Sometimes it is convenient to define a class
explicitly, by stating the individuals the class contains. An
OntClass.OneOf
is exactly the class whose members are the given individuals. For
example, we know that the class of PrimaryColours contains exactly
red, green and blue, and no others.
In Jena, an enumerated class is created in a similar way to other
classes. The set of values that comprise the enumeration is
described by an RDFList. For example, here’s a class defining the
countries that comprise the United Kingdom:
<owl:Class rdf:ID="UKCountries">
  <owl:oneOf rdf:parseType="Collection">
    <eswc:Place rdf:about="#england"/>
    <eswc:Place rdf:about="#scotland"/>
    <eswc:Place rdf:about="#wales"/>
    <eswc:Place rdf:about="#northern_ireland"/>
  </owl:oneOf>
</owl:Class>

To list the contents of this enumeration, we could do the
following:
OntClass place = m.getOntClass( ns + "Place" );

OntClass.OneOf ukCountries = m.createObjectOneOf(
        place.createIndividual( ns + "england" ),
        place.createIndividual( ns + "scotland" ),
        place.createIndividual( ns + "wales" ),
        place.createIndividual( ns + "northern_ireland" )
);

ukCountries.getList().members().forEach( System.out::println );

Listing classes
In many applications, we need to inspect the set of classes
in an ontology.
The primary method to list any OntObject’s, including OntClasses,
is <T extends OntObject> OntModel#ontObjects(Class<T>), which returns java Stream.
In additional to that, there are more specialized methods:
public Stream<OntClass.Named> classes();
public Stream<OntClass> hierarchyRoots();

In OWL, class
expressions are typically not named, but are denoted by anonymous
resources (aka bNodes). In many applications, such as displaying
an ontology in a user interface, we want to pick out the named
classes only, ignoring those denoted by bNodes. This is what
classes() does. The method hierarchyRoots()
identifies the classes that are uppermost in the class hierarchy
contained in the given model. These are the classes that have no
super-classes. The iteration returned by
hierarchyRoots() may contain anonymous classes.
You should also note that it is important to close the Stream
returned from the list methods, particularly when the underlying
store is a database. This is necessary so that any state (e.g., the
database connection resources) can be released. Closing happens
automatically when the hasNext() method on the underlying iterator returns
false. If your code does not iterate all the way to the end of the
iterator, you should call the Stream#close() method explicitly. Note
also that the values returned by these streams will depend on the
asserted data and the reasoner being used.
Ontology DataRanges
The concept of OWL DataRange is similar to class expressions.
There is also named data range, called datatype
(OntDataRange.Named),
and five kinds of anonymous data range expressions:
data ComplementOf, data IntersectionOf, data UnionOf, data OneOf and datatype restriction (see table below).
See the
OntDataRange javadoc
for more details.
Example:
m.createDataRestriction(
    XSD.integer.inModel(m).as(OntDataRange.Named.class),
    m.createFacetRestriction(OntFacetRestriction.FractionDigits.class, m.createTypedLiteral(42))
);

The following data range expressions are supported:

  
      
          Java Class
          OWL2 construct
      
  
  
      
          OntDataRange.Named
          Datatype Entity
      
      
          OntDataRange.ComplementOf
          Complement of Data Ranges,
      
      
          OntDataRange.IntersectionOf
          Intersection of Data Ranges,
      
      
          OntDataRange.UnionOf
          Union of Data Ranges,
      
      
          OntDataRange.OneOf
          Enumeration of Literals
      
      
          OntDataRange.Restriction
          Datatype Restrictions.
      
  

Ontology properties
In an ontology, a property denotes the name of a relationship
between resources, or between a resource and a data value.
Usually it corresponds to a predicate in logic representations, with one exception:
in OWL2 there is also Inverse Object Property Expression.
One interesting aspect of RDFS and OWL is that
properties are not defined as aspects of some enclosing class, but
are first-class objects in their own right. This means that
ontologies and ontology-applications can store, retrieve and make
assertions about properties directly. Consequently, Jena has a set
of Java classes that allow you to conveniently manipulate the
properties represented in an ontology model.
A named property in an ontology model is an extension of the core Jena
API class
Property
and allows access to the additional information that can be
asserted about properties in an ontology language. The common API
super-class for representing named and anonymous ontology properties in Java is
OntProperty.
There is also OntNamedProperty supertype,
which extends standard RDF Property, and OntRelationalProperty, which is supertype for OntDataProperty and OntObjectProperty.
Again, using the pattern of add, set, get, list, has, and remove
methods, we can access the following attributes of an
OntProperty:

  
      
          Attribute
          Meaning
      
  
  
      
          subProperty
          A sub property of this property; i.e. a property which is declared to be a rdfs:subPropertyOf this property. If p is a sub property of q, and we know that A p B is true, we can infer that A q B is also true. For OntObjectProperty there is also ObjectPropertyChain.
      
      
          superProperty
          A super property of this property, i.e. a property that this property is a rdfs:subPropertyOf
      
      
          domain
          Denotes the class or classes that form the domain of this property. Multiple domain values are interpreted as a conjunction. The domain denotes the class of value the property maps from.
      
      
          range
          Denotes the class or classes (for object properties) or datarange or dataranges (for datatype properties) that form the range of this property. Multiple range values are interpreted as a conjunction. The range denotes the class of values the property maps to.
      
      
          equivalentProperty
          Denotes a property that is the same as this property. This attribute is only for OntRealProperty.
      
      
          disjointProperty
          A disjoint object properties axiom states that all of the object property expressions OPEi, 1 ≤ i ≤ n, are pairwise disjoint; that is, no individual x can be connected to an individual y by both OPEi and OPEj for i ≠ j. Applicable only for OntRealPropery
      
      
          inverse
          Denotes a property that is the inverse of this property. Thus if q is the inverse of p, and we know that A q B, then we can infer that B p A. This attribute is only for OntObjectProperty.
      
  

In the example ontology, the property hasProgramme has a domain
of OrganizedEvent, a range of Programme and the human-readable label “has programme”.
We can reconstruct this definition in an
empty ontology model as follows:
OntModel m = OntModelFactory.createModel( OntSpecification.OWL2_FULL_MEM );
OntClass programme = m.createOntClass( NS + "Programme" );
OntClass orgEvent = m.createOntClass( NS + "OrganizedEvent" );

OntObjectProperty hasProgramme = m.createObjectProperty( NS + "hasProgramme" );

hasProgramme.addDomain( orgEvent );
hasProgramme.addRange( programme );
hasProgramme.addLabel( "has programme", "en" );

As a further example, we can alternatively add information to an
existing ontology. To add a super-property hasDeadline, to
generalise the separate properties denoting the submission
deadline, notification deadline and camera-ready deadline, do:
String ns = "http://www.eswc2006.org/technologies/ontology#";
OntModel m = OntModelFactory.createModel( OntSpecification.OWL2_FULL_MEM );
m.read( "https://raw.githubusercontent.com/apache/jena/main/jena-core/src-examples/data/eswc-2006-09-21.rdf" );

OntDataProperty subDeadline = m.getDataProperty( ns + "hasSubmissionDeadline" );
OntDataProperty notifyDeadline = m.getDataProperty( ns + "hasNotificationDeadline" );
OntDataProperty cameraDeadline = m.getDataProperty( ns + "hasCameraReadyDeadline" );

OntDataProperty deadline = m.createDataProperty( ns + "deadline" );
deadline.addDomain( m.getOntClass( ns + "Call" ) );
deadline.addRange( m.getDatatype(XSD.dateTime) );

deadline.addSubProperty( subDeadline );
deadline.addSubProperty( notifyDeadline );
deadline.addSubProperty( cameraDeadline );

Note that, although we called the addSubProperty method on the
object representing the new super-property, the serialized form of
the ontology will contain rdfs:subPropertyOf axioms on each of
the sub-property resources, since this is what the language
defines. Jena will, in general, try to allow symmetric access to
sub-properties and sub-classes from either direction.
Object and Datatype properties
OWL refines the basic property type from RDF into two
sub-types: object properties and datatype properties. The
difference between them is that an object property can have only
individuals in its range, while a datatype property has concrete
data literals (only) in its range. Some OWL reasoners are able to
exploit the differences between object and datatype properties to
perform more efficient reasoning over ontologies. OWL also adds an
annotation property, which is defined to have no semantic
entailments, and so is useful when annotating ontology documents,
for example.
Functional properties
OWL permits object and datatype properties to be functional –
that is, for a given individual in the domain, the range value will
always be the same. In particular, if father is a functional
property, and individual :jane has father :jim and
father :james, a reasoner is entitled to conclude that :jim and
:james denote the same individual. A functional property is
equivalent to stating that the property has a maximum cardinality
of one.
To declare a functional property, expression property.setFunctional(true) can be used.
Other property types
There are several additional characteristics of ObjectProperty that
represent additional capabilities of ontology properties:
transitive,
symmetric,
asymmetric,
inverse-functional,
reflexive,
irreflexive.
Transitive property means that if p is transitive, and we know :a p :b and also
b p :c, we can infer that :a p :c. A
Symmetric property means that if p is symmetric, and we know :a p :b, we can infer
:b p :a.
An inverse functional property
means that for any given range element, the domain value is unique.
An object property asymmetry axiom states
that the object property expression p is asymmetric — that is,
if an individual x is connected by p to an individual y, then y cannot be connected by p to x.
An object property reflexivity axiom states
that the object property expression p is reflexive — that is,
each individual is connected by p to itself.
An object property irreflexivity axiom states
that the object property expression p is irreflexive — that is,
no individual is connected by p to itself.
Instances or individuals
The Individual (or Instance in terms of legacy OntModel) is present
by the class OntIndividual.
The definition of individual is a class-assertion a rdf:type C., where C is OntClass and a is IRI or Blank Node.
Thus, unlike legacy Jena OntModel, in general not every resource can be represented as an OntIndividual,
although this is true in some specifications, such as OntSpecification.OWL1_FULL_MEM_RDFS_INF.
There are several ways to create individuals.
OntClass c = m.createOntClass( NS + "SomeClass" );

// first way: use a call on OntModel
OntIndividual ind0 = m.createOntIndividual( NS + "ind0", c );
OntIndividual ind1 = m.createOntIndividual( null, c );

// second way: create a named (uri) individual; this way works for OWL2 ontologies
OntIndividual ind2 = m.createOntIndividual( NS + "ind0" );

// third way: use a call on OntClass
OntIndividual ind3 = c.createIndividual( NS + "ind1" );
OntIndividual ind4 = c.createIndividual();

There is a wide range of methods for listing and manipulating related individuals, classes and properties.
For listing methods see the table:

  
      
          Method
          Effect
      
  
  
      
          sameIndividuals
          Lists all same individuals. The pattern to search for is ai owl:sameAs aj, where ai is this individual.
      
      
          disjoints
          Lists all OntDisjoint sections where this individual is a member.
      
      
          differentIndividuals
          Lists all different individuals. The pattern to search for is ai owl:differentFrom aj, where ai is this individual.
      
      
          positiveAssertions
          Lists all positive assertions for this individual (ai PN aj, a R v, where PN is named object property, R is a data property, v is a literal).
      
      
          negativeAssertions
          Lists all negative property assertions for this individual.
      
      
          classes
          Returns all class types
      
  

The most important method here is classes.
The interface OntIndividual provides a set of methods for testing and manipulating
the ontology classes to which an individual belongs. This is a
convenience: OWL and RDFS denote class membership through the
rdf:type property.
There are methods OntIndividual#classes(boolean direct), #classes(), addClassAssertion, hasOntClass, ontClass,
attachClass, dettachClass for listing,
getting and setting the rdf:type of an individual,
which denotes a class to which the resource belongs (noting that, in RDF and OWL, a resource can belong to many classes at once).
The rdf:type property is one for which many entailment rules are defined in the semantic models of the various ontology languages.
Therefore, the values that classes() returns is more than usually dependent on the reasoner bound to the ontology model.
For example, suppose we have class A, class B which is a subclass of A, and resource x whose asserted rdf:type is B.
With no reasoner, listing x’s RDF types will return only B.
If the reasoner is able to calculate the closure of the subclass hierarchy (and most can),
x’s RDF types would also include A.
A complete OWL reasoner would also infer that x has rdf:type owl:Thing and rdf:Resource.
For some tasks, getting a complete list of the RDF types of a resource is exactly what is needed.
For other tasks, this is not the case.
If you are developing an ontology editor, for example,
you may want to distinguish in its display between inferred and asserted types.
In the above example, only x rdf:type B is asserted, everything else is inferred.
One way to make this distinction is to make use of the base model (see Figure 4).
Getting the resource from the base model and listing the type properties
there would return only the asserted values.
For example:
// create the base model
String source = "https://www.w3.org/TR/2003/PR-owl-guide-20031215/wine";
String ns = "http://www.w3.org/TR/2003/PR-owl-guide-20031209/wine#";
OntModel base = OntModelFactory.createModel( OntSpecification.OWL2_DL_MEM );
base.read( source, "RDF/XML" );

// create the reasoning model using the base
OntModel inf = OntModelFactory.createModel( base.getGraph(), OntSpecification.OWL2_DL_MEM_RDFS_INF );

// create a country for this example
OntIndividual p1 = base.getIndividual( ns + "CorbansPrivateBinSauvignonBlanc");

// list the asserted types
p1.classes().forEach(clazz -> System.out.println( p1.getURI() + " is asserted in class " + clazz ));

// list the inferred types
OntIndividual p2 = inf.getIndividual( ns + "CorbansPrivateBinSauvignonBlanc");
p2.classes().forEach(clazz -> System.out.println( p2.getURI() + " is inferred to be in class " + clazz ));

For other user interface or presentation tasks,
we may want something between the complete list of types and the base list of only the asserted values.
Consider the class hierarchy in figure 5 (i):

Figure 5: asserted and inferred relationships
Figure 5 (i) shows a base model, containing a class hierarchy and an instance x.
Figure 5 (ii) shows the full set of relationships that might be inferred from this base model.
In Figure 5 (iii), we see only direct or maximally specific relationships.
For example, in 5 (iii) x does not have rdf:type A,
since this is a relationship covered by the fact that x has rdf:type D,
and D is a subclass of A.
Notice also that the rdf:type B link is also removed from the direct graph, for a similar reason.
Thus, the direct graph hides relationships from both the inferred and asserted graphs.
When displaying instance x in a user interface, particularly in a tree view of some kind,
the direct graph is often the most useful as it contains the useful information in the most compact form.
Ontology meta-data
In OWL, but not RDFS, meta-data about the ontology
itself is encoded as properties on a resource of type
owl:Ontology. By convention,
the URI of this individual is the URL, or web address, of the ontology document
itself. In the XML serialisation, this is typically shown as:
<owl:Ontology rdf:about="">
</owl:Ontology>

Note that the construct rdf:about="" does not indicate a
resource with no URI; it is in fact a shorthand way of referencing
the base URI of the document containing the ontology. The base
URI may be stated in the document through an xml:base declaration
in the XML preamble. The base URI can also be specified when
reading the document via Jena’s Model API (see the read() methods
on OntModel
for reference).
We can attach various meta-data statements to this object to
indicate attributes of the ontology as a whole, using the Java object
OntID:
m.getID()
        .annotate(m.getAnnotationProperty(OWL2.backwardCompatibleWith), m.createResource("http://example.com/v1"))
        .annotate(m.getRDFSSeeAlso(), m.createResource("http://example.com/v2"))
        .addComment("xxx");

In the Jena API, the ontology’s metadata properties can be accessed
through the
OntID
interface. Suppose we wish to know the list of URI’s that the
ontology imports. First, we must obtain the resource representing the
ontology itself:
OntModel m = ...;  
OntID id = m.getID();
id.imports().forEach( System.out::println );

Note that in OWL2 ontology document should contain one and only one ontology header (i.e. OntID).
The OntModel#getID method will generate the ontology header if it is missing.
A common practice is also to use the Ontology element to attach
Dublin Core metadata
to the ontology document. Jena provides a copy
of the Dublin Core vocabulary, in org.apache.jena.vocabulary.DCTerms.
To attach a statement saying that the ontology was authored by John
Smith, we can say:
OntID ont = m.getID();
ont.addProperty( DCTerms.creator, "John Smith" );

It is also possible to programmatically add imports and other
meta-data to a model, for example:
String base = ...; // the base URI of the ontology
OntModel m = ...;

OntID ont = m.setID( base );
ont.addImport( "http://example.com/import1" );
ont.addImport( "http://example.com/import2" );

Note that under default conditions, simply adding (or removing) an
owl:imports statement to a model will not cause the corresponding
document to be imported (or removed).
However, if model created with GraphRepository attached, it will start noticing
the addition or removal of owl:imports statements.
Ontology inference: overview
You have the choice of whether to use the Ontology API with Jena’s
reasoning capability turned on, and, if so, which of the various
reasoners to use. Sometimes a reasoner will add information to the
ontology model that it is not useful for your application to see. A
good example is an ontology editor. Here, you may wish to present
your users with the information they have entered in to their
ontology; the addition of the entailed information into the
editor’s display would be very confusing. Since Jena does not have
a means for distinguishing inferred statements from those
statements asserted into the base model, a common choice for
ontology editors and similar applications is to run with no
reasoner.
In many other cases, however, it is the addition of the reasoner
that makes the ontology useful. For example, if we know that John
is the father of Mary, we would expect a ‘yes’ if we query whether
John is the parent of Mary. The parent relationship is not
asserted, but we know from our ontology that fatherOf is a
sub-property of parentOf. If ‘John fatherOf Mary’ is true, then
‘John parentOf Mary’ is also true. The integrated reasoning
capability in Jena exists to allow just such entailments to be seen
and used.
For a complete and thorough description of Jena’s inference
capabilities, please see the
reasoner documentation. This section of
of the ontology API documentation is intended to serve as only a
brief guide and overview.
Recall from the introduction that the reasoners in Jena operate by
making it appear that triples entailed by the inference engine
are part of the model in just the same way as the asserted triples
(see Figure 2). The underlying architecture allows the reasoner to
be part of the same Java virtual machine (as is the case with the
built-in rule-based reasoners), or in a separate process on the
local computer, or even a remote computer. Of course, each of these
choices will have different characteristics of what reasoning
capabilities are supported, and what the implications for
performance are.
The reasoner attached to an ontology model, if any, is specified
through the
OntSpecification.
The Java object OntSpecification has two parameters: OntPersonality and ReasonerFactory.
The ReasonerRegistry provides a collection of pre-built reasoners –
see the reasoner documentation for more details. However, it is
also possible for you to define your own reasoner that conforms to
the appropriate interface. For example, there is an in-process
interface to the open-source
Pellet reasoner.
To facilitate the choice of reasoners for a given model, some
common choices have been included in the pre-built ontology model
specifications available as static fields on OntSpecification. The
available choices are described in the section on
ont model specifications, above.
Depending on which of these choices is made, the statements
returned from queries to a given ontology model may vary
considerably.
Additional notes
Jena’s inference machinery defines some specialised services that
are not exposed through the addition of extra triples to the model.
These are exposed by the
InfModel
interface; for convenience there is the method OntModel#asInferenceModel() to make
these services directly available to the user. Please note that
calling this method on an ontology model that does
not contain a reasoner will cause an error.
In general, inference models will add many additional
statements to a given model, including the axioms appropriate to
the ontology language. This is typically not something you will
want in the output when the model is serialized, so
write() on an ontology model will only write the statements from the base model.
This is typically the desired behaviour, but there are occasions
(e.g. during debugging) when you may want to write the entire
model, virtual triples included. The easiest way to achieve this is
to call the writeAll() method on OntModel. An alternative
technique, which can sometimes be useful for a variety of
use-cases, including debugging, is to snapshot the model by
constructing a temporary plain model and adding to it: the contents
of the ontology model:
OntModel m = ...

// snapshot the contents of ont model om
Model snapshot = ModelFactory.createDefaultModel();
snapshot.add( om );

Working with persistent ontologies
A common way to work with ontology data is to load the ontology
axioms and instances at run-time from a set of source documents.
This is a very flexible approach, but has limitations. In
particular, your application must parse the source documents each
time it is run. For large ontologies, this can be a source of
significant overhead. Jena provides an implementation of the RDF
model interface that stores the triples persistently in a database.
This saves the overhead of loading the model each time, and means
that you can store RDF models significantly larger than the
computer’s main memory, but at the expense of a higher overhead (a
database interaction) to retrieve and update RDF data from the
model. In this section we briefly discuss using the ontology API with
Jena’s persistent database models.
For information on setting-up and accessing the persistent models
themselves, see the TDB
reference sections.
There are two somewhat separate requirements for persistently
storing ontology data. The first is making the main or base model
itself persistent. The second is re-using or creating persistent
models for the imports of an ontology. These two requirements are
handled slightly differently.
To retrieve a Jena model from the database API, we have to know its
name. Fortunately, common practice for ontologies on the Semantic
Web is that each is named with a URI. We use this URI to name the
model that is stored in the database. Note carefully what is
actually happening here: we are exploiting a feature of the
database sub-system to make persistently stored ontologies easy to
retrieve, but we are not in any sense resolving the URI of the
model. Once placed into the database, the name of the model is
treated as an opaque string.
To create a persistent model for the ontology
http://example.org/Customers, we create a graph maker that will
access our underlying database, and use the ontology URI as the
database name. We then take the resulting persistent model, and use
it as the base model when constructing an ontology model:
Graph base = getMaker().createGraph( "http://example.org/Customers" );
OntModel m = OntModelFactory.createModel( base, OntSpecification.OWL2_DL_MEM );

Here we assume that the getMaker() method returns a suitably
initialized GraphMaker that will open the connection to the
database. This step only creates a persistent model named with the
ontology URI. To initialize the content, we must either add
statements to the model using the OntModel API, or do a one-time
read from a document:
m.read( "http://example.org/Customers" );

Once this step is completed, the model contents may be accessed in
future without needing to read again.
A GraphMaker may be wrapped
by PersistentGraphRepository using the method GraphRepository#createPersistentGraphRepository(GraphMaker).
This allows managing ontology relationships automatically, without interacting with GraphMaker directly.
Note on performance The built-in Jena reasoners, including the
rule reasoners, make many small queries into the model in order to
propagate the effects of rules firing. When using a persistent
database model, each of these small queries creates an SQL
interaction with the database engine. This is a very inefficient
way to interact with a database system, and performance suffers as
a result. Efficient reasoning over large, persistent databases is
currently an open research challenge. Our best suggested
work-around is, where possible, to snapshot the contents of the
database-backed model into RAM for the duration of processing by
the inference engine. An alternative solution, that may be
applicable if your application does not write to the datastore
often, is to precompute the inference closure of the ontology and
data in-memory, then store that into a database model to be queried
by the run-time application. Such an off-line processing
architecture will clearly not be applicable to every application
problem.
Utilities
There are several utilities, which can be used for various purposes.
Graphs
is a collection of methods for working with various types of graphs, including UnionGraph.
StdModels is for working with general-purpose Models,
and OntModels is
for working with OntModels.
Some of the useful methods are:

OntModels#getLCA( OntClass u, OntClass v ) - determine the lowest common ancestor for classes u and v.
This is the class that is lowest in the class hierarchy, and which includes both u and v among its sub-classes.
StdModels#findShortestPath( Model m, Resource start, RDFNode end, Filter onPath ) - breadth-first search, including a cycle check,
to locate the shortest path from start to end, in which every triple on the path returns true to the onPath predicate.
OntModels#namedHierarchyRoots( OntModel m ) - compute a list containing the uppermost fringe of the class hierarchy
in the given model which consists only of named classes.


  
  
  
    On this page
    
  
    
      
        Prerequisites
      
    
    Overview
      
        Further assistance
      
    
    General concepts
      
        RDFS
        OWL
        Ontology languages and the Jena Ontology API
        Ontologies and reasoning
        RDF-level polymorphism and Java
      
    
    Running example: the ESWC ontology
    Creating ontology models
    Compound ontology documents and imports processing
    GraphRepository
    GraphMaker
    OntModel triple representation: OntStatement
    The generic ontology type: OntObject
    Ontology entities
    Ontology classes
      
        Complex class expressions
        Restriction class expressions
        Boolean Connectives and Enumeration of Individuals
          
            Intersection, union and complement class expressions
            Enumeration of Individuals
          
        
        Listing classes
      
    
    Ontology DataRanges
    Ontology properties
      
        Object and Datatype properties
        Functional properties
        Other property types
      
    
    Instances or individuals
    Ontology meta-data
    Ontology inference: overview
      
        Additional notes
      
    
    Working with persistent ontologies
    Utilities\n\n\n\nJena Permissions is a SecurityEvaluator interface and a set of dynamic proxies that apply that interface to Jena Graphs, Models, and associated methods and classes. It does not implement any specific security policy but provides a framework for developers or integrators to implement any desired policy.
Documentation

Overview
Usage Notes
Design
Security Evaluator implementation
Assembler for a Secured Model
Adding Jena Permissions to Fuseki

Overview
Jena Permissions transparently intercepts calls to the Graph or Model interface, evaluates access restrictions and either allows or rejects the access. The system is authentication agnostic and will work with most authentication systems. The system uses dynamic proxies to wrap any Graph or Model implementation. The Jena Permissions module includes an Assembler module to extend the standard Assembler to include the ability to create secured models and graphs. A complete example application is also available.
The developer using Jena Permissions is required to implement a SecurityEvaluator that provides access to the Principal (User) using the system and also determines if that Principal has the proper access to execute a method. Through the SecurityEvaluator the developer may apply full CRUD (Create, Read, Update, and Delete) restrictions to graphs and optionally triples within the graphs.
The javadocs have additional annotations that specify what permissions at graph and triple levels are required for the user to execute the method.
There is an example jar that contains configuration examples for both a stand alone application and a Fuseki configuration option.
Usage Notes
When the system is correctly configured the developer creates a SecuredGraph by calling Factory.getInstance( SecurityEvaluator, String, Graph );. Once created the resulting graph automatically makes the appropriate calls to the SecurityEvaluator before passing any approved requests to the underlying graph.
Secured models are created by calling Factory.getInstance( SecurityEvaluator, String, Model ); or ModelFactory.createModelForGraph( SecuredGraph );
NOTE: when creating a model by wrapping a secured graph (e.g. ModelFactory.createModelForGraph( SecuredGraph );) the resulting Model does not have the same security requirements that the standard secured model. For example When creating a list on a secured model calling model.createList( RDFNode[] );, the standard secured model verifies that the user has the right to update the triples and allows or denies the entire operation accordingly. The wrapped secured graph does not have visibility to the createList() command and can only operate on the instructions issued by the model.createList() implementation. In the standard implementation the model requests the graph to delete one triple and then insert another. Thus the user must have delete and add permissions, not the update permission.
There are several other cases where the difference in the layer can trip up the security system. In all known cases the result is a tighter security definition than was requested. For simplicity sake we recommend that the wrapped secured graph only be used in cases where access to the graph as a whole is granted/denied. In these cases the user either has all CRUD capabilities or none.\n\nOn this page
    
  
    Documentation
    Overview
    Usage Notes
  

  
  
    Jena Permissions is a SecurityEvaluator interface and a set of dynamic proxies that apply that interface to Jena Graphs, Models, and associated methods and classes. It does not implement any specific security policy but provides a framework for developers or integrators to implement any desired policy.
Documentation

Overview
Usage Notes
Design
Security Evaluator implementation
Assembler for a Secured Model
Adding Jena Permissions to Fuseki

Overview
Jena Permissions transparently intercepts calls to the Graph or Model interface, evaluates access restrictions and either allows or rejects the access. The system is authentication agnostic and will work with most authentication systems. The system uses dynamic proxies to wrap any Graph or Model implementation. The Jena Permissions module includes an Assembler module to extend the standard Assembler to include the ability to create secured models and graphs. A complete example application is also available.
The developer using Jena Permissions is required to implement a SecurityEvaluator that provides access to the Principal (User) using the system and also determines if that Principal has the proper access to execute a method. Through the SecurityEvaluator the developer may apply full CRUD (Create, Read, Update, and Delete) restrictions to graphs and optionally triples within the graphs.
The javadocs have additional annotations that specify what permissions at graph and triple levels are required for the user to execute the method.
There is an example jar that contains configuration examples for both a stand alone application and a Fuseki configuration option.
Usage Notes
When the system is correctly configured the developer creates a SecuredGraph by calling Factory.getInstance( SecurityEvaluator, String, Graph );. Once created the resulting graph automatically makes the appropriate calls to the SecurityEvaluator before passing any approved requests to the underlying graph.
Secured models are created by calling Factory.getInstance( SecurityEvaluator, String, Model ); or ModelFactory.createModelForGraph( SecuredGraph );
NOTE: when creating a model by wrapping a secured graph (e.g. ModelFactory.createModelForGraph( SecuredGraph );) the resulting Model does not have the same security requirements that the standard secured model. For example When creating a list on a secured model calling model.createList( RDFNode[] );, the standard secured model verifies that the user has the right to update the triples and allows or denies the entire operation accordingly. The wrapped secured graph does not have visibility to the createList() command and can only operate on the instructions issued by the model.createList() implementation. In the standard implementation the model requests the graph to delete one triple and then insert another. Thus the user must have delete and add permissions, not the update permission.
There are several other cases where the difference in the layer can trip up the security system. In all known cases the result is a tighter security definition than was requested. For simplicity sake we recommend that the wrapped secured graph only be used in cases where access to the graph as a whole is granted/denied. In these cases the user either has all CRUD capabilities or none.

  
  
  
    On this page
    
  
    Documentation
    Overview
    Usage Notes\n\n\n\nOverview
Query Builder provides implementations of Ask, Construct, Select and Update builders that allow developers to create queries without resorting to StringBuilders or similar solutions.  The Query Builder module is an extra package and is found in the jena-querybuilder jar.
Each of the builders has a series of methods to define the query.  Each method returns the builder for easy chaining.  The  example:
SelectBuilder sb = new SelectBuilder()
    .addVar( "*" )
    .addWhere( "?s", "?p", "?o" );

Query q = sb.build() ;
produces
SELECT *
WHERE
  { ?s ?p ?o }
Standard Java variables can be used in the various clauses as long as the datatype has a registered Datatype within Jena.  For example:
Integer five = Integer.valueof(5);
SelectBuilder sb = new SelectBuilder()
    .addVar( "*" )
    .addWhere( "?s", "?p", five );

Query q = sb.build() ;
produces
SELECT *
WHERE
  { ?s ?p "5"^^<http://www.w3.org/2001/XMLSchema#integer> }
Java Collections are properly expanded to RDF collections within the query builder provided there is a registered Datatype for the elements.  Nested collections are expanded. Collections can also be defined with the standard SPARQL shorthand.  So the following produce equivalent queries:
SelectBuilder sb = new SelectBuilder()
    .addVar( "*" )
    .addWhere( "?s", "?p", List.of( "a", "b", "c") );

Query q = sb.build() ;
and
SelectBuilder sb = new SelectBuilder()
    .addVar( "*" )
    .addWhere( "?s", "?p", "('a' 'b' 'c')" );

Query q = sb.build() ;
It is common to create Var objects and use them in complex queries to make the query more readable.  For example:
Var node = Var.alloc("node");
Var x = Var.alloc("x");
Var y = Var.alloc("y");
SelectBuilder sb = new SelectBuilder()
  .addVar(x).addVar(y)
  .addWhere(node, RDF.type, Namespace.Obst)
  .addWhere(node, Namespace.x, x)
  .addWhere(node, Namespace.y, y);
Constructing Expressions
Expressions are primarily used in filter and bind statements as well as in select clauses.  All the standard expressions are implemented in the ExprFactory class.  An ExprFactory can be retrieved from any Builder by calling the getExprFactory() method.  This will create a Factory that has the same prefix mappings and the query.  An alternative is to construct the ExprFactory directly, this factory will not have the prefixes defined in PrefixMapping.Extended.
SelectBuilder builder = new SelectBuilder();
ExprFactory exprF = builder.getExprFactory()
    .addPrefix( "cf",
        "http://vocab.nerc.ac.uk/collection/P07/current/CFSN0023/");
builder.addVar( exprF.floor( ?v ), ?floor )
    .addWhere( ?s, "cf:air_temperature", ?v );
Update Builder
The UpdateBuilder is used to create Update, UpdateDeleteWhere or UpdateRequest objects.  When an UpdateRequest is built is contains a single Update object as defined by the UpdateBuilder.  Update objects can  be added to an UpdateRequest using the appendTo() method.
Var subj = Var.alloc( "s" );
Var obj = Var.alloc( "o" );

UpdateBuilder builder = new UpdateBuilder( PrefixMapping.Standard)
    .addInsert( subj, "rdfs:comment", obj )
    .addWhere( subj, "dc:title", obj);

UpdateRequest req = builder.buildRequest();

UpdateBuilder builder2 = new UpdateBuilder()
    .addPrefix( "dc", "http://purl.org/dc/elements/1.1/")
    .addDelete( subj, "?p", obj)
    .where( subj, dc:creator, "me")
    .appendTo( req );
Where Builder
In some use cases it is desirable to create a where clause without constructing an entire query.  The WhereBuilder is designed to fit this need.  For example to construct the query:
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX foaf: <http://xmlns.com/foaf/0.1/>

SELECT ?page ?type WHERE
{
    ?s foaf:page ?page .
    { ?s rdfs:label "Microsoft"@en . BIND ("A" as ?type) }
    UNION
    { ?s rdfs:label "Apple"@en . BIND ("B" as ?type) }
}
You could use a WhereBuilder to construct the union queries and add them to a Select or other query builder.
WhereBuilder whereBuilder = new WhereBuilder()
    .addPrefix( "rdfs",  "http://www.w3.org/2000/01/rdf-schema#" )
    addWhere( "?s", "rdfs:label", "'Microsoft'@en" )
    .addBind( "'A'", "?type")
    .addUnion( new WhereBuilder()
        .addPrefix( "rdfs",  "http://www.w3.org/2000/01/rdf-schema#" )
        .addWhere( "?s", "rdfs:label", "'Apple'@en" )
        .addBind( "'B'", "?type")
    );

SelectBuilder builder = new SelectBuilder()
   .addPrefix( "rdfs",  "http://www.w3.org/2000/01/rdf-schema#" )
   .addPrefix( "foaf", "http://xmlns.com/foaf/0.1/" );
   .addVar( "?page")
   .addVar( "?type" )
   .addWhere( "?s", "foaf:page",  "?page" )
   .addWhere( whereBuilder );
The where clauses could be built inline as:
SelectBuilder builder = new SelectBuilder()
  .addPrefixs( PrefixMapping.Standard )
  .addPrefix( "foaf", "http://xmlns.com/foaf/0.1/" );
  .addVar( "?page")
  .addVar( "?type" )
  .addWhere( "?s", "foaf:page",  "?page" )
  .addWhere( new WhereBuilder()
      .addPrefix( "rdfs",  "http://www.w3.org/2000/01/rdf-schema#" )
      .addWhere( "?s", "rdfs:label", "'Microsoft'@en" )
      .addBind( "'A'", "?type")
      .addUnion( new WhereBuilder()
          .addPrefix( "rdfs",  "http://www.w3.org/2000/01/rdf-schema#" )
          .addWhere( "?s", "rdfs:label", "'Apple'@en" )
          .addBind( "'B'", "?type")
      )
  );
Template Usage
In addition to making it easier to build valid queries the QueryBuilder has a clone method.
Using this a developer can create as “Template” query and add to it as necessary.
For example using the above query as the “template” with this code:
SelectBuilder sb2 = sb.clone();
sb2.addPrefix( "foaf", "http://xmlns.com/foaf/0.1/" )
   .addWhere( ?s, RDF.type, "foaf:Person") ;
produces
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT *
WHERE
  { ?s ?p ?o .
    ?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> foaf:person .
  }
Prepared Statement Usage
The query builders have the ability to replace variables with other values.  This can be
SelectBuilder sb = new SelectBuilder()
    .addVar( "*" )
    .addWhere( "?s", "?p", "?o" );

sb.setVar( Var.alloc( "?o" ), NodeFactory.createURI( "http://xmlns.com/foaf/0.1/Person" ) ) ;
Query q = sb.build();
produces
SELECT *
WHERE
  { ?s ?p <http://xmlns.com/foaf/0.1/Person> }\n\nOn this page
    
  
    Overview
  

  
    Update Builder
    Where Builder
    Template Usage
    Prepared Statement Usage
  

  
  
    Overview
Query Builder provides implementations of Ask, Construct, Select and Update builders that allow developers to create queries without resorting to StringBuilders or similar solutions.  The Query Builder module is an extra package and is found in the jena-querybuilder jar.
Each of the builders has a series of methods to define the query.  Each method returns the builder for easy chaining.  The  example:
SelectBuilder sb = new SelectBuilder()
    .addVar( "*" )
    .addWhere( "?s", "?p", "?o" );

Query q = sb.build() ;
produces
SELECT *
WHERE
  { ?s ?p ?o }
Standard Java variables can be used in the various clauses as long as the datatype has a registered Datatype within Jena.  For example:
Integer five = Integer.valueof(5);
SelectBuilder sb = new SelectBuilder()
    .addVar( "*" )
    .addWhere( "?s", "?p", five );

Query q = sb.build() ;
produces
SELECT *
WHERE
  { ?s ?p "5"^^<http://www.w3.org/2001/XMLSchema#integer> }
Java Collections are properly expanded to RDF collections within the query builder provided there is a registered Datatype for the elements.  Nested collections are expanded. Collections can also be defined with the standard SPARQL shorthand.  So the following produce equivalent queries:
SelectBuilder sb = new SelectBuilder()
    .addVar( "*" )
    .addWhere( "?s", "?p", List.of( "a", "b", "c") );

Query q = sb.build() ;
and
SelectBuilder sb = new SelectBuilder()
    .addVar( "*" )
    .addWhere( "?s", "?p", "('a' 'b' 'c')" );

Query q = sb.build() ;
It is common to create Var objects and use them in complex queries to make the query more readable.  For example:
Var node = Var.alloc("node");
Var x = Var.alloc("x");
Var y = Var.alloc("y");
SelectBuilder sb = new SelectBuilder()
  .addVar(x).addVar(y)
  .addWhere(node, RDF.type, Namespace.Obst)
  .addWhere(node, Namespace.x, x)
  .addWhere(node, Namespace.y, y);
Constructing Expressions
Expressions are primarily used in filter and bind statements as well as in select clauses.  All the standard expressions are implemented in the ExprFactory class.  An ExprFactory can be retrieved from any Builder by calling the getExprFactory() method.  This will create a Factory that has the same prefix mappings and the query.  An alternative is to construct the ExprFactory directly, this factory will not have the prefixes defined in PrefixMapping.Extended.
SelectBuilder builder = new SelectBuilder();
ExprFactory exprF = builder.getExprFactory()
    .addPrefix( "cf",
        "http://vocab.nerc.ac.uk/collection/P07/current/CFSN0023/");
builder.addVar( exprF.floor( ?v ), ?floor )
    .addWhere( ?s, "cf:air_temperature", ?v );
Update Builder
The UpdateBuilder is used to create Update, UpdateDeleteWhere or UpdateRequest objects.  When an UpdateRequest is built is contains a single Update object as defined by the UpdateBuilder.  Update objects can  be added to an UpdateRequest using the appendTo() method.
Var subj = Var.alloc( "s" );
Var obj = Var.alloc( "o" );

UpdateBuilder builder = new UpdateBuilder( PrefixMapping.Standard)
    .addInsert( subj, "rdfs:comment", obj )
    .addWhere( subj, "dc:title", obj);

UpdateRequest req = builder.buildRequest();

UpdateBuilder builder2 = new UpdateBuilder()
    .addPrefix( "dc", "http://purl.org/dc/elements/1.1/")
    .addDelete( subj, "?p", obj)
    .where( subj, dc:creator, "me")
    .appendTo( req );
Where Builder
In some use cases it is desirable to create a where clause without constructing an entire query.  The WhereBuilder is designed to fit this need.  For example to construct the query:
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX foaf: <http://xmlns.com/foaf/0.1/>

SELECT ?page ?type WHERE
{
    ?s foaf:page ?page .
    { ?s rdfs:label "Microsoft"@en . BIND ("A" as ?type) }
    UNION
    { ?s rdfs:label "Apple"@en . BIND ("B" as ?type) }
}
You could use a WhereBuilder to construct the union queries and add them to a Select or other query builder.
WhereBuilder whereBuilder = new WhereBuilder()
    .addPrefix( "rdfs",  "http://www.w3.org/2000/01/rdf-schema#" )
    addWhere( "?s", "rdfs:label", "'Microsoft'@en" )
    .addBind( "'A'", "?type")
    .addUnion( new WhereBuilder()
        .addPrefix( "rdfs",  "http://www.w3.org/2000/01/rdf-schema#" )
        .addWhere( "?s", "rdfs:label", "'Apple'@en" )
        .addBind( "'B'", "?type")
    );

SelectBuilder builder = new SelectBuilder()
   .addPrefix( "rdfs",  "http://www.w3.org/2000/01/rdf-schema#" )
   .addPrefix( "foaf", "http://xmlns.com/foaf/0.1/" );
   .addVar( "?page")
   .addVar( "?type" )
   .addWhere( "?s", "foaf:page",  "?page" )
   .addWhere( whereBuilder );
The where clauses could be built inline as:
SelectBuilder builder = new SelectBuilder()
  .addPrefixs( PrefixMapping.Standard )
  .addPrefix( "foaf", "http://xmlns.com/foaf/0.1/" );
  .addVar( "?page")
  .addVar( "?type" )
  .addWhere( "?s", "foaf:page",  "?page" )
  .addWhere( new WhereBuilder()
      .addPrefix( "rdfs",  "http://www.w3.org/2000/01/rdf-schema#" )
      .addWhere( "?s", "rdfs:label", "'Microsoft'@en" )
      .addBind( "'A'", "?type")
      .addUnion( new WhereBuilder()
          .addPrefix( "rdfs",  "http://www.w3.org/2000/01/rdf-schema#" )
          .addWhere( "?s", "rdfs:label", "'Apple'@en" )
          .addBind( "'B'", "?type")
      )
  );
Template Usage
In addition to making it easier to build valid queries the QueryBuilder has a clone method.
Using this a developer can create as “Template” query and add to it as necessary.
For example using the above query as the “template” with this code:
SelectBuilder sb2 = sb.clone();
sb2.addPrefix( "foaf", "http://xmlns.com/foaf/0.1/" )
   .addWhere( ?s, RDF.type, "foaf:Person") ;
produces
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT *
WHERE
  { ?s ?p ?o .
    ?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> foaf:person .
  }
Prepared Statement Usage
The query builders have the ability to replace variables with other values.  This can be
SelectBuilder sb = new SelectBuilder()
    .addVar( "*" )
    .addWhere( "?s", "?p", "?o" );

sb.setVar( Var.alloc( "?o" ), NodeFactory.createURI( "http://xmlns.com/foaf/0.1/Person" ) ) ;
Query q = sb.build();
produces
SELECT *
WHERE
  { ?s ?p <http://xmlns.com/foaf/0.1/Person> }

  
  
  
    On this page
    
  
    Overview
  

  
    Update Builder
    Where Builder
    Template Usage
    Prepared Statement Usage\n\n\n\nThis section provides some basic reference notes on the core Jena RDF API.
For a more tutorial introduction, please see the tutorials.
Core concepts
Graphs, models
In Jena, all state information provided by a collection of RDF triples is
contained in a data structure called a Model. The model denotes an
RDF graph, so called because it contains a collection of RDF nodes,
attached to each other by labelled relations. Each relationship goes
only in one direction, so the triple:
example:ijd foaf:name "Ian"
can be read as ‘resource example:ijd has property foaf:name with value "Ian"’.
Clearly the reverse is not true. Mathematically, this makes the model an instance of a
directed graph.
In Java terms, we use the class Model as the primary container of RDF information
contained in graph form. Model is designed to have a rich API, with many methods
intended to make it easier to write RDF-based programs and applications. One of
Model’s other roles is to provide an abstraction over different ways of storing
the RDF nodes and relations: in-memory data structures, disk-based persistent stores
and inference engines, for example, all provide Model as a core API.
While this common abstraction is appealing to API users, it is less convenient when trying
to create a new abstraction over a different storage medium. For example, suppose we
wanted to present an RDF triples view of an LDAP store by wrapping it as a Jena Model.
Internally, Jena uses a much simpler abstraction, Graph as the common interface to
low-level RDF stores. Graph has a much simpler API, so is easier to re-implement
for different store substrates.
In summary there are three distinct concepts of RDF containers in Jena:

graph, a mathematical view of the directed relations between nodes in a connected structure
Model, a rich Java API with many convenience methods for Java application developers
Graph, a simpler Java API intended for extending Jena’s functionality.

As an application developer, you will mostly be concerned with Model.
Nodes: resources, literals and blank nodes
So if RDF information is contained in a graph of connected nodes, what do the nodes themselves
look like? There are two distinct types of nodes: URI references and literals. Essentially, these
denote, respectively, some resource about which we wish to make some assertions, and concrete data values that
appear in those assertions. In the example above, example:ijd is a resource, denoting a person,
and "Ian" denotes the value of a property of that resource (that property being first name, in this case).
The resource is denoted by a URI, shown in abbreviated form here (about which more below).
What is the nature of the relationship between the resource node in the graph (example:ijd) and
an actual person (the author of this document)? That turns out to be a surprisingly subtle and
complex matter, which we won’t dwell on here.
See this very good summary of the issues
by Jeni Tennison for a detailed analysis. Suffice to say here that resources - somehow - denote
the things we want to describe in an RDF model.
A resource represented as a URI denotes a named thing - it has an identity. We can use that identity
to refer to directly the resource, as we will see below. Another kind of node in the graph is a literal,
which just represents a data value such as the string "ten" or the number 10. Literals representing
values other than strings may have an attached datatype, which helps an RDF processor correctly
convert the string representation of the literal into the correct value in the computer. By default,
RDF assumes the datatypes used XSD are available, but in fact
any datatype URI may be used.
RDF allows one special case of resources, in which we don’t actually know the identity (i.e. the URI)
of the resource. Consider the sentence “I gave my friend five dollars”. We know from this claim
that I have friend, but we don’t know who that friend is. We also know a property of the friend -
namely that he or she is five dollars better off than before.  In RDF, we can model this situation by
using a special type of resource called an anonymous resource. In the RDF semantics, an anonymous
resource is represented as having an identity which is blank, so they are often referred to
as nodes in the graph with blank identities, or blank nodes, typically shortened to bNodes.
In Jena, the Java interface Resource represents both ordinary URI resources and bNodes (in the case
of a bNode, the getURI() method returns null, and the isAnon() method returns true).
The Java interface Literal represents literals. Since both resources and literals may appear
as nodes in a graph, the common interface RDFNode is a super-class of both Resource and Literal.
Triples
In an RDF graph, the relationships always connect one subject resource to one other resource or
one literal. For example:
example:ijd foaf:firstName "Ian".
example:ijd foaf:knows example:mary.
The relationship, or predicate, always connects two nodes (formally, it has arity two). The first
argument of the predicate is node we are linking from, and the second is the node we are linking
to. We will often refer to these as the subject and object of the RDF statement, respectively.
The pattern subject-predicate-object is sufficiently commonplace that we will sometimes use the
abbreviation SPO. More commonly, we refer to a statement of one subject, predicate and object as a triple,
leading naturally to the term triplestore to refer to a means of storing RDF information.
In Jena, the Java class used to represent a single triple is Statement. According to the RDF
specification, only resources can be the subject of an RDF triple, whereas the object can be a
resource or a literal. The key methods for extracting the elements of a Statement are then:

getSubject() returning a Resource
getObject() returning an RDFNode
getPredicate() returning a Property (see below for more on Properties)

The predicate of a triple corresponds to the label on an edge in the RDF graph. So in the figure
below, the two representations are equivalent:

Technically, an RDF graph corresponds to a set of RDF triples. This means that an RDF resource
can only be the subject of at most one triple with the same predicate and object (because sets do
not contain any duplicates).
Properties
As mentioned above, the connection between two resources or a resource and a literal in an RDF graph
is labelled with the identity of the property. Just as RDF itself uses URI’s as names for resources,
minimising the chances of accidental name collisions, so too are properties identified with URI’s. In fact,
RDF Properties are just a special case of RDF Resources. Properties are denoted in Jena by the Property
object, which is a Java sub-class of Resource (itself a Java sub-class of RDFNode).
One difference between properties and resources in general is that RDF does not permit anonymous
properties, so you can’t use a bNode in place of a Property in the graph.
Namespaces
Suppose two companies, Acme Inc, and Emca Inc, decide to encode their product catalogues in RDF. A key
piece of information to include in the graph is the price of the product, so both decide to use a price
predicate to denote the relationship between a product and its current price. However, Acme wants the
price to include applicable sales taxes, whereas Emca wants to exclude them. So the notion of price
is slightly different in each case. However, using the name ‘price’ on its own risks losing this
distinction.
Fortunately, RDF specifies that a property is identified by a URI, and ‘price’ on its own is not a URI.
A logical solution is for both Acme and Emca to use their own web spaces to provide different
base URIs on which to construct the URI for the property:
http://acme.example/schema/products#price
http://emca.example/ontology/catalogue/price
These are clearly now two distinct identities, and so each company can define the semantics of the
price property without interfering with the other. Writing out such long strings each time, however,
can be unwieldy and a source of error. A compact URI or curie
is an abbreviated form in which a namespace and name are separated by a colon character:
acme-product:price
emca-catalogue:price
where acme-product is defined to be http://acme.example/schema/products#. This can be defined,
for example, in Turtle:
PREFIX acme-product: <http://acme.example/schema/products#>

acme-product:widget acme-product:price "44.99"^^xsd:decimal.
The datatype xsd:decimal is another example of an abbreviated URI. Note that no PREFIX rules
are defined by RDF or Turtle: authors of RDF content should ensure that all prefixes used in curies
are defined before use.
Note
Jena does not treat namespaces in a special way. A Model will remember any prefixes defined
in the input RDF (see the PrefixMapping
interface; all Jena Model objects extend PrefixMapping), and the output writers which
serialize a model to XML or Turtle will normally attempt to use prefixes to abbreviate URI’s.
However internally, a Resource URI is not separated into a namespace and local-name pair.
The method getLocalName() on Resource will attempt to calculate what a reasonable local
name might have been, but it may not always recover the pairing that was used in the
input document.
can be used as the subject of statements about the properties
of that resource, as above, but also as the value of a statement. For example, the property
is-a-friend-of might typically connect two resources denoting people
Jena packages
As a guide to the various features of Jena, here’s a description of the main Java packages.
For brevity, we shorten org.apache.jena to oaj.

  
      
          Package
          Description
          More information
      
  
  
      
          oaj.jena.rdf.model
          The Jena core. Creating and manipulating RDF graphs.
          
      
      
          oaj.riot
          Reading and Writing RDF.
          
      
      
          oaj.jena.datatypes
          Provides the core interfaces through which datatypes are described to Jena.
          Typed literals
      
      
          oaj.jena.ontology
          Abstractions and convenience classes for accessing and manipulating ontologies represented in RDF.
          Ontology API
      
      
          oaj.jena.rdf.listeners
          Listening for changes to the statements in a model
          
      
      
          oaj.jena.reasoner
          The reasoner subsystem is supports a range of inference engines which derive additional information from an RDF model
          Reasoner how-to
      
      
          oaj.jena.shared
          Common utility classes
          
      
      
          oaj.jena.vocabulary
          A package containing constant classes with predefined constant objects for classes and properties defined in well known vocabularies.
          
      
      
          oaj.jena.xmloutput
          Writing RDF/XML.
          I/O index\n\nOn this page
    
  
    Core concepts
      
        Graphs, models
        Nodes: resources, literals and blank nodes
        Triples
        Properties
        Namespaces
      
    
    Jena packages
  

  
  
    This section provides some basic reference notes on the core Jena RDF API.
For a more tutorial introduction, please see the tutorials.
Core concepts
Graphs, models
In Jena, all state information provided by a collection of RDF triples is
contained in a data structure called a Model. The model denotes an
RDF graph, so called because it contains a collection of RDF nodes,
attached to each other by labelled relations. Each relationship goes
only in one direction, so the triple:
example:ijd foaf:name "Ian"
can be read as ‘resource example:ijd has property foaf:name with value "Ian"’.
Clearly the reverse is not true. Mathematically, this makes the model an instance of a
directed graph.
In Java terms, we use the class Model as the primary container of RDF information
contained in graph form. Model is designed to have a rich API, with many methods
intended to make it easier to write RDF-based programs and applications. One of
Model’s other roles is to provide an abstraction over different ways of storing
the RDF nodes and relations: in-memory data structures, disk-based persistent stores
and inference engines, for example, all provide Model as a core API.
While this common abstraction is appealing to API users, it is less convenient when trying
to create a new abstraction over a different storage medium. For example, suppose we
wanted to present an RDF triples view of an LDAP store by wrapping it as a Jena Model.
Internally, Jena uses a much simpler abstraction, Graph as the common interface to
low-level RDF stores. Graph has a much simpler API, so is easier to re-implement
for different store substrates.
In summary there are three distinct concepts of RDF containers in Jena:

graph, a mathematical view of the directed relations between nodes in a connected structure
Model, a rich Java API with many convenience methods for Java application developers
Graph, a simpler Java API intended for extending Jena’s functionality.

As an application developer, you will mostly be concerned with Model.
Nodes: resources, literals and blank nodes
So if RDF information is contained in a graph of connected nodes, what do the nodes themselves
look like? There are two distinct types of nodes: URI references and literals. Essentially, these
denote, respectively, some resource about which we wish to make some assertions, and concrete data values that
appear in those assertions. In the example above, example:ijd is a resource, denoting a person,
and "Ian" denotes the value of a property of that resource (that property being first name, in this case).
The resource is denoted by a URI, shown in abbreviated form here (about which more below).
What is the nature of the relationship between the resource node in the graph (example:ijd) and
an actual person (the author of this document)? That turns out to be a surprisingly subtle and
complex matter, which we won’t dwell on here.
See this very good summary of the issues
by Jeni Tennison for a detailed analysis. Suffice to say here that resources - somehow - denote
the things we want to describe in an RDF model.
A resource represented as a URI denotes a named thing - it has an identity. We can use that identity
to refer to directly the resource, as we will see below. Another kind of node in the graph is a literal,
which just represents a data value such as the string "ten" or the number 10. Literals representing
values other than strings may have an attached datatype, which helps an RDF processor correctly
convert the string representation of the literal into the correct value in the computer. By default,
RDF assumes the datatypes used XSD are available, but in fact
any datatype URI may be used.
RDF allows one special case of resources, in which we don’t actually know the identity (i.e. the URI)
of the resource. Consider the sentence “I gave my friend five dollars”. We know from this claim
that I have friend, but we don’t know who that friend is. We also know a property of the friend -
namely that he or she is five dollars better off than before.  In RDF, we can model this situation by
using a special type of resource called an anonymous resource. In the RDF semantics, an anonymous
resource is represented as having an identity which is blank, so they are often referred to
as nodes in the graph with blank identities, or blank nodes, typically shortened to bNodes.
In Jena, the Java interface Resource represents both ordinary URI resources and bNodes (in the case
of a bNode, the getURI() method returns null, and the isAnon() method returns true).
The Java interface Literal represents literals. Since both resources and literals may appear
as nodes in a graph, the common interface RDFNode is a super-class of both Resource and Literal.
Triples
In an RDF graph, the relationships always connect one subject resource to one other resource or
one literal. For example:
example:ijd foaf:firstName "Ian".
example:ijd foaf:knows example:mary.
The relationship, or predicate, always connects two nodes (formally, it has arity two). The first
argument of the predicate is node we are linking from, and the second is the node we are linking
to. We will often refer to these as the subject and object of the RDF statement, respectively.
The pattern subject-predicate-object is sufficiently commonplace that we will sometimes use the
abbreviation SPO. More commonly, we refer to a statement of one subject, predicate and object as a triple,
leading naturally to the term triplestore to refer to a means of storing RDF information.
In Jena, the Java class used to represent a single triple is Statement. According to the RDF
specification, only resources can be the subject of an RDF triple, whereas the object can be a
resource or a literal. The key methods for extracting the elements of a Statement are then:

getSubject() returning a Resource
getObject() returning an RDFNode
getPredicate() returning a Property (see below for more on Properties)

The predicate of a triple corresponds to the label on an edge in the RDF graph. So in the figure
below, the two representations are equivalent:

Technically, an RDF graph corresponds to a set of RDF triples. This means that an RDF resource
can only be the subject of at most one triple with the same predicate and object (because sets do
not contain any duplicates).
Properties
As mentioned above, the connection between two resources or a resource and a literal in an RDF graph
is labelled with the identity of the property. Just as RDF itself uses URI’s as names for resources,
minimising the chances of accidental name collisions, so too are properties identified with URI’s. In fact,
RDF Properties are just a special case of RDF Resources. Properties are denoted in Jena by the Property
object, which is a Java sub-class of Resource (itself a Java sub-class of RDFNode).
One difference between properties and resources in general is that RDF does not permit anonymous
properties, so you can’t use a bNode in place of a Property in the graph.
Namespaces
Suppose two companies, Acme Inc, and Emca Inc, decide to encode their product catalogues in RDF. A key
piece of information to include in the graph is the price of the product, so both decide to use a price
predicate to denote the relationship between a product and its current price. However, Acme wants the
price to include applicable sales taxes, whereas Emca wants to exclude them. So the notion of price
is slightly different in each case. However, using the name ‘price’ on its own risks losing this
distinction.
Fortunately, RDF specifies that a property is identified by a URI, and ‘price’ on its own is not a URI.
A logical solution is for both Acme and Emca to use their own web spaces to provide different
base URIs on which to construct the URI for the property:
http://acme.example/schema/products#price
http://emca.example/ontology/catalogue/price
These are clearly now two distinct identities, and so each company can define the semantics of the
price property without interfering with the other. Writing out such long strings each time, however,
can be unwieldy and a source of error. A compact URI or curie
is an abbreviated form in which a namespace and name are separated by a colon character:
acme-product:price
emca-catalogue:price
where acme-product is defined to be http://acme.example/schema/products#. This can be defined,
for example, in Turtle:
PREFIX acme-product: <http://acme.example/schema/products#>

acme-product:widget acme-product:price "44.99"^^xsd:decimal.
The datatype xsd:decimal is another example of an abbreviated URI. Note that no PREFIX rules
are defined by RDF or Turtle: authors of RDF content should ensure that all prefixes used in curies
are defined before use.
Note
Jena does not treat namespaces in a special way. A Model will remember any prefixes defined
in the input RDF (see the PrefixMapping
interface; all Jena Model objects extend PrefixMapping), and the output writers which
serialize a model to XML or Turtle will normally attempt to use prefixes to abbreviate URI’s.
However internally, a Resource URI is not separated into a namespace and local-name pair.
The method getLocalName() on Resource will attempt to calculate what a reasonable local
name might have been, but it may not always recover the pairing that was used in the
input document.
can be used as the subject of statements about the properties
of that resource, as above, but also as the value of a statement. For example, the property
is-a-friend-of might typically connect two resources denoting people
Jena packages
As a guide to the various features of Jena, here’s a description of the main Java packages.
For brevity, we shorten org.apache.jena to oaj.

  
      
          Package
          Description
          More information
      
  
  
      
          oaj.jena.rdf.model
          The Jena core. Creating and manipulating RDF graphs.
          
      
      
          oaj.riot
          Reading and Writing RDF.
          
      
      
          oaj.jena.datatypes
          Provides the core interfaces through which datatypes are described to Jena.
          Typed literals
      
      
          oaj.jena.ontology
          Abstractions and convenience classes for accessing and manipulating ontologies represented in RDF.
          Ontology API
      
      
          oaj.jena.rdf.listeners
          Listening for changes to the statements in a model
          
      
      
          oaj.jena.reasoner
          The reasoner subsystem is supports a range of inference engines which derive additional information from an RDF model
          Reasoner how-to
      
      
          oaj.jena.shared
          Common utility classes
          
      
      
          oaj.jena.vocabulary
          A package containing constant classes with predefined constant objects for classes and properties defined in well known vocabularies.
          
      
      
          oaj.jena.xmloutput
          Writing RDF/XML.
          I/O index
      
  


  
  
  
    On this page
    
  
    Core concepts
      
        Graphs, models
        Nodes: resources, literals and blank nodes
        Triples
        Properties
        Namespaces
      
    
    Jena packages\n\n\n\nRDFConnection provides a unified set of operations for working on RDF
with SPARQL operations. It provides SPARQL Query,
SPARQL Update and the
SPARQL Graph Store operations.
The interface is uniform - the same interface applies to local data and to remote
data using HTTP and the SPARQL protocols (SPARQL protocol)
and SPARQL Graph Store Protocol).
Outline
RDFConnection provides a number of different styles for working with RDF
data in Java.  It provides support for try-resource and functional code
passing styles, as well the more basic sequence of methods calls.
For example: using try-resources to manage the connection, and perform two operations, one to load
some data, and one to make a query can be written as:
try ( RDFConnection conn = RDFConnection.connect(...) ) {
  conn.load("data.ttl") ;
  conn.querySelect("SELECT DISTINCT ?s { ?s ?p ?o }", (qs) -> {
     Resource subject = qs.getResource("s") ;
     System.out.println("Subject: " + subject) ;
  }) ;
}
This could have been written as (approximately – the error handling is better
in the example above):
RDFConnection conn = RDFConnection.connect(...)
conn.load("data.ttl") ;
QueryExecution qExec = conn.query("SELECT DISTINCT ?s { ?s ?p ?o }") ;
ResultSet rs = qExec.execSelect() ;
while(rs.hasNext()) {
  QuerySolution qs = rs.next() ;
  Resource subject = qs.getResource("s") ;
  System.out.println("Subject: " + subject) ;
}
qExec.close() ;
conn.close() ;
Transactions
Transactions are the preferred way to work with RDF data.
Operations on an RDFConnection outside of an application-controlled
transaction will cause the system to add one for the duration of the
operation. This “autocommit” feature may lead to inefficient operations due
to excessive overhead.
The Txn class provides a Java8-style transaction API.  Transactions are
code passed in the Txn library that handles the transaction lifecycle.
try ( RDFConnection conn = RDFConnection.connect(...) ) {
  Txn.execWrite(conn, () -> {
      conn.load("data1.ttl") ;
      conn.load("data2.ttl") ;
      conn.querySelect("SELECT DISTINCT ?s { ?s ?p ?o }", (qs) ->
         Resource subject = qs.getResource("s") ;
         System.out.println("Subject: " + subject) ;
      }) ;
  }) ;
}
The traditional style of explicit begin, commit, abort is also available.
try ( RDFConnection conn = RDFConnection.connect(...) ) {
    conn.begin(ReadWrite.WRITE) ;
    try {
        conn.load("data1.ttl") ;
        conn.load("data2.ttl") ;
        conn.querySelect("SELECT DISTINCT ?s { ?s ?p ?o }", (qs) -> {
           Resource subject = qs.getResource("s") ;
           System.out.println("Subject: " + subject) ;
        }) ;
        conn.commit() ;
    } finally { conn.end() ; }
}
The use of try-finally ensures that transactions are properly finished.
The conn.end() provides an abort in case an exception occurs in the
transaction and a commit has not been issued.  The use of try-finally
ensures that transactions are properly finished.
Txn is wrapping these steps up and calling the application supplied code
for the transaction body.
Remote Transactions
SPARQL does not define a remote transaction standard protocol. Each remote
operation should be atomic (all happens or nothing happens) - this is the
responsibility of the remote server.
An RDFConnection will at least provide the client-side locking features.
This means that overlapping operations that change data are naturally
handled by the transaction pattern within a single JVM.
Configuring a remote RDFConnection.
The default settings on a remote connection should work for any SPARQL
triple store endpoint which supports HTTP content negotiation. Sometimes
different settings are desirable or required and RDFConnectionRemote provides a
builder to construct RDFConnectionRemotes.
At its simplest, it is:
RDFConnectionRemoteBuilder builder = RDFConnection.create()
          .destination("http://host/triplestore");
which uses default settings used by RDFConenctionFactory.connect.
See example
4
and example
5.
There are many options, including setting HTTP headers for content types
(javadoc)
and providing detailed configuration with
Apache HttpComponents HttpClient.
Fuseki Specific Connection
If the remote destination is an Apache Jena Fuseki server, then the
default general settings work, but it is possible to have a specialised connection
RDFConnectionRemoteBuilder builder = RDFConnectionFuseki.create()
     .destination("http://host/fuseki");
which uses settings tuned to Fuseki, including round-trip handling of
blank nodes.
See example
6.
Graph Store Protocol
The SPARQL Graph Store Protocol
(GSP) is a set of operations to work on whole graphs in a
dataset.  It provides a standardised way to manage the data in a dataset.
The operations are to fetch a graph, set the RDF data in a graph,
add more RDF data into a graph, and delete a graph from a dataset.
For example: load two files:
try ( RDFConnection conn = RDFConnection.connect(...) ) {
    conn.load("data1.ttl") ;
    conn.load("data2.nt") ;
}
The file extension is used to determine the syntax.
There is also a set of scripts to help do these operations from the command
line with SOH.
It is possible to write curl scripts as well.  The SPARQL Graph
Store Protocol provides a standardised way to manage the data in a dataset.
In addition, RDFConnection provides an extension to give the same style
of operation to work on a whole dataset (deleting the dataset is not
provided).
conn.loadDataset("data-complete.trig") ;
Local vs Remote
GSP operations work on whole models and datasets. When used on a remote connection,
the result of a GSP operation is a separate copy of the remote RDF data.  When working
with local connections, 3 isolation modes are available:

Copy – the models and datasets returned are independent copies.
Updates are made to the return copy only. This is most like
a remote connection and is useful for testing.
Read-only – the models and datasets are made read-only but any changes
to the underlying RDF data by changes by another route will be visible.
This provides a form of checking for large datasets when “copy” is impractical.
None – the models and datasets are passed back with no additional wrappers,
and they can be updated with the changes being made the underlying dataset.

The default for a local RDFConnection is “none”. When used with TDB,
accessing returned models must be done with transactions
in this mode.
Query Usage
RDFConnection provides methods for each of the SPARQL query forms (SELECT,
CONSTRUCT, DESCRIBE, ASK) as well as a way to get the
QueryExecution for specialized configuration. When creating an
QueryExecution explicitly, care should be taken to close it.
If the application wishes to capture the result set from a SELECT query and
retain it across the lifetime of the transaction or QueryExecution, then
the application should create a copy which is not attached to any external system
with ResultSetFactory.copyResults.
try ( RDFConnection conn = RDFConnection.connect("https://...") ) {
    ResultSet safeCopy =
        Txn.execReadReturn(conn, () -> {
            // Process results by row:
            conn.querySelect("SELECT DISTINCT ?s { ?s ?p ?o }", (qs) -> {
                Resource subject = qs.getResource("s") ;
                System.out.println("Subject: "+subject) ;
            }) ;
            ResultSet rs = conn.query("SELECT * { ?s ?p ?o }").execSelect() ;
            return ResultSetFactory.copyResults(rs) ;
        }) ;
}
Update Usage
SPARQL Update operations can be performed and mixed with other operations.
try ( RDFConnection conn = RDFConnection.connect(...) ) {
    Txn.execWrite(conn, () -> {
       conn.update("DELETE DATA { ... }" ) ;
       conn.load("data.ttl") ;
    }) ;
}
Dataset operations
In addition to the SPARQL Graph Store Protocol, operations on whole
datasets are provided for fetching (HTTP GET), adding data (HTTP POST) and
setting the data (HTTP PUT) on a dataset URL.  This assumes the remote
server supported these REST-style operations.  Apache Jena Fuseki does
provide these.
Subinterfaces
To help structure code, the RDFConnection consists of a number of
different interfaces.  An RDFConnection can be passed to application code
as one of these interfaces so that only certain subsets of the full
operations are visible to the called code.

query via SparqlQueryConnection
update via SparqlUpdateConnection
graph store protocol RDFDatasetAccessConnection (read operations),
and RDFDatasetConnection (read and write operations).

Examples

for simple usage examples see https://github.com/apache/jena/tree/main/jena-examples/src/main/java/rdfconnection/examples.
for example of how to use with StreamRDF see https://github.com/apache/jena/blob/main/jena-examples/src/main/java/org/apache/jena/example/streaming/StreamRDFToConnection.java.\n\nOn this page
    
  
    Outline
    Transactions
      
        Remote Transactions
      
    
    Configuring a remote RDFConnection.
      
        Fuseki Specific Connection
      
    
    Graph Store Protocol
      
        Local vs Remote
      
    
    Query Usage
    Update Usage
    Dataset operations
    Subinterfaces
    Examples
  

  
  
    RDFConnection provides a unified set of operations for working on RDF
with SPARQL operations. It provides SPARQL Query,
SPARQL Update and the
SPARQL Graph Store operations.
The interface is uniform - the same interface applies to local data and to remote
data using HTTP and the SPARQL protocols (SPARQL protocol)
and SPARQL Graph Store Protocol).
Outline
RDFConnection provides a number of different styles for working with RDF
data in Java.  It provides support for try-resource and functional code
passing styles, as well the more basic sequence of methods calls.
For example: using try-resources to manage the connection, and perform two operations, one to load
some data, and one to make a query can be written as:
try ( RDFConnection conn = RDFConnection.connect(...) ) {
  conn.load("data.ttl") ;
  conn.querySelect("SELECT DISTINCT ?s { ?s ?p ?o }", (qs) -> {
     Resource subject = qs.getResource("s") ;
     System.out.println("Subject: " + subject) ;
  }) ;
}
This could have been written as (approximately – the error handling is better
in the example above):
RDFConnection conn = RDFConnection.connect(...)
conn.load("data.ttl") ;
QueryExecution qExec = conn.query("SELECT DISTINCT ?s { ?s ?p ?o }") ;
ResultSet rs = qExec.execSelect() ;
while(rs.hasNext()) {
  QuerySolution qs = rs.next() ;
  Resource subject = qs.getResource("s") ;
  System.out.println("Subject: " + subject) ;
}
qExec.close() ;
conn.close() ;
Transactions
Transactions are the preferred way to work with RDF data.
Operations on an RDFConnection outside of an application-controlled
transaction will cause the system to add one for the duration of the
operation. This “autocommit” feature may lead to inefficient operations due
to excessive overhead.
The Txn class provides a Java8-style transaction API.  Transactions are
code passed in the Txn library that handles the transaction lifecycle.
try ( RDFConnection conn = RDFConnection.connect(...) ) {
  Txn.execWrite(conn, () -> {
      conn.load("data1.ttl") ;
      conn.load("data2.ttl") ;
      conn.querySelect("SELECT DISTINCT ?s { ?s ?p ?o }", (qs) ->
         Resource subject = qs.getResource("s") ;
         System.out.println("Subject: " + subject) ;
      }) ;
  }) ;
}
The traditional style of explicit begin, commit, abort is also available.
try ( RDFConnection conn = RDFConnection.connect(...) ) {
    conn.begin(ReadWrite.WRITE) ;
    try {
        conn.load("data1.ttl") ;
        conn.load("data2.ttl") ;
        conn.querySelect("SELECT DISTINCT ?s { ?s ?p ?o }", (qs) -> {
           Resource subject = qs.getResource("s") ;
           System.out.println("Subject: " + subject) ;
        }) ;
        conn.commit() ;
    } finally { conn.end() ; }
}
The use of try-finally ensures that transactions are properly finished.
The conn.end() provides an abort in case an exception occurs in the
transaction and a commit has not been issued.  The use of try-finally
ensures that transactions are properly finished.
Txn is wrapping these steps up and calling the application supplied code
for the transaction body.
Remote Transactions
SPARQL does not define a remote transaction standard protocol. Each remote
operation should be atomic (all happens or nothing happens) - this is the
responsibility of the remote server.
An RDFConnection will at least provide the client-side locking features.
This means that overlapping operations that change data are naturally
handled by the transaction pattern within a single JVM.
Configuring a remote RDFConnection.
The default settings on a remote connection should work for any SPARQL
triple store endpoint which supports HTTP content negotiation. Sometimes
different settings are desirable or required and RDFConnectionRemote provides a
builder to construct RDFConnectionRemotes.
At its simplest, it is:
RDFConnectionRemoteBuilder builder = RDFConnection.create()
          .destination("http://host/triplestore");
which uses default settings used by RDFConenctionFactory.connect.
See example
4
and example
5.
There are many options, including setting HTTP headers for content types
(javadoc)
and providing detailed configuration with
Apache HttpComponents HttpClient.
Fuseki Specific Connection
If the remote destination is an Apache Jena Fuseki server, then the
default general settings work, but it is possible to have a specialised connection
RDFConnectionRemoteBuilder builder = RDFConnectionFuseki.create()
     .destination("http://host/fuseki");
which uses settings tuned to Fuseki, including round-trip handling of
blank nodes.
See example
6.
Graph Store Protocol
The SPARQL Graph Store Protocol
(GSP) is a set of operations to work on whole graphs in a
dataset.  It provides a standardised way to manage the data in a dataset.
The operations are to fetch a graph, set the RDF data in a graph,
add more RDF data into a graph, and delete a graph from a dataset.
For example: load two files:
try ( RDFConnection conn = RDFConnection.connect(...) ) {
    conn.load("data1.ttl") ;
    conn.load("data2.nt") ;
}
The file extension is used to determine the syntax.
There is also a set of scripts to help do these operations from the command
line with SOH.
It is possible to write curl scripts as well.  The SPARQL Graph
Store Protocol provides a standardised way to manage the data in a dataset.
In addition, RDFConnection provides an extension to give the same style
of operation to work on a whole dataset (deleting the dataset is not
provided).
conn.loadDataset("data-complete.trig") ;
Local vs Remote
GSP operations work on whole models and datasets. When used on a remote connection,
the result of a GSP operation is a separate copy of the remote RDF data.  When working
with local connections, 3 isolation modes are available:

Copy – the models and datasets returned are independent copies.
Updates are made to the return copy only. This is most like
a remote connection and is useful for testing.
Read-only – the models and datasets are made read-only but any changes
to the underlying RDF data by changes by another route will be visible.
This provides a form of checking for large datasets when “copy” is impractical.
None – the models and datasets are passed back with no additional wrappers,
and they can be updated with the changes being made the underlying dataset.

The default for a local RDFConnection is “none”. When used with TDB,
accessing returned models must be done with transactions
in this mode.
Query Usage
RDFConnection provides methods for each of the SPARQL query forms (SELECT,
CONSTRUCT, DESCRIBE, ASK) as well as a way to get the
QueryExecution for specialized configuration. When creating an
QueryExecution explicitly, care should be taken to close it.
If the application wishes to capture the result set from a SELECT query and
retain it across the lifetime of the transaction or QueryExecution, then
the application should create a copy which is not attached to any external system
with ResultSetFactory.copyResults.
try ( RDFConnection conn = RDFConnection.connect("https://...") ) {
    ResultSet safeCopy =
        Txn.execReadReturn(conn, () -> {
            // Process results by row:
            conn.querySelect("SELECT DISTINCT ?s { ?s ?p ?o }", (qs) -> {
                Resource subject = qs.getResource("s") ;
                System.out.println("Subject: "+subject) ;
            }) ;
            ResultSet rs = conn.query("SELECT * { ?s ?p ?o }").execSelect() ;
            return ResultSetFactory.copyResults(rs) ;
        }) ;
}
Update Usage
SPARQL Update operations can be performed and mixed with other operations.
try ( RDFConnection conn = RDFConnection.connect(...) ) {
    Txn.execWrite(conn, () -> {
       conn.update("DELETE DATA { ... }" ) ;
       conn.load("data.ttl") ;
    }) ;
}
Dataset operations
In addition to the SPARQL Graph Store Protocol, operations on whole
datasets are provided for fetching (HTTP GET), adding data (HTTP POST) and
setting the data (HTTP PUT) on a dataset URL.  This assumes the remote
server supported these REST-style operations.  Apache Jena Fuseki does
provide these.
Subinterfaces
To help structure code, the RDFConnection consists of a number of
different interfaces.  An RDFConnection can be passed to application code
as one of these interfaces so that only certain subsets of the full
operations are visible to the called code.

query via SparqlQueryConnection
update via SparqlUpdateConnection
graph store protocol RDFDatasetAccessConnection (read operations),
and RDFDatasetConnection (read and write operations).

Examples

for simple usage examples see https://github.com/apache/jena/tree/main/jena-examples/src/main/java/rdfconnection/examples.
for example of how to use with StreamRDF see https://github.com/apache/jena/blob/main/jena-examples/src/main/java/org/apache/jena/example/streaming/StreamRDFToConnection.java.


  
  
  
    On this page
    
  
    Outline
    Transactions
      
        Remote Transactions
      
    
    Configuring a remote RDFConnection.
      
        Fuseki Specific Connection
      
    
    Graph Store Protocol
      
        Local vs Remote
      
    
    Query Usage
    Update Usage
    Dataset operations
    Subinterfaces
    Examples\n\n\n\n\n\njena-shacl is an implementation of the
W3C Shapes Constraint Language (SHACL).
It implements SHACL Core and SHACL SPARQL Constraints.
In addition, it provides:

SHACL Compact Syntax
SPARQL-based targets

Command line
The command shacl introduces shacl operations; it takes a sub-command
argument.
To validate:
shacl validate --shapes SHAPES.ttl --data DATA.ttl
shacl v -s SHAPES.ttl -d DATA.ttl
The shapes and data files can be the same; the --shapes is optional and
defaults to the same as --data.  This includes running individual W3C Working
Group tests.
To parse a file:
shacl parse FILE
shacl p FILE
which writes out a text format.
shacl p --out=FMT FILE
writes out in text(t), compact(c), rdf(r) formats. Multiple formats can be given,
separated by “,” and format all outputs all 3 formats.
Integration with Apache Jena Fuseki
Fuseki has a new service operation fuseki:shacl:
<#serviceWithShacl>; rdf:type fuseki:Service ;
    rdfs:label                   "Dataset with SHACL validation" ;
    fuseki:name                  "<i>ds</i>" ;
    fuseki:serviceReadWriteGraphStore "" ;
    fuseki:endpoint [ fuseki:operation fuseki:shacl ; fuseki:name "shacl" ] ;
    fuseki:dataset <#dataset> ;
    .
This requires a “new style” endpoint declaration:  see
“Fuseki Endpoint Configuration”.
This is not installed into a dataset setup by default; a configuration file using
fuseki:endpoint [ fuseki:operation fuseki:shacl ;
                  fuseki:name "shacl" ];
is necessary (or programmatic setup for Fuseki Main).
The service accepts a shapes graph posted as RDF to /ds/shacl with
content negotiation.
There is a graph argument, ?graph=, that specifies the graph to validate. It
is the URI of a named graph, default for the unnamed, default graph (and
this is the assumed value of ?graph if not present), or union for union of
all named graphs in the dataset.
Further, an argument target=uri validates a specific node in the data.
Upload data in file fu-data.ttl:
curl -XPOST --data-binary @fu-data.ttl    \  
     --header 'Content-type: text/turtle' \  
     'http://localhost:3030/ds?default'
Validate with shapes in fu-shapes.ttl and get back a validation report:
curl -XPOST --data-binary @fu-shapes.ttl  \  
     --header 'Content-type: text/turtle' \  
     'http://localhost:3030/ds/shacl?graph=default'
API
The package org.apache.jena.shacl has the main classes.

ShaclValidator for parsing and validation
GraphValidation for updating graphs with validation

API Examples
https://github.com/apache/jena/tree/main/jena-examples/src/main/java/shacl/examples/
Example
Shacl01_validateGraph
shows validation and printing of the validation report in a text form and in RDF:
public static void main(String ...args) {
    String SHAPES = "shapes.ttl";
    String DATA = "data1.ttl";

    Graph shapesGraph = RDFDataMgr.loadGraph(SHAPES);
    Graph dataGraph = RDFDataMgr.loadGraph(DATA);

    Shapes shapes = Shapes.parse(shapesGraph);

    ValidationReport report = ShaclValidator.get().validate(shapes, dataGraph);
    ShLib.printReport(report);
    System.out.println();
    RDFDataMgr.write(System.out, report.getModel(), Lang.TTL);
}
Example
Shacl02_validateTransaction
shows how to update a graph only if, after the changes, the graph is validated
according to the shapes provided.
SHACL Compact Syntax
Apache Jena supports
SHACL Compact Syntax (SHACL-C)
for both reading and writing.
The file extensions for SHACL-C are .shc and .shaclc and there is a registered language
constant Lang.SHACLC.
RDFDataMgr.load("shapes.shc");

RDFDataMgr.read("file:compactShapes", Lang.SHACLC);

RDFDataMgr.write(System.out, shapesGraph, Lang.SHACLC);
SHACL-C is managed by the SHACL Community Group. It does not cover all possible shapes.
When outputting SHACL-C, SHACL shapes not expressible in SHACL-C will cause an
exception and data in the RDF graph that is not relevant will not be output. In
other words, SHACL-C is a lossy format for RDF.
The Jena SHACL-C writer will output any valid SHACL-C document.
Extensions:

The constraint grammar rule allows a shape reference to a node shape.
The propertyParam grammar rule provides “group”, “order”, “name”,
“description” and “defaultValue” to align with nodeParam.
The nodeParam grammar rule supports “targetClass” (normally written
with the shorthand ->) as well as the defined
“targetNode”, “targetObjectsOf”, “targetSubjectsOf”

SPARQL-based targets
SPARQL-based targets allow the target nodes to be calculated with a SPARQL
SELECT query.
See SPARQL-based targets
for details.
ex:example
    sh:target [
        a sh:SPARQLTarget ;
        sh:select """
            SELECT ?this
            WHERE {
              ...
            }
            """ ;
    ] ;
ValidationListener
When given a ValidationListener the SHACL validation code emits events at each step of validation:

when validation of a shape starts or finishes
when the focus nodes of the shape have been identified
when validation of a constraint begins, ends and yields positive or negative results

For example, the following listener will just record all events in a List:
public class RecordingValidationListener implements ValidationListener {
        private final List<ValidationEvent> events = new ArrayList<>();

        @Override public void onValidationEvent(ValidationEvent e) {
            events.add(e);
        }

        public List<ValidationEvent> getEvents() {
            return events;
        }
    }
The listener must be passed to the constructor of the ValidationContext.
The following example validates the dataGraph according to the shapesGraph using the ValidationListener above:
Graph shapesGraph = RDFDataMgr.loadGraph(shapesGraphUri); //assuming shapesGraphUri points to an RDF file
Graph dataGraph = RDFDataMgr.loadGraph(dataGraphUri); //assuming dataGraphUri points to an RDF file
RecordingValidationListener listener = new RecordingValidationListener();  // see above
Shapes shapes = Shapes.parse(shapesGraph);
ValidationContext vCtx = ValidationContext.create(shapes, dataGraph, listener); // pass listener here
for (Shape shape : shapes.getTargetShapes()) {
    Collection<Node> focusNodes = VLib.focusNodes(dataGraph, shape);
    for (Node focusNode : focusNodes) {
        VLib.validateShape(vCtx, dataGraph, shape, focusNode);
    }
}
List<ValidationEvent> actualEvents = listener.getEvents(); // all events have been recorded
The events thus generated might look like this (event.toString(), one per line):
FocusNodeValidationStartedEvent{focusNode=http://datashapes.org/sh/tests/core/node/class-001.test#Someone, shape=NodeShape[http://datashapes.org/sh/tests/core/node/class-001.test#TestShape]}
ConstraintEvaluationForNodeShapeStartedEvent{constraint=ClassConstraint[<http://datashapes.org/sh/tests/core/node/class-001.test#Person>], focusNode=http://datashapes.org/sh/tests/core/node/class-001.test#Someone, shape=NodeShape[http://datashapes.org/sh/tests/core/node/class-001.test#TestShape]}
ConstraintEvaluatedOnFocusNodeEvent{constraint=ClassConstraint[<http://datashapes.org/sh/tests/core/node/class-001.test#Person>], focusNode=http://datashapes.org/sh/tests/core/node/class-001.test#Someone, shape=NodeShape[http://datashapes.org/sh/tests/core/node/class-001.test#TestShape], valid=true}
ConstraintEvaluationForNodeShapeFinishedEvent{constraint=ClassConstraint[<http://datashapes.org/sh/tests/core/node/class-001.test#Person>], focusNode=http://datashapes.org/sh/tests/core/node/class-001.test#Someone, shape=NodeShape[http://datashapes.org/sh/tests/core/node/class-001.test#TestShape]}
FocusNodeValidationFinishedEvent{focusNode=http://datashapes.org/sh/tests/core/node/class-001.test#Someone, shape=NodeShape[http://datashapes.org/sh/tests/core/node/class-001.test#TestShape]}
[...]    
Many use cases can be addressed with the HandlerBasedValidationListener, which allows for registering event handlers on a per-event basis.
For example:
ValidationListener myListener = HandlerBasedValidationListener
    .builder()
    .forEventType(FocusNodeValidationStartedEvent.class)
    .addSimpleHandler(e -> {
       // ... 
    })
    .forEventType(ConstraintEvaluatedEvent.class)
    .addHandler(c -> c
        .iff(EventPredicates.isValid()) // use a Predicate<ValidationEvent> to select events
        .handle(e -> {
            // ...
        })
    )
    .build();\n\nOn this page
    
  
    Command line
    Integration with Apache Jena Fuseki
    API
    API Examples
    SHACL Compact Syntax
    SPARQL-based targets
    ValidationListener
  

  
  
    jena-shacl is an implementation of the
W3C Shapes Constraint Language (SHACL).
It implements SHACL Core and SHACL SPARQL Constraints.
In addition, it provides:

SHACL Compact Syntax
SPARQL-based targets

Command line
The command shacl introduces shacl operations; it takes a sub-command
argument.
To validate:
shacl validate --shapes SHAPES.ttl --data DATA.ttl
shacl v -s SHAPES.ttl -d DATA.ttl
The shapes and data files can be the same; the --shapes is optional and
defaults to the same as --data.  This includes running individual W3C Working
Group tests.
To parse a file:
shacl parse FILE
shacl p FILE
which writes out a text format.
shacl p --out=FMT FILE
writes out in text(t), compact(c), rdf(r) formats. Multiple formats can be given,
separated by “,” and format all outputs all 3 formats.
Integration with Apache Jena Fuseki
Fuseki has a new service operation fuseki:shacl:
<#serviceWithShacl>; rdf:type fuseki:Service ;
    rdfs:label                   "Dataset with SHACL validation" ;
    fuseki:name                  "<i>ds</i>" ;
    fuseki:serviceReadWriteGraphStore "" ;
    fuseki:endpoint [ fuseki:operation fuseki:shacl ; fuseki:name "shacl" ] ;
    fuseki:dataset <#dataset> ;
    .
This requires a “new style” endpoint declaration:  see
“Fuseki Endpoint Configuration”.
This is not installed into a dataset setup by default; a configuration file using
fuseki:endpoint [ fuseki:operation fuseki:shacl ;
                  fuseki:name "shacl" ];
is necessary (or programmatic setup for Fuseki Main).
The service accepts a shapes graph posted as RDF to /ds/shacl with
content negotiation.
There is a graph argument, ?graph=, that specifies the graph to validate. It
is the URI of a named graph, default for the unnamed, default graph (and
this is the assumed value of ?graph if not present), or union for union of
all named graphs in the dataset.
Further, an argument target=uri validates a specific node in the data.
Upload data in file fu-data.ttl:
curl -XPOST --data-binary @fu-data.ttl    \  
     --header 'Content-type: text/turtle' \  
     'http://localhost:3030/ds?default'
Validate with shapes in fu-shapes.ttl and get back a validation report:
curl -XPOST --data-binary @fu-shapes.ttl  \  
     --header 'Content-type: text/turtle' \  
     'http://localhost:3030/ds/shacl?graph=default'
API
The package org.apache.jena.shacl has the main classes.

ShaclValidator for parsing and validation
GraphValidation for updating graphs with validation

API Examples
https://github.com/apache/jena/tree/main/jena-examples/src/main/java/shacl/examples/
Example
Shacl01_validateGraph
shows validation and printing of the validation report in a text form and in RDF:
public static void main(String ...args) {
    String SHAPES = "shapes.ttl";
    String DATA = "data1.ttl";

    Graph shapesGraph = RDFDataMgr.loadGraph(SHAPES);
    Graph dataGraph = RDFDataMgr.loadGraph(DATA);

    Shapes shapes = Shapes.parse(shapesGraph);

    ValidationReport report = ShaclValidator.get().validate(shapes, dataGraph);
    ShLib.printReport(report);
    System.out.println();
    RDFDataMgr.write(System.out, report.getModel(), Lang.TTL);
}
Example
Shacl02_validateTransaction
shows how to update a graph only if, after the changes, the graph is validated
according to the shapes provided.
SHACL Compact Syntax
Apache Jena supports
SHACL Compact Syntax (SHACL-C)
for both reading and writing.
The file extensions for SHACL-C are .shc and .shaclc and there is a registered language
constant Lang.SHACLC.
RDFDataMgr.load("shapes.shc");

RDFDataMgr.read("file:compactShapes", Lang.SHACLC);

RDFDataMgr.write(System.out, shapesGraph, Lang.SHACLC);
SHACL-C is managed by the SHACL Community Group. It does not cover all possible shapes.
When outputting SHACL-C, SHACL shapes not expressible in SHACL-C will cause an
exception and data in the RDF graph that is not relevant will not be output. In
other words, SHACL-C is a lossy format for RDF.
The Jena SHACL-C writer will output any valid SHACL-C document.
Extensions:

The constraint grammar rule allows a shape reference to a node shape.
The propertyParam grammar rule provides “group”, “order”, “name”,
“description” and “defaultValue” to align with nodeParam.
The nodeParam grammar rule supports “targetClass” (normally written
with the shorthand ->) as well as the defined
“targetNode”, “targetObjectsOf”, “targetSubjectsOf”

SPARQL-based targets
SPARQL-based targets allow the target nodes to be calculated with a SPARQL
SELECT query.
See SPARQL-based targets
for details.
ex:example
    sh:target [
        a sh:SPARQLTarget ;
        sh:select """
            SELECT ?this
            WHERE {
              ...
            }
            """ ;
    ] ;
ValidationListener
When given a ValidationListener the SHACL validation code emits events at each step of validation:

when validation of a shape starts or finishes
when the focus nodes of the shape have been identified
when validation of a constraint begins, ends and yields positive or negative results

For example, the following listener will just record all events in a List:
public class RecordingValidationListener implements ValidationListener {
        private final List<ValidationEvent> events = new ArrayList<>();

        @Override public void onValidationEvent(ValidationEvent e) {
            events.add(e);
        }

        public List<ValidationEvent> getEvents() {
            return events;
        }
    }
The listener must be passed to the constructor of the ValidationContext.
The following example validates the dataGraph according to the shapesGraph using the ValidationListener above:
Graph shapesGraph = RDFDataMgr.loadGraph(shapesGraphUri); //assuming shapesGraphUri points to an RDF file
Graph dataGraph = RDFDataMgr.loadGraph(dataGraphUri); //assuming dataGraphUri points to an RDF file
RecordingValidationListener listener = new RecordingValidationListener();  // see above
Shapes shapes = Shapes.parse(shapesGraph);
ValidationContext vCtx = ValidationContext.create(shapes, dataGraph, listener); // pass listener here
for (Shape shape : shapes.getTargetShapes()) {
    Collection<Node> focusNodes = VLib.focusNodes(dataGraph, shape);
    for (Node focusNode : focusNodes) {
        VLib.validateShape(vCtx, dataGraph, shape, focusNode);
    }
}
List<ValidationEvent> actualEvents = listener.getEvents(); // all events have been recorded
The events thus generated might look like this (event.toString(), one per line):
FocusNodeValidationStartedEvent{focusNode=http://datashapes.org/sh/tests/core/node/class-001.test#Someone, shape=NodeShape[http://datashapes.org/sh/tests/core/node/class-001.test#TestShape]}
ConstraintEvaluationForNodeShapeStartedEvent{constraint=ClassConstraint[<http://datashapes.org/sh/tests/core/node/class-001.test#Person>], focusNode=http://datashapes.org/sh/tests/core/node/class-001.test#Someone, shape=NodeShape[http://datashapes.org/sh/tests/core/node/class-001.test#TestShape]}
ConstraintEvaluatedOnFocusNodeEvent{constraint=ClassConstraint[<http://datashapes.org/sh/tests/core/node/class-001.test#Person>], focusNode=http://datashapes.org/sh/tests/core/node/class-001.test#Someone, shape=NodeShape[http://datashapes.org/sh/tests/core/node/class-001.test#TestShape], valid=true}
ConstraintEvaluationForNodeShapeFinishedEvent{constraint=ClassConstraint[<http://datashapes.org/sh/tests/core/node/class-001.test#Person>], focusNode=http://datashapes.org/sh/tests/core/node/class-001.test#Someone, shape=NodeShape[http://datashapes.org/sh/tests/core/node/class-001.test#TestShape]}
FocusNodeValidationFinishedEvent{focusNode=http://datashapes.org/sh/tests/core/node/class-001.test#Someone, shape=NodeShape[http://datashapes.org/sh/tests/core/node/class-001.test#TestShape]}
[...]    
Many use cases can be addressed with the HandlerBasedValidationListener, which allows for registering event handlers on a per-event basis.
For example:
ValidationListener myListener = HandlerBasedValidationListener
    .builder()
    .forEventType(FocusNodeValidationStartedEvent.class)
    .addSimpleHandler(e -> {
       // ... 
    })
    .forEventType(ConstraintEvaluatedEvent.class)
    .addHandler(c -> c
        .iff(EventPredicates.isValid()) // use a Predicate<ValidationEvent> to select events
        .handle(e -> {
            // ...
        })
    )
    .build();

  
  
  
    On this page
    
  
    Command line
    Integration with Apache Jena Fuseki
    API
    API Examples
    SHACL Compact Syntax
    SPARQL-based targets
    ValidationListener\n\n\n\njena-shex is an implementation of the
ShEx (Shape Expressions) language.
Status
jena-shex reads ShExC (the compact syntax) files.
Not currently supported:

semantic actions
EXTERNAL

Blank node label validation is meaningless in Jena because a blank node label is
scoped to the file, and not retained after the file has been read.
Command line
The command shex introduces ShEx operations; it takes a sub-command
argument.
To validate:
shex validate --schema SCHEMA.shex --map MAP.smap --data DATA.ttl
shex v -s SCHEMA.shex -m MAP.smap -d data.ttl
To parse a file:
shex parse FILE
shex p FILE
which writes out the parser results in a text format.

API
The package org.apache.jena.shex has the main classes.

Shex for reading ShEx related formats.
ShexValidation for validation.

API Examples
Examples:
https://github.com/apache/jena/tree/main/jena-examples/src/main/java/shex/examples/
public static void main(String ...args) {
    String SHAPES     = "examples/schema.shex";
    String SHAPES_MAP = "examples/shape-map.shexmap";
    String DATA       = "examples/data.ttl";

    System.out.println("Read data");
    Graph dataGraph = RDFDataMgr.loadGraph(DATA);

    System.out.println("Read schema");
    ShexSchema shapes = Shex.readSchema(SHAPES);

    // Shapes map.
    System.out.println("Read shapes map");
    ShapeMap shapeMap = Shex.readShapeMap(SHAPES_MAP);

    // ShexReport
    System.out.println("Validate");
    ShexReport report = ShexValidator.get().validate(dataGraph, shapes, shapeMap);

    System.out.println();
    // Print report.
    ShexLib.printReport(report);
}\n\nOn this page
    
  
    Status
    Command line
    API
    API Examples
  

  
  
    jena-shex is an implementation of the
ShEx (Shape Expressions) language.
Status
jena-shex reads ShExC (the compact syntax) files.
Not currently supported:

semantic actions
EXTERNAL

Blank node label validation is meaningless in Jena because a blank node label is
scoped to the file, and not retained after the file has been read.
Command line
The command shex introduces ShEx operations; it takes a sub-command
argument.
To validate:
shex validate --schema SCHEMA.shex --map MAP.smap --data DATA.ttl
shex v -s SCHEMA.shex -m MAP.smap -d data.ttl
To parse a file:
shex parse FILE
shex p FILE
which writes out the parser results in a text format.

API
The package org.apache.jena.shex has the main classes.

Shex for reading ShEx related formats.
ShexValidation for validation.

API Examples
Examples:
https://github.com/apache/jena/tree/main/jena-examples/src/main/java/shex/examples/
public static void main(String ...args) {
    String SHAPES     = "examples/schema.shex";
    String SHAPES_MAP = "examples/shape-map.shexmap";
    String DATA       = "examples/data.ttl";

    System.out.println("Read data");
    Graph dataGraph = RDFDataMgr.loadGraph(DATA);

    System.out.println("Read schema");
    ShexSchema shapes = Shex.readSchema(SHAPES);

    // Shapes map.
    System.out.println("Read shapes map");
    ShapeMap shapeMap = Shex.readShapeMap(SHAPES_MAP);

    // ShexReport
    System.out.println("Validate");
    ShexReport report = ShexValidator.get().validate(dataGraph, shapes, shapeMap);

    System.out.println();
    // Print report.
    ShexLib.printReport(report);
}

  
  
  
    On this page
    
  
    Status
    Command line
    API
    API Examples\n\n\n\nTDB is a component of Jena for RDF storage and
query.  It supports the full range of Jena APIs.  TDB can be used as a high
performance RDF store on a single machine.  This documentation describes the
latest version, unless otherwise noted.
This is the documentation for the current standard version of TDB.
This is also called TDB1 to distinguish it from the next generation version
TDB2
TDB1 and TDB2 databases are not compatible.
A TDB store can be accessed and managed with the provided command
line scripts and via the Jena API.  When accessed using transactions
a TDB dataset is protected against corruption, unexpected process terminations and system crashes.
A TDB dataset should only be directly accessed from a single JVM at a time otherwise data corruption
may occur.  From 1.1.0 onwards TDB includes automatic protection against multi-JVM usage which prevents this
under most circumstances.
If you wish to share a TDB dataset between multiple applications please use our
Fuseki component which provides a SPARQL server that
can use TDB for persistent storage and provides the SPARQL protocols
for query, update and REST update over HTTP.
Documentation

Using TDB from Java through the API
Command line utilities
Transactions
Assemblers for Graphs and Datasets
Datasets and Named Graphs
Dynamic Datasets:  Query a subset of the named graphs
Quad filtering: Hide information in the dataset
The TDB Optimizer
TDB Configuration
Value Canonicalization
TDB Design
Use on 64 bit or 32 bit Java systems
FAQs\n\nOn this page
    
  
    Documentation
  

  
  
    TDB is a component of Jena for RDF storage and
query.  It supports the full range of Jena APIs.  TDB can be used as a high
performance RDF store on a single machine.  This documentation describes the
latest version, unless otherwise noted.
This is the documentation for the current standard version of TDB.
This is also called TDB1 to distinguish it from the next generation version
TDB2
TDB1 and TDB2 databases are not compatible.
A TDB store can be accessed and managed with the provided command
line scripts and via the Jena API.  When accessed using transactions
a TDB dataset is protected against corruption, unexpected process terminations and system crashes.
A TDB dataset should only be directly accessed from a single JVM at a time otherwise data corruption
may occur.  From 1.1.0 onwards TDB includes automatic protection against multi-JVM usage which prevents this
under most circumstances.
If you wish to share a TDB dataset between multiple applications please use our
Fuseki component which provides a SPARQL server that
can use TDB for persistent storage and provides the SPARQL protocols
for query, update and REST update over HTTP.
Documentation

Using TDB from Java through the API
Command line utilities
Transactions
Assemblers for Graphs and Datasets
Datasets and Named Graphs
Dynamic Datasets:  Query a subset of the named graphs
Quad filtering: Hide information in the dataset
The TDB Optimizer
TDB Configuration
Value Canonicalization
TDB Design
Use on 64 bit or 32 bit Java systems
FAQs


  
  
  
    On this page
    
  
    Documentation\n\n\n\nTDB2 is a component of Apache Jena for RDF storage
and query.  It supports the full range of Jena APIs.  TDB2 can be used as a high
performance RDF store on a single machine.  TDB2 can be used with Apache Jena
Fuseki.
TDB1 is the previous generation native storage system for Jena.
Compared to TDB1:

No size limits on transactions : bulk uploads into a live Fuseki can be 100’s
of millions of triples.
Models and Graphs can be passed across transactions
Transactional only (there is currently no “autocommit” mode).
Better transaction control

No queue of delayed updates
No backlog problems.
“Writer pays” - readers don’t


Datatypes of numerics preserved; xsd:doubles supported.

TDB2 is not compatible with TDB1
Documentation

Migrating from TDB1
Use with Fuseki2
Command line tools
Database administration\n\nOn this page
    
  
    Documentation
  

  
  
    TDB2 is a component of Apache Jena for RDF storage
and query.  It supports the full range of Jena APIs.  TDB2 can be used as a high
performance RDF store on a single machine.  TDB2 can be used with Apache Jena
Fuseki.
TDB1 is the previous generation native storage system for Jena.
Compared to TDB1:

No size limits on transactions : bulk uploads into a live Fuseki can be 100’s
of millions of triples.
Models and Graphs can be passed across transactions
Transactional only (there is currently no “autocommit” mode).
Better transaction control

No queue of delayed updates
No backlog problems.
“Writer pays” - readers don’t


Datatypes of numerics preserved; xsd:doubles supported.

TDB2 is not compatible with TDB1
Documentation

Migrating from TDB1
Use with Fuseki2
Command line tools
Database administration


  
  
  
    On this page
    
  
    Documentation\n\n\n\nThis extension to ARQ combines SPARQL and full text search via
Lucene.
It gives applications the ability to perform indexed full text
searches within SPARQL queries. Here is a version compatibility table:

  
      
           Jena 
           Lucene 
           Solr 
           ElasticSearch 
      
  
  
      
          upto 3.2.0
          5.x or 6.x
          5.x or 6.x
          not supported
      
      
          3.3.0 - 3.9.0
          6.4.x
          not supported
          5.2.2 - 5.2.13
      
      
          3.10.0
          7.4.0
          not supported
          6.4.2
      
      
          3.15.0 - 3.17.0
          7.7.x
          not supported
          6.8.6
      
      
          4.0.0 - 4.6.1
          8.8.x
          not supported
          not supported
      
      
          4.7.0 - current
          9.4.x
          not supported
          not supported
      
  

Note: In Lucene 9, the default setup of the StandardAnalyzer changed to having
no stop words. For more details, see analyzer specifications below.
SPARQL allows the use of
regex
in FILTERs which is a test on a value retrieved earlier in the query
so its use is not indexed. For example, if you’re
searching for occurrences of "printer" in the rdfs:label of a bunch
of products:
PREFIX   ex: <http://www.example.org/resources#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?s ?lbl
WHERE { 
  ?s a ex:Product ;
     rdfs:label ?lbl
  FILTER regex(?lbl, "printer", "i")
}
then the search will need to examine all selected rdfs:label
statements and apply the regular expression to each label in turn. If
there are many such statements and many such uses of regex, then it
may be appropriate to consider using this extension to take advantage of
the performance potential of full text indexing.
Text indexes provide additional information for accessing the RDF graph
by allowing the application to have indexed access to the internal
structure of string literals rather than treating such literals as
opaque items.  Unlike FILTER, an index can set the values of variables.
Assuming appropriate configuration, the
above query can use full text search via the
ARQ property function extension, text:query:
PREFIX   ex: <http://www.example.org/resources#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX text: <http://jena.apache.org/text#>

SELECT ?s ?lbl
WHERE { 
	?s a ex:Product ;
	   text:query (rdfs:label 'printer') ;
	   rdfs:label ?lbl
}

This query makes a text query for 'printer' on the rdfs:label
property; and then looks in the RDF data and retrieves the complete
label for each match.
The full text engine can be either Apache
Lucene hosted with Jena on a single
machine, or Elasticsearch for a large scale
enterprise search application where the full text engine is potentially
distributed across separate machines.
This example code
illustrates creating an in-memory dataset with a Lucene index.
Architecture
In general, a text index engine (Lucene or Elasticsearch) indexes
documents where each document is a collection of fields, the values
of which are indexed so that searches matching contents of specified
fields can return a reference to the document containing the fields with
matching values.
There are two models for extending Jena with text indexing and search:

One Jena triple equals one Lucene document
One Lucene document equals one Jena entity

One triple equals one document
The basic Jena text extension associates a triple with
a document and the property of the triple with a field of a document
and the object of the triple (which must be a literal) with the value
of the field in the document. The subject of the triple then becomes
another field of the document that is returned as the result of a search
match to identify what was matched. (NB, the particular triple that
matched is not identified. Only, its subject and optionally the matching
literal and match score.)
In this manner, the text index provides an inverted index that maps
query string matches to subject URIs.
A text-indexed dataset is configured with a description of which
properties are to be indexed. When triples are added, any properties
matching the description cause a document to be added to the index by
analyzing the literal value of the triple object and mapping to the
subject URI. On the other hand, it is necessary to specifically
configure the text-indexed dataset to delete index
entries when the corresponding triples are
dropped from the RDF store.
The text index uses the native query language of the index:
Lucene query language
(with restrictions)
or
Elasticsearch query language.
One document equals one entity
There are two approaches to creating indexed documents that contain more
than one indexed field:

Using an externally maintained Lucene index
Multiple fields per document

When using this integration model, text:query returns the subject URI
for the document on which additional triples of metadata may be associated,
and optionally the Lucene score for the match.
External content
When document content is externally indexed via Lucene and accessed in Jena
via a text:TextDataset then the subject URI returned for a search result
is considered to refer to the external content, and metadata about the
document is represented as triples in Jena with the subject URI.
There is no requirement that the indexed document content be present
in the RDF data.  As long as the index contains the index text documents to
match the index description, then text search can be performed with queries that explicitly mention indexed fields in the document.
That is, if the content of a collection of documents is externally indexed
and the URI naming the document is the result of the text search, then an RDF
dataset with the document metadata can be combined with accessing the
content by URI.
The maintenance of the index is external to the RDF data store.
External applications
By using Elasticsearch, other applications can share the text index with
SPARQL search.
Document structure
As mentioned above, when using the (default) one-triple equals one-document model,
text indexing of a triple involves associating a Lucene document with the triple.
How is this done?
Lucene documents are composed of Fields. Indexing and searching are performed
over the contents of these Fields. For an RDF triple to be indexed in Lucene the
property of the triple must be
configured in the entity map of a TextIndex.
This associates a Lucene analyzer with the property which will be used
for indexing and search. The property becomes the searchable Lucene
Field in the resulting document.
A Lucene index includes a default Field, which is specified in the configuration,
that is the field to search if not otherwise named in the query. In jena-text
this field is configured via the text:defaultField property which is then mapped
to a specific RDF property via text:predicate (see entity map
below).
There are several additional Fields that will be included in the
document that is passed to the Lucene IndexWriter depending on the
configuration options that are used. These additional fields are used to
manage the interface between Jena and Lucene and are not generally
searchable per se.
The most important of these additional Fields is the text:entityField.
This configuration property defines the name of the Field that will contain
the URI or blank node id of the subject of the triple being indexed. This property does
not have a default and must be specified for most uses of jena-text. This
Field is often given the name, uri, in examples. It is via this Field
that ?s is bound in a typical use such as:
select ?s
where {
    ?s text:query "some text"
}

Other Fields that may be configured: text:uidField, text:graphField,
and so on are discussed below.
Given the triple:
ex:SomeOne skos:prefLabel "zorn protégé a prés"@fr ;

The following is an abbreviated illustration a Lucene document that Jena will create and
request Lucene to index:
Document<
    <uri:http://example.org/SomeOne> 
    <graph:urn:x-arq:DefaultGraphNode> 
    <label:zorn protégé a prés> 
    <lang:fr> 
    <uid:28959d0130121b51e1459a95bdac2e04f96efa2e6518ff3c090dfa7a1e6dcf00> 
    >

It may be instructive to refer back to this example when considering the various
points below.
Query with SPARQL
The URI of the text extension property function is
http://jena.apache.org/text#query more conveniently written:
PREFIX text: <http://jena.apache.org/text#>

...   text:query ...

Syntax
The following forms are all legal:
?s text:query 'word'                              # query
?s text:query ('word' 10)                         # with limit on results
?s text:query (rdfs:label 'word')                 # query specific property if multiple
?s text:query (rdfs:label 'protégé' 'lang:fr')    # restrict search to French
(?s ?score) text:query 'word'                     # query capturing also the score
(?s ?score ?literal) text:query 'word'            # ... and original literal value
(?s ?score ?literal ?g) text:query 'word'         # ... and the graph

The most general form when using the default one-triple equals one-document
integration model is:
 ( ?s ?score ?literal ?g ) text:query ( property* 'query string' limit 'lang:xx' 'highlight:yy' )

while for the one-document equals one-entity model, the general form is:
 ( ?s ?score ) text:query ( 'query string' limit )

and if only the subject URI is needed:
 ?s text:query ( 'query string' limit )

Input arguments:

  
      
           Argument 
            Definition 
      
  
  
      
          property
          (zero or more) property URIs (including prefix name form)
      
      
          query string
          Lucene query string fragment
      
      
          limit
          (optional) int limit on the number of results
      
      
          lang:xx
          (optional) language tag spec
      
      
          highlight:yy
          (optional) highlighting options
      
  

The property URI is only necessary if multiple properties have been
indexed and the property being searched over is not the default field
of the index.
Since 3.13.0, property may be a list of zero or more (prior to 3.13.0 zero or one) Lucene indexed properties, or a defined
text:propList of indexed properties.
The meaning is an OR of searches on a variety of properties. This can be used in place of SPARQL level UNIONs of
individual text:querys. For example, instead of:
select ?foo where {
  {
    (?s ?sc ?lit) text:query ( rdfs:label "some query" ).
  }
  union
  {
    (?s ?sc ?lit) text:query ( skos:altLabel "some query" ).
  }
  union
  { 
    (?s ?sc ?lit) text:query ( skos:prefLabel "some query" ).
  }
}

it can be more performant to push the unions into the Lucene query by rewriting as:
(?s ?sc ?lit) text:query ( rdfs:label skos:prefLabel skos:altLabel "some query" )

which creates a Lucene query:
(altLabel:"some query" OR prefLabel:"some query" OR label:"some query")

The query string syntax conforms to the underlying
Lucene,
or when appropriate,
Elasticsearch.
In the case of the default one-triple equals one-document model, the Lucene query syntax is restricted to Terms, Term modifiers,
Boolean Operators applied to Terms, and Grouping of terms.
Additionally, the use of Fields within the query string is supported when using the one-document equals one-entity text integration model.
When using the default model,
use of Fields in the query string will generally lead to unpredictable results.
The optional limit indicates the maximum hits to be returned by Lucene.
The lang:xx specification is an optional string, where xx is
a BCP-47 language tag. This restricts searches to field values that were originally
indexed with the tag xx. Searches may be restricted to field values with no
language tag via "lang:none".
The highlight:yy specification is an optional string where yy are options that control the highlighting of search
result literals. See below for details.
If both limit and one or more of lang:xx or highlight:yy are present, then limit must precede these arguments.
If only the query string is required, the surrounding ( ) may be omitted.
Output arguments:

  
      
           Argument 
            Definition 
      
  
  
      
          subject URI
          The subject of the indexed RDF triple.
      
      
          score
          (optional) The score for the match.
      
      
          literal
          (optional) The matched object literal.
      
      
          graph URI
          (optional) The graph URI of the triple.
      
      
          property URI
          (optional) The property URI of the matched triple
      
  

The results include the subject URI; the score assigned by the
text search engine; and the entire matched literal (if the index has
been configured to store literal values).
The subject URI may be a variable, e.g., ?s, or a URI. In the
latter case the search is restricted to triples with the specified
subject. The score, literal, graph URI, and property URI must be variables.
The property URI is meaningful when two or more properties are used in the query.
Query strings
There are several points that need to be considered when formulating
SPARQL queries using either of the Lucene integration models.
As mentioned above, in the case of the default model
the query string syntax is restricted to Terms, Term modifiers, Boolean Operators
applied to Terms, and Grouping of terms.
Explicit use of Fields in the query string is only useful with the
one-document equals one-entity model;
and otherwise will generally produce unexpected results.
See Queries across multiple Fields.
Simple queries
The simplest use of the jena-text Lucene integration is like:
?s text:query "some phrase"

This will bind ?s to each entity URI that is the subject of a triple
that has the default property and an object literal that matches
the argument string, e.g.:
ex:AnEntity skos:prefLabel "this is some phrase to match"

This query form will indicate the subjects that have literals that match
for the default property which is determined via the configuration of
the text:predicate of the text:defaultField
(in the above this has been assumed to be skos:prefLabel.
For a non-default property it is necessary to specify the property as
an input argument to the text:query:
?s text:query (rdfs:label "protégé")

(see below for how RDF property names
are mapped to Lucene Field names).
If this use case is sufficient for your needs you can skip on to the
sections on configuration.
Please note that the query:
?s text:query "some phrase"

when using the Lucene StandardAnalyzer or similar will treat the query string
as an OR of terms: some and phrase. If a phrase search is required then
it is necessary to surround the phrase by double quotes, ":
?s text:query "\"some phrase\""

This will only match strings that contain "some phrase", while the former
query will match strings like: "there is a phrase for some" or
"this is some of the various sorts of phrase that might be matched".
Queries with language tags
When working with rdf:langStrings it is necessary that the
text:langField has been configured. Then it is
as simple as writing queries such as:
?s text:query "protégé"@fr

to return results where the given term or phrase has been
indexed under French in the text:defaultField.
It is also possible to use the optional lang:xx argument, for example:
?s text:query ("protégé" 'lang:fr') .

In general, the presence of a language tag, xx, on the query string or
lang:xx in the text:query adds AND lang:xx to the query sent to Lucene,
so the above example becomes the following Lucene query:
"label:protégé AND lang:fr"

For non-default properties the general form is used:
?s text:query (skos:altLabel "protégé" 'lang:fr')

Note that an explicit language tag on the query string takes precedence
over the lang:xx, so the following
?s text:query ("protégé"@fr 'lang:none')

will find French matches rather than matches indexed without a language tag.
Queries that retrieve literals
It is possible to retrieve the literals that Lucene finds matches for
assuming that
<#TextIndex#> text:storeValues true ;

has been specified in the TextIndex configuration. So
(?s ?sc ?lit) text:query (rdfs:label "protégé")

will bind the matching literals to ?lit, e.g.,
"zorn protégé a prés"@fr

Note it is necessary to include a variable to capture the Lucene score
even if this value is not otherwise needed since the literal variable
is determined by position.
Queries with graphs
Assuming that the text:graphField has been configured,
then, when a triple is indexed, the graph that the triple resides in is
included in the document and may be used to restrict searches or to retrieve the graph that a matching triple resides in.
For example:
select ?s ?lit
where {
  graph ex:G2 { (?s ?sc ?lit) text:query "zorn" } .
}

will restrict searches to triples with the default property that reside
in graph, ex:G2.
On the other hand:
select ?g ?s ?lit
where {
  graph ?g { (?s ?sc ?lit) text:query "zorn" } .
}

will iterate over the graphs in the dataset, searching each in turn for
matches.
If there is suitable structure to the graphs, e.g., a known rdf:type and
depending on the selectivity of the text query and number of graphs,
it may be more performant to express the query as follows:
select ?g ?s ?lit
where {
  (?s ?sc ?lit) text:query "zorn" .
  graph ?g { ?s a ex:Item } .
}

Further, if tdb:unionDefaultGraph true for a TDB dataset backing a Lucene index then it is possible to retrieve the graphs that contain triples resulting from a Lucene search via the fourth output argument to text:query:
select ?g ?s ?lit
where {
  (?s ?sc ?lit ?g) text:query "zorn" .
}

This will generally perform much better than either of the previous approaches when there are
large numbers of graphs since the Lucene search will run once and the returned documents carry
the containing graph URI for free as it were.
Queries across multiple Fields
As mentioned earlier, the Lucene text index uses the
native Lucene query language.
Multiple fields in the default integration model
For the default integration model, since each document
has only one field containing searchable text, searching for documents containing
multiple fields will generally not find any results.
Note that the default model provides three Lucene Fields
in a document that are used during searching:

the field corresponding to the property of the indexed triple,
the field for the language of the literal (if configured), and
the graph that the triple is in (if configured).

Given these, it should be clear from the above that the
default model
constructs a Lucene query from the property, query string, lang:xx, and
SPARQL graph arguments.
For example, consider the following triples:
ex:SomePrinter 
    rdfs:label     "laser printer" ;
    ex:description "includes a large capacity cartridge" .

assuming an appropriate configuration, if we try to retrieve ex:SomePrinter
with the following Lucene query string:
?s text:query "label:printer AND description:\"large capacity cartridge\""

then this query can not find the expected results since the AND is interpreted
by Lucene to indicate that all documents that contain a matching label field and
a matching description field are to be returned; yet, from the discussion above
regarding the structure of Lucene documents in jena-text it
is evident that there is not one but rather in fact two separate documents one with a
label field and one with a description field so an effective SPARQL query is:
?s text:query (rdfs:label "printer") .
?s text:query (ex:description "large capacity cartridge") .

which leads to ?s being bound to ex:SomePrinter.
In other words when a query is to involve two or more properties of a given entity
then it is expressed at the SPARQL level, as it were, versus in Lucene’s query language.
It is worth noting that the equivalent of a Lucene OR of Fields can be expressed
using SPARQL union, though since 3.13.0 this can be expressed in Jena text
using a property list - see Input arguments:
{ ?s text:query (rdfs:label "printer") . }
union
{ ?s text:query (ex:description "large capacity cartridge") . }

Suppose the matching literals are required for the above then it should be clear
from the above that:
(?s ?sc1 ?lit1) text:query (skos:prefLabel "printer") .
(?s ?sc2 ?lit2) text:query (ex:description "large capacity cartridge") .

will be the appropriate form to retrieve the subject and the associated literals, ?lit1 and ?lit2. (Obviously, in general, the score variables, ?sc1 and ?sc2
must be distinct since it is very unlikely that the scores of the two Lucene queries
will ever match).
There is no loss of expressiveness of the Lucene query language versus the jena-text
integration of Lucene. Any cross-field ANDs are replaced by concurrent SPARQL calls to
text:query as illustrated above and uses of Lucene OR can be converted to SPARQL
unions. Uses of Lucene NOT are converted to appropriate SPARQL filters.
Multiple fields in the one-document equals one-entity model
If Lucene documents have been indexed with multiple searchable fields
then compound queries expressed directly in the Lucene query language can significantly improve search
performance, in particular, where the individual components of the Lucene query generate
a lot of results which must be combined in SPARQL.
It is possible to have text queries that search multiple fields within a text query.
Doing this is more complex as it requires the use of either an externally managed
text index or code must be provided to build the multi-field text documents to be indexed.
See Multiple fields per document.
Queries with Boolean Operators and Term Modifiers
On the other hand the various features of the Lucene query language
are all available to be used for searches within a Field.
For example, Boolean Operators on Terms:
?s text:query (ex:description "(large AND cartridge)")

and
(?s ?sc ?lit) text:query (ex:description "(includes AND (large OR capacity))")

or fuzzy searches:
?s text:query (ex:description "include~")

and so on will work as expected.
Always surround the query string with ( ) if more than a single term or phrase
are involved.
Highlighting
The highlighting option uses the Lucene Highlighter and SimpleHTMLFormatter to insert highlighting markup into the literals returned from search results (hence the text dataset must be configured to store the literals). The highlighted results are returned via the literal output argument. This highlighting feature, introduced in version 3.7.0, does not require re-indexing by Lucene.
The simplest way to request highlighting is via 'highlight:'. This will apply all the defaults:

  
      
           Option 
           Key 
           Default 
      
  
  
      
          maxFrags
          m:
          3
      
      
          fragSize
          z:
          128
      
      
          start
          s:
          RIGHT_ARROW
      
      
          end
          e:
          LEFT_ARROW
      
      
          fragSep
          f:
          DIVIDES
      
      
          joinHi
          jh:
          true
      
      
          joinFrags
          jf:
          true
      
  

to the highlighting of the search results. For example if the query is:
(?s ?sc ?lit) text:query ( "brown fox" "highlight:" ) 

then a resulting literal binding might be:
"the quick ↦brown fox↤ jumped over the lazy baboon"

The RIGHT_ARROW is Unicode \u21a6 and the LEFT_ARROW is Unicode \u21a4. These are chosen to be single characters that in most situations will be very unlikely to occur in resulting literals. The fragSize of 128 is chosen to be large enough that in many situations the matches will result in single fragments. If the literal is larger than 128 characters and there are several matches in the literal then there may be additional fragments separated by the DIVIDES, Unicode \u2223.
Depending on the analyzer used and the tokenizer, the highlighting will result in marking each token rather than an entire phrase. The joinHi option is by default true so that entire phrases are highlighted together rather than as individual tokens as in:
"the quick ↦brown↤ ↦fox↤ jumped over the lazy baboon"

which would result from:
(?s ?sc ?lit) text:query ( "brown fox" "highlight:jh:n" )

The jh and jf boolean options are set false via n. Any other value is true. The defaults for these options have been selected to be reasonable for most applications.
The joining is performed post highlighting via Java String replaceAll rather than using the Lucene Unified Highlighter facility which requires that term vectors and positions be stored. The joining deletes extra highlighting with only intervening Unicode separators, \p{Z}.
The more conventional output of the Lucene SimpleHTMLFormatter with html emphasis markup is achieved via, "highlight:s:<em class='hiLite'> | e:</em>" (highlight options are separated by a Unicode vertical line, \u007c. The spaces are not necessary). The result with the above example will be:
"the quick <em class='hiLite'>brown fox</em> jumped over the lazy baboon"

which would result from the query:
(?s ?sc ?lit) text:query ( "brown fox" "highlight:s:<em class='hiLite'> | e:</em>" )

Good practice
From the above it should be clear that best practice, except in the simplest cases
is to use explicit text:query forms such as:
(?s ?sc ?lit) text:query (ex:someProperty "a single Field query")

possibly with limit and lang:xx arguments.
Further, the query engine does not have information about the selectivity of the
text index and so effective query plans cannot be determined
programmatically.  It is helpful to be aware of the following two
general query patterns.
Query pattern 1 – Find in the text index and refine results
Access to the text index is first in the query and used to find a number of
items of interest; further information is obtained about these items from
the RDF data.
SELECT ?s
{ ?s text:query (rdfs:label 'word' 10) ; 
     rdfs:label ?label ;
     rdf:type   ?type 
}

The text:query limit argument is useful when working with large indexes to limit results to the
higher scoring results – results are returned in the order of scoring by the text search engine.
Query pattern 2 – Filter results via the text index
By finding items of interest first in the RDF data, the text search can be
used to restrict the items found still further.
SELECT ?s
{ ?s rdf:type     :book ;
     dc:creator  "John" .
  ?s text:query   (dc:title 'word') ; 
}

Configuration
The usual way to describe a text index is with a
Jena assembler description.  Configurations can
also be built with code. The assembler describes a ’text
dataset’ which has an underlying RDF dataset and a text index. The text
index describes the text index technology (Lucene or Elasticsearch) and the details
needed for each.
A text index has an “entity map” which defines the properties to
index, the name of the Lucene/Elasticsearch field and field used for storing the URI
itself.
For simple RDF use, there will be one field, mapping a property to a text
index field. More complex setups, with multiple properties per entity
(URI) are possible.
The assembler file can be either default configuration file (…/run/config.ttl)
or a custom file in …run/configuration folder. Note that you can use several files
simultaneously.
You have to edit the file (see comments in the assembler code below):

provide values for paths and a fixed URI for tdb:DatasetTDB
modify the entity map : add the fields you want to index and desired options (filters, tokenizers…)

If your assembler file is run/config.ttl, you can index the dataset with this command :
java -cp ./fuseki-server.jar jena.textindexer --desc=run/config.ttl

Once configured, any data added to the text dataset is automatically
indexed as well: Building a Text Index.
Text Dataset Assembler
The following is an example of an assembler file defining a TDB dataset with a Lucene text index.
######## Example of a TDB dataset and text index#########################
# The main doc sources are:
#  - https://jena.apache.org/documentation/fuseki2/fuseki-configuration.html
#  - https://jena.apache.org/documentation/assembler/assembler-howto.html
#  - https://jena.apache.org/documentation/assembler/assembler.ttl
# See https://jena.apache.org/documentation/fuseki2/fuseki-layout.html for the destination of this file.
#########################################################################

PREFIX :        <http://localhost/jena_example/#>
PREFIX rdf:     <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs:    <http://www.w3.org/2000/01/rdf-schema#>
PREFIX tdb:     <http://jena.hpl.hp.com/2008/tdb#>
PREFIX text:    <http://jena.apache.org/text#>
PREFIX skos:    <http://www.w3.org/2004/02/skos/core#>
PREFIX fuseki:  <http://jena.apache.org/fuseki#>

[] rdf:type fuseki:Server ;
   fuseki:services (
     :myservice
   ) .

:myservice rdf:type fuseki:Service ;
    # e.g : `s-query --service=http://localhost:3030/myds "select * ..."`
    fuseki:name               "myds" ;
    # SPARQL query service : /myds
    fuseki:endpoint [ 
        fuseki:operation fuseki:query ;
    ];
    # SPARQL query service : /myds/query
    fuseki:endpoint [ 
        fuseki:operation fuseki:query ;
        fuseki:name "query"
    ];
    # SPARQL update service : /myds/update
    fuseki:endpoint [
        fuseki:operation fuseki:update ;
        fuseki:name "update"
    ];
    # SPARQL Graph store protocol (read and write) : /myds/data
    fuseki:endpoint [
        fuseki:operation fuseki:gsp-rw ; 
        fuseki:name "data" 
    ];
    # The text-enabled dataset
    fuseki:dataset                    :text_dataset ;
    .

## ---------------------------------------------------------------

# A TextDataset is a regular dataset with a text index.
:text_dataset rdf:type     text:TextDataset ;
    text:dataset   :mydataset ; # <-- replace `:my_dataset` with the desired URI
    text:index     <#indexLucene> ;
.

# A TDB dataset used for RDF storage
:mydataset rdf:type      tdb:DatasetTDB ; # <-- replace `:my_dataset` with the desired URI - as above
    tdb:location "DB" ;
    tdb:unionDefaultGraph true ; # Optional
.

# Text index description
<#indexLucene> a text:TextIndexLucene ;
    text:directory <file:path> ;  # <-- replace `<file:path>` with your path (e.g., `<file:/.../fuseki/run/databases/MY_INDEX>`)
    text:entityMap <#entMap> ;
    text:storeValues true ; 
    text:analyzer [ a text:StandardAnalyzer ] ;
    text:queryAnalyzer [ a text:KeywordAnalyzer ] ;
    text:queryParser text:AnalyzingQueryParser ;
    text:propLists ( [ . . . ] . . . ) ;
    text:defineAnalyzers ( [ . . . ] . . . ) ;
    text:multilingualSupport true ; # optional
.
# Entity map (see documentation for other options)
<#entMap> a text:EntityMap ;
    text:defaultField     "label" ;
    text:entityField      "uri" ;
    text:uidField         "uid" ;
    text:langField        "lang" ;
    text:graphField       "graph" ;
    text:map (
        [ text:field "label" ; 
          text:predicate skos:prefLabel ]
    ) .

See below for more on defining an entity map
The text:TextDataset has two properties:


a text:dataset, e.g., a tdb:DatasetTDB, to contain
the RDF triples; and


an index configured to use either text:TextIndexLucene or text:TextIndexES.


The <#indexLucene> instance of text:TextIndexLucene, above, has two required properties:


the text:directory
file URI which specifies the directory that will contain the Lucene index files – if this has the
value "mem" then the index resides in memory;


the text:entityMap, <#entMap> that will define
what properties are to be indexed and other features of the index; and


and several optional properties:


text:storeValues controls the storing of literal values.
It indicates whether values are stored or not – values must be stored for the
?literal return value to be available in text:query in SPARQL.


text:analyzer specifies the default analyzer configuration to used
during indexing and querying. The default analyzer defaults to Lucene’s StandardAnalyzer.


text:queryAnalyzer specifies an optional analyzer for query that will be
used to analyze the query string. If not set the analyzer used to index a given field is used.


text:queryParser is optional and specifies an alternative query parser


text:propLists is optional and allows to specify lists of indexed properties for use in text:query


text:defineAnalyzers is optional and allows specification of additional analyzers, tokenizers and filters


text:multilingualSupport enables Multilingual Support


If using Elasticsearch then an index would be configured as follows:
<#indexES> a text:TextIndexES ;
      # A comma-separated list of Host:Port values of the ElasticSearch Cluster nodes.
    text:serverList "127.0.0.1:9300" ; 
      # Name of the ElasticSearch Cluster. If not specified defaults to 'elasticsearch'
    text:clusterName "elasticsearch" ; 
      # The number of shards for the index. Defaults to 1
    text:shards "1" ;
      # The number of replicas for the index. Defaults to 1
    text:replicas "1" ;         
      # Name of the Index. defaults to jena-text
    text:indexName "jena-text" ;
    text:entityMap <#entMap> ;
    .

and text:index  <#indexES> ; would be used in the configuration of :text_dataset.
To use a text index assembler configuration in Java code is it necessary
to identify the dataset URI to be assembled, such as in:
Dataset ds = DatasetFactory.assemble(
    "text-config.ttl", 
    "http://localhost/jena_example/#text_dataset") ;

since the assembler contains two dataset definitions, one for the text
dataset, one for the base data.  Therefore, the application needs to
identify the text dataset by it’s URI
http://localhost/jena_example/#text_dataset.
Lists of Indexed Properties
Since 3.13.0, an optional text:TextIndexLucene feature, text:propLists allows to define lists of Lucene indexed
properties that may be used in text:querys. For example:
text:propLists (
    [ text:propListProp ex:labels ;
      text:props ( skos:prefLabel 
                   skos:altLabel 
                   rdfs:label ) ;
    ]
    [ text:propListProp ex:workStmts ;
      text:props ( ex:workColophon 
                   ex:workAuthorshipStatement 
                   ex:workEditionStatement ) ;
    ]
) ;

The text:propLists is a list of property list definitions. Each property list defines a new property,
text:propListProp that will be used to refer to the list in a text:query, for example, ex:labels and
ex:workStmts, above. The text:props is a list of Lucene indexed properties that will be searched over when the
property list property is referred to in a text:query. For example:
?s text:query ( ex:labels "some text" ) .

will request Lucene to search for documents representing triples, ?s ?p ?o, where ?p is one of: rdfs:label OR
skos:prefLbael OR skos:altLabel, matching the query string.
Entity Map definition
A text:EntityMap has several properties that condition what is indexed, what information is stored, and
what analyzers are used.
<#entMap> a text:EntityMap ;
    text:defaultField     "label" ;
    text:entityField      "uri" ;
    text:uidField         "uid" ;
    text:langField        "lang" ;
    text:graphField       "graph" ;
    text:map (
         [ text:field "label" ; 
           text:predicate rdfs:label ]
         ) .

Default text field
The text:defaultField specifies the default field name that Lucene will use in a query that does
not otherwise specify a field. For example,
?s text:query "\"bread and butter\""

will perform a search in the label field for the phrase "bread and butter"
Entity field
The text:entityField  specifies the field name of the field that will contain the subject URI that
is returned on a match. The value of the property is arbitrary so long as it is unique among the
defined names.
UID Field and automatic document deletion
When the text:uidField is defined in the EntityMap then dropping a triple will result in the
corresponding document, if any, being deleted from the text index. The value, "uid", is arbitrary
and defines the name of a stored field in Lucene that holds a unique ID that represents the triple.
If you configure the index via Java code, you need to set this parameter to the
EntityDefinition instance, e.g.
EntityDefinition docDef = new EntityDefinition(entityField, defaultField);
docDef.setUidField("uid");

Note: If you migrate from an index without deletion support to an index with automatic deletion,
you will need to rebuild the index to ensure that the uid information is stored.
Language Field
The text:langField is the name of the field that will store the language attribute of the literal
in the case of an rdf:langString. This Entity Map property is a key element of the
Linguistic support with Lucene index
Graph Field
Setting the text:graphField allows graph-specific indexing of the text
index to limit searching to a specified graph when a SPARQL query targets a single named graph. The
field value is arbitrary and serves to store the graph ID that a triple belongs to when the index is
updated.
The Analyzer Map
The text:map is a list of analyzer specifications as described below.
Configuring an Analyzer
Text to be indexed is passed through a text analyzer that divides it into tokens
and may perform other transformations such as eliminating stop words. If a Lucene
or Elasticsearch text index is used, then by default the Lucene StandardAnalyzer is used.
As of Jena 4.7.x / Lucene 9.x onwards, the StandardAnalyzer does not default to having
English stopwords if no stop words are provided. The setting up until
Apache Lucene 8 had the stopwords:
      "a"  "an"  "and"  "are"  "as"  "at"  "be"  "but"  "by"  "for"  "if"  "in"  "into"  "is" 
      "it"  "no"  "not"  "of"  "on"  "or"  "such"  "that"  "the"  "their"  "then"  "there" 
      "these"  "they"  "this"  "to"  "was"  "will"  "with"

In case of a TextIndexLucene the default analyzer can be replaced by another analyzer with
the text:analyzer property on the text:TextIndexLucene resource in the
text dataset assembler,  for example with a SimpleAnalyzer:
<#indexLucene> a text:TextIndexLucene ;
        text:directory <file:Lucene> ;
        text:analyzer [
            a text:SimpleAnalyzer
        ]
        . 

It is possible to configure an alternative analyzer for each field indexed in a
Lucene index.  For example:
<#entMap> a text:EntityMap ;
    text:entityField      "uri" ;
    text:defaultField     "text" ;
    text:map (
         [ text:field "text" ; 
           text:predicate rdfs:label ;
           text:analyzer [
               a text:StandardAnalyzer ;
               text:stopWords ("a" "an" "and" "but")
           ]
         ]
         ) .

will configure the index to analyze values of the ’text’ field
using a StandardAnalyzer with the given list of stop words.
Other analyzer types that may be specified are SimpleAnalyzer and
KeywordAnalyzer, neither of which has any configuration parameters. See
the Lucene documentation for details of what these analyzers do. Jena also
provides LowerCaseKeywordAnalyzer, which is a case-insensitive version of
KeywordAnalyzer, and ConfigurableAnalyzer.
Support for the new LocalizedAnalyzer has been introduced in Jena 3.0.0 to
deal with Lucene language specific analyzers. See Linguistic Support with
Lucene Index for details.
Support for GenericAnalyzers has been introduced in Jena 3.4.0 to allow
the use of Analyzers that do not have built-in support, e.g., BrazilianAnalyzer;
require constructor parameters not otherwise supported, e.g., a stop words FileReader or
a stemExclusionSet; and finally use of Analyzers not included in the bundled
Lucene distribution, e.g., a SanskritIASTAnalyzer. See Generic and Defined
Analyzer Support
ConfigurableAnalyzer
ConfigurableAnalyzer was introduced in Jena 3.0.1. It allows more detailed
configuration of text analysis parameters by independently selecting a
Tokenizer and zero or more TokenFilters which are applied in order after
tokenization. See the Lucene documentation for details on what each
tokenizer and token filter does.
The available Tokenizer implementations are:

StandardTokenizer
KeywordTokenizer
WhitespaceTokenizer
LetterTokenizer

The available TokenFilter implementations are:

StandardFilter
LowerCaseFilter
ASCIIFoldingFilter
SelectiveFoldingFilter

Configuration is done using Jena assembler like this:
text:analyzer [
  a text:ConfigurableAnalyzer ;
  text:tokenizer text:KeywordTokenizer ;
  text:filters (text:ASCIIFoldingFilter, text:LowerCaseFilter)
]

From Jena 3.7.0, it is possible to define tokenizers and filters in addition to the built-in
choices above that may be used with the ConfigurableAnalyzer. Tokenizers and filters are
defined via text:defineAnalyzers in the text:TextIndexLucene assembler section
using text:GenericTokenizer and text:GenericFilter.
Analyzer for Query
New in Jena 2.13.0.
There is an ability to specify an analyzer to be used for the query
string itself.  It will find terms in the query text.  If not set, then
the analyzer used for the document will be used.  The query analyzer is
specified on the TextIndexLucene resource:
<#indexLucene> a text:TextIndexLucene ;
    text:directory <file:Lucene> ;
    text:entityMap <#entMap> ;
    text:queryAnalyzer [
        a text:KeywordAnalyzer
    ]
    .

Alternative Query Parsers
New in Jena 3.1.0.
It is possible to select a query parser other than the default QueryParser.
The available QueryParser implementations are:


AnalyzingQueryParser: Performs analysis for wildcard queries . This
is useful in combination with accent-insensitive wildcard queries.


ComplexPhraseQueryParser: Permits complex phrase query syntax. Eg:
“(john jon jonathan~) peters*”.  This is useful for performing wildcard
or fuzzy queries on individual terms in a phrase.


SurroundQueryParser: Provides positional operators (w and n)
that accept a numeric distance, as well as boolean
operators (and, or, and not, wildcards (* and ?), quoting (with “),
and boosting (via ^).


The query parser is specified on
the TextIndexLucene resource:
<#indexLucene> a text:TextIndexLucene ;
    text:directory <file:Lucene> ;
    text:entityMap <#entMap> ;
    text:queryParser text:AnalyzingQueryParser .

Elasticsearch currently doesn’t support Analyzers beyond Standard Analyzer.
Configuration by Code
A text dataset can also be constructed in code as might be done for a
purely in-memory setup:
    // Example of building a text dataset with code.
    // Example is in-memory.
    // Base dataset
    Dataset ds1 = DatasetFactory.createMem() ; 

    EntityDefinition entDef = new EntityDefinition("uri", "text", RDFS.label) ;

    // Lucene, in memory.
    Directory dir =  new RAMDirectory();
    
    // Join together into a dataset
    Dataset ds = TextDatasetFactory.createLucene(ds1, dir, entDef) ;

Graph-specific Indexing
jena-text supports storing information about the source graph into the
text index. This allows for more efficient text queries when the query
targets only a single named graph. Without graph-specific indexing, text
queries do not distinguish named graphs and will always return results
from all graphs.
Support for graph-specific indexing is enabled by defining the name of the
index field to use for storing the graph identifier.
If you use an assembler configuration, set the graph field using the
text:graphField property on the EntityMap, e.g.
# Mapping in the index
# URI stored in field "uri"
# Graph stored in field "graph"
# rdfs:label is mapped to field "text"
<#entMap> a text:EntityMap ;
    text:entityField      "uri" ;
    text:graphField       "graph" ;
    text:defaultField     "text" ;
    text:map (
         [ text:field "text" ; text:predicate rdfs:label ]
         ) .

If you configure the index in Java code, you need to use one of the
EntityDefinition constructors that support the graphField parameter, e.g.
    EntityDefinition entDef = new EntityDefinition("uri", "text", "graph", RDFS.label.asNode()) ;

Note: If you migrate from a global (non-graph-aware) index to a graph-aware index,
you need to rebuild the index to ensure that the graph information is stored.
Linguistic support with Lucene index
Language tags associated with rdfs:langStrings occurring as literals in triples may
be used to enhance indexing and queries. Sub-sections below detail different settings with the index, and use cases with SPARQL queries.
Explicit Language Field in the Index
The language tag for object literals of triples can be stored (during triple insert/update)
into the index to extend query capabilities.
For that, the text:langField property must be set in the EntityMap assembler :
<#entMap> a text:EntityMap ;
    text:entityField      "uri" ;
    text:defaultField     "text" ;        
    text:langField        "lang" ;       
    . 

If you configure the index via Java code, you need to set this parameter to the
EntityDefinition instance, e.g.
EntityDefinition docDef = new EntityDefinition(entityField, defaultField);
docDef.setLangField("lang");

Note that configuring the text:langField does not determine a language specific
analyzer. It merely records the tag associated with an indexed rdfs:langString.
SPARQL Linguistic Clause Forms
Once the langField is set, you can use it directly inside SPARQL queries. For that the lang:xx
argument allows you to target specific localized values. For example:
//target english literals
?s text:query (rdfs:label 'word' 'lang:en' ) 

//target unlocalized literals
?s text:query (rdfs:label 'word' 'lang:none') 

//ignore language field
?s text:query (rdfs:label 'word')

Refer above for further discussion on querying.
LocalizedAnalyzer
You can specify a LocalizedAnalyzer in order to benefit from Lucene language
specific analyzers (stemming, stop words,…). Like any other analyzers, it can
be done for default text indexing, for each different field or for query.
Using an assembler configuration, the text:language property needs to
be provided, e.g :
<#indexLucene> a text:TextIndexLucene ;
    text:directory <file:Lucene> ;
    text:entityMap <#entMap> ;
    text:analyzer [
        a text:LocalizedAnalyzer ;
        text:language "fr"
    ]
    .

will configure the index to analyze values of the default property field using a
FrenchAnalyzer.
To configure the same example via Java code, you need to provide the analyzer to the
index configuration object:
    TextIndexConfig config = new TextIndexConfig(def);
    Analyzer analyzer = Util.getLocalizedAnalyzer("fr");
    config.setAnalyzer(analyzer);
    Dataset ds = TextDatasetFactory.createLucene(ds1, dir, config) ;

Where def, ds1 and dir are instances of EntityDefinition, Dataset and
Directory classes.
Note: You do not have to set the text:langField property with a single
localized analyzer. Also note that the above configuration will use the
FrenchAnalyzer for all strings indexed under the default property regardless
of the language tag associated with the literal (if any).
Multilingual Support
Let us suppose that we have many triples with many localized literals in
many different languages. It is possible to take all these languages
into account for future mixed localized queries.  Configure the
text:multilingualSupport property to enable indexing and search via localized
analyzers based on the language tag:
<#indexLucene> a text:TextIndexLucene ;
    text:directory "mem" ;
    text:multilingualSupport true;     
    .

Via Java code, set the multilingual support flag :
    TextIndexConfig config = new TextIndexConfig(def);
    config.setMultilingualSupport(true);
    Dataset ds = TextDatasetFactory.createLucene(ds1, dir, config) ;

This multilingual index combines dynamically all localized analyzers of existing
languages and the storage of langField properties.
The multilingual analyzer becomes the default analyzer and the Lucene
StandardAnalyzer is the default analyzer used when there is no language tag.
It is straightforward to refer to different languages in the same text search query:
SELECT ?s
WHERE {
    { ?s text:query ( rdfs:label 'institut' 'lang:fr' ) }
    UNION
    { ?s text:query ( rdfs:label 'institute' 'lang:en' ) }
}

Hence, the result set of the query will contain “institute” related
subjects (institution, institutional,…) in French and in English.
Note When multilingual indexing is enabled for a property, e.g., rdfs:label,
there will actually be two copies of each literal indexed. One under the Field name,
“label”, and one under the name “label_xx”, where “xx” is the language tag.
Generic and Defined Analyzer Support
There are many Analyzers that do not have built-in support, e.g.,
BrazilianAnalyzer; require constructor parameters not otherwise
supported, e.g., a stop words FileReader or a stemExclusionSet; or
make use of Analyzers not included in the bundled Lucene distribution,
e.g., a SanskritIASTAnalyzer. Two features have been added to enhance
the utility of jena-text: 1) text:GenericAnalyzer; and 2)
text:DefinedAnalyzer. Further, since Jena 3.7.0, features to allow definition of
tokenizers and filters are included.
Generic Analyzers, Tokenizers and Filters
A text:GenericAnalyzer includes a text:class which is the fully
qualified class name of an Analyzer that is accessible on the jena
classpath. This is trivial for Analyzer classes that are included in the
bundled Lucene distribution and for other custom Analyzers a simple
matter of including a jar containing the custom Analyzer and any
associated Tokenizer and Filters on the classpath.
Similarly, text:GenericTokenizer and text:GenericFilter allow to access any tokenizers
or filters that are available on the Jena classpath. These two types are used only to define
tokenizer and filter configurations that may be referred to when specifying a
ConfigurableAnalyzer.
In addition to the text:class it is generally useful to include an
ordered list of text:params that will be used to select an appropriate
constructor of the Analyzer class. If there are no text:params in the
analyzer specification or if the text:params is an empty list then the
nullary constructor is used to instantiate the analyzer. Each element of
the list of text:params includes:

an optional text:paramName of type Literal that is useful to identify the purpose of a
parameter in the assembler configuration
a text:paramType which is one of:


  
      
           Type 
            Description 
      
  
  
      
          text:TypeAnalyzer
          a subclass of org.apache.lucene.analysis.Analyzer
      
      
          text:TypeBoolean
          a java boolean
      
      
          text:TypeFile
          the String path to a file materialized as a java.io.FileReader
      
      
          text:TypeInt
          a java int
      
      
          text:TypeString
          a java String
      
      
          text:TypeSet
          an org.apache.lucene.analysis.CharArraySet
      
  

and is required for the types text:TypeAnalyzer, text:TypeFile and text:TypeSet, but,
since Jena 3.7.0, may be implied by the form of the literal for the types: text:TypeBoolean,
text:TypeInt and text:TypeString.

a required text:paramValue with an object of the type corresponding to text:paramType

In the case of an analyzer parameter the text:paramValue is any text:analyzer resource as
describe throughout this document.
An example of the use of text:GenericAnalyzer to configure an EnglishAnalyzer with stop
words and stem exclusions is:
text:map (
     [ text:field "text" ; 
       text:predicate rdfs:label;
       text:analyzer [
           a text:GenericAnalyzer ;
           text:class "org.apache.lucene.analysis.en.EnglishAnalyzer" ;
           text:params (
                [ text:paramName "stopwords" ;
                  text:paramType text:TypeSet ;
                  text:paramValue ("the" "a" "an") ]
                [ text:paramName "stemExclusionSet" ;
                  text:paramType text:TypeSet ;
                  text:paramValue ("ing" "ed") ]
                )
       ] .

Here is an example of defining an instance of ShingleAnalyzerWrapper:
text:map (
     [ text:field "text" ; 
       text:predicate rdfs:label;
       text:analyzer [
           a text:GenericAnalyzer ;
           text:class "org.apache.lucene.analysis.shingle.ShingleAnalyzerWrapper" ;
           text:params (
                [ text:paramName "defaultAnalyzer" ;
                  text:paramType text:TypeAnalyzer ;
                  text:paramValue [ a text:SimpleAnalyzer ] ]
                [ text:paramName "maxShingleSize" ;
                  text:paramType text:TypeInt ;
                  text:paramValue 3 ]
                )
       ] .

If there is need of using an analyzer with constructor parameter types not included here then
one approach is to define an AnalyzerWrapper that uses available parameter types, such as
file, to collect the information needed to instantiate the desired analyzer. An example of
such an analyzer is the Kuromoji morphological analyzer for Japanese text that uses constructor
parameters of types: UserDictionary, JapaneseTokenizer.Mode, CharArraySet and Set<String>.
As mentioned above, the simple types: TypeInt, TypeBoolean, and TypeString may be written
without explicitly including text:paramType in the parameter specification. For example:
                [ text:paramName "maxShingleSize" ;
                  text:paramValue 3 ]

is sufficient to specify the parameter.
Defined Analyzers
The text:defineAnalyzers feature allows to extend the Multilingual Support
defined above. Further, this feature can also be used to name analyzers defined via text:GenericAnalyzer
so that a single (perhaps complex) analyzer configuration can be used is several places.
Further, since Jena 3.7.0, this feature is also used to name tokenizers and filters that
can be referred to in the specification of a ConfigurableAnalyzer.
The text:defineAnalyzers is used with text:TextIndexLucene to provide a list of analyzer
definitions:
<#indexLucene> a text:TextIndexLucene ;
    text:directory <file:Lucene> ;
    text:entityMap <#entMap> ;
    text:defineAnalyzers (
        [ text:addLang "sa-x-iast" ;
          text:analyzer [ . . . ] ]
        [ text:defineAnalyzer <#foo> ;
          text:analyzer [ . . . ] ]
    )
    .

References to a defined analyzer may be made in the entity map like:
text:analyzer [
    a text:DefinedAnalyzer
    text:useAnalyzer <#foo> ]

Since Jena 3.7.0, a ConfigurableAnalyzer specification can refer to any defined tokenizer
and filters, as in:
text:defineAnalyzers (
     [ text:defineAnalyzer :configuredAnalyzer ;
       text:analyzer [
            a text:ConfigurableAnalyzer ;
            text:tokenizer :ngram ;
            text:filters ( :asciiff text:LowerCaseFilter ) ] ]
     [ text:defineTokenizer :ngram ;
       text:tokenizer [
            a text:GenericTokenizer ;
            text:class "org.apache.lucene.analysis.ngram.NGramTokenizer" ;
            text:params (
                 [ text:paramName "minGram" ;
                   text:paramValue 3 ]
                 [ text:paramName "maxGram" ;
                   text:paramValue 7 ]
                 ) ] ]
     [ text:defineFilter :asciiff ;
       text:filter [
            a text:GenericFilter ;
            text:class "org.apache.lucene.analysis.miscellaneous.ASCIIFoldingFilter" ;
            text:params (
                 [ text:paramName "preserveOriginal" ;
                   text:paramValue true ]
                 ) ] ]
     ) ;

And after 3.8.0 users are able to use the JenaText custom filter SelectiveFoldingFilter.
This filter is not part of the Apache Lucene, but rather a custom implementation available
for JenaText users.
It is based on the Apache Lucene’s ASCIIFoldingFilter, but with the addition of a
white-list for characters that must not be replaced. This is especially useful for languages
where some special characters and diacritical marks are useful when searching.
Here’s an example:
text:defineAnalyzers (
     [ text:defineAnalyzer :configuredAnalyzer ;
       text:analyzer [
            a text:ConfigurableAnalyzer ;
            text:tokenizer :tokenizer ;
            text:filters ( :selectiveFoldingFilter text:LowerCaseFilter ) ] ]
     [ text:defineTokenizer :tokenizer ;
       text:tokenizer [
            a text:GenericTokenizer ;
            text:class "org.apache.lucene.analysis.core.LowerCaseTokenizer" ] ]
     [ text:defineFilter :selectiveFoldingFilter ;
       text:filter [
            a text:GenericFilter ;
            text:class "org.apache.jena.query.text.filter.SelectiveFoldingFilter" ;
            text:params (
                 [ text:paramName "whitelisted" ;
                   text:paramType text:TypeSet ;
                   text:paramValue ("ç" "ä") ]
                 ) ] ]
     ) ;

Extending multilingual support
The Multilingual Support described above allows for a limited set of
ISO 2-letter codes to be used to select from among built-in analyzers using the nullary constructor
associated with each analyzer. So if one is wanting to use:

a language not included, e.g., Brazilian; or
use additional constructors defining stop words, stem exclusions and so on; or
refer to custom analyzers that might be associated with generalized BCP-47 language tags,
such as, sa-x-iast for Sanskrit in the IAST transliteration,

then text:defineAnalyzers with text:addLang will add the desired analyzers to the
multilingual support so that fields with the appropriate language tags will use the appropriate
custom analyzer.
When text:defineAnalyzers is used with text:addLang then text:multilingualSupport is
implicitly added if not already specified and a warning is put in the log:
    text:defineAnalyzers (
        [ text:addLang "sa-x-iast" ;
          text:analyzer [ . . . ] ]

this adds an analyzer to be used when the text:langField has the value sa-x-iast during
indexing and search.
Multilingual enhancements for multi-encoding searches
There are two multilingual search situations that are supported as of 3.8.0:

Search in one encoding and retrieve results that may have been entered in other encodings. For example, searching via Simplified Chinese (Hans) and retrieving results that may have been entered in Traditional Chinese (Hant) or Pinyin. This will simplify applications by permitting encoding independent retrieval without additional layers of transcoding and so on. It’s all done under the covers in Lucene.
Search with queries entered in a lossy, e.g., phonetic, encoding and retrieve results entered with accurate encoding. For example, searching via Pinyin without diacritics and retrieving all possible Hans and Hant triples.

The first situation arises when entering triples that include languages with multiple encodings that for various reasons are not normalized to a single encoding. In this situation it is helpful to be able to retrieve appropriate result sets without regard for the encodings used at the time that the triples were inserted into the dataset.
There are several suchlanguages of interest: Chinese, Tibetan, Sanskrit, Japanese and Korean. There are various Romanizations and ideographic variants.
Encodings may not be normalized when inserting triples for a variety of reasons. A principle one is that the rdf:langString object often must be entered in the same encoding that it occurs in some physical text that is being catalogued. Another is that metadata may be imported from sources that use different encoding conventions and it is desirable to preserve the original form.
The second situation arises to provide simple support for phonetic or other forms of lossy search at the time that triples are indexed directly in the Lucene system.
To handle the first situation a text assembler predicate, text:searchFor, is introduced that specifies a list of language tags that provides a list of language variants that should be searched whenever a query string of a given encoding (language tag) is used. For example, the following text:defineAnalyzers fragment :
    [ text:addLang "bo" ; 
      text:searchFor ( "bo" "bo-x-ewts" "bo-alalc97" ) ;
      text:analyzer [ 
        a text:GenericAnalyzer ;
        text:class "io.bdrc.lucene.bo.TibetanAnalyzer" ;
        text:params (
            [ text:paramName "segmentInWords" ;
              text:paramValue false ]
            [ text:paramName "lemmatize" ;
              text:paramValue true ]
            [ text:paramName "filterChars" ;
              text:paramValue false ]
            [ text:paramName "inputMode" ;
              text:paramValue "unicode" ]
            [ text:paramName "stopFilename" ;
              text:paramValue "" ]
            )
        ] ; 
      ]

indicates that when using a search string such as “རྡོ་རྗེ་སྙིང་"@bo the Lucene index should also be searched for matches tagged as bo-x-ewts and bo-alalc97.
This is made possible by a Tibetan Analyzer that tokenizes strings in all three encodings into Tibetan Unicode. This is feasible since the bo-x-ewts and bo-alalc97 encodings are one-to-one with Unicode Tibetan. Since all fields with these language tags will have a common set of indexed terms, i.e., Tibetan Unicode, it suffices to arrange for the query analyzer to have access to the language tag for the query string along with the various fields that need to be considered.
Supposing that the query is:
(?s ?sc ?lit) text:query ("rje"@bo-x-ewts)

Then the query formed in TextIndexLucene will be:
label_bo:rje label_bo-x-ewts:rje label_bo-alalc97:rje

which is translated using a suitable Analyzer, QueryMultilingualAnalyzer, via Lucene’s QueryParser to:
+(label_bo:རྗེ label_bo-x-ewts:རྗེ label_bo-alalc97:རྗེ)

which reflects the underlying Tibetan Unicode term encoding. During IndexSearcher.search all documents with one of the three fields in the index for term, “རྗེ”, will be returned even though the value in the fields label_bo-x-ewts and label_bo-alalc97 for the returned documents will be the original value “rje”.
This support simplifies applications by permitting encoding independent retrieval without additional layers of transcoding and so on. It’s all done under the covers in Lucene.
Solving the second situation simplifies applications by adding appropriate fields and indexing via configuration in the text:defineAnalyzers. For example, the following fragment:
    [ text:defineAnalyzer :hanzAnalyzer ; 
      text:analyzer [ 
        a text:GenericAnalyzer ;
        text:class "io.bdrc.lucene.zh.ChineseAnalyzer" ;
        text:params (
            [ text:paramName "profile" ;
              text:paramValue "TC2SC" ]
            [ text:paramName "stopwords" ;
              text:paramValue false ]
            [ text:paramName "filterChars" ;
              text:paramValue 0 ]
            )
        ] ; 
      ]  
    [ text:defineAnalyzer :han2pinyin ; 
      text:analyzer [ 
        a text:GenericAnalyzer ;
        text:class "io.bdrc.lucene.zh.ChineseAnalyzer" ;
        text:params (
            [ text:paramName "profile" ;
              text:paramValue "TC2PYstrict" ]
            [ text:paramName "stopwords" ;
              text:paramValue false ]
            [ text:paramName "filterChars" ;
              text:paramValue 0 ]
            )
        ] ; 
      ]
    [ text:defineAnalyzer :pinyin ; 
      text:analyzer [ 
        a text:GenericAnalyzer ;
        text:class "io.bdrc.lucene.zh.ChineseAnalyzer" ;
        text:params (
            [ text:paramName "profile" ;
              text:paramValue "PYstrict" ]
            )
        ] ; 
      ]
    [ text:addLang "zh-hans" ; 
      text:searchFor ( "zh-hans" "zh-hant" ) ;
      text:auxIndex ( "zh-aux-han2pinyin" ) ;
      text:analyzer [
        a text:DefinedAnalyzer ;
        text:useAnalyzer :hanzAnalyzer ] ; 
      ]
    [ text:addLang "zh-hant" ; 
      text:searchFor ( "zh-hans" "zh-hant" ) ;
      text:auxIndex ( "zh-aux-han2pinyin" ) ;
      text:analyzer [
        a text:DefinedAnalyzer ;
        text:useAnalyzer :hanzAnalyzer ] ; 
      ]
    [ text:addLang "zh-latn-pinyin" ;
      text:searchFor ( "zh-latn-pinyin" "zh-aux-han2pinyin" ) ;
      text:analyzer [
        a text:DefinedAnalyzer ;
        text:useAnalyzer :pinyin ] ; 
      ]        
    [ text:addLang "zh-aux-han2pinyin" ;
      text:searchFor ( "zh-latn-pinyin" "zh-aux-han2pinyin" ) ;
      text:analyzer [
        a text:DefinedAnalyzer ;
        text:useAnalyzer :pinyin ] ; 
      text:indexAnalyzer :han2pinyin ; 
      ]

defines language tags for Traditional, Simplified, Pinyin and an auxiliary tag zh-aux-han2pinyin associated with an Analyzer, :han2pinyin. The purpose of the auxiliary tag is to define an Analyzer that will be used during indexing and to specify a list of tags that should be searched when the auxiliary tag is used with a query string.
Searching is then done via the multi-encoding support discussed above. In this example the Analyzer, :han2pinyin, tokenizes strings in zh-hans and zh-hant as the corresponding pinyin so that at search time a pinyin query will retrieve appropriate triples inserted in Traditional or Simplified Chinese. Such a query would appear as:
(?s ?sc ?lit ?g) text:query ("jīng"@zh-aux-han2pinyin)

The auxiliary field support is needed to accommodate situations such as pinyin or sound-ex which are not exact, i.e., one-to-many rather than one-to-one as in the case of Simplified and Traditional.
TextIndexLucene adds a field for each of the auxiliary tags associated with the tag of the triple object being indexed. These fields are in addition to the un-tagged field and the field tagged with the language of the triple object literal.
Naming analyzers for later use
Repeating a text:GenericAnalyzer specification for use with multiple fields in an entity map
may be cumbersome. The text:defineAnalyzer is used in an element of a text:defineAnalyzers
list to associate a resource with an analyzer so that it may be referred to later in a
text:analyzer object. Assuming that an analyzer definition such as the following has appeared
among the text:defineAnalyzers list:
[ text:defineAnalyzer <#foo>
  text:analyzer [ . . . ] ]

then in a text:analyzer specification in an entity map, for example, a reference to analyzer <#foo>
is made via:
text:map (
     [ text:field "text" ; 
       text:predicate rdfs:label;
       text:analyzer [
           a text:DefinedAnalyzer
           text:useAnalyzer <#foo> ]

This makes it straightforward to refer to the same (possibly complex) analyzer definition in multiple fields.
Storing Literal Values
New in Jena 3.0.0.
It is possible to configure the text index to store enough information in the
text index to be able to access the original indexed literal values at query time.
This is controlled by two configuration options. First, the text:storeValues property
must be set to true for the text index:
<#indexLucene> a text:TextIndexLucene ;
    text:directory "mem" ;
    text:storeValues true;     
    .

Or using Java code, used the setValueStored method of TextIndexConfig:
    TextIndexConfig config = new TextIndexConfig(def);
    config.setValueStored(true);

Additionally, setting the langField configuration option is recommended. See
Linguistic Support with Lucene Index
for details. Without the langField setting, the stored literals will not have
language tag or datatype information.
At query time, the stored literals can be accessed by using a 3-element list
of variables as the subject of the text:query property function. The literal
value will be bound to the third variable:
(?s ?score ?literal) text:query 'word'

Working with Fuseki
The Fuseki configuration simply points to the text dataset as the
fuseki:dataset of the service.
<#service_text_tdb> rdf:type fuseki:Service ;
    rdfs:label                      "TDB/text service" ;
    fuseki:name                     "ds" ;
    fuseki:serviceQuery             "query" ;
    fuseki:serviceQuery             "sparql" ;
    fuseki:serviceUpdate            "update" ;
    fuseki:serviceReadGraphStore    "get" ;
    fuseki:serviceReadWriteGraphStore    "data" ;
    fuseki:dataset                  :text_dataset ;
    .

Building a Text Index
When working at scale, or when preparing a published, read-only, SPARQL
service, creating the index by loading the text dataset is impractical.
The index and the dataset can be built using command line tools in two
steps: first load the RDF data, second create an index from the existing
RDF dataset.
Step 1 - Building a TDB dataset
Note: If you have an existing TDB dataset then you can skip this step
Build the TDB dataset:
java -cp $FUSEKI_HOME/fuseki-server.jar tdb.tdbloader --tdb=assembler_file data_file

using the copy of TDB included with Fuseki.
Alternatively, use one of the
TDB utilities tdbloader or tdbloader2 which are better for bulk loading:
$JENA_HOME/bin/tdbloader --loc=directory  data_file

Step 2 - Build the Text Index
You can then build the text index with the jena.textindexer tool:
java -cp $FUSEKI_HOME/fuseki-server.jar jena.textindexer --desc=assembler_file

Because a Fuseki assembler description can have several datasets descriptions,
and several text indexes, it may be necessary to extract a single dataset and index description
into a separate assembler file for use in loading.
Updating the index
If you allow updates to the dataset through Fuseki, the configured index
will automatically be updated on every modification.  This means that you
do not have to run the above mentioned jena.textindexer after updates,
only when you want to rebuild the index from scratch.
Configuring Alternative TextDocProducers
Default Behavior
The default behavior when performing text indexing
is to index a single property as a single field, generating a different Document
for each indexed triple. This behavior may be augmented by
writing and configuring an alternative TextDocProducer.
Please note that TextDocProducer.change(...) is called once for each triple that is
ADDed or DELETEd, and thus can not be directly used to accumulate multiple properties
for use in composing a single multi-fielded Lucene document. See below.
To configure a TextDocProducer, say net.code.MyProducer in a dataset assembly,
use the property textDocProducer, eg:
<#ds-with-lucene> rdf:type text:TextDataset;
	text:index <#indexLucene> ;
	text:dataset <#ds> ;
	text:textDocProducer <java:net.code.MyProducer> ;
	.

where CLASSNAME is the full java class name. It must have either
a single-argument constructor of type TextIndex, or a two-argument
constructor (DatasetGraph, TextIndex). The TextIndex argument
will be the configured text index, and the DatasetGraph argument
will be the graph of the configured dataset.
For example, to explicitly create the default TextDocProducer use:
...
    text:textDocProducer <java:org.apache.jena.query.text.TextDocProducerTriples> ;
...

TextDocProducerTriples produces a new document for each subject/field
added to the dataset, using TextIndex.addEntity(Entity).
Example
The example class below is a TextDocProducer that only indexes
ADDs of quads for which the subject already had at least one
property-value. It uses the two-argument constructor to give it
access to the dataset so that it count the (?G, S, P, ?O) quads
with that subject and predicate, and delegates the indexing to
TextDocProducerTriples if there are at least two values for
that property (one of those values, of course, is the one that
gives rise to this change()).
  public class Example extends TextDocProducerTriples {
  
      final DatasetGraph dg;
      
      public Example(DatasetGraph dg, TextIndex indexer) {
          super(indexer);
          this.dg = dg;
      }
      
      public void change(QuadAction qaction, Node g, Node s, Node p, Node o) {
          if (qaction == QuadAction.ADD) {
              if (alreadyHasOne(s, p)) super.change(qaction, g, s, p, o);
          }
      }
  
      private boolean alreadyHasOne(Node s, Node p) {
          int count = 0;
          Iterator<Quad> quads = dg.find( null, s, p, null );
          while (quads.hasNext()) { quads.next(); count += 1; }
          return count > 1;
      }
  }

Multiple fields per document
In principle it should be possible to extend Jena to allow for creating documents with
multiple searchable fields by extending org.apache.jena.sparql.core.DatasetChangesBatched
such as with org.apache.jena.query.text.TextDocProducerEntities; however, this form of
extension is not currently (Jena 3.13.1) functional.
Maven Dependency
The jena-text module is included in Fuseki.  To use it within application code,
then use the following maven dependency:
<dependency>
  <groupId>org.apache.jena</groupId>
  <artifactId>jena-text</artifactId>
  <version>X.Y.Z</version>
</dependency>

adjusting the version X.Y.Z as necessary.  This will automatically
include a compatible version of Lucene.
For Elasticsearch implementation, you can include the following Maven Dependency:
<dependency>
  <groupId>org.apache.jena</groupId>
  <artifactId>jena-text-es</artifactId>
  <version>X.Y.Z</version>
</dependency>

adjusting the version X.Y.Z as necessary.\n\nOn this page
    
  
    Architecture
      
        One triple equals one document
        One document equals one entity
          
            External content
          
        
        External applications
        Document structure
      
    
    Query with SPARQL
      
        Syntax
          
            Input arguments:
            Output arguments:
          
        
        Query strings
          
            Simple queries
            Queries with language tags
            Queries that retrieve literals
            Queries with graphs
            Queries across multiple Fields
            Queries with Boolean Operators and Term Modifiers
            Highlighting
          
        
        Good practice
          
            Query pattern 1 – Find in the text index and refine results
            Query pattern 2 – Filter results via the text index
          
        
      
    
    Configuration
      
        Text Dataset Assembler
          
            Lists of Indexed Properties
          
        
        Entity Map definition
          
            Default text field
            Entity field
            UID Field and automatic document deletion
            Language Field
            Graph Field
            The Analyzer Map
          
        
        Configuring an Analyzer
          
            ConfigurableAnalyzer
            Analyzer for Query
            Alternative Query Parsers
          
        
        Configuration by Code
        Graph-specific Indexing
        Linguistic support with Lucene index
          
            Explicit Language Field in the Index
            SPARQL Linguistic Clause Forms
            LocalizedAnalyzer
            Multilingual Support
          
        
        Generic and Defined Analyzer Support
          
            Generic Analyzers, Tokenizers and Filters
            Defined Analyzers
            Extending multilingual support
            Multilingual enhancements for multi-encoding searches
            Naming analyzers for later use
          
        
        Storing Literal Values
      
    
    Working with Fuseki
    Building a Text Index
      
        Step 1 - Building a TDB dataset
        Step 2 - Build the Text Index
          
            Updating the index
          
        
      
    
  

  
    Default Behavior
      
        Example
      
    
    Multiple fields per document
    Maven Dependency
  

  
  
    This extension to ARQ combines SPARQL and full text search via
Lucene.
It gives applications the ability to perform indexed full text
searches within SPARQL queries. Here is a version compatibility table:

  
      
           Jena 
           Lucene 
           Solr 
           ElasticSearch 
      
  
  
      
          upto 3.2.0
          5.x or 6.x
          5.x or 6.x
          not supported
      
      
          3.3.0 - 3.9.0
          6.4.x
          not supported
          5.2.2 - 5.2.13
      
      
          3.10.0
          7.4.0
          not supported
          6.4.2
      
      
          3.15.0 - 3.17.0
          7.7.x
          not supported
          6.8.6
      
      
          4.0.0 - 4.6.1
          8.8.x
          not supported
          not supported
      
      
          4.7.0 - current
          9.4.x
          not supported
          not supported
      
  

Note: In Lucene 9, the default setup of the StandardAnalyzer changed to having
no stop words. For more details, see analyzer specifications below.
SPARQL allows the use of
regex
in FILTERs which is a test on a value retrieved earlier in the query
so its use is not indexed. For example, if you’re
searching for occurrences of "printer" in the rdfs:label of a bunch
of products:
PREFIX   ex: <http://www.example.org/resources#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?s ?lbl
WHERE { 
  ?s a ex:Product ;
     rdfs:label ?lbl
  FILTER regex(?lbl, "printer", "i")
}
then the search will need to examine all selected rdfs:label
statements and apply the regular expression to each label in turn. If
there are many such statements and many such uses of regex, then it
may be appropriate to consider using this extension to take advantage of
the performance potential of full text indexing.
Text indexes provide additional information for accessing the RDF graph
by allowing the application to have indexed access to the internal
structure of string literals rather than treating such literals as
opaque items.  Unlike FILTER, an index can set the values of variables.
Assuming appropriate configuration, the
above query can use full text search via the
ARQ property function extension, text:query:
PREFIX   ex: <http://www.example.org/resources#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX text: <http://jena.apache.org/text#>

SELECT ?s ?lbl
WHERE { 
	?s a ex:Product ;
	   text:query (rdfs:label 'printer') ;
	   rdfs:label ?lbl
}

This query makes a text query for 'printer' on the rdfs:label
property; and then looks in the RDF data and retrieves the complete
label for each match.
The full text engine can be either Apache
Lucene hosted with Jena on a single
machine, or Elasticsearch for a large scale
enterprise search application where the full text engine is potentially
distributed across separate machines.
This example code
illustrates creating an in-memory dataset with a Lucene index.
Architecture
In general, a text index engine (Lucene or Elasticsearch) indexes
documents where each document is a collection of fields, the values
of which are indexed so that searches matching contents of specified
fields can return a reference to the document containing the fields with
matching values.
There are two models for extending Jena with text indexing and search:

One Jena triple equals one Lucene document
One Lucene document equals one Jena entity

One triple equals one document
The basic Jena text extension associates a triple with
a document and the property of the triple with a field of a document
and the object of the triple (which must be a literal) with the value
of the field in the document. The subject of the triple then becomes
another field of the document that is returned as the result of a search
match to identify what was matched. (NB, the particular triple that
matched is not identified. Only, its subject and optionally the matching
literal and match score.)
In this manner, the text index provides an inverted index that maps
query string matches to subject URIs.
A text-indexed dataset is configured with a description of which
properties are to be indexed. When triples are added, any properties
matching the description cause a document to be added to the index by
analyzing the literal value of the triple object and mapping to the
subject URI. On the other hand, it is necessary to specifically
configure the text-indexed dataset to delete index
entries when the corresponding triples are
dropped from the RDF store.
The text index uses the native query language of the index:
Lucene query language
(with restrictions)
or
Elasticsearch query language.
One document equals one entity
There are two approaches to creating indexed documents that contain more
than one indexed field:

Using an externally maintained Lucene index
Multiple fields per document

When using this integration model, text:query returns the subject URI
for the document on which additional triples of metadata may be associated,
and optionally the Lucene score for the match.
External content
When document content is externally indexed via Lucene and accessed in Jena
via a text:TextDataset then the subject URI returned for a search result
is considered to refer to the external content, and metadata about the
document is represented as triples in Jena with the subject URI.
There is no requirement that the indexed document content be present
in the RDF data.  As long as the index contains the index text documents to
match the index description, then text search can be performed with queries that explicitly mention indexed fields in the document.
That is, if the content of a collection of documents is externally indexed
and the URI naming the document is the result of the text search, then an RDF
dataset with the document metadata can be combined with accessing the
content by URI.
The maintenance of the index is external to the RDF data store.
External applications
By using Elasticsearch, other applications can share the text index with
SPARQL search.
Document structure
As mentioned above, when using the (default) one-triple equals one-document model,
text indexing of a triple involves associating a Lucene document with the triple.
How is this done?
Lucene documents are composed of Fields. Indexing and searching are performed
over the contents of these Fields. For an RDF triple to be indexed in Lucene the
property of the triple must be
configured in the entity map of a TextIndex.
This associates a Lucene analyzer with the property which will be used
for indexing and search. The property becomes the searchable Lucene
Field in the resulting document.
A Lucene index includes a default Field, which is specified in the configuration,
that is the field to search if not otherwise named in the query. In jena-text
this field is configured via the text:defaultField property which is then mapped
to a specific RDF property via text:predicate (see entity map
below).
There are several additional Fields that will be included in the
document that is passed to the Lucene IndexWriter depending on the
configuration options that are used. These additional fields are used to
manage the interface between Jena and Lucene and are not generally
searchable per se.
The most important of these additional Fields is the text:entityField.
This configuration property defines the name of the Field that will contain
the URI or blank node id of the subject of the triple being indexed. This property does
not have a default and must be specified for most uses of jena-text. This
Field is often given the name, uri, in examples. It is via this Field
that ?s is bound in a typical use such as:
select ?s
where {
    ?s text:query "some text"
}

Other Fields that may be configured: text:uidField, text:graphField,
and so on are discussed below.
Given the triple:
ex:SomeOne skos:prefLabel "zorn protégé a prés"@fr ;

The following is an abbreviated illustration a Lucene document that Jena will create and
request Lucene to index:
Document<
    <uri:http://example.org/SomeOne> 
    <graph:urn:x-arq:DefaultGraphNode> 
    <label:zorn protégé a prés> 
    <lang:fr> 
    <uid:28959d0130121b51e1459a95bdac2e04f96efa2e6518ff3c090dfa7a1e6dcf00> 
    >

It may be instructive to refer back to this example when considering the various
points below.
Query with SPARQL
The URI of the text extension property function is
http://jena.apache.org/text#query more conveniently written:
PREFIX text: <http://jena.apache.org/text#>

...   text:query ...

Syntax
The following forms are all legal:
?s text:query 'word'                              # query
?s text:query ('word' 10)                         # with limit on results
?s text:query (rdfs:label 'word')                 # query specific property if multiple
?s text:query (rdfs:label 'protégé' 'lang:fr')    # restrict search to French
(?s ?score) text:query 'word'                     # query capturing also the score
(?s ?score ?literal) text:query 'word'            # ... and original literal value
(?s ?score ?literal ?g) text:query 'word'         # ... and the graph

The most general form when using the default one-triple equals one-document
integration model is:
 ( ?s ?score ?literal ?g ) text:query ( property* 'query string' limit 'lang:xx' 'highlight:yy' )

while for the one-document equals one-entity model, the general form is:
 ( ?s ?score ) text:query ( 'query string' limit )

and if only the subject URI is needed:
 ?s text:query ( 'query string' limit )

Input arguments:

  
      
           Argument 
            Definition 
      
  
  
      
          property
          (zero or more) property URIs (including prefix name form)
      
      
          query string
          Lucene query string fragment
      
      
          limit
          (optional) int limit on the number of results
      
      
          lang:xx
          (optional) language tag spec
      
      
          highlight:yy
          (optional) highlighting options
      
  

The property URI is only necessary if multiple properties have been
indexed and the property being searched over is not the default field
of the index.
Since 3.13.0, property may be a list of zero or more (prior to 3.13.0 zero or one) Lucene indexed properties, or a defined
text:propList of indexed properties.
The meaning is an OR of searches on a variety of properties. This can be used in place of SPARQL level UNIONs of
individual text:querys. For example, instead of:
select ?foo where {
  {
    (?s ?sc ?lit) text:query ( rdfs:label "some query" ).
  }
  union
  {
    (?s ?sc ?lit) text:query ( skos:altLabel "some query" ).
  }
  union
  { 
    (?s ?sc ?lit) text:query ( skos:prefLabel "some query" ).
  }
}

it can be more performant to push the unions into the Lucene query by rewriting as:
(?s ?sc ?lit) text:query ( rdfs:label skos:prefLabel skos:altLabel "some query" )

which creates a Lucene query:
(altLabel:"some query" OR prefLabel:"some query" OR label:"some query")

The query string syntax conforms to the underlying
Lucene,
or when appropriate,
Elasticsearch.
In the case of the default one-triple equals one-document model, the Lucene query syntax is restricted to Terms, Term modifiers,
Boolean Operators applied to Terms, and Grouping of terms.
Additionally, the use of Fields within the query string is supported when using the one-document equals one-entity text integration model.
When using the default model,
use of Fields in the query string will generally lead to unpredictable results.
The optional limit indicates the maximum hits to be returned by Lucene.
The lang:xx specification is an optional string, where xx is
a BCP-47 language tag. This restricts searches to field values that were originally
indexed with the tag xx. Searches may be restricted to field values with no
language tag via "lang:none".
The highlight:yy specification is an optional string where yy are options that control the highlighting of search
result literals. See below for details.
If both limit and one or more of lang:xx or highlight:yy are present, then limit must precede these arguments.
If only the query string is required, the surrounding ( ) may be omitted.
Output arguments:

  
      
           Argument 
            Definition 
      
  
  
      
          subject URI
          The subject of the indexed RDF triple.
      
      
          score
          (optional) The score for the match.
      
      
          literal
          (optional) The matched object literal.
      
      
          graph URI
          (optional) The graph URI of the triple.
      
      
          property URI
          (optional) The property URI of the matched triple
      
  

The results include the subject URI; the score assigned by the
text search engine; and the entire matched literal (if the index has
been configured to store literal values).
The subject URI may be a variable, e.g., ?s, or a URI. In the
latter case the search is restricted to triples with the specified
subject. The score, literal, graph URI, and property URI must be variables.
The property URI is meaningful when two or more properties are used in the query.
Query strings
There are several points that need to be considered when formulating
SPARQL queries using either of the Lucene integration models.
As mentioned above, in the case of the default model
the query string syntax is restricted to Terms, Term modifiers, Boolean Operators
applied to Terms, and Grouping of terms.
Explicit use of Fields in the query string is only useful with the
one-document equals one-entity model;
and otherwise will generally produce unexpected results.
See Queries across multiple Fields.
Simple queries
The simplest use of the jena-text Lucene integration is like:
?s text:query "some phrase"

This will bind ?s to each entity URI that is the subject of a triple
that has the default property and an object literal that matches
the argument string, e.g.:
ex:AnEntity skos:prefLabel "this is some phrase to match"

This query form will indicate the subjects that have literals that match
for the default property which is determined via the configuration of
the text:predicate of the text:defaultField
(in the above this has been assumed to be skos:prefLabel.
For a non-default property it is necessary to specify the property as
an input argument to the text:query:
?s text:query (rdfs:label "protégé")

(see below for how RDF property names
are mapped to Lucene Field names).
If this use case is sufficient for your needs you can skip on to the
sections on configuration.
Please note that the query:
?s text:query "some phrase"

when using the Lucene StandardAnalyzer or similar will treat the query string
as an OR of terms: some and phrase. If a phrase search is required then
it is necessary to surround the phrase by double quotes, ":
?s text:query "\"some phrase\""

This will only match strings that contain "some phrase", while the former
query will match strings like: "there is a phrase for some" or
"this is some of the various sorts of phrase that might be matched".
Queries with language tags
When working with rdf:langStrings it is necessary that the
text:langField has been configured. Then it is
as simple as writing queries such as:
?s text:query "protégé"@fr

to return results where the given term or phrase has been
indexed under French in the text:defaultField.
It is also possible to use the optional lang:xx argument, for example:
?s text:query ("protégé" 'lang:fr') .

In general, the presence of a language tag, xx, on the query string or
lang:xx in the text:query adds AND lang:xx to the query sent to Lucene,
so the above example becomes the following Lucene query:
"label:protégé AND lang:fr"

For non-default properties the general form is used:
?s text:query (skos:altLabel "protégé" 'lang:fr')

Note that an explicit language tag on the query string takes precedence
over the lang:xx, so the following
?s text:query ("protégé"@fr 'lang:none')

will find French matches rather than matches indexed without a language tag.
Queries that retrieve literals
It is possible to retrieve the literals that Lucene finds matches for
assuming that
<#TextIndex#> text:storeValues true ;

has been specified in the TextIndex configuration. So
(?s ?sc ?lit) text:query (rdfs:label "protégé")

will bind the matching literals to ?lit, e.g.,
"zorn protégé a prés"@fr

Note it is necessary to include a variable to capture the Lucene score
even if this value is not otherwise needed since the literal variable
is determined by position.
Queries with graphs
Assuming that the text:graphField has been configured,
then, when a triple is indexed, the graph that the triple resides in is
included in the document and may be used to restrict searches or to retrieve the graph that a matching triple resides in.
For example:
select ?s ?lit
where {
  graph ex:G2 { (?s ?sc ?lit) text:query "zorn" } .
}

will restrict searches to triples with the default property that reside
in graph, ex:G2.
On the other hand:
select ?g ?s ?lit
where {
  graph ?g { (?s ?sc ?lit) text:query "zorn" } .
}

will iterate over the graphs in the dataset, searching each in turn for
matches.
If there is suitable structure to the graphs, e.g., a known rdf:type and
depending on the selectivity of the text query and number of graphs,
it may be more performant to express the query as follows:
select ?g ?s ?lit
where {
  (?s ?sc ?lit) text:query "zorn" .
  graph ?g { ?s a ex:Item } .
}

Further, if tdb:unionDefaultGraph true for a TDB dataset backing a Lucene index then it is possible to retrieve the graphs that contain triples resulting from a Lucene search via the fourth output argument to text:query:
select ?g ?s ?lit
where {
  (?s ?sc ?lit ?g) text:query "zorn" .
}

This will generally perform much better than either of the previous approaches when there are
large numbers of graphs since the Lucene search will run once and the returned documents carry
the containing graph URI for free as it were.
Queries across multiple Fields
As mentioned earlier, the Lucene text index uses the
native Lucene query language.
Multiple fields in the default integration model
For the default integration model, since each document
has only one field containing searchable text, searching for documents containing
multiple fields will generally not find any results.
Note that the default model provides three Lucene Fields
in a document that are used during searching:

the field corresponding to the property of the indexed triple,
the field for the language of the literal (if configured), and
the graph that the triple is in (if configured).

Given these, it should be clear from the above that the
default model
constructs a Lucene query from the property, query string, lang:xx, and
SPARQL graph arguments.
For example, consider the following triples:
ex:SomePrinter 
    rdfs:label     "laser printer" ;
    ex:description "includes a large capacity cartridge" .

assuming an appropriate configuration, if we try to retrieve ex:SomePrinter
with the following Lucene query string:
?s text:query "label:printer AND description:\"large capacity cartridge\""

then this query can not find the expected results since the AND is interpreted
by Lucene to indicate that all documents that contain a matching label field and
a matching description field are to be returned; yet, from the discussion above
regarding the structure of Lucene documents in jena-text it
is evident that there is not one but rather in fact two separate documents one with a
label field and one with a description field so an effective SPARQL query is:
?s text:query (rdfs:label "printer") .
?s text:query (ex:description "large capacity cartridge") .

which leads to ?s being bound to ex:SomePrinter.
In other words when a query is to involve two or more properties of a given entity
then it is expressed at the SPARQL level, as it were, versus in Lucene’s query language.
It is worth noting that the equivalent of a Lucene OR of Fields can be expressed
using SPARQL union, though since 3.13.0 this can be expressed in Jena text
using a property list - see Input arguments:
{ ?s text:query (rdfs:label "printer") . }
union
{ ?s text:query (ex:description "large capacity cartridge") . }

Suppose the matching literals are required for the above then it should be clear
from the above that:
(?s ?sc1 ?lit1) text:query (skos:prefLabel "printer") .
(?s ?sc2 ?lit2) text:query (ex:description "large capacity cartridge") .

will be the appropriate form to retrieve the subject and the associated literals, ?lit1 and ?lit2. (Obviously, in general, the score variables, ?sc1 and ?sc2
must be distinct since it is very unlikely that the scores of the two Lucene queries
will ever match).
There is no loss of expressiveness of the Lucene query language versus the jena-text
integration of Lucene. Any cross-field ANDs are replaced by concurrent SPARQL calls to
text:query as illustrated above and uses of Lucene OR can be converted to SPARQL
unions. Uses of Lucene NOT are converted to appropriate SPARQL filters.
Multiple fields in the one-document equals one-entity model
If Lucene documents have been indexed with multiple searchable fields
then compound queries expressed directly in the Lucene query language can significantly improve search
performance, in particular, where the individual components of the Lucene query generate
a lot of results which must be combined in SPARQL.
It is possible to have text queries that search multiple fields within a text query.
Doing this is more complex as it requires the use of either an externally managed
text index or code must be provided to build the multi-field text documents to be indexed.
See Multiple fields per document.
Queries with Boolean Operators and Term Modifiers
On the other hand the various features of the Lucene query language
are all available to be used for searches within a Field.
For example, Boolean Operators on Terms:
?s text:query (ex:description "(large AND cartridge)")

and
(?s ?sc ?lit) text:query (ex:description "(includes AND (large OR capacity))")

or fuzzy searches:
?s text:query (ex:description "include~")

and so on will work as expected.
Always surround the query string with ( ) if more than a single term or phrase
are involved.
Highlighting
The highlighting option uses the Lucene Highlighter and SimpleHTMLFormatter to insert highlighting markup into the literals returned from search results (hence the text dataset must be configured to store the literals). The highlighted results are returned via the literal output argument. This highlighting feature, introduced in version 3.7.0, does not require re-indexing by Lucene.
The simplest way to request highlighting is via 'highlight:'. This will apply all the defaults:

  
      
           Option 
           Key 
           Default 
      
  
  
      
          maxFrags
          m:
          3
      
      
          fragSize
          z:
          128
      
      
          start
          s:
          RIGHT_ARROW
      
      
          end
          e:
          LEFT_ARROW
      
      
          fragSep
          f:
          DIVIDES
      
      
          joinHi
          jh:
          true
      
      
          joinFrags
          jf:
          true
      
  

to the highlighting of the search results. For example if the query is:
(?s ?sc ?lit) text:query ( "brown fox" "highlight:" ) 

then a resulting literal binding might be:
"the quick ↦brown fox↤ jumped over the lazy baboon"

The RIGHT_ARROW is Unicode \u21a6 and the LEFT_ARROW is Unicode \u21a4. These are chosen to be single characters that in most situations will be very unlikely to occur in resulting literals. The fragSize of 128 is chosen to be large enough that in many situations the matches will result in single fragments. If the literal is larger than 128 characters and there are several matches in the literal then there may be additional fragments separated by the DIVIDES, Unicode \u2223.
Depending on the analyzer used and the tokenizer, the highlighting will result in marking each token rather than an entire phrase. The joinHi option is by default true so that entire phrases are highlighted together rather than as individual tokens as in:
"the quick ↦brown↤ ↦fox↤ jumped over the lazy baboon"

which would result from:
(?s ?sc ?lit) text:query ( "brown fox" "highlight:jh:n" )

The jh and jf boolean options are set false via n. Any other value is true. The defaults for these options have been selected to be reasonable for most applications.
The joining is performed post highlighting via Java String replaceAll rather than using the Lucene Unified Highlighter facility which requires that term vectors and positions be stored. The joining deletes extra highlighting with only intervening Unicode separators, \p{Z}.
The more conventional output of the Lucene SimpleHTMLFormatter with html emphasis markup is achieved via, "highlight:s:<em class='hiLite'> | e:</em>" (highlight options are separated by a Unicode vertical line, \u007c. The spaces are not necessary). The result with the above example will be:
"the quick <em class='hiLite'>brown fox</em> jumped over the lazy baboon"

which would result from the query:
(?s ?sc ?lit) text:query ( "brown fox" "highlight:s:<em class='hiLite'> | e:</em>" )

Good practice
From the above it should be clear that best practice, except in the simplest cases
is to use explicit text:query forms such as:
(?s ?sc ?lit) text:query (ex:someProperty "a single Field query")

possibly with limit and lang:xx arguments.
Further, the query engine does not have information about the selectivity of the
text index and so effective query plans cannot be determined
programmatically.  It is helpful to be aware of the following two
general query patterns.
Query pattern 1 – Find in the text index and refine results
Access to the text index is first in the query and used to find a number of
items of interest; further information is obtained about these items from
the RDF data.
SELECT ?s
{ ?s text:query (rdfs:label 'word' 10) ; 
     rdfs:label ?label ;
     rdf:type   ?type 
}

The text:query limit argument is useful when working with large indexes to limit results to the
higher scoring results – results are returned in the order of scoring by the text search engine.
Query pattern 2 – Filter results via the text index
By finding items of interest first in the RDF data, the text search can be
used to restrict the items found still further.
SELECT ?s
{ ?s rdf:type     :book ;
     dc:creator  "John" .
  ?s text:query   (dc:title 'word') ; 
}

Configuration
The usual way to describe a text index is with a
Jena assembler description.  Configurations can
also be built with code. The assembler describes a ’text
dataset’ which has an underlying RDF dataset and a text index. The text
index describes the text index technology (Lucene or Elasticsearch) and the details
needed for each.
A text index has an “entity map” which defines the properties to
index, the name of the Lucene/Elasticsearch field and field used for storing the URI
itself.
For simple RDF use, there will be one field, mapping a property to a text
index field. More complex setups, with multiple properties per entity
(URI) are possible.
The assembler file can be either default configuration file (…/run/config.ttl)
or a custom file in …run/configuration folder. Note that you can use several files
simultaneously.
You have to edit the file (see comments in the assembler code below):

provide values for paths and a fixed URI for tdb:DatasetTDB
modify the entity map : add the fields you want to index and desired options (filters, tokenizers…)

If your assembler file is run/config.ttl, you can index the dataset with this command :
java -cp ./fuseki-server.jar jena.textindexer --desc=run/config.ttl

Once configured, any data added to the text dataset is automatically
indexed as well: Building a Text Index.
Text Dataset Assembler
The following is an example of an assembler file defining a TDB dataset with a Lucene text index.
######## Example of a TDB dataset and text index#########################
# The main doc sources are:
#  - https://jena.apache.org/documentation/fuseki2/fuseki-configuration.html
#  - https://jena.apache.org/documentation/assembler/assembler-howto.html
#  - https://jena.apache.org/documentation/assembler/assembler.ttl
# See https://jena.apache.org/documentation/fuseki2/fuseki-layout.html for the destination of this file.
#########################################################################

PREFIX :        <http://localhost/jena_example/#>
PREFIX rdf:     <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs:    <http://www.w3.org/2000/01/rdf-schema#>
PREFIX tdb:     <http://jena.hpl.hp.com/2008/tdb#>
PREFIX text:    <http://jena.apache.org/text#>
PREFIX skos:    <http://www.w3.org/2004/02/skos/core#>
PREFIX fuseki:  <http://jena.apache.org/fuseki#>

[] rdf:type fuseki:Server ;
   fuseki:services (
     :myservice
   ) .

:myservice rdf:type fuseki:Service ;
    # e.g : `s-query --service=http://localhost:3030/myds "select * ..."`
    fuseki:name               "myds" ;
    # SPARQL query service : /myds
    fuseki:endpoint [ 
        fuseki:operation fuseki:query ;
    ];
    # SPARQL query service : /myds/query
    fuseki:endpoint [ 
        fuseki:operation fuseki:query ;
        fuseki:name "query"
    ];
    # SPARQL update service : /myds/update
    fuseki:endpoint [
        fuseki:operation fuseki:update ;
        fuseki:name "update"
    ];
    # SPARQL Graph store protocol (read and write) : /myds/data
    fuseki:endpoint [
        fuseki:operation fuseki:gsp-rw ; 
        fuseki:name "data" 
    ];
    # The text-enabled dataset
    fuseki:dataset                    :text_dataset ;
    .

## ---------------------------------------------------------------

# A TextDataset is a regular dataset with a text index.
:text_dataset rdf:type     text:TextDataset ;
    text:dataset   :mydataset ; # <-- replace `:my_dataset` with the desired URI
    text:index     <#indexLucene> ;
.

# A TDB dataset used for RDF storage
:mydataset rdf:type      tdb:DatasetTDB ; # <-- replace `:my_dataset` with the desired URI - as above
    tdb:location "DB" ;
    tdb:unionDefaultGraph true ; # Optional
.

# Text index description
<#indexLucene> a text:TextIndexLucene ;
    text:directory <file:path> ;  # <-- replace `<file:path>` with your path (e.g., `<file:/.../fuseki/run/databases/MY_INDEX>`)
    text:entityMap <#entMap> ;
    text:storeValues true ; 
    text:analyzer [ a text:StandardAnalyzer ] ;
    text:queryAnalyzer [ a text:KeywordAnalyzer ] ;
    text:queryParser text:AnalyzingQueryParser ;
    text:propLists ( [ . . . ] . . . ) ;
    text:defineAnalyzers ( [ . . . ] . . . ) ;
    text:multilingualSupport true ; # optional
.
# Entity map (see documentation for other options)
<#entMap> a text:EntityMap ;
    text:defaultField     "label" ;
    text:entityField      "uri" ;
    text:uidField         "uid" ;
    text:langField        "lang" ;
    text:graphField       "graph" ;
    text:map (
        [ text:field "label" ; 
          text:predicate skos:prefLabel ]
    ) .

See below for more on defining an entity map
The text:TextDataset has two properties:


a text:dataset, e.g., a tdb:DatasetTDB, to contain
the RDF triples; and


an index configured to use either text:TextIndexLucene or text:TextIndexES.


The <#indexLucene> instance of text:TextIndexLucene, above, has two required properties:


the text:directory
file URI which specifies the directory that will contain the Lucene index files – if this has the
value "mem" then the index resides in memory;


the text:entityMap, <#entMap> that will define
what properties are to be indexed and other features of the index; and


and several optional properties:


text:storeValues controls the storing of literal values.
It indicates whether values are stored or not – values must be stored for the
?literal return value to be available in text:query in SPARQL.


text:analyzer specifies the default analyzer configuration to used
during indexing and querying. The default analyzer defaults to Lucene’s StandardAnalyzer.


text:queryAnalyzer specifies an optional analyzer for query that will be
used to analyze the query string. If not set the analyzer used to index a given field is used.


text:queryParser is optional and specifies an alternative query parser


text:propLists is optional and allows to specify lists of indexed properties for use in text:query


text:defineAnalyzers is optional and allows specification of additional analyzers, tokenizers and filters


text:multilingualSupport enables Multilingual Support


If using Elasticsearch then an index would be configured as follows:
<#indexES> a text:TextIndexES ;
      # A comma-separated list of Host:Port values of the ElasticSearch Cluster nodes.
    text:serverList "127.0.0.1:9300" ; 
      # Name of the ElasticSearch Cluster. If not specified defaults to 'elasticsearch'
    text:clusterName "elasticsearch" ; 
      # The number of shards for the index. Defaults to 1
    text:shards "1" ;
      # The number of replicas for the index. Defaults to 1
    text:replicas "1" ;         
      # Name of the Index. defaults to jena-text
    text:indexName "jena-text" ;
    text:entityMap <#entMap> ;
    .

and text:index  <#indexES> ; would be used in the configuration of :text_dataset.
To use a text index assembler configuration in Java code is it necessary
to identify the dataset URI to be assembled, such as in:
Dataset ds = DatasetFactory.assemble(
    "text-config.ttl", 
    "http://localhost/jena_example/#text_dataset") ;

since the assembler contains two dataset definitions, one for the text
dataset, one for the base data.  Therefore, the application needs to
identify the text dataset by it’s URI
http://localhost/jena_example/#text_dataset.
Lists of Indexed Properties
Since 3.13.0, an optional text:TextIndexLucene feature, text:propLists allows to define lists of Lucene indexed
properties that may be used in text:querys. For example:
text:propLists (
    [ text:propListProp ex:labels ;
      text:props ( skos:prefLabel 
                   skos:altLabel 
                   rdfs:label ) ;
    ]
    [ text:propListProp ex:workStmts ;
      text:props ( ex:workColophon 
                   ex:workAuthorshipStatement 
                   ex:workEditionStatement ) ;
    ]
) ;

The text:propLists is a list of property list definitions. Each property list defines a new property,
text:propListProp that will be used to refer to the list in a text:query, for example, ex:labels and
ex:workStmts, above. The text:props is a list of Lucene indexed properties that will be searched over when the
property list property is referred to in a text:query. For example:
?s text:query ( ex:labels "some text" ) .

will request Lucene to search for documents representing triples, ?s ?p ?o, where ?p is one of: rdfs:label OR
skos:prefLbael OR skos:altLabel, matching the query string.
Entity Map definition
A text:EntityMap has several properties that condition what is indexed, what information is stored, and
what analyzers are used.
<#entMap> a text:EntityMap ;
    text:defaultField     "label" ;
    text:entityField      "uri" ;
    text:uidField         "uid" ;
    text:langField        "lang" ;
    text:graphField       "graph" ;
    text:map (
         [ text:field "label" ; 
           text:predicate rdfs:label ]
         ) .

Default text field
The text:defaultField specifies the default field name that Lucene will use in a query that does
not otherwise specify a field. For example,
?s text:query "\"bread and butter\""

will perform a search in the label field for the phrase "bread and butter"
Entity field
The text:entityField  specifies the field name of the field that will contain the subject URI that
is returned on a match. The value of the property is arbitrary so long as it is unique among the
defined names.
UID Field and automatic document deletion
When the text:uidField is defined in the EntityMap then dropping a triple will result in the
corresponding document, if any, being deleted from the text index. The value, "uid", is arbitrary
and defines the name of a stored field in Lucene that holds a unique ID that represents the triple.
If you configure the index via Java code, you need to set this parameter to the
EntityDefinition instance, e.g.
EntityDefinition docDef = new EntityDefinition(entityField, defaultField);
docDef.setUidField("uid");

Note: If you migrate from an index without deletion support to an index with automatic deletion,
you will need to rebuild the index to ensure that the uid information is stored.
Language Field
The text:langField is the name of the field that will store the language attribute of the literal
in the case of an rdf:langString. This Entity Map property is a key element of the
Linguistic support with Lucene index
Graph Field
Setting the text:graphField allows graph-specific indexing of the text
index to limit searching to a specified graph when a SPARQL query targets a single named graph. The
field value is arbitrary and serves to store the graph ID that a triple belongs to when the index is
updated.
The Analyzer Map
The text:map is a list of analyzer specifications as described below.
Configuring an Analyzer
Text to be indexed is passed through a text analyzer that divides it into tokens
and may perform other transformations such as eliminating stop words. If a Lucene
or Elasticsearch text index is used, then by default the Lucene StandardAnalyzer is used.
As of Jena 4.7.x / Lucene 9.x onwards, the StandardAnalyzer does not default to having
English stopwords if no stop words are provided. The setting up until
Apache Lucene 8 had the stopwords:
      "a"  "an"  "and"  "are"  "as"  "at"  "be"  "but"  "by"  "for"  "if"  "in"  "into"  "is" 
      "it"  "no"  "not"  "of"  "on"  "or"  "such"  "that"  "the"  "their"  "then"  "there" 
      "these"  "they"  "this"  "to"  "was"  "will"  "with"

In case of a TextIndexLucene the default analyzer can be replaced by another analyzer with
the text:analyzer property on the text:TextIndexLucene resource in the
text dataset assembler,  for example with a SimpleAnalyzer:
<#indexLucene> a text:TextIndexLucene ;
        text:directory <file:Lucene> ;
        text:analyzer [
            a text:SimpleAnalyzer
        ]
        . 

It is possible to configure an alternative analyzer for each field indexed in a
Lucene index.  For example:
<#entMap> a text:EntityMap ;
    text:entityField      "uri" ;
    text:defaultField     "text" ;
    text:map (
         [ text:field "text" ; 
           text:predicate rdfs:label ;
           text:analyzer [
               a text:StandardAnalyzer ;
               text:stopWords ("a" "an" "and" "but")
           ]
         ]
         ) .

will configure the index to analyze values of the ’text’ field
using a StandardAnalyzer with the given list of stop words.
Other analyzer types that may be specified are SimpleAnalyzer and
KeywordAnalyzer, neither of which has any configuration parameters. See
the Lucene documentation for details of what these analyzers do. Jena also
provides LowerCaseKeywordAnalyzer, which is a case-insensitive version of
KeywordAnalyzer, and ConfigurableAnalyzer.
Support for the new LocalizedAnalyzer has been introduced in Jena 3.0.0 to
deal with Lucene language specific analyzers. See Linguistic Support with
Lucene Index for details.
Support for GenericAnalyzers has been introduced in Jena 3.4.0 to allow
the use of Analyzers that do not have built-in support, e.g., BrazilianAnalyzer;
require constructor parameters not otherwise supported, e.g., a stop words FileReader or
a stemExclusionSet; and finally use of Analyzers not included in the bundled
Lucene distribution, e.g., a SanskritIASTAnalyzer. See Generic and Defined
Analyzer Support
ConfigurableAnalyzer
ConfigurableAnalyzer was introduced in Jena 3.0.1. It allows more detailed
configuration of text analysis parameters by independently selecting a
Tokenizer and zero or more TokenFilters which are applied in order after
tokenization. See the Lucene documentation for details on what each
tokenizer and token filter does.
The available Tokenizer implementations are:

StandardTokenizer
KeywordTokenizer
WhitespaceTokenizer
LetterTokenizer

The available TokenFilter implementations are:

StandardFilter
LowerCaseFilter
ASCIIFoldingFilter
SelectiveFoldingFilter

Configuration is done using Jena assembler like this:
text:analyzer [
  a text:ConfigurableAnalyzer ;
  text:tokenizer text:KeywordTokenizer ;
  text:filters (text:ASCIIFoldingFilter, text:LowerCaseFilter)
]

From Jena 3.7.0, it is possible to define tokenizers and filters in addition to the built-in
choices above that may be used with the ConfigurableAnalyzer. Tokenizers and filters are
defined via text:defineAnalyzers in the text:TextIndexLucene assembler section
using text:GenericTokenizer and text:GenericFilter.
Analyzer for Query
New in Jena 2.13.0.
There is an ability to specify an analyzer to be used for the query
string itself.  It will find terms in the query text.  If not set, then
the analyzer used for the document will be used.  The query analyzer is
specified on the TextIndexLucene resource:
<#indexLucene> a text:TextIndexLucene ;
    text:directory <file:Lucene> ;
    text:entityMap <#entMap> ;
    text:queryAnalyzer [
        a text:KeywordAnalyzer
    ]
    .

Alternative Query Parsers
New in Jena 3.1.0.
It is possible to select a query parser other than the default QueryParser.
The available QueryParser implementations are:


AnalyzingQueryParser: Performs analysis for wildcard queries . This
is useful in combination with accent-insensitive wildcard queries.


ComplexPhraseQueryParser: Permits complex phrase query syntax. Eg:
“(john jon jonathan~) peters*”.  This is useful for performing wildcard
or fuzzy queries on individual terms in a phrase.


SurroundQueryParser: Provides positional operators (w and n)
that accept a numeric distance, as well as boolean
operators (and, or, and not, wildcards (* and ?), quoting (with “),
and boosting (via ^).


The query parser is specified on
the TextIndexLucene resource:
<#indexLucene> a text:TextIndexLucene ;
    text:directory <file:Lucene> ;
    text:entityMap <#entMap> ;
    text:queryParser text:AnalyzingQueryParser .

Elasticsearch currently doesn’t support Analyzers beyond Standard Analyzer.
Configuration by Code
A text dataset can also be constructed in code as might be done for a
purely in-memory setup:
    // Example of building a text dataset with code.
    // Example is in-memory.
    // Base dataset
    Dataset ds1 = DatasetFactory.createMem() ; 

    EntityDefinition entDef = new EntityDefinition("uri", "text", RDFS.label) ;

    // Lucene, in memory.
    Directory dir =  new RAMDirectory();
    
    // Join together into a dataset
    Dataset ds = TextDatasetFactory.createLucene(ds1, dir, entDef) ;

Graph-specific Indexing
jena-text supports storing information about the source graph into the
text index. This allows for more efficient text queries when the query
targets only a single named graph. Without graph-specific indexing, text
queries do not distinguish named graphs and will always return results
from all graphs.
Support for graph-specific indexing is enabled by defining the name of the
index field to use for storing the graph identifier.
If you use an assembler configuration, set the graph field using the
text:graphField property on the EntityMap, e.g.
# Mapping in the index
# URI stored in field "uri"
# Graph stored in field "graph"
# rdfs:label is mapped to field "text"
<#entMap> a text:EntityMap ;
    text:entityField      "uri" ;
    text:graphField       "graph" ;
    text:defaultField     "text" ;
    text:map (
         [ text:field "text" ; text:predicate rdfs:label ]
         ) .

If you configure the index in Java code, you need to use one of the
EntityDefinition constructors that support the graphField parameter, e.g.
    EntityDefinition entDef = new EntityDefinition("uri", "text", "graph", RDFS.label.asNode()) ;

Note: If you migrate from a global (non-graph-aware) index to a graph-aware index,
you need to rebuild the index to ensure that the graph information is stored.
Linguistic support with Lucene index
Language tags associated with rdfs:langStrings occurring as literals in triples may
be used to enhance indexing and queries. Sub-sections below detail different settings with the index, and use cases with SPARQL queries.
Explicit Language Field in the Index
The language tag for object literals of triples can be stored (during triple insert/update)
into the index to extend query capabilities.
For that, the text:langField property must be set in the EntityMap assembler :
<#entMap> a text:EntityMap ;
    text:entityField      "uri" ;
    text:defaultField     "text" ;        
    text:langField        "lang" ;       
    . 

If you configure the index via Java code, you need to set this parameter to the
EntityDefinition instance, e.g.
EntityDefinition docDef = new EntityDefinition(entityField, defaultField);
docDef.setLangField("lang");

Note that configuring the text:langField does not determine a language specific
analyzer. It merely records the tag associated with an indexed rdfs:langString.
SPARQL Linguistic Clause Forms
Once the langField is set, you can use it directly inside SPARQL queries. For that the lang:xx
argument allows you to target specific localized values. For example:
//target english literals
?s text:query (rdfs:label 'word' 'lang:en' ) 

//target unlocalized literals
?s text:query (rdfs:label 'word' 'lang:none') 

//ignore language field
?s text:query (rdfs:label 'word')

Refer above for further discussion on querying.
LocalizedAnalyzer
You can specify a LocalizedAnalyzer in order to benefit from Lucene language
specific analyzers (stemming, stop words,…). Like any other analyzers, it can
be done for default text indexing, for each different field or for query.
Using an assembler configuration, the text:language property needs to
be provided, e.g :
<#indexLucene> a text:TextIndexLucene ;
    text:directory <file:Lucene> ;
    text:entityMap <#entMap> ;
    text:analyzer [
        a text:LocalizedAnalyzer ;
        text:language "fr"
    ]
    .

will configure the index to analyze values of the default property field using a
FrenchAnalyzer.
To configure the same example via Java code, you need to provide the analyzer to the
index configuration object:
    TextIndexConfig config = new TextIndexConfig(def);
    Analyzer analyzer = Util.getLocalizedAnalyzer("fr");
    config.setAnalyzer(analyzer);
    Dataset ds = TextDatasetFactory.createLucene(ds1, dir, config) ;

Where def, ds1 and dir are instances of EntityDefinition, Dataset and
Directory classes.
Note: You do not have to set the text:langField property with a single
localized analyzer. Also note that the above configuration will use the
FrenchAnalyzer for all strings indexed under the default property regardless
of the language tag associated with the literal (if any).
Multilingual Support
Let us suppose that we have many triples with many localized literals in
many different languages. It is possible to take all these languages
into account for future mixed localized queries.  Configure the
text:multilingualSupport property to enable indexing and search via localized
analyzers based on the language tag:
<#indexLucene> a text:TextIndexLucene ;
    text:directory "mem" ;
    text:multilingualSupport true;     
    .

Via Java code, set the multilingual support flag :
    TextIndexConfig config = new TextIndexConfig(def);
    config.setMultilingualSupport(true);
    Dataset ds = TextDatasetFactory.createLucene(ds1, dir, config) ;

This multilingual index combines dynamically all localized analyzers of existing
languages and the storage of langField properties.
The multilingual analyzer becomes the default analyzer and the Lucene
StandardAnalyzer is the default analyzer used when there is no language tag.
It is straightforward to refer to different languages in the same text search query:
SELECT ?s
WHERE {
    { ?s text:query ( rdfs:label 'institut' 'lang:fr' ) }
    UNION
    { ?s text:query ( rdfs:label 'institute' 'lang:en' ) }
}

Hence, the result set of the query will contain “institute” related
subjects (institution, institutional,…) in French and in English.
Note When multilingual indexing is enabled for a property, e.g., rdfs:label,
there will actually be two copies of each literal indexed. One under the Field name,
“label”, and one under the name “label_xx”, where “xx” is the language tag.
Generic and Defined Analyzer Support
There are many Analyzers that do not have built-in support, e.g.,
BrazilianAnalyzer; require constructor parameters not otherwise
supported, e.g., a stop words FileReader or a stemExclusionSet; or
make use of Analyzers not included in the bundled Lucene distribution,
e.g., a SanskritIASTAnalyzer. Two features have been added to enhance
the utility of jena-text: 1) text:GenericAnalyzer; and 2)
text:DefinedAnalyzer. Further, since Jena 3.7.0, features to allow definition of
tokenizers and filters are included.
Generic Analyzers, Tokenizers and Filters
A text:GenericAnalyzer includes a text:class which is the fully
qualified class name of an Analyzer that is accessible on the jena
classpath. This is trivial for Analyzer classes that are included in the
bundled Lucene distribution and for other custom Analyzers a simple
matter of including a jar containing the custom Analyzer and any
associated Tokenizer and Filters on the classpath.
Similarly, text:GenericTokenizer and text:GenericFilter allow to access any tokenizers
or filters that are available on the Jena classpath. These two types are used only to define
tokenizer and filter configurations that may be referred to when specifying a
ConfigurableAnalyzer.
In addition to the text:class it is generally useful to include an
ordered list of text:params that will be used to select an appropriate
constructor of the Analyzer class. If there are no text:params in the
analyzer specification or if the text:params is an empty list then the
nullary constructor is used to instantiate the analyzer. Each element of
the list of text:params includes:

an optional text:paramName of type Literal that is useful to identify the purpose of a
parameter in the assembler configuration
a text:paramType which is one of:


  
      
           Type 
            Description 
      
  
  
      
          text:TypeAnalyzer
          a subclass of org.apache.lucene.analysis.Analyzer
      
      
          text:TypeBoolean
          a java boolean
      
      
          text:TypeFile
          the String path to a file materialized as a java.io.FileReader
      
      
          text:TypeInt
          a java int
      
      
          text:TypeString
          a java String
      
      
          text:TypeSet
          an org.apache.lucene.analysis.CharArraySet
      
  

and is required for the types text:TypeAnalyzer, text:TypeFile and text:TypeSet, but,
since Jena 3.7.0, may be implied by the form of the literal for the types: text:TypeBoolean,
text:TypeInt and text:TypeString.

a required text:paramValue with an object of the type corresponding to text:paramType

In the case of an analyzer parameter the text:paramValue is any text:analyzer resource as
describe throughout this document.
An example of the use of text:GenericAnalyzer to configure an EnglishAnalyzer with stop
words and stem exclusions is:
text:map (
     [ text:field "text" ; 
       text:predicate rdfs:label;
       text:analyzer [
           a text:GenericAnalyzer ;
           text:class "org.apache.lucene.analysis.en.EnglishAnalyzer" ;
           text:params (
                [ text:paramName "stopwords" ;
                  text:paramType text:TypeSet ;
                  text:paramValue ("the" "a" "an") ]
                [ text:paramName "stemExclusionSet" ;
                  text:paramType text:TypeSet ;
                  text:paramValue ("ing" "ed") ]
                )
       ] .

Here is an example of defining an instance of ShingleAnalyzerWrapper:
text:map (
     [ text:field "text" ; 
       text:predicate rdfs:label;
       text:analyzer [
           a text:GenericAnalyzer ;
           text:class "org.apache.lucene.analysis.shingle.ShingleAnalyzerWrapper" ;
           text:params (
                [ text:paramName "defaultAnalyzer" ;
                  text:paramType text:TypeAnalyzer ;
                  text:paramValue [ a text:SimpleAnalyzer ] ]
                [ text:paramName "maxShingleSize" ;
                  text:paramType text:TypeInt ;
                  text:paramValue 3 ]
                )
       ] .

If there is need of using an analyzer with constructor parameter types not included here then
one approach is to define an AnalyzerWrapper that uses available parameter types, such as
file, to collect the information needed to instantiate the desired analyzer. An example of
such an analyzer is the Kuromoji morphological analyzer for Japanese text that uses constructor
parameters of types: UserDictionary, JapaneseTokenizer.Mode, CharArraySet and Set<String>.
As mentioned above, the simple types: TypeInt, TypeBoolean, and TypeString may be written
without explicitly including text:paramType in the parameter specification. For example:
                [ text:paramName "maxShingleSize" ;
                  text:paramValue 3 ]

is sufficient to specify the parameter.
Defined Analyzers
The text:defineAnalyzers feature allows to extend the Multilingual Support
defined above. Further, this feature can also be used to name analyzers defined via text:GenericAnalyzer
so that a single (perhaps complex) analyzer configuration can be used is several places.
Further, since Jena 3.7.0, this feature is also used to name tokenizers and filters that
can be referred to in the specification of a ConfigurableAnalyzer.
The text:defineAnalyzers is used with text:TextIndexLucene to provide a list of analyzer
definitions:
<#indexLucene> a text:TextIndexLucene ;
    text:directory <file:Lucene> ;
    text:entityMap <#entMap> ;
    text:defineAnalyzers (
        [ text:addLang "sa-x-iast" ;
          text:analyzer [ . . . ] ]
        [ text:defineAnalyzer <#foo> ;
          text:analyzer [ . . . ] ]
    )
    .

References to a defined analyzer may be made in the entity map like:
text:analyzer [
    a text:DefinedAnalyzer
    text:useAnalyzer <#foo> ]

Since Jena 3.7.0, a ConfigurableAnalyzer specification can refer to any defined tokenizer
and filters, as in:
text:defineAnalyzers (
     [ text:defineAnalyzer :configuredAnalyzer ;
       text:analyzer [
            a text:ConfigurableAnalyzer ;
            text:tokenizer :ngram ;
            text:filters ( :asciiff text:LowerCaseFilter ) ] ]
     [ text:defineTokenizer :ngram ;
       text:tokenizer [
            a text:GenericTokenizer ;
            text:class "org.apache.lucene.analysis.ngram.NGramTokenizer" ;
            text:params (
                 [ text:paramName "minGram" ;
                   text:paramValue 3 ]
                 [ text:paramName "maxGram" ;
                   text:paramValue 7 ]
                 ) ] ]
     [ text:defineFilter :asciiff ;
       text:filter [
            a text:GenericFilter ;
            text:class "org.apache.lucene.analysis.miscellaneous.ASCIIFoldingFilter" ;
            text:params (
                 [ text:paramName "preserveOriginal" ;
                   text:paramValue true ]
                 ) ] ]
     ) ;

And after 3.8.0 users are able to use the JenaText custom filter SelectiveFoldingFilter.
This filter is not part of the Apache Lucene, but rather a custom implementation available
for JenaText users.
It is based on the Apache Lucene’s ASCIIFoldingFilter, but with the addition of a
white-list for characters that must not be replaced. This is especially useful for languages
where some special characters and diacritical marks are useful when searching.
Here’s an example:
text:defineAnalyzers (
     [ text:defineAnalyzer :configuredAnalyzer ;
       text:analyzer [
            a text:ConfigurableAnalyzer ;
            text:tokenizer :tokenizer ;
            text:filters ( :selectiveFoldingFilter text:LowerCaseFilter ) ] ]
     [ text:defineTokenizer :tokenizer ;
       text:tokenizer [
            a text:GenericTokenizer ;
            text:class "org.apache.lucene.analysis.core.LowerCaseTokenizer" ] ]
     [ text:defineFilter :selectiveFoldingFilter ;
       text:filter [
            a text:GenericFilter ;
            text:class "org.apache.jena.query.text.filter.SelectiveFoldingFilter" ;
            text:params (
                 [ text:paramName "whitelisted" ;
                   text:paramType text:TypeSet ;
                   text:paramValue ("ç" "ä") ]
                 ) ] ]
     ) ;

Extending multilingual support
The Multilingual Support described above allows for a limited set of
ISO 2-letter codes to be used to select from among built-in analyzers using the nullary constructor
associated with each analyzer. So if one is wanting to use:

a language not included, e.g., Brazilian; or
use additional constructors defining stop words, stem exclusions and so on; or
refer to custom analyzers that might be associated with generalized BCP-47 language tags,
such as, sa-x-iast for Sanskrit in the IAST transliteration,

then text:defineAnalyzers with text:addLang will add the desired analyzers to the
multilingual support so that fields with the appropriate language tags will use the appropriate
custom analyzer.
When text:defineAnalyzers is used with text:addLang then text:multilingualSupport is
implicitly added if not already specified and a warning is put in the log:
    text:defineAnalyzers (
        [ text:addLang "sa-x-iast" ;
          text:analyzer [ . . . ] ]

this adds an analyzer to be used when the text:langField has the value sa-x-iast during
indexing and search.
Multilingual enhancements for multi-encoding searches
There are two multilingual search situations that are supported as of 3.8.0:

Search in one encoding and retrieve results that may have been entered in other encodings. For example, searching via Simplified Chinese (Hans) and retrieving results that may have been entered in Traditional Chinese (Hant) or Pinyin. This will simplify applications by permitting encoding independent retrieval without additional layers of transcoding and so on. It’s all done under the covers in Lucene.
Search with queries entered in a lossy, e.g., phonetic, encoding and retrieve results entered with accurate encoding. For example, searching via Pinyin without diacritics and retrieving all possible Hans and Hant triples.

The first situation arises when entering triples that include languages with multiple encodings that for various reasons are not normalized to a single encoding. In this situation it is helpful to be able to retrieve appropriate result sets without regard for the encodings used at the time that the triples were inserted into the dataset.
There are several suchlanguages of interest: Chinese, Tibetan, Sanskrit, Japanese and Korean. There are various Romanizations and ideographic variants.
Encodings may not be normalized when inserting triples for a variety of reasons. A principle one is that the rdf:langString object often must be entered in the same encoding that it occurs in some physical text that is being catalogued. Another is that metadata may be imported from sources that use different encoding conventions and it is desirable to preserve the original form.
The second situation arises to provide simple support for phonetic or other forms of lossy search at the time that triples are indexed directly in the Lucene system.
To handle the first situation a text assembler predicate, text:searchFor, is introduced that specifies a list of language tags that provides a list of language variants that should be searched whenever a query string of a given encoding (language tag) is used. For example, the following text:defineAnalyzers fragment :
    [ text:addLang "bo" ; 
      text:searchFor ( "bo" "bo-x-ewts" "bo-alalc97" ) ;
      text:analyzer [ 
        a text:GenericAnalyzer ;
        text:class "io.bdrc.lucene.bo.TibetanAnalyzer" ;
        text:params (
            [ text:paramName "segmentInWords" ;
              text:paramValue false ]
            [ text:paramName "lemmatize" ;
              text:paramValue true ]
            [ text:paramName "filterChars" ;
              text:paramValue false ]
            [ text:paramName "inputMode" ;
              text:paramValue "unicode" ]
            [ text:paramName "stopFilename" ;
              text:paramValue "" ]
            )
        ] ; 
      ]

indicates that when using a search string such as “རྡོ་རྗེ་སྙིང་"@bo the Lucene index should also be searched for matches tagged as bo-x-ewts and bo-alalc97.
This is made possible by a Tibetan Analyzer that tokenizes strings in all three encodings into Tibetan Unicode. This is feasible since the bo-x-ewts and bo-alalc97 encodings are one-to-one with Unicode Tibetan. Since all fields with these language tags will have a common set of indexed terms, i.e., Tibetan Unicode, it suffices to arrange for the query analyzer to have access to the language tag for the query string along with the various fields that need to be considered.
Supposing that the query is:
(?s ?sc ?lit) text:query ("rje"@bo-x-ewts)

Then the query formed in TextIndexLucene will be:
label_bo:rje label_bo-x-ewts:rje label_bo-alalc97:rje

which is translated using a suitable Analyzer, QueryMultilingualAnalyzer, via Lucene’s QueryParser to:
+(label_bo:རྗེ label_bo-x-ewts:རྗེ label_bo-alalc97:རྗེ)

which reflects the underlying Tibetan Unicode term encoding. During IndexSearcher.search all documents with one of the three fields in the index for term, “རྗེ”, will be returned even though the value in the fields label_bo-x-ewts and label_bo-alalc97 for the returned documents will be the original value “rje”.
This support simplifies applications by permitting encoding independent retrieval without additional layers of transcoding and so on. It’s all done under the covers in Lucene.
Solving the second situation simplifies applications by adding appropriate fields and indexing via configuration in the text:defineAnalyzers. For example, the following fragment:
    [ text:defineAnalyzer :hanzAnalyzer ; 
      text:analyzer [ 
        a text:GenericAnalyzer ;
        text:class "io.bdrc.lucene.zh.ChineseAnalyzer" ;
        text:params (
            [ text:paramName "profile" ;
              text:paramValue "TC2SC" ]
            [ text:paramName "stopwords" ;
              text:paramValue false ]
            [ text:paramName "filterChars" ;
              text:paramValue 0 ]
            )
        ] ; 
      ]  
    [ text:defineAnalyzer :han2pinyin ; 
      text:analyzer [ 
        a text:GenericAnalyzer ;
        text:class "io.bdrc.lucene.zh.ChineseAnalyzer" ;
        text:params (
            [ text:paramName "profile" ;
              text:paramValue "TC2PYstrict" ]
            [ text:paramName "stopwords" ;
              text:paramValue false ]
            [ text:paramName "filterChars" ;
              text:paramValue 0 ]
            )
        ] ; 
      ]
    [ text:defineAnalyzer :pinyin ; 
      text:analyzer [ 
        a text:GenericAnalyzer ;
        text:class "io.bdrc.lucene.zh.ChineseAnalyzer" ;
        text:params (
            [ text:paramName "profile" ;
              text:paramValue "PYstrict" ]
            )
        ] ; 
      ]
    [ text:addLang "zh-hans" ; 
      text:searchFor ( "zh-hans" "zh-hant" ) ;
      text:auxIndex ( "zh-aux-han2pinyin" ) ;
      text:analyzer [
        a text:DefinedAnalyzer ;
        text:useAnalyzer :hanzAnalyzer ] ; 
      ]
    [ text:addLang "zh-hant" ; 
      text:searchFor ( "zh-hans" "zh-hant" ) ;
      text:auxIndex ( "zh-aux-han2pinyin" ) ;
      text:analyzer [
        a text:DefinedAnalyzer ;
        text:useAnalyzer :hanzAnalyzer ] ; 
      ]
    [ text:addLang "zh-latn-pinyin" ;
      text:searchFor ( "zh-latn-pinyin" "zh-aux-han2pinyin" ) ;
      text:analyzer [
        a text:DefinedAnalyzer ;
        text:useAnalyzer :pinyin ] ; 
      ]        
    [ text:addLang "zh-aux-han2pinyin" ;
      text:searchFor ( "zh-latn-pinyin" "zh-aux-han2pinyin" ) ;
      text:analyzer [
        a text:DefinedAnalyzer ;
        text:useAnalyzer :pinyin ] ; 
      text:indexAnalyzer :han2pinyin ; 
      ]

defines language tags for Traditional, Simplified, Pinyin and an auxiliary tag zh-aux-han2pinyin associated with an Analyzer, :han2pinyin. The purpose of the auxiliary tag is to define an Analyzer that will be used during indexing and to specify a list of tags that should be searched when the auxiliary tag is used with a query string.
Searching is then done via the multi-encoding support discussed above. In this example the Analyzer, :han2pinyin, tokenizes strings in zh-hans and zh-hant as the corresponding pinyin so that at search time a pinyin query will retrieve appropriate triples inserted in Traditional or Simplified Chinese. Such a query would appear as:
(?s ?sc ?lit ?g) text:query ("jīng"@zh-aux-han2pinyin)

The auxiliary field support is needed to accommodate situations such as pinyin or sound-ex which are not exact, i.e., one-to-many rather than one-to-one as in the case of Simplified and Traditional.
TextIndexLucene adds a field for each of the auxiliary tags associated with the tag of the triple object being indexed. These fields are in addition to the un-tagged field and the field tagged with the language of the triple object literal.
Naming analyzers for later use
Repeating a text:GenericAnalyzer specification for use with multiple fields in an entity map
may be cumbersome. The text:defineAnalyzer is used in an element of a text:defineAnalyzers
list to associate a resource with an analyzer so that it may be referred to later in a
text:analyzer object. Assuming that an analyzer definition such as the following has appeared
among the text:defineAnalyzers list:
[ text:defineAnalyzer <#foo>
  text:analyzer [ . . . ] ]

then in a text:analyzer specification in an entity map, for example, a reference to analyzer <#foo>
is made via:
text:map (
     [ text:field "text" ; 
       text:predicate rdfs:label;
       text:analyzer [
           a text:DefinedAnalyzer
           text:useAnalyzer <#foo> ]

This makes it straightforward to refer to the same (possibly complex) analyzer definition in multiple fields.
Storing Literal Values
New in Jena 3.0.0.
It is possible to configure the text index to store enough information in the
text index to be able to access the original indexed literal values at query time.
This is controlled by two configuration options. First, the text:storeValues property
must be set to true for the text index:
<#indexLucene> a text:TextIndexLucene ;
    text:directory "mem" ;
    text:storeValues true;     
    .

Or using Java code, used the setValueStored method of TextIndexConfig:
    TextIndexConfig config = new TextIndexConfig(def);
    config.setValueStored(true);

Additionally, setting the langField configuration option is recommended. See
Linguistic Support with Lucene Index
for details. Without the langField setting, the stored literals will not have
language tag or datatype information.
At query time, the stored literals can be accessed by using a 3-element list
of variables as the subject of the text:query property function. The literal
value will be bound to the third variable:
(?s ?score ?literal) text:query 'word'

Working with Fuseki
The Fuseki configuration simply points to the text dataset as the
fuseki:dataset of the service.
<#service_text_tdb> rdf:type fuseki:Service ;
    rdfs:label                      "TDB/text service" ;
    fuseki:name                     "ds" ;
    fuseki:serviceQuery             "query" ;
    fuseki:serviceQuery             "sparql" ;
    fuseki:serviceUpdate            "update" ;
    fuseki:serviceReadGraphStore    "get" ;
    fuseki:serviceReadWriteGraphStore    "data" ;
    fuseki:dataset                  :text_dataset ;
    .

Building a Text Index
When working at scale, or when preparing a published, read-only, SPARQL
service, creating the index by loading the text dataset is impractical.
The index and the dataset can be built using command line tools in two
steps: first load the RDF data, second create an index from the existing
RDF dataset.
Step 1 - Building a TDB dataset
Note: If you have an existing TDB dataset then you can skip this step
Build the TDB dataset:
java -cp $FUSEKI_HOME/fuseki-server.jar tdb.tdbloader --tdb=assembler_file data_file

using the copy of TDB included with Fuseki.
Alternatively, use one of the
TDB utilities tdbloader or tdbloader2 which are better for bulk loading:
$JENA_HOME/bin/tdbloader --loc=directory  data_file

Step 2 - Build the Text Index
You can then build the text index with the jena.textindexer tool:
java -cp $FUSEKI_HOME/fuseki-server.jar jena.textindexer --desc=assembler_file

Because a Fuseki assembler description can have several datasets descriptions,
and several text indexes, it may be necessary to extract a single dataset and index description
into a separate assembler file for use in loading.
Updating the index
If you allow updates to the dataset through Fuseki, the configured index
will automatically be updated on every modification.  This means that you
do not have to run the above mentioned jena.textindexer after updates,
only when you want to rebuild the index from scratch.
Configuring Alternative TextDocProducers
Default Behavior
The default behavior when performing text indexing
is to index a single property as a single field, generating a different Document
for each indexed triple. This behavior may be augmented by
writing and configuring an alternative TextDocProducer.
Please note that TextDocProducer.change(...) is called once for each triple that is
ADDed or DELETEd, and thus can not be directly used to accumulate multiple properties
for use in composing a single multi-fielded Lucene document. See below.
To configure a TextDocProducer, say net.code.MyProducer in a dataset assembly,
use the property textDocProducer, eg:
<#ds-with-lucene> rdf:type text:TextDataset;
	text:index <#indexLucene> ;
	text:dataset <#ds> ;
	text:textDocProducer <java:net.code.MyProducer> ;
	.

where CLASSNAME is the full java class name. It must have either
a single-argument constructor of type TextIndex, or a two-argument
constructor (DatasetGraph, TextIndex). The TextIndex argument
will be the configured text index, and the DatasetGraph argument
will be the graph of the configured dataset.
For example, to explicitly create the default TextDocProducer use:
...
    text:textDocProducer <java:org.apache.jena.query.text.TextDocProducerTriples> ;
...

TextDocProducerTriples produces a new document for each subject/field
added to the dataset, using TextIndex.addEntity(Entity).
Example
The example class below is a TextDocProducer that only indexes
ADDs of quads for which the subject already had at least one
property-value. It uses the two-argument constructor to give it
access to the dataset so that it count the (?G, S, P, ?O) quads
with that subject and predicate, and delegates the indexing to
TextDocProducerTriples if there are at least two values for
that property (one of those values, of course, is the one that
gives rise to this change()).
  public class Example extends TextDocProducerTriples {
  
      final DatasetGraph dg;
      
      public Example(DatasetGraph dg, TextIndex indexer) {
          super(indexer);
          this.dg = dg;
      }
      
      public void change(QuadAction qaction, Node g, Node s, Node p, Node o) {
          if (qaction == QuadAction.ADD) {
              if (alreadyHasOne(s, p)) super.change(qaction, g, s, p, o);
          }
      }
  
      private boolean alreadyHasOne(Node s, Node p) {
          int count = 0;
          Iterator<Quad> quads = dg.find( null, s, p, null );
          while (quads.hasNext()) { quads.next(); count += 1; }
          return count > 1;
      }
  }

Multiple fields per document
In principle it should be possible to extend Jena to allow for creating documents with
multiple searchable fields by extending org.apache.jena.sparql.core.DatasetChangesBatched
such as with org.apache.jena.query.text.TextDocProducerEntities; however, this form of
extension is not currently (Jena 3.13.1) functional.
Maven Dependency
The jena-text module is included in Fuseki.  To use it within application code,
then use the following maven dependency:
<dependency>
  <groupId>org.apache.jena</groupId>
  <artifactId>jena-text</artifactId>
  <version>X.Y.Z</version>
</dependency>

adjusting the version X.Y.Z as necessary.  This will automatically
include a compatible version of Lucene.
For Elasticsearch implementation, you can include the following Maven Dependency:
<dependency>
  <groupId>org.apache.jena</groupId>
  <artifactId>jena-text-es</artifactId>
  <version>X.Y.Z</version>
</dependency>

adjusting the version X.Y.Z as necessary.

  
  
  
    On this page
    
  
    Architecture
      
        One triple equals one document
        One document equals one entity
          
            External content
          
        
        External applications
        Document structure
      
    
    Query with SPARQL
      
        Syntax
          
            Input arguments:
            Output arguments:
          
        
        Query strings
          
            Simple queries
            Queries with language tags
            Queries that retrieve literals
            Queries with graphs
            Queries across multiple Fields
            Queries with Boolean Operators and Term Modifiers
            Highlighting
          
        
        Good practice
          
            Query pattern 1 – Find in the text index and refine results
            Query pattern 2 – Filter results via the text index
          
        
      
    
    Configuration
      
        Text Dataset Assembler
          
            Lists of Indexed Properties
          
        
        Entity Map definition
          
            Default text field
            Entity field
            UID Field and automatic document deletion
            Language Field
            Graph Field
            The Analyzer Map
          
        
        Configuring an Analyzer
          
            ConfigurableAnalyzer
            Analyzer for Query
            Alternative Query Parsers
          
        
        Configuration by Code
        Graph-specific Indexing
        Linguistic support with Lucene index
          
            Explicit Language Field in the Index
            SPARQL Linguistic Clause Forms
            LocalizedAnalyzer
            Multilingual Support
          
        
        Generic and Defined Analyzer Support
          
            Generic Analyzers, Tokenizers and Filters
            Defined Analyzers
            Extending multilingual support
            Multilingual enhancements for multi-encoding searches
            Naming analyzers for later use
          
        
        Storing Literal Values
      
    
    Working with Fuseki
    Building a Text Index
      
        Step 1 - Building a TDB dataset
        Step 2 - Build the Text Index
          
            Updating the index
          
        
      
    
  

  
    Default Behavior
      
        Example
      
    
    Multiple fields per document
    Maven Dependency\n\n\n\nJena Core
Fuseki JavaDoc

Fuseki2 Webapp
Fuseki2 Main


ARQ(SPARQL)
Ontology
RDF Connection
TDB
Text search
SHACL
ShEx
RDF Patch
GeoSPARQL
Query Builder
Service Enhancer
Security Permissions JavaDoc\n\nOn this page
    
  
  
    
Jena Core
Fuseki JavaDoc

Fuseki2 Webapp
Fuseki2 Main


ARQ(SPARQL)
Ontology
RDF Connection
TDB
Text search
SHACL
ShEx
RDF Patch
GeoSPARQL
Query Builder
Service Enhancer
Security Permissions JavaDoc


  
  
  
    On this page\n\n\n\norg.apache.jena.arq/module-summary.html\n\n\n\norg.apache.jena.core/module-summary.html\n\n\n\norg.apache.jena.ontapi/module-summary.html\n\n\n\norg.apache.jena.fuseki.core/module-summary.html\n\n\n\norg.apache.jena.shacl/module-summary.html\n\n\n\norg.apache.jena.shex/module-summary.html\n\n\n\norg.apache.jena.tdb2/module-summary.html\n\n\n\norg.apache.jena.text/module-summary.html\n\n\n\nApache Jena - Security Permissions 5.3.0

JenaSecurity is a SecurityEvaluator interface and a set of
                dynamic proxies that apply that interface to Jena Graphs, Models, and
                associated methods and classes.
        
                The SecurityEvaluator class must be implemented. This class provides
                the interface to the authentication results (e.g.
                getPrincipal()
                ) and the authorization system.
        
                Create a SecuredGraph by calling Factory.getInstance(
                                SecurityEvaluator, String, Graph );
                
                Create a SecuredModel by calling Factory.getInstance(
                                SecurityEvaluator, String, Model )
                
                It is not recommended that you use the Jena ModelFactory.createModelForGraph(
                                SecuredGraph ) See Differences Between Graph and Model below for
                        reasons.
                
        
        
                See SecurityEvaluator documentation for description of
                        cascading security checks
                Secured methods are annotated with: @sec.graph for
                        permissions required on the graph to execute the method.
                        @sec.triple for permissions required on the associated triples
                        (if any) to execute the method.
                It is possible to implement a SecurityEvaluator that does not
                        enforce security at the triple level. See SecurityEvaluator
                        documentation for details
        
        
                Differences Between
                Graph
                and
                Model
        
        
                The Graph interface does not have the concept of "update". Thus all
                updates are implemented as a delete and an insert. The Model interface
                does have the concept of update as evidenced by the
                replace()
                method in the
                RDFList
                class. This difference means that a
                Model
                created by calling
                ModelFactory.createModelForGraph( SecuredGraph )
                will yield a model that evaluates
                Update
                actions differently from one created with
                Factory.getInstance( SecurityEvaluator, modelIRI, model)
                .
        
                Models created by the Jena ModelFactory will require that the
                        user have both delete and create permissions on the underlying graph
                        to perform the update. And will delete the existing triple before
                        attempting to create the new one. Since the graph interface does not
                        have visibility to the model's request for update these are, to the
                        graph, separate events. It is possible that the delete may succeed
                        while the create fails.
                Models created by the JenaSecurity Factory will require that
                        the user have update permissions on the underlying model to perform
                        the update. As long as the user has the update permission on the
                        graph, and the triple where required, the update is performed as a
                        single event.
        
        
        This is the well documented case of differences between the two secured
        model creation methods. For this reason it is recommended that the
        model be created with the
        Factory.getInsance()
        method.
        

Modules

Module
Description
org.apache.jena.permissions\n\n\n\norg.apache.jena.querybuilder/module-summary.html\n\n\n\norg.apache.jena.geosparql/module-summary.html\n\n\n\nWe are always happy to help you get your Jena project going. Jena has been around
for many years, there are many archives of past questions, tutorials and articles
on the web. A quick search may well answer your question directly! If not, please
feel free to post a question to the user support list (details below).
Email support lists
The main user support list is users@jena.apache.org. To join this
list, please send an email to: 
users-subscribe@jena.apache.org from the email account you want
to subscribe with. This list is a good place to ask for advice on developing Jena-based
applications, or solving  a problem with using Jena. Please see below for notes
on asking good questions. The list is archived at
lists.apache.org.
The developers list is dev@jena.apache.org. To join this
list, please send an email to: 
dev-subscribe@jena.apache.org from the email account you want
to subscribe with. This list is a good place to discuss the development of the Jena
platform itself, including patches you want to submit.
To unsubscribe from a mailing list, send email to LIST-unsubscribe@jena.apache.org.
Full details of Apache mailing lists: https://www.apache.org/foundation/mailinglists.html.
Other resources
There are curated collections of Jena questions on StackOverflow
tagged ‘jena’ and
‘apache-jena’.
There are also questions and answers about SPARQL.
How to ask a good question
Asking good questions is the best way to get good answers. Try to follow these tips:


Make the question precise and specific. “My code doesn’t work”, for example, does not help us to help you
as much as “The following SPARQL query gave me an answer I didn’t expect”.


Show that you’ve tried to solve the problem yourself. Everyone who answers questions on the list
has a full-time job or study to do; no-one gets paid for answering these support questions. Spend
their goodwill wisely: “Here’s the code I tried…” or “I read in the documentation that …” shows that
you’ve at least made some effort to find things out for yourself.


Where appropriate show a complete test case. Seeing where your code goes wrong is generally
much easier if we can run it our computers. Corollaries: don’t post your entire project - take some
time to reduce it down to a minimal test case. Include enough data - runnable code is no help if
critical resources like *.rdf files are missing. Reducing your code down to a minimal test case
is often enough for you to figure out the problem yourself, which is always satisfying!


Don’t re-post your question after only a few hours. People are busy, and may be in a different timezone
to you. If you’re not sure if your question made it to the list, look in the archive.


Adding lots of exclamation marks or other punctuation will not move your question up the queue. Quite the
reverse, in fact.


Ask questions on the list, rather than emailing the developers directly. This gives us the chance to share the
load of answering questions, and also ensures that answers are archived in case they’re of use to others in the future.\n\nOn this page
    
  
    Email support lists
    Other resources
    How to ask a good question
  

  
  
    We are always happy to help you get your Jena project going. Jena has been around
for many years, there are many archives of past questions, tutorials and articles
on the web. A quick search may well answer your question directly! If not, please
feel free to post a question to the user support list (details below).
Email support lists
The main user support list is users@jena.apache.org. To join this
list, please send an email to: 
users-subscribe@jena.apache.org from the email account you want
to subscribe with. This list is a good place to ask for advice on developing Jena-based
applications, or solving  a problem with using Jena. Please see below for notes
on asking good questions. The list is archived at
lists.apache.org.
The developers list is dev@jena.apache.org. To join this
list, please send an email to: 
dev-subscribe@jena.apache.org from the email account you want
to subscribe with. This list is a good place to discuss the development of the Jena
platform itself, including patches you want to submit.
To unsubscribe from a mailing list, send email to LIST-unsubscribe@jena.apache.org.
Full details of Apache mailing lists: https://www.apache.org/foundation/mailinglists.html.
Other resources
There are curated collections of Jena questions on StackOverflow
tagged ‘jena’ and
‘apache-jena’.
There are also questions and answers about SPARQL.
How to ask a good question
Asking good questions is the best way to get good answers. Try to follow these tips:


Make the question precise and specific. “My code doesn’t work”, for example, does not help us to help you
as much as “The following SPARQL query gave me an answer I didn’t expect”.


Show that you’ve tried to solve the problem yourself. Everyone who answers questions on the list
has a full-time job or study to do; no-one gets paid for answering these support questions. Spend
their goodwill wisely: “Here’s the code I tried…” or “I read in the documentation that …” shows that
you’ve at least made some effort to find things out for yourself.


Where appropriate show a complete test case. Seeing where your code goes wrong is generally
much easier if we can run it our computers. Corollaries: don’t post your entire project - take some
time to reduce it down to a minimal test case. Include enough data - runnable code is no help if
critical resources like *.rdf files are missing. Reducing your code down to a minimal test case
is often enough for you to figure out the problem yourself, which is always satisfying!


Don’t re-post your question after only a few hours. People are busy, and may be in a different timezone
to you. If you’re not sure if your question made it to the list, look in the archive.


Adding lots of exclamation marks or other punctuation will not move your question up the queue. Quite the
reverse, in fact.


Ask questions on the list, rather than emailing the developers directly. This gives us the chance to share the
load of answering questions, and also ensures that answers are archived in case they’re of use to others in the future.



  
  
  
    On this page
    
  
    Email support lists
    Other resources
    How to ask a good question\n\n\n\nWe welcome your contribution towards making Jena a better platform for semantic web and linked data applications.
We appreciate feature suggestions, bug reports and patches for code or documentation.
If you need help using Jena, please see our getting help page.
How to contribute
You can help us sending your suggestions, feature requests and bug reports (as well as patches) using
Jena’s GitHub Issues.
You can discuss your contribution on the dev@jena.apache.org mailing list.
You can also help other users by answering their questions on the users@jena.apache.org mailing list.
See the subscription instructions for details.
Please see the Reviewing Contributions page for details of what committers will be looking for when reviewing contributions.
Improving the Website
You can also help us improve the documentation on this website via Pull Request.
The website source lives in an Apache git repository at gitbox.apache.org repo
jena-site. There is also a
full read-write mirror on GitHub, see
jena-site on GitHub:
git clone https://github.com/apache/jena-site.git
cd jena-site

You can then make a branch, prepare your changes and submit a pull request.  Please see the README.md in that repository for more details.
SNAPSHOTs
If you use Apache Maven and you are not afraid of being on the bleeding-edge, you can help us by testing our SNAPSHOTs which you can find in the Apache Maven repository.
Here is, for example, how you can add TDB version X.Y.Z-SNAPSHOT to your project (please ask if you are unsure what the latest snapshot version number currently is):
<dependency>
    <groupId>org.apache.jena</groupId>
    <artifactId>jena-tdb</artifactId>
    <version>X.Y.Z-SNAPSHOT</version>
</dependency>

See also how to use Jena with Maven.
If you have problems with any of our SNAPSHOTs, let us know.
You can check the state of each Jena development builds
on the Apache Jenkins continuous integration server.
Git repository
You can find the Jena source code in the Apache git repository:
https://gitbox.apache.org/repos/asf/jena.git
There is also a full read-write mirror of Jena on GitHub:
git clone https://github.com/apache/jena.git
cd jena
mvn clean install

You can fork Jena on GitHub and also submit pull requests to
contribute your suggested changes to the code.
Open issues
Apache Jena manages issues using github open issues.
Submit your patches
You can develop new contributions and work on patches using either the
Apache-hosted git repository or the mirror on GitHub.
Alternatively, patches can be attached directly to issues in github.
Please, inspect your contribution/patch and make sure it includes all (and
only) the relevant changes for a single issue. Don’t forget tests!
If you want to test if a patch applies cleanly you can use:
patch -p0 < JENA-XYZ.patch

If you use Eclipse: right click on the project name in Package Explorer,
select Team > Create Patch or Team > Apply Patch.
You can also use git:
git format-patch origin/trunk

IRC channel
Some Jena developers hang out on #jena on irc.freenode.net.
How Apache Software Foundation works
To better understand how to get involved and how the Apache Software Foundation works we recommend you read:

https://www.apache.org/foundation/getinvolved.html
https://www.apache.org/foundation/how-it-works.html
https://www.apache.org/dev/contributors.html\n\nOn this page
    
  
    
      
        How to contribute
          
            Improving the Website
          
        
        SNAPSHOTs
        Git repository
        Open issues
        Submit your patches
        IRC channel
        How Apache Software Foundation works
      
    
  

  
  
    We welcome your contribution towards making Jena a better platform for semantic web and linked data applications.
We appreciate feature suggestions, bug reports and patches for code or documentation.
If you need help using Jena, please see our getting help page.
How to contribute
You can help us sending your suggestions, feature requests and bug reports (as well as patches) using
Jena’s GitHub Issues.
You can discuss your contribution on the dev@jena.apache.org mailing list.
You can also help other users by answering their questions on the users@jena.apache.org mailing list.
See the subscription instructions for details.
Please see the Reviewing Contributions page for details of what committers will be looking for when reviewing contributions.
Improving the Website
You can also help us improve the documentation on this website via Pull Request.
The website source lives in an Apache git repository at gitbox.apache.org repo
jena-site. There is also a
full read-write mirror on GitHub, see
jena-site on GitHub:
git clone https://github.com/apache/jena-site.git
cd jena-site

You can then make a branch, prepare your changes and submit a pull request.  Please see the README.md in that repository for more details.
SNAPSHOTs
If you use Apache Maven and you are not afraid of being on the bleeding-edge, you can help us by testing our SNAPSHOTs which you can find in the Apache Maven repository.
Here is, for example, how you can add TDB version X.Y.Z-SNAPSHOT to your project (please ask if you are unsure what the latest snapshot version number currently is):
<dependency>
    <groupId>org.apache.jena</groupId>
    <artifactId>jena-tdb</artifactId>
    <version>X.Y.Z-SNAPSHOT</version>
</dependency>

See also how to use Jena with Maven.
If you have problems with any of our SNAPSHOTs, let us know.
You can check the state of each Jena development builds
on the Apache Jenkins continuous integration server.
Git repository
You can find the Jena source code in the Apache git repository:
https://gitbox.apache.org/repos/asf/jena.git
There is also a full read-write mirror of Jena on GitHub:
git clone https://github.com/apache/jena.git
cd jena
mvn clean install

You can fork Jena on GitHub and also submit pull requests to
contribute your suggested changes to the code.
Open issues
Apache Jena manages issues using github open issues.
Submit your patches
You can develop new contributions and work on patches using either the
Apache-hosted git repository or the mirror on GitHub.
Alternatively, patches can be attached directly to issues in github.
Please, inspect your contribution/patch and make sure it includes all (and
only) the relevant changes for a single issue. Don’t forget tests!
If you want to test if a patch applies cleanly you can use:
patch -p0 < JENA-XYZ.patch

If you use Eclipse: right click on the project name in Package Explorer,
select Team > Create Patch or Team > Apply Patch.
You can also use git:
git format-patch origin/trunk

IRC channel
Some Jena developers hang out on #jena on irc.freenode.net.
How Apache Software Foundation works
To better understand how to get involved and how the Apache Software Foundation works we recommend you read:

https://www.apache.org/foundation/getinvolved.html
https://www.apache.org/foundation/how-it-works.html
https://www.apache.org/dev/contributors.html


  
  
  
    On this page
    
  
    
      
        How to contribute
          
            Improving the Website
          
        
        SNAPSHOTs
        Git repository
        Open issues
        Submit your patches
        IRC channel
        How Apache Software Foundation works\n\n\n\nPlease report bugs using Jena’s GitHub Issues.
General suggestions or requests for changes can also be discussed on the user list
or Jena’s GitHub Discussions
but are less likely to be accidentally forgotten if you log them in a GitHub Issue.
For any security issues please refer to our Security Advisories
page for how those should be reported and handled.
Patches and other code contributions are made via git pull requests.
See ‘Getting Involved’
Please note that ASF requires that all contributions must be covered by an
appropriate license.\n\nOn this page
    
  
  
    Please report bugs using Jena’s GitHub Issues.
General suggestions or requests for changes can also be discussed on the user list
or Jena’s GitHub Discussions
but are less likely to be accidentally forgotten if you log them in a GitHub Issue.
For any security issues please refer to our Security Advisories
page for how those should be reported and handled.
Patches and other code contributions are made via git pull requests.
See ‘Getting Involved’
Please note that ASF requires that all contributions must be covered by an
appropriate license.

  
  
  
    On this page\n\n\n\nJena is a Java framework for building Semantic Web applications. It provides an
extensive Java libraries for helping developers develop code that
handles RDF, RDFS, RDFa, OWL and SPARQL in line with
published W3C recommendations.
Jena includes a rule-based inference engine
to perform reasoning based on OWL and RDFS ontologies, and a variety of storage
strategies to store RDF triples in memory or on disk.
History
Jena was originally developed by researchers in HP Labs,
starting in Bristol, UK, in 2000. Jena has always been an open-source project,
and has been extensively used in a wide variety of semantic web applications and
demonstrators. In 2009, HP decided to refocus development activity away from direct
support of development of Jena, though remaining supportive of the project’s aims.
The project team successfully applied to have Jena adopted by the Apache
Software Foundation
in November 2010  (see the vote result).
Current status
Jena entered incubation with the Apache in November 2010, and graduated as a top-level
project in April 2012.
Thanks
YourKit is kindly supporting open source projects with its full-featured Java Profiler.
YourKit, LLC is the creator of innovative and intelligent tools for profiling
Java and .NET applications. Take a look at YourKit’s leading software products:
YourKit Java Profiler and
YourKit .NET Profiler.\n\nOn this page
    
  
    
      
        History
        Current status
        Thanks
      
    
  

  
  
    Jena is a Java framework for building Semantic Web applications. It provides an
extensive Java libraries for helping developers develop code that
handles RDF, RDFS, RDFa, OWL and SPARQL in line with
published W3C recommendations.
Jena includes a rule-based inference engine
to perform reasoning based on OWL and RDFS ontologies, and a variety of storage
strategies to store RDF triples in memory or on disk.
History
Jena was originally developed by researchers in HP Labs,
starting in Bristol, UK, in 2000. Jena has always been an open-source project,
and has been extensively used in a wide variety of semantic web applications and
demonstrators. In 2009, HP decided to refocus development activity away from direct
support of development of Jena, though remaining supportive of the project’s aims.
The project team successfully applied to have Jena adopted by the Apache
Software Foundation
in November 2010  (see the vote result).
Current status
Jena entered incubation with the Apache in November 2010, and graduated as a top-level
project in April 2012.
Thanks
YourKit is kindly supporting open source projects with its full-featured Java Profiler.
YourKit, LLC is the creator of innovative and intelligent tools for profiling
Java and .NET applications. Take a look at YourKit’s leading software products:
YourKit Java Profiler and
YourKit .NET Profiler.

  
  
  
    On this page
    
  
    
      
        History
        Current status
        Thanks\n\n\n\nThere’s quite a lot of code inside Jena, and it can be daunting for new Jena
users to find their way around. On this page we’ll summarise the
key features and interfaces in Jena, as a general overview and guide to the
more detailed documentation.
At its core, Jena stores information as RDF triples in directed graphs, and allows
your code to add, remove, manipulate, store and publish that information. We
tend to think of Jena as a number of major subsystems with clearly
defined interfaces between them. First let’s start with the big picture:

RDF triples and graphs, and their various components, are accessed through Jena’s
RDF API. Typical abstractions here are Resource representing an
RDF resource (whether named with a URI or anonymous), Literal for data
values (numbers, strings, dates, etc.), Statement representing an RDF
triple and Model representing the whole graph. The RDF API has basic
facilities for adding and removing triples to graphs and finding triples that match
particular patterns. Here you can also read in RDF from external sources, whether
files or URL’s, and serialize a graph in correctly-formatted text form. Both input
and output support most of the commonly-used RDF syntaxes.
While the programming interface to Model is quite rich,
internally, the RDF graph is stored in a much simpler abstraction named Graph.
This allows Jena to use a variety of different storage strategies equivalently, as long
as they conform to the Graph interface. Out-of-the box, Jena can store
a graph as an in-memory store, or as a persistent store using a
custom disk-based tuple index. The graph interface is also a convenient extension point
for connecting other stores to Jena, such as LDAP, by writing an adapter that allows
the calls from the Graph API to work on that store.
A key feature of semantic web applications is that the semantic rules of RDF, RDFS and
OWL can be used to infer information that is not explicitly stated in the graph. For example,
if class C is a sub-class of class B, and B a sub-class of A, then by implication C is
a sub-class of A. Jena’s inference API provides the means to make these entailed triples
appear in the store just as if they had been added explicitly. The inference API provides
a number of rule engines to perform this job, either using the built-in rulesets for OWL
and RDFS, or using application custom rules. Alternatively, the inference API can be
connected up to an external reasoner, such as description logic (DL) engine, to perform
the same job with different, specialised, reasoning algorithms.
The collection of standards that define semantic web technologies includes SPARQL -
the query language for RDF. Jena conforms to all of the published standards, and tracks
the revisions and updates in the under-development areas of the standard. Handling
SPARQL, both for query and update, is the responsibility of the SPARQL API.
Ontologies are also key to many semantic web applications. Ontologies are formal logical
descriptions, or models, of some aspect of the real-world that applications have to
deal with. Ontologies can be shared with other developers and researchers, making it a
good basis for building linked-data applications. There are two ontology languages
for RDF: RDFS, which is rather weak, and OWL which is much more expressive. Both languages
are supported in Jena though the Ontology API, which provides convenience methods that
know about the richer representation forms available to applications through OWL and RDFS.
While the above capabilities are typically accessed by applications directly through the
Java API, publishing data over the Internet is a common requirement in modern applications.
Fuseki is a data publishing server, which can present, and update, RDF models over the
web using SPARQL and HTTP.
There are many other pieces to Jena, including command-line tools, specialised indexes for
text-based lookup, etc. These, and further details on the pieces outlined above, can be
found in the detailed documentation on this site.\n\nOn this page
    
  
  
    There’s quite a lot of code inside Jena, and it can be daunting for new Jena
users to find their way around. On this page we’ll summarise the
key features and interfaces in Jena, as a general overview and guide to the
more detailed documentation.
At its core, Jena stores information as RDF triples in directed graphs, and allows
your code to add, remove, manipulate, store and publish that information. We
tend to think of Jena as a number of major subsystems with clearly
defined interfaces between them. First let’s start with the big picture:

RDF triples and graphs, and their various components, are accessed through Jena’s
RDF API. Typical abstractions here are Resource representing an
RDF resource (whether named with a URI or anonymous), Literal for data
values (numbers, strings, dates, etc.), Statement representing an RDF
triple and Model representing the whole graph. The RDF API has basic
facilities for adding and removing triples to graphs and finding triples that match
particular patterns. Here you can also read in RDF from external sources, whether
files or URL’s, and serialize a graph in correctly-formatted text form. Both input
and output support most of the commonly-used RDF syntaxes.
While the programming interface to Model is quite rich,
internally, the RDF graph is stored in a much simpler abstraction named Graph.
This allows Jena to use a variety of different storage strategies equivalently, as long
as they conform to the Graph interface. Out-of-the box, Jena can store
a graph as an in-memory store, or as a persistent store using a
custom disk-based tuple index. The graph interface is also a convenient extension point
for connecting other stores to Jena, such as LDAP, by writing an adapter that allows
the calls from the Graph API to work on that store.
A key feature of semantic web applications is that the semantic rules of RDF, RDFS and
OWL can be used to infer information that is not explicitly stated in the graph. For example,
if class C is a sub-class of class B, and B a sub-class of A, then by implication C is
a sub-class of A. Jena’s inference API provides the means to make these entailed triples
appear in the store just as if they had been added explicitly. The inference API provides
a number of rule engines to perform this job, either using the built-in rulesets for OWL
and RDFS, or using application custom rules. Alternatively, the inference API can be
connected up to an external reasoner, such as description logic (DL) engine, to perform
the same job with different, specialised, reasoning algorithms.
The collection of standards that define semantic web technologies includes SPARQL -
the query language for RDF. Jena conforms to all of the published standards, and tracks
the revisions and updates in the under-development areas of the standard. Handling
SPARQL, both for query and update, is the responsibility of the SPARQL API.
Ontologies are also key to many semantic web applications. Ontologies are formal logical
descriptions, or models, of some aspect of the real-world that applications have to
deal with. Ontologies can be shared with other developers and researchers, making it a
good basis for building linked-data applications. There are two ontology languages
for RDF: RDFS, which is rather weak, and OWL which is much more expressive. Both languages
are supported in Jena though the Ontology API, which provides convenience methods that
know about the richer representation forms available to applications through OWL and RDFS.
While the above capabilities are typically accessed by applications directly through the
Java API, publishing data over the Internet is a common requirement in modern applications.
Fuseki is a data publishing server, which can present, and update, RDF models over the
web using SPARQL and HTTP.
There are many other pieces to Jena, including command-line tools, specialised indexes for
text-based lookup, etc. These, and further details on the pieces outlined above, can be
found in the detailed documentation on this site.

  
  
  
    On this page\n\n\n\nThe name of the project is “Apache Jena”. That should appear as the first
use in a paper and in a reference. After that “Jena” can be used.
It is also a trademark
of the Apache Software Foundation. This is also the industry practice.
The reference should indicate the website https://jena.apache.org/
(https is preferable). If relevant to reproducibility, or discussing
performance, the release version number MUST also be included. The date
of access would also be helpful to the reader.
You can use names such as “TDB” and “Fuseki” on their own. They are informal
names to parts of the whole system. They also change over time and versions.
You could say “Apache Jena Fuseki” for the triplestore but as the components
function as part of the whole, “Apache Jena” would be accurate.
The first paper citing Jena is Jena: implementing the semantic web recommendations.
That only covers the API and its implementation. Some parts of the system
mentioned in that paper have been dropped a long time ago (e.g. the “RDB”
system). The paper is also prior to the move to under the Apache Software
Foundation. It is also good to acknowledge Brian McBride, who started the
project.
Here is an example of what a citation may look like:
Apache Software Foundation, 2021. Apache Jena, Available at: https://jena.apache.org/.\n\nOn this page
    
  
  
    The name of the project is “Apache Jena”. That should appear as the first
use in a paper and in a reference. After that “Jena” can be used.
It is also a trademark
of the Apache Software Foundation. This is also the industry practice.
The reference should indicate the website https://jena.apache.org/
(https is preferable). If relevant to reproducibility, or discussing
performance, the release version number MUST also be included. The date
of access would also be helpful to the reader.
You can use names such as “TDB” and “Fuseki” on their own. They are informal
names to parts of the whole system. They also change over time and versions.
You could say “Apache Jena Fuseki” for the triplestore but as the components
function as part of the whole, “Apache Jena” would be accurate.
The first paper citing Jena is Jena: implementing the semantic web recommendations.
That only covers the API and its implementation. Some parts of the system
mentioned in that paper have been dropped a long time ago (e.g. the “RDB”
system). The paper is also prior to the move to under the Apache Software
Foundation. It is also good to acknowledge Brian McBride, who started the
project.
Here is an example of what a citation may look like:
Apache Software Foundation, 2021. Apache Jena, Available at: https://jena.apache.org/.

  
  
  
    On this page\n\n\n\nIn first name alphabetical order:

Aaron Coburn (acoburn) C
Adam Soroka (ajs6f) CP
Andy Seaborne (andy) CP VP
Bruno Kinoshita (kinow) CP
Chris Dollin (chrisdollin) CP
Chris Tomlinson (codeferret) CP
Claude Warren (claude) CP
Damian Steer (damian) CP
Dave Reynolds (der) CP
Ian Dickinson (ijd) CP
Lorenz Buehmann (lbuehmann) C
Osma Suominen (osma) CP
Paolo Castagna (castagna) CP
Rob Vesse (rvesse) CP
Stephen Allen (sallen) CP
Ying Jiang (jpz6311whu) C

Emeritus and Mentors:

Benson Margulies C
Dave Johnson
Leo Simons
Ross Gardler

Key

C
a committer
P
a PMC member
VP
project chair and Apache Foundation Vice-President\n\nOn this page
    
  
    
      
        Key
      
    
  

  
  
    In first name alphabetical order:

Aaron Coburn (acoburn) C
Adam Soroka (ajs6f) CP
Andy Seaborne (andy) CP VP
Bruno Kinoshita (kinow) CP
Chris Dollin (chrisdollin) CP
Chris Tomlinson (codeferret) CP
Claude Warren (claude) CP
Damian Steer (damian) CP
Dave Reynolds (der) CP
Ian Dickinson (ijd) CP
Lorenz Buehmann (lbuehmann) C
Osma Suominen (osma) CP
Paolo Castagna (castagna) CP
Rob Vesse (rvesse) CP
Stephen Allen (sallen) CP
Ying Jiang (jpz6311whu) C

Emeritus and Mentors:

Benson Margulies C
Dave Johnson
Leo Simons
Ross Gardler

Key

C
a committer
P
a PMC member
VP
project chair and Apache Foundation Vice-President


  
  
  
    On this page
    
  
    
      
        Key\n\n\n\nThis page lists various projects and tools related to Jena - classes, packages,
libraries, applications, or ontologies that enhance Jena or are
built on top of it. These projects are not part of the Jena project itself, but
may be useful to Jena users.
This list is provided for information purposes only, and is not meant as an
endorsement of the mentioned projects by the Jena team.
If you wish your contribution to appear on this page, please raise a
GitHub issue with the details to be published.
Related projects

  
      
          Name
          Description
          License
          Creator
          URL
      
  
  
      
          GeoSPARQL Jena
          Implementation of GeoSPARQL 1.0 standard using Apache Jena for SPARQL query or API.
          Apache 2.0
          Greg Albiston and Haozhe Chen
          geosparql-jena at GitHub
      
      
          GeoSPARQL Fuseki
          HTTP server application compliant with the GeoSPARQL standard using GeoSPARQL Jena library and Apache Jena Fuseki server
          Apache 2.0
          Greg Albiston
          geosparql-fuseki at GitHub
      
      
          Jastor
          Code generator that emits Java Beans from OWL Web Ontologies
          Common Public License
          Ben Szekely and Joe Betz
          Jastor website
      
      
          NG4J
          Named Graphs API for Jena
          BSD license
          Chris Bizer
          NG4J website
      
      
          Micro Jena (uJena)
          Reduced version of Jena for mobile devices
          as per Jena
          Fulvio Crivellaro and Gabriele Genovese and Giorgio Orsi
          Micro Jena
      
      
          Gloze
          XML to RDF, RDF to XML, XSD to OWL mapping tool
          as per Jena
          Steve Battle
          jena files page
      
      
          WYMIWYG KnoBot
          A fully Jena based semantic CMS. Implements URIQA. File-based persistence.
          Apache
          Reto Bachmann-Gmuer / wymiwyg.org
          Download KnoBot
      
      
          Infinite Graph
          An infinite graph implementation for RDF graphs
          BSD
          UTD
          Infinite Graph for Jena
      
      
          Twinkle
          A GUI interface for working with SPARQL queries
          Public Domain
          Leigh Dodds
          Twinkle project homepage
      
      
          GLEEN
          A path expression (a.k.a. “regular paths”) property function library for ARQ SparQL
          Apache 2.0
          Todd Detwiler - University of Washington Structural Informatics Group
          GLEEN home
      
      
          Jena Sesame Model
          Jena Sesame Model - Sesame triple store for Jena models
          GNU
          Weijian Fang
          Jena Sesame Model
      
      
          D2RQ
          Treats non-RDF databases as virtual Jena RDF graphs
          GNU GPL License
          Chris Bizer
          D2RQ website
      
      
          GeoSpatialWeb
          This projects adds geo-spatial predicates and reasoning features to Jena property functions.
          GNU GPL License
          Marco Neumann and Taylor Cowan
          GeoSpatialWeb
      
      
          Jenabean
          Jenabean uses Jena’s flexible RDF/OWL API to persist Java beans.
          Apache 2.0
          Taylor Cowan and David Donohue
          Jenabean project page
      
      
          Persistence Annotations 4 RDF
          Persistence Annotation for RDF (PAR) is a set of annotations and an entity manager that provides JPA like functionality on top of an RDF store while accounting for and exploiting the fundamental differences between graph storage and relational storage. PAR introduces three (3) annotations that map a RDF triple (subject, predicate, object) to a Plain Old Java Object (POJO) using Java’s dynamic proxy capabilities.
          Apache 2.0
          Claude Warren
          PA4RDF at Sourceforge
      
      
          Semantic_Forms
          Swiss army knife for data management and social networking.
          open source
          Jean-Marc Vanel
          Semantic_Forms
      
      
          JDBC 4 SPARQL
          JDBC 4 SPARQL is a type 4 JDBC Driver that uses a SPARQL endpoint (or Jena Model) as the data store.  Presents graph data as relational data to tools that understand SQL and utilize JDBC
          Apache 2.0 (Some components GNU LGPL V3.0)
          Claude Warren
          jdbc4sparql at GitHub\n\nOn this page
    
  
    Related projects
  

  
  
    This page lists various projects and tools related to Jena - classes, packages,
libraries, applications, or ontologies that enhance Jena or are
built on top of it. These projects are not part of the Jena project itself, but
may be useful to Jena users.
This list is provided for information purposes only, and is not meant as an
endorsement of the mentioned projects by the Jena team.
If you wish your contribution to appear on this page, please raise a
GitHub issue with the details to be published.
Related projects

  
      
          Name
          Description
          License
          Creator
          URL
      
  
  
      
          GeoSPARQL Jena
          Implementation of GeoSPARQL 1.0 standard using Apache Jena for SPARQL query or API.
          Apache 2.0
          Greg Albiston and Haozhe Chen
          geosparql-jena at GitHub
      
      
          GeoSPARQL Fuseki
          HTTP server application compliant with the GeoSPARQL standard using GeoSPARQL Jena library and Apache Jena Fuseki server
          Apache 2.0
          Greg Albiston
          geosparql-fuseki at GitHub
      
      
          Jastor
          Code generator that emits Java Beans from OWL Web Ontologies
          Common Public License
          Ben Szekely and Joe Betz
          Jastor website
      
      
          NG4J
          Named Graphs API for Jena
          BSD license
          Chris Bizer
          NG4J website
      
      
          Micro Jena (uJena)
          Reduced version of Jena for mobile devices
          as per Jena
          Fulvio Crivellaro and Gabriele Genovese and Giorgio Orsi
          Micro Jena
      
      
          Gloze
          XML to RDF, RDF to XML, XSD to OWL mapping tool
          as per Jena
          Steve Battle
          jena files page
      
      
          WYMIWYG KnoBot
          A fully Jena based semantic CMS. Implements URIQA. File-based persistence.
          Apache
          Reto Bachmann-Gmuer / wymiwyg.org
          Download KnoBot
      
      
          Infinite Graph
          An infinite graph implementation for RDF graphs
          BSD
          UTD
          Infinite Graph for Jena
      
      
          Twinkle
          A GUI interface for working with SPARQL queries
          Public Domain
          Leigh Dodds
          Twinkle project homepage
      
      
          GLEEN
          A path expression (a.k.a. “regular paths”) property function library for ARQ SparQL
          Apache 2.0
          Todd Detwiler - University of Washington Structural Informatics Group
          GLEEN home
      
      
          Jena Sesame Model
          Jena Sesame Model - Sesame triple store for Jena models
          GNU
          Weijian Fang
          Jena Sesame Model
      
      
          D2RQ
          Treats non-RDF databases as virtual Jena RDF graphs
          GNU GPL License
          Chris Bizer
          D2RQ website
      
      
          GeoSpatialWeb
          This projects adds geo-spatial predicates and reasoning features to Jena property functions.
          GNU GPL License
          Marco Neumann and Taylor Cowan
          GeoSpatialWeb
      
      
          Jenabean
          Jenabean uses Jena’s flexible RDF/OWL API to persist Java beans.
          Apache 2.0
          Taylor Cowan and David Donohue
          Jenabean project page
      
      
          Persistence Annotations 4 RDF
          Persistence Annotation for RDF (PAR) is a set of annotations and an entity manager that provides JPA like functionality on top of an RDF store while accounting for and exploiting the fundamental differences between graph storage and relational storage. PAR introduces three (3) annotations that map a RDF triple (subject, predicate, object) to a Plain Old Java Object (POJO) using Java’s dynamic proxy capabilities.
          Apache 2.0
          Claude Warren
          PA4RDF at Sourceforge
      
      
          Semantic_Forms
          Swiss army knife for data management and social networking.
          open source
          Jean-Marc Vanel
          Semantic_Forms
      
      
          JDBC 4 SPARQL
          JDBC 4 SPARQL is a type 4 JDBC Driver that uses a SPARQL endpoint (or Jena Model) as the data store.  Presents graph data as relational data to tools that understand SQL and utilize JDBC
          Apache 2.0 (Some components GNU LGPL V3.0)
          Claude Warren
          jdbc4sparql at GitHub
      
  


  
  
  
    On this page
    
  
    Related projects\n\n\n\nYou can view a list of the open issues on Github.
Pull requests, patches and other contributions welcome!\n\nOn this page
    
  
  
    You can view a list of the open issues on Github.
Pull requests, patches and other contributions welcome!

  
  
  
    On this page\n\n\n\n\n\nThis section contains detailed information about the various Jena
sub-systems, aimed at developers using Jena. For more general introductions,
please refer to the Getting started and Tutorial
sections.
Documentation index

The RDF API - the core RDF API in Jena
SPARQL - querying and updating RDF models using the SPARQL standards
Fuseki - SPARQL server which can present RDF data and answer SPARQL queries over HTTP
I/O - reading and writing RDF data
RDF Connection - a SPARQL API for local datasets and remote services
Assembler - describing recipes for constructing Jena models declaratively using RDF
Inference - using the Jena rules engine and other inference algorithms to derive consequences from RDF models
Ontology - support for handling OWL models in Jena
Data and RDFS - apply RDFS to graphs in a dataset
TDB2 - a fast persistent triple store that stores directly to disk
TDB - Original TDB database
SHACL - SHACL processor for Jena
ShEx - ShEx processor for Jena
Text Search - enhanced indexes using Lucene for more efficient searching of text literals in Jena models and datasets.
GeoSPARQL - support for GeoSPARQL
Permissions - a permissions wrapper around Jena RDF implementation
Tools - various command-line tools and utilities to help developers manage RDF data and other aspects of Jena
How-To’s - various topic-specific how-to documents
QueryBuilder - Classes to simplify the programmatic building of various query and update statements.
Extras - various modules that provide utilities and larger packages that make Apache Jena development or usage easier but that do not fall within the standard Jena framework.
Javadoc - JavaDoc generated from the Jena source\n\nOn this page
    
  
    Documentation index
  

  
  
    This section contains detailed information about the various Jena
sub-systems, aimed at developers using Jena. For more general introductions,
please refer to the Getting started and Tutorial
sections.
Documentation index

The RDF API - the core RDF API in Jena
SPARQL - querying and updating RDF models using the SPARQL standards
Fuseki - SPARQL server which can present RDF data and answer SPARQL queries over HTTP
I/O - reading and writing RDF data
RDF Connection - a SPARQL API for local datasets and remote services
Assembler - describing recipes for constructing Jena models declaratively using RDF
Inference - using the Jena rules engine and other inference algorithms to derive consequences from RDF models
Ontology - support for handling OWL models in Jena
Data and RDFS - apply RDFS to graphs in a dataset
TDB2 - a fast persistent triple store that stores directly to disk
TDB - Original TDB database
SHACL - SHACL processor for Jena
ShEx - ShEx processor for Jena
Text Search - enhanced indexes using Lucene for more efficient searching of text literals in Jena models and datasets.
GeoSPARQL - support for GeoSPARQL
Permissions - a permissions wrapper around Jena RDF implementation
Tools - various command-line tools and utilities to help developers manage RDF data and other aspects of Jena
How-To’s - various topic-specific how-to documents
QueryBuilder - Classes to simplify the programmatic building of various query and update statements.
Extras - various modules that provide utilities and larger packages that make Apache Jena development or usage easier but that do not fall within the standard Jena framework.
Javadoc - JavaDoc generated from the Jena source


  
  
  
    On this page
    
  
    Documentation index\n\n\n\nApache Jena Fuseki is a SPARQL server.  It can run as a standalone server, or embedded in an
application.
Fuseki provides the
SPARQL 1.1 protocols for query and update
as well as the
SPARQL Graph Store protocol.
Fuseki is integrated with TDB to provide a robust,
transactional, persistent storage layer. Fuseki also incorporates
Jena text query.
Contents

Download
Getting Started
Running Fuseki Server

As a standalone server
As a service
As a web application
Security with Apache Shiro


Running Fuseki Plain

Setup
As a Docker container
As an embedded SPARQL server
Security and data access control
Logging


Fuseki Configuration
Server Statistics and Metrics
How to Contribute
Client access

Use from Java


Extending Fuseki with Fuseki Modules
Links to Standards

The Jena users mailing is the place to get help with Fuseki.
Email support lists
Download Fuseki
Releases of Apache Jena Fuseki can be downloaded from:
Jena Downloads
Fuseki download files

  
      
          Filename
          Description
      
  
  
      
          apache-jena-fuseki-*VER*.zip
          The Fuseki server and UI
      
  

The Fuseki engine is also available as a Maven artifact:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-main</artifactId>
   <version>X.Y.Z</version>
</dependency>

and the UI is available as:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-ui</artifactId>
   <version>X.Y.Z</version>
</dependency>

A WAR file is also available from the Jena download page.
Previous releases
While previous releases are available, we strongly recommend that wherever
possible users use the latest official Apache releases of Jena in
preference to using any older versions of Jena.
Previous Apache Jena releases can be found in the Apache archive area
at https://archive.apache.org/dist/jena
Development Builds
Regular development builds of all of Jena are available
(these are not formal releases) from the
Apache snapshots maven repository.
This includes the packaged build of Fuseki.
How to Contribute
We welcome contributions towards making Jena a better platform for semantic
web and linked data applications.  We appreciate feature suggestions, bug
reports and patches for code or documentation.
See “Getting Involved” for ways to
contribute to Jena and Fuseki, including patches and making github
pull-requests.
Source code
The development codebase is available from git.
Development builds (not a formal release):
SNAPSHOT
Source code:
https://github.com/apache/jena/tree/main/jena-fuseki2
The Fuseki code is under “jena-fuseki2/”:

  
      
          Code
          Purpose
      
  
  
      
          jena-fuseki-main
          The Fuseki server
      
      
          jena-fuseki-core
          The Fuseki engine
      
      
          jena-fuseki-server
          Build the combined jar for Fuseki server
      
      
          jena-fuseki-access
          Data access control
      
      
          apache-jena-fuseki
          The download for Fuseki
      
      
          Other
          
      
      
          jena-fuseki-docker
          Build a docker container for Fuseki
      
      
          jena-fuseki-geosparql
          Integration for GeoSPARQL
      
      
          Webapp
          
      
      
          jena-fuseki-webapp
          Web application and command line startup
      
      
          jena-fuseki-fulljar
          Build the combined jar for Fuseki/UI server
      
      
          jena-fuseki-war
          Build the war file for Fuseki/UI server\n\nOn this page
    
  
    Contents
    Download Fuseki
      
        Previous releases
        Development Builds
      
    
    How to Contribute
      
        Source code
      
    
  

  
  
    Apache Jena Fuseki is a SPARQL server.  It can run as a standalone server, or embedded in an
application.
Fuseki provides the
SPARQL 1.1 protocols for query and update
as well as the
SPARQL Graph Store protocol.
Fuseki is integrated with TDB to provide a robust,
transactional, persistent storage layer. Fuseki also incorporates
Jena text query.
Contents

Download
Getting Started
Running Fuseki Server

As a standalone server
As a service
As a web application
Security with Apache Shiro


Running Fuseki Plain

Setup
As a Docker container
As an embedded SPARQL server
Security and data access control
Logging


Fuseki Configuration
Server Statistics and Metrics
How to Contribute
Client access

Use from Java


Extending Fuseki with Fuseki Modules
Links to Standards

The Jena users mailing is the place to get help with Fuseki.
Email support lists
Download Fuseki
Releases of Apache Jena Fuseki can be downloaded from:
Jena Downloads
Fuseki download files

  
      
          Filename
          Description
      
  
  
      
          apache-jena-fuseki-*VER*.zip
          The Fuseki server and UI
      
  

The Fuseki engine is also available as a Maven artifact:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-main</artifactId>
   <version>X.Y.Z</version>
</dependency>

and the UI is available as:
<dependency>
   <groupId>org.apache.jena</groupId>
   <artifactId>jena-fuseki-ui</artifactId>
   <version>X.Y.Z</version>
</dependency>

A WAR file is also available from the Jena download page.
Previous releases
While previous releases are available, we strongly recommend that wherever
possible users use the latest official Apache releases of Jena in
preference to using any older versions of Jena.
Previous Apache Jena releases can be found in the Apache archive area
at https://archive.apache.org/dist/jena
Development Builds
Regular development builds of all of Jena are available
(these are not formal releases) from the
Apache snapshots maven repository.
This includes the packaged build of Fuseki.
How to Contribute
We welcome contributions towards making Jena a better platform for semantic
web and linked data applications.  We appreciate feature suggestions, bug
reports and patches for code or documentation.
See “Getting Involved” for ways to
contribute to Jena and Fuseki, including patches and making github
pull-requests.
Source code
The development codebase is available from git.
Development builds (not a formal release):
SNAPSHOT
Source code:
https://github.com/apache/jena/tree/main/jena-fuseki2
The Fuseki code is under “jena-fuseki2/”:

  
      
          Code
          Purpose
      
  
  
      
          jena-fuseki-main
          The Fuseki server
      
      
          jena-fuseki-core
          The Fuseki engine
      
      
          jena-fuseki-server
          Build the combined jar for Fuseki server
      
      
          jena-fuseki-access
          Data access control
      
      
          apache-jena-fuseki
          The download for Fuseki
      
      
          Other
          
      
      
          jena-fuseki-docker
          Build a docker container for Fuseki
      
      
          jena-fuseki-geosparql
          Integration for GeoSPARQL
      
      
          Webapp
          
      
      
          jena-fuseki-webapp
          Web application and command line startup
      
      
          jena-fuseki-fulljar
          Build the combined jar for Fuseki/UI server
      
      
          jena-fuseki-war
          Build the war file for Fuseki/UI server
      
  


  
  
  
    On this page
    
  
    Contents
    Download Fuseki
      
        Previous releases
        Development Builds
      
    
    How to Contribute
      
        Source code\n\n\n\n