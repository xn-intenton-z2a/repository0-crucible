Documentation
    

  
  
    
      
      
      Tutorials
      
        
            
              
              Getting started with RDF4J
              
              Starting a new Maven project in Eclipse
              
              Creating custom SPARQL functions
              
              Creating SPARQL Queries with the SparqlBuilder
              
          
      
      
      
      
      Programming with RDF4J
      
        
            
              
              Setting up your development environment
              
              The RDF Model API
              
              The Repository API
              
              Parsing and Writing RDF with Rio
              
              The LMDB Store
              
              Full-text indexing with the Lucene SAIL
              
              Reasoning and Validation with SPIN
              
              Validation with SHACL
              
              Federation with FedX
              
              Integration with Spring
              
              GeoSPARQL
              
              RDF-star and SPARQL-star
              
          
      
      
      
      
      Tools
      
        
            
              
              RDF4J Console
              
              RDF4J Server and Workbench
              
              Application directory configuration
              
              Repository configuration templates
              
          
      
      
      
      
      Reference
      
        
RDF4J REST API
RDF4J API Javadoc


            
              
              The SAIL API
              
              RDF4J Binary RDF Format
              
              Repository and SAIL configuration
              
              Sesame to Eclipse RDF4J migration
              
          
      
      
      
      
      Info for RDF4J Developers
      
        
            
              
              Developer workflow and project management
              
              RDF4J merge strategy
              
              Release management
              
              Squashing Commits
              
          
      
      
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nDocumentation
    

  
  
    
      
      
      Tutorials
      
        
            
              
              Getting started with RDF4J
              
              Starting a new Maven project in Eclipse
              
              Creating custom SPARQL functions
              
              Creating SPARQL Queries with the SparqlBuilder
              
          
      
      
      
      
      Programming with RDF4J
      
        
            
              
              Setting up your development environment
              
              The RDF Model API
              
              The Repository API
              
              Parsing and Writing RDF with Rio
              
              The LMDB Store
              
              Full-text indexing with the Lucene SAIL
              
              Reasoning and Validation with SPIN
              
              Validation with SHACL
              
              Federation with FedX
              
              Integration with Spring
              
              GeoSPARQL
              
              RDF-star and SPARQL-star
              
          
      
      
      
      
      Tools
      
        
            
              
              RDF4J Console
              
              RDF4J Server and Workbench
              
              Application directory configuration
              
              Repository configuration templates
              
          
      
      
      
      
      Reference
      
        
RDF4J REST API
RDF4J API Javadoc


            
              
              The SAIL API
              
              RDF4J Binary RDF Format
              
              Repository and SAIL configuration
              
              Sesame to Eclipse RDF4J migration
              
          
      
      
      
      
      Info for RDF4J Developers
      
        
            
              
              Developer workflow and project management
              
              RDF4J merge strategy
              
              Release management
              
              Squashing Commits
              
          
      
      
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nWhat's new
  
  
    
      
  
  
    
      Wed, Apr 16, 2025
      
        RDF4J 4.3.16 released 
      
      
        RDF4J 4.3.16 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.

        
      
    
  
  
  
    
      Tue, Apr 15, 2025
      
        RDF4J 5.1.3 released
      
      
        RDF4J 5.1.3 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
    
      Tue, Feb 11, 2025
      
        RDF4J 5.1.2 released
      
      
        RDF4J 5.1.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
View all news


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.
  
  
  


        
      
    
  

  What's new
  
  
    
      
  
  
    
      Wed, Apr 16, 2025
      
        RDF4J 4.3.16 released 
      
      
        RDF4J 4.3.16 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.

        
      
    
  
  
  
    
      Tue, Apr 15, 2025
      
        RDF4J 5.1.3 released
      
      
        RDF4J 5.1.3 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
    
      Tue, Feb 11, 2025
      
        RDF4J 5.1.2 released
      
      
        RDF4J 5.1.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
View all news\n\n\n\nNews
    

  
   
  
  
    
      
    
      Wed, Apr 16, 2025
      
        RDF4J 4.3.16 released
      
      
        RDF4J 4.3.16 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Apr 15, 2025
      
        RDF4J 5.1.3 released
      
      
        RDF4J 5.1.3 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Feb 11, 2025
      
        RDF4J 5.1.2 released
      
      
        RDF4J 5.1.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Jan 28, 2025
      
        RDF4J 5.1.1 released
      
      
        RDF4J 5.1.1 is now available. This is a patch release fixing 9 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Thu, Nov 21, 2024
      
        RDF4J 5.1.0 released
      
      
        RDF4J 5.1.0 is now available.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Nov 10, 2024
      
        RDF4J 5.0.3 released
      
      
        RDF4J 5.0.3 is now available. This is a patch release fixing 11 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Nov 10, 2024
      
        RDF4J 5.1.0 Milestone 1
      
      
        Milestone number 1 of the upcoming 5.1.0 release of RDF4J is now available for download.
RDF4J 5.1.0 is a minor release focusing on … .
This milestone build is not yet feature-complete, but we are putting it out to receive early feedback on all the improvements we have put in.
        
      
      
        [read more]
      
    
    

    
      
    
      Thu, Nov 7, 2024
      
        RDF4J 4.3.15 released
      
      
        RDF4J 4.3.15 is now available. This is a patch release fixing 4 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Wed, Oct 2, 2024
      
        RDF4J 4.3.14 released
      
      
        RDF4J 4.3.14 is now available. This is a patch release fixing 1 bug.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Fri, Aug 2, 2024
      
        RDF4J 5.0.2 released
      
      
        RDF4J 5.0.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
    
    
      
        ««
      
      
        «
      
      
        1
      
      
        2
      
      
        3
      
      
        4
      
      
        5
      
      
        »
      
      
        »»
      
    
  

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nSupport
    

  
  Ask about or discuss RDF4J
RDF4J Github Discussions is the best place to ask questions, propose new ideas or discuss other issues related to RDF4J.
You can also find us on Gitter, if you prefer an instant messaging style of communication. However, we make no promises that any of the RDF4J developers will be online at any given time.
We previously used a Google group called rdf4j-users. This group has now been archived. You can still browse the archive, but no new posts will be accepted.
The RDF4J development team uses the rdf4j-dev@eclipse.org mailinglist to discuss development progress.
Reporting bugs and requests for improvement
If you think you’ve found a bug in RDF4J, or wish to log a request for a new feature or improvement, please use the RDF4J issue tracker. Before you add your new issue, though, please have a look around to see if it’s not already there.


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nTutorials
    

  
  
    
        
          
          Getting started with RDF4JIn this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
          
          Starting a new Maven project in EclipseIf you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.
          
          Creating custom SPARQL functionsIn this short tutoral, we’ll create a simple custom function and add it RDF4J’s SPARQL engine.
          
          Creating SPARQL Queries with the SparqlBuilderRDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
          
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nStarting a New Maven Project in Eclipse
    

  
  If you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.

    
    
You don't have to use Apache Maven or Eclipse IDE if you want to work with RDF4J. These are simply very useful tools for quickly getting a Java project started. Maven is good because it allows you to just define which libraries you want to use and never worry about any further third-party libraries you might need, and Eclipse IDE is good because it has good integration with Maven, code completion features, and is just generally a great Java development environment. But you can work with RDF4J just as well in a different build tool or IDE.



In this tutorial, we assume that you have a basic understanding of programming in Java, and have at least an inkling of what RDF is. However, we do not assume that you know how to use either RDF4J, Maven, or Eclipse, so we’ll go through it all one step at a time. If anything is already sufficiently familiar to you, you are of course free to skip ahead!
Setting up your environment
Before we kick off, you will need to install the Eclipse IDE. In this tutorial, we will use Eclipse for Java Developers version 4.18.0 (2020-12), but it shouldn’t matter too much which exact version you have installed. Select either the “Eclipse for Java developers” or “Eclipse for Java EE developers” installation option.
Eclipse IDE comes with a Maven plugin (called m2e) already installed. We will use this plugin to work with Maven. You are of course free to also install the Maven command line tools, but we won’t be using those directly in this tutorial.
When starting Eclipse for the first time, you will be asked for a workspace directory – this will be where Eclipse stores all project data as well as some configuration information. Feel free to pick/create a directory of your choice, or just accept the default.
Once Eclipse is started (and any welcome messages have been closed), you will be presented with a screen that should look roughly like this:

Tweaking Eclipse preferences
Before we start creating our actual project in Eclipse, there are a few preferences that we should tweak. This step is not required, but it will make things a little easier in the rest of this tutorial.
From the menu, select Eclipse -> Preferences. The Preferences dialog pops up. On the left, select Maven. You will see the Maven configuration settings. The options “Download Artifact Sources” and “Download Artifact JavaDoc” are unchecked by default. Check them both, then click OK.

The reason we want this, by the way, is that having either the sources or the Javadoc available really helps when you use code autocompletion in Eclipse – Eclipse will automatically read the Javadoc and provide the documentation in a little tooltip. As said: not required, but very useful.
Creating a new project
Now that we’re all set up, we can kick things off by creating a new project. Select the File menu, then New -> New Project... . A dialog will appear. In this dialog, select the option Maven Project:

Once you click Next, you will be presented with a screen to create a new Maven Project. Make you sure the option ‘Create a simple project’ is checked:

Click Next again. In the following screen you define further details of your Maven project, such as group and artifact ids, version number, and so on. For this tutorial,  you only have to fill in three fields:

group id – typically you use something like a Java package name for this. We will use org.example.
artifact id –  name of the maven artifact if you publish our project as one. We will use rdf4j-getting-started.
name – the project name. We will use HelloRDF4J .


Once this has been filled in you can click Finish. We now have our first Eclipse project, huzzah!
Defining the POM
So we have a project. That’s great, but it’s still empty: we haven’t added any code or necessary libraries (such as the RDF4J libraries) yet.
We will remedy the library-situation first. In Maven, requirements for libraries (we’re talking about jar files here) are specified in a special project configuration file called pom.xml, often referred to as “the POM”. Open your project and you will see that a file with that name is already present. Doubleclick it to open it in the POM editor:

As you can see, the POM already contains some information – in fact the information that we added when we created our project. However, we need to add more information about our project. Specifically, we want to add which external Java libraries we want to include in our project. In Maven, you do this by specifying dependencies.
The first dependency we will add is for RDF4J itself. RDF4J is not a single library, but consists of a large collection of separate modules, allowing you to pick and choose which functionality you need. To make handling of all the various modules and the dependencies they all need easier, we will first add a “Bill Of Materials” (BOM) dependency. The easiest way to do this is to edit the XML source code directly. Switch to the ‘pom.xml’ tab of the POM editor, and add the following XML fragment:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.6.0</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
It should look like this after you’ve added this and saved the file:

Next we need to add the actual RDF4J modules we want to use. As said, this is where you could pick and choose the specific modules (parsers, databases, etc) that you want. For this tutorial, however, we’ll be lazy and just add the entire core framework. We do this by adding the rdf4j-storage dependency, as follows:
<dependencies>
  <dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
  </dependency>
</dependencies>
Notice that because we already have a BOM added, we don’t need to specify a version number for this dependency. Once you have done this and save the file, Eclipse will automatically begin downloading the RDF4J libraries. This may take a little while, just let it do its thing. Once it’s complete, your POM should look like this:

We are going to add one more dependency, namely for a logging framework. RDF4J uses the SLF4J logging API, which requires a logging implementation, so we’ll provide one, in this case slf4j-simple (which is, as the name implies, a very simple logging library that by default just logs to the console). Add a new dependency, with group id org.slf4j, artifact id slf4j-simple. Again we don’t need to specify a version number: the RDF4J Bill of Materials already specifies which version of slf4j is compatible with RDF4J:
<dependency>
  <groupId>org.slf4j</groupId>
  <artifactId>slf4j-simple</artifactId>
  <scope>runtime</scope>
</dependency>
Once you have saved your changes to the POM, your project should be automatically refreshed by Eclipse again, and you should see a new section called ‘Maven dependencies’ in the Package Explorer (If your project does not automatically refresh and you don’t see the above section, right-click on your project in the Package Explorer, and select Maven -> Update Project...). This section, when opened, shows a list of jar files which can now be used in your project:

As you can see, this list contains not just all the RDF4J libraries, but also a lot of other libraries that RDF4J depends on.

    
    
If you are interested in having more control over which libraries are and aren't included, please see Setting up your development environment.



Configuring the Java compiler
Unfortunately the default Maven archetype that we used to create our new project uses a very old Java compiler (1.5) by default. Since RDF4J requires Java 8 at a minimum (we actually recommend Java 11 for better performance), we will need to change this.
Copy-paste (or type if you prefer) the following section into the xml file (put it just above the dependencyManagement section we added in earlier). This will set the version to Java 11.
<properties>
  <maven.compiler.source>11</maven.compiler.source>
  <maven.compiler.target>11</maven.compiler.target>
</properties>
Once you have done this, and saved your POM, you will need to manually update your Maven project for Eclipse to accept the changes. You do this by right-clicking on your project (in the project explorer), and then selecting ‘Maven’ -> ‘Update Project…':

Programming our first Semantic Web application
We’re done preparing, we can start programming! Right-click on src/main/java and select New -> Class. In the dialog shown, set the package name to org.example, the class name to HelloRDF4J, and make sure the option public static void main (String[] args) is checked:

Click ‘Finish’ and Eclipse will create the new class and automatically open it in an editor.
Since we will want to work with RDF data, we will start by creating something to keep that RDF data in. In RDF4J, RDF data is usually stored in a Repository. There many different types of Repository, both for local access as well as for accessing remote services. In this tutorial, we will create a simple local repository, namely one that uses an in-memory store, without any sort of persistent storage functionality (so the data will be lost when the program exits). Add the following code to your main method:
Repository rep = new SailRepository(new MemoryStore());
Once you have done this and saved the file, you will notice some red lines appearing:

This is Eclipse telling you that there is something wrong with your code. In this case, the problem is that several import statements are missing (note that Eclipse tells you what is wrong in detail as well in the ‘Problems’ tab underneath the editor). We’ll need to add those import statements. You can add each import manually of course, but luckily Eclipse has a shortcut. Hit Ctrl+Shift+O and Eclipse should automatically resolve all missing imports for you. It will pop up a dialog if there is more than one possibility for a particular import, which will like happen for the Repository interface. Just pick the one from org.eclipse.rdf4j:

Hit ‘Finish’, then save your code.
Now that we have created our repository, we are going to put some data in it, and then get that data out again and print it to the console.
Adding data can be done in numerous ways. In this tutorial, we will be creating a few RDF statements directly in Java (rather than, say, loading them from a file). To do this, we need a namespace that we can use to create any new IRIs we need. With this namespace, we can create a new IRI. We will create an identifier for a person called “John”, about whom we will later add some data to our repository:
Namespace ex = Values.namespace("ex", "http://example.org/");
IRI john = Values.iri(ex, "john");
We use a static factory(the Values class) to easily create new Namespaces, IRIs and other types of values.
We have now created a IRI, but have not yet added any data to our repository. To do this, we first need to open a RepositoryConnection. Such a connection allows us to add, remove, and retrieve data from our Repository. We do this as follows:
RepositoryConnection conn = rep.getConnection();
It is important to keep track of open connections and to close them again when we are finished with them, to free up any resources the connection may keep hold of. RDF4J connections are “AutoCloseable”, which means we can let Java handle the closing of the connection when we’re done with it, rather than having to deal with this ourselves. We use a try-with-resources construction for this, like so:
try (RepositoryConnection conn = rep.getConnection()) {
}
Your code should now look as follows:

Using the connection, we can start adding statements to our repository. We will add two triples, one to assert that John is a Person, and one to assert that John’s name is “John”:
conn.add(john, RDF.TYPE, FOAF.PERSON);
conn.add(john, RDFS.LABEL, Values.literal("John"));
Note how for well-known RDF vocabularies, such as RDF, RDFS, and FOAF, RDF4J provides constants that you can easily reuse to create/read data with. Also, you see another use of the Values static factory here, this time to create a Literal object.
Once we have added our data, we can retrieve it back from the repository again. You can do this in several ways, with a SPARQL query or using the RepositoryConnection API methods directly. Here we will use the latter:
RepositoryResult<Statement> statements = conn.getStatements(null, null, null);
We use the getStatements() method to retrieve data from the repository. The three arguments (all null) are for the subject, predicate, and object we wish to match. Since they are all set to null, this will retrieve all statements.
The object returned by getStatements() is a RepositoryResult, which implements both Iteration and Iterable: it allows you to process the result in a streaming fashion, using its hasNext() and next() methods, as well as using Java’s for-each loop to iterate over it. Almost all the methods that retrieve data from a Repository return such a streaming/iterating object: they are very useful for processsing very large result sets, because they do not require that the entire result is kept in memory all at the same time.
It is important to always call close() on this kind of result object when you are done with them, to avoid memory leaks and other problems. Since the RepositoryResult is also a AutoCloseable, you can use a try-with-resources construction (similar to what we used for opening the connection) to handle this.
However, since our Repository only contains 2 statements, we can safely just transfer the entire result into a Java Collection. RDF4J has the Model interface for this, which is an extension of the standard Java collections that has some special tricks up its sleeve for dealing with RDF data, but which you can also use in the same manner as a Set or a List. We convert our result to a Model as follows:
Model model = QueryResults.asModel(statements);
The QueryResults utility retrieves all the statements from the supplied result, and puts them in a Model. It also automatically closes the result object for you.
Finally, we want to print out our data to the console. This too is something you can do in numerous ways, but let’s just say we would like to print out our data in Turtle format. Here’s how:
Rio.write(model, System.out, RDFFormat.TURTLE);
Rio (which stands for “RDF I/O”) is the RDF4J parser/writer toolkit. We can just pass it our model, specify the outputstream, and the format in which we’d like it written, and Rio does the rest. Your code should now look like this:

You now have created your first Semantic Web application! After saving, just right-click on HelloRDF4J.java (in the project explorer) and select Run as -> Java application. You will see the following output in the Console:
<http://example.org/john> a <http://xmlns.com/foaf/0.1/Person> ;
         <http://www.w3.org/2000/01/rdf-schema#label> "John" .
As you can see, it contains exactly the two statements that we added earlier, in Turtle syntax.
However, it is a bit ugly, using all those long IRIs. We can make the output a bit prettier, by adding some namespace definitions to our Model:
model.setNamespace(RDF.NS);
model.setNamespace(RDFS.NS);
model.setNamespace(FOAF.NS);
model.setNamespace(ex);
Add these lines to your code, directly after where you have created the model. Then run your program again. You will now see the following:

That’s it! Obviously there is still loads to learn about how to use RDF4J effectively, but you’ve got the basics under control now: you can set up a new project using RDF4J,  and have seen some basic ways to add, write, and retrieve data. The rest is up to you. Good sources of further documentation are the Getting Started tutorial, Programming with RDF4J, and of course the API Javadoc.

  

     
      
        
          

  Table of Contents

  
  
    Setting up your environment
      
        Tweaking Eclipse preferences
      
    
    Creating a new project
    Defining the POM
    Configuring the Java compiler
    Programming our first Semantic Web application\n\n\n\nCreating Custom SPARQL Functions
    

  
  In this short tutoral, we’ll create a simple custom function and add it RDF4J’s SPARQL engine.
The SPARQL query language is extensible by nature: it allows you to add your own custom functions if the standard set of operators is not sufficient for your needs. The RDF4J SPARQL engine has been designed with this extensibility in mind: you can define your own custom function and use it as part of your SPARQL queries.
Here, we are going to implement a boolean function that detects if some string literal is a palindrome.
The palindrome function
Suppose we have the following RDF data:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix ex: <http://example.org/> .

ex:a rdfs:label "step on no pets" .
ex:b rdfs:label "go on, try it" .
We would like to be able to formulate a SPARQL query that allows us to retrieve all resources that have a palindrome as their label:
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX cfn: <http://example.org/custom-function/>
SELECT ?x ?label
WHERE {
   ?x rdfs:label ?label .
   FILTER(cfn:palindrome(str(?label)))
}
The expected result of this query, given the above data, would be:



x
label




ex:a
"step on no pets"



Unfortunately, the function cfn:palindrome is not a standard SPARQL function, so this query won’t work: the RDF4J SPARQL engine will simply report an error.
We could of course retrieve all label values in the database and then do some checking ourselves on these values, to detect if they’re palindromes. However if we add a custom function instead, we remove the need to scan over the entire database: the SPARQL engine itself can determine if a value is a valid palindrome or not, which removes the need for us to loop over all possible values.
There’s two basic steps in adding custom functions to RDF4J:

implementing a Java class for the function;
creating a JAR file with your function code in it and an Service Provider Interface (SPI) configuration.

Implementing the custom function as a Java class
In the RDF4J SPARQL engine, functions are expected to implement the Function
 interface.
package org.eclipser.rdf4j.examples.function;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

public class PalindromeFunction implements Function { }
The Function interface defines two methods: evaluate() and getURI(). The latter of these is a simple method that returns a string representation of the URI of the function:
// define a constant for the namespace of our custom function
public static final String NAMESPACE = "http://example.org/custom-function/";

/**
 * return the URI 'http://example.org/custom-function/palindrome' as a String
 */
public String getURI() {
    return NAMESPACE + "palindrome";
}
The real proof of the pudding is in the evaluate() method: this is where the function logic is implemented. In other words, in this method we check the incoming value to see if it is, first of all, a valid argument for the function, and second of all, a palindrome, and return the result.
Example 1
 show how we put everything together:
package org.eclipse.rdf4j.examples.function;

import static org.eclipse.rdf4j.model.util.Values.literal;

import org.eclipse.rdf4j.model.Literal;
import org.eclipse.rdf4j.model.Value;
import org.eclipse.rdf4j.query.algebra.evaluation.ValueExprEvaluationException;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

/**
 * An example custom SPARQL function that detects palindromes
 *
 * @author Jeen Broekstra
 */
public class PalindromeFunction implements Function {

    // define a constant for the namespace of our custom function
    public static final String NAMESPACE = "http://example.org/custom-function/";

    /**
     * return the URI 'http://example.org/custom-function/palindrome' as a
     * String
     */
    public String getURI() {
	return NAMESPACE + "palindrome";
    }

    /**
     * Executes the palindrome function.
     *
     * @return A boolean literal representing true if the input argument is a
     *         palindrome, false otherwise.
     * @throws ValueExprEvaluationException
     *         if more than one argument is supplied or if the supplied argument
     *         is not a literal.
     */
    public Value evaluate(TripleSource tripleSource, Value... args)
	throws ValueExprEvaluationException
	{
	    // our palindrome function expects only a single argument, so throw an error
	    // if there's more than one
	    if (args.length != 1) {
		throw new ValueExprEvaluationException(
			"palindrome function requires"
			+ "exactly 1 argument, got "
			+ args.length);
	    }
	    Value arg = args[0];
	    // check if the argument is a literal, if not, we throw an error
	    if (!(arg instanceof Literal)) {
		throw new ValueExprEvaluationException(
			"invalid argument (literal expected): " + arg);
	    }

	    // get the actual string value that we want to check for palindrome-ness.
	    String label = ((Literal)arg).getLabel();
	    // we invert our string
	    String inverted = "";
	    for (int i = label.length() - 1; i >= 0; i--) {
		inverted += label.charAt(i);
	    }
	    // a string is a palindrome if it is equal to its own inverse
	    boolean palindrome = inverted.equalsIgnoreCase(label);

	    // a function is always expected to return a Value object, so we
	    // return our boolean result as a Literal
	    return literal(palindrome);
	}
}
You are completely free to implement your function logic: in the above example, we have created a function that only returns true or false, but since the actual return type of an RDF4J function is Value
, you can create functions that return string literals, numbers, dates, or even IRIs or blank nodes.
In addition, the evaluate method accepts a TripleSource as input parameter, which you can use to inspect the underlying database, and query it for further information (for a simple/silly example see the Existing Palindrome function
, which in addition to checking that the argument is a palindrome, also checks if that palindrome already exists in the database).
There are two important things to keep in mind though:

the evaluate() method is invoked for every single solution in the query result. So you should make sure that the implementation of your function is not overly complex and memory-intensive.
RDF4J treats functions as singletons. This means that you should not “keep state” as part of your function; for example storing intermediate results in a private object field. This state will carry over between different uses of the function and even between different queries using it, making your results inconsistent.

Once we have created the Java class for our function, we need some way to make the RDF4J SPARQL engine aware of it. This is where the Service Provider Interface (SPI) comes into play.
Creating an SPI configuration
RDF4J’s set of SPARQL functions is dynamically determined through the use of a java.util.ServiceLoader class. Specifically, RDF4J has a class called FunctionRegistry
 which keeps track of all implementations of the Function interface. Java’s SPI mechanism depends on the presence of configuration files in the JAR files that contain service implementations. This configuration file is expected to be present in the directory META-INF/services in your JAR file.
In the case of the SPARQL function registry, the name of this configuration file should be org.eclipse.rdf4j.query.algebra.evaluation.function.Function (in other words, the file name is equal to the fully-qualified name of the service interface we are providing an implementation for). The contents are really quite simple: an SPI configuration is a text file, containing the fully-qualified names of each Java class that provides an SPI implementation, one on each line. So in our case, the contents of the file would be:
org.eclipse.rdf4j.example.function.PalindromeFunction
Apart from this configuration file, your JAR file should of course also contain the actual compiled class. All of this is fairly easy to do, for example from your Eclipse project:

create a directory META-INF and a subdirectory META-INF/services within the src directory of your project (or, if you use Maven, within src/main/resources) See our example resources dir for an example;
Add a text file named org.eclipse.rdf4j.query.algebra.evaluation.function.Function to this new directory. Make sure it contains a single line with the fully qualified name of your custom function class (in our example, that’s org.eclipse.rdf4j.example.function.PalindromeFunction);
Use Eclipse’s export function (or alternatively Maven’s package command) to create a JAR file (select the project, click ‘File’ -> ‘Export’ -> ‘JAR file’). Make sure the JAR file produced contains your compiled code and the sevice registry config file.

Once you have a proper JAR file, you need to add it the runtime classpath of your RDF4J project (or if you’re aiming to use this in an RDF4J Server, add it to the RDF4J Server webapp classpath and restart). After that, you’re done: RDF4J should automatically pick up your new custom function, you can from now on use it in your SPARQL queries.
Further reading

Introduction to the Service Provider Interface - Oracle Java documentation.

If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    The palindrome function
    Implementing the custom function as a Java class
    Creating an SPI configuration
    Further reading\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nProgramming With RDF4J
    

  
  
    
        
          
          Setting up your development environmentThis chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
          
          The RDF Model APIThe RDF Model API is the core of the RDF4J framework. It provides the basic building blocks for manipulating RDF data in Java.
          
          The Repository APIThe Repository API is the central access point for RDF4J-compatible RDF databases (a.k.a. triplestores), as well as for SPARQL endpoints. This is what you use to execute SPARQL queries and update your data.
          
          Parsing and Writing RDF with RioThe RDF4J framework includes a set of parsers and writers for RDF called Rio. Rio (“RDF I/O”) is a toolkit that can be used independently from the rest of RDF4J.
          
          The LMDB StoreNew in RDF4J 4.0

Experimental

The RDF4J LMDB Store is a new SAIL database, using the Symas Lightning
Memory-Mapped Database: a fast embeddable
key-value database using memory-mapped IO for great performance and stability.
          
          Full-text indexing with the Lucene SAILThe LuceneSail enables you to add full text search of RDF literals to find subject resources to any Sail stack.
          
          Reasoning and Validation with SPINThe SPARQL Inferencing Notation (SPIN) is a way to represent a wide range of business rules on top of an RDF dataset. These rules can be anything from constraint validation to inferred property value calculation.
          
          Validation with SHACLThe SHapes Constraint Language (SHACL) is a language for validating RDF graphs.
          
          Federation with FedXFedX provides transparent federation of multiple SPARQL endpoints under a single virtual endpoint.
          
          Integration with SpringThe rdf4j-spring
 module allows for using an RDF4J repository as the data backend of a spring application.
          
          GeoSPARQLRDF4J offers an extended algebra for partial GeoSPARQL support. When enabled, this offers additional geospatial functionality as part of the SPARQL engine, on top of any RDF4J repository, using the well-known Spatial4J and JTS libraries for geospatial reasoning.
          
          RDF-star and SPARQL-starRDF4J has (experimental) support for RDF-star and SPARQL-star.
          
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nThe RDF Model API
    

  
  The RDF Model API is the core of the RDF4J framework. It provides the basic building blocks for manipulating RDF data in Java.
RDF Building Blocks: IRIs, literals, blank nodes and statements
The core of the RDF4J framework is the RDF Model API (see the Model API Javadoc), defined in package org.eclipse.rdf4j.model. This API defines how the building blocks of RDF (statements, IRIs, blank nodes, literals, and models) are represented.
RDF statements are represented by the Statement
 interface. Each Statement has a subject, predicate, object and (optionally) a context. Each of these 4 items is a Value
. The Value interface is further specialized into Resource
, and Literal
. Resource represents any RDF value that is either a BNode
 or an IRI
. Literal represents RDF literal values (strings, dates, integer numbers, and so on).
Creating new building blocks: the Values and Statements factory methods
 New in RDF4J 3.5 

To create new values and statements, you can use the Values
  and Statements
 static factory methods, which provide easy creation of new IRIs, Literals, BNodes, Triples and Statements based on a variety of different input objects.
import static org.eclipse.rdf4j.model.util.Statements.statement;
import static org.eclipse.rdf4j.model.util.Values.iri;
import static org.eclipse.rdf4j.model.util.Values.literal;

IRI bob = iri("http://example.org/bob");
IRI nameProp = iri("http://example.org/name");
Literal bobsName = literal("Bob");
Literal bobsAge = literal(42);

Statement st = statement(bob, nameProp, bobsName, null);
Using a ValueFactory
If you want more control than the static factory methods provide, you can also use a ValueFactory
 instance. You can obtain one from Values
, or you can directly use a singleton ValueFactory implementation called SimpleValueFactory
:
import org.eclipse.rdf4j.model.ValueFactory;
import org.eclipse.rdf4j.model.impl.SimpleValueFactory;

ValueFactory factory = SimpleValueFactory.getInstance();
For performance reasons, the SimpleValueFactory provides only basic input validation. The ValidatingValueFactory
 is stricter, albeit somewhat slower (though this should not be noticeable unless you are working with very significant amounts of data).
You can also obtain a ValueFactory from the Repository
 you are working with, and in fact, this is the recommend approach. For more information about this see the Repository API documentation.
Regardless of how you obtain your ValueFactory, once you have it, you can use it to create new IRIs, Literals, and Statements:
IRI bob = iri(factory, "http://example.org/bob");
IRI name = iri(factory,"http://example.org/name");
Literal bobsName = literal(factory, "Bob");
Statement nameStatement = statement(factory, bob, name, bobsName);
Or if you prefer, using the Valuefactory directly:
IRI bob = factory.createIRI("http://example.org/bob");
IRI name = factory.createIRI("http://example.org/name");
Literal bobsName = factory.createLiteral("Bob");
Statement nameStatement = factory.createStatement(bob, name, bobsName);
The Model API also provides pre-defined IRIs for several well-known vocabularies, such as RDF, RDFS, OWL, DC (Dublin Core), FOAF (Friend-of-a-Friend), and more. These constants can all be found in the org.eclipse.rdf4j.model.vocabulary package, and can be quite handy in quick creation of RDF statements (or in querying a Repository, as we shall see later):
Statement typeStatement = Values.statement(bob, RDF.TYPE, FOAF.PERSON);
The Model interface
The above interfaces and classes show how we can create the individual building blocks that make up an RDF model. However, an actual collection of RDF data is just that: a collection. In order to deal with collections of RDF statements, we can use the org.eclipse.rdf4j.model.Model
 interface.
Model is an extension of the default Java Collection class java.util.Set<Statement>. This means that you can use a Model like any other Java collection in your code:
// create a new Model to put statements in
Model model = DynamicModelFactory.createEmptyModel();
// add an RDF statement
model.add(typeStatement);
// add another RDF statement by simply providing subject, predicate, and object.
model.add(bob, name, bobsName);

// iterate over every statement in the Model
for (Statement statement: model) {
	   ...
}
In addition, however, Model offers a number of useful methods to quickly get subsets of statements and otherwise search/filter your collection of statements. For example, to quickly iterate over all statements that make a resource an instance of the class foaf:Person, you can do:
for (Statement typeStatement: model.filter(null, RDF.TYPE, FOAF.PERSON)) {
  // ...
}
Even more convenient is that you can quickly retrieve the building blocks that make up the statements. For example, to immediately iterate over all subject-resources that are of type foaf:Person and then retrieve each person’s name, you can do something like the following:
for (Resource person: model.filter(null, RDF.TYPE, FOAF.PERSON).subjects()) {
  // get the name of the person (if it exists)
  Optional<Literal> name = Models.objectLiteral(model.filter(person, FOAF.NAME, null));
}
The filter() method returns a Model again. However, the Model returned by this method is still backed by the original Model. Thus, changes that you make to this returned Model will automatically be reflected in the original Model as well.
RDF4J provides three default implementations of the Model interface: org.eclipse.rdf4j.model.impl.DynamicModel
, org.eclipse.rdf4j.model.impl.LinkedHashModel
, and org.eclipse.rdf4j.model.impl.TreeModel
. The difference between them is in their performance for different kinds of lookups and insertion patterns (see their respective javadoc entries for details). These differences are only really noticeable when dealing with quite large collections of statements, however.
Building RDF Models with the ModelBuilder
Since version 2.1, RDF4J provides a ModelBuilder
 utility. The ModelBuilder provides a fluent API to quickly and efficiently create RDF models programmatically.
Here’s a simple code example that demonstrates how to quickly create an RDF graph with some FOAF data:
ModelBuilder builder = new ModelBuilder();

// set some namespaces
builder.setNamespace("ex", "http://example.org/").setNamespace(FOAF.NS);

builder.namedGraph("ex:graph1")      // add a new named graph to the model
       .subject("ex:john")        // add  several statements about resource ex:john
	 .add(FOAF.NAME, "John")  // add the triple (ex:john, foaf:name "John") to the named graph
	 .add(FOAF.AGE, 42)
	 .add(FOAF.MBOX, "john@example.org");

// add a triple to the default graph
builder.defaultGraph().add("ex:graph1", RDF.TYPE, "ex:Graph");

// return the Model object
Model m = builder.build();
The ModelBuilder offers several conveniences:

you can specify a subject/predicate IRI as a prefixed name string (for example “ex:john”), so you don’t have to use a ValueFactory to create an IRI object first.
you can add a literal object as a String, an int, or several other supported Java primitive types.
the subject() method makes it easier to take a resource-centric view when building an RDF Model.

Quickly accessing data with the Models utility
The Models
 utility class offers a number of useful methods for convenient access and manipulation of data in a Model object. We have already shown some examples of its use in previous sections. For example, to retrieve the value of the foaf:name properties for all resources of type foaf:Person:
for (Resource person: model.filter(null, RDF.TYPE, FOAF.PERSON).subjects()) {
  // get the name of the person (if it exists)
  Optional<Literal> name = Models.objectLiteral(model.filter(person, FOAF.NAME, null));
}
The Models.objectLiteral method retrieves an arbitrary object literal value from the statements in the supplied Model. Since the supplied Model is filtered to only contain the foaf:name statements for the given person, the resulting object literal value is the name value for this person. Note that if the model happens to contain more than one name value for this person, this will just return an arbitrary one.
The Models utility provides variants for retrieving different types of object values: Models.object() retrieves a Value, Models.objectResource() a Resource, Models.objectIRI() an IRI.
Property-centric access
To provide quicker access to a property’s value(s), the Models class offers some further shortcuts that bypass the need to first filter the Model. For example, to retrieve the name literal, we can replace the `objectLiteral call from the previous example like so:
for (Resource person: model.filter(null, RDF.TYPE, FOAF.PERSON).subjects()) {
  // get the name of the person (if it exists)
  Optional<Literal> name = Models.getPropertyLiteral(model, person, FOAF.NAME);
}
Models also provides methods that allow retrieving all values, instead of one arbitrary one:
for (Resource person: model.filter(null, RDF.TYPE, FOAF.PERSON).subjects()) {
  // get all name-values of the person
  Set<Literal> names = Models.getPropertyLiterals(model, person, FOAF.NAME);
}
For both retrieval types, Models also provides variants that retrieve other value types such as IRIs. The Models
 javadoc is worth exploring for a complete overview of all methods.
In addition to retrieving values in a property-centric manner, Models also provides a setProperty method, which can be used to quickly give a resoure’s property a new value. For example:
Literal newName = vf.createLiteral("John");
Models.setProperty(person, FOAF.NAME, newName);
This will remove any existing name-properties for the given person, and set it to the single new value “John”.
RDF Collections
To model closed lists of items, RDF provides a Collection vocabulary . RDF Collections are represented as a list of items using a Lisp-like structure. The list starts with a head resource (typically a blank node), which is connected to the first collection member via the rdf:first relation. The head resource is then connected to the rest of the list via an rdf:rest relation. The last resource in the list is marked using the rdf:nil node.
As an example, a list containing three values, “A”, “B”, and “C” looks like this as an RDF Collection:

Here, the blank node _:n1 is the head resource of the list. In this example it is declared an instance of rdf:List, however this is not required for the collection to be considered well-formed. For each collection member, a new node is added (linked to the previous node via the rdf:rest property), and the actual member value is linked to to this node via the rdf:first property. The last member member of the list is marked by the fact that the value of its rdf:rest property is set to rdf:nil.
Working with this kind of structure directly is rather cumbersome. To make life a little easier, rdf4j provides several utilities to convert between Java Collections and RDF Collections.
Converting to/from Java Collections
As an example, suppose we wish to add the above list of three string literals as a property value for the property ex:favoriteLetters of ex:John .
The RDFCollections
 utility allows us to do this, as follows:
import static org.eclipse.rdf4j.model.util.Values.bnode;
import static org.eclipse.rdf4j.model.util.Values.iri;
import static org.eclipse.rdf4j.model.util.Values.literal;

String ns = "http://example.org/";
// IRI for ex:favoriteLetters
IRI favoriteLetters = iri(ns, "favoriteLetters");
// IRI for ex:John
IRI john = iri(ns, "John");
// create a list of letters
List<Literal> letters = Arrays.asList(new Literal[] { literal("A"), literal("B"), literal("C") });
// create a head resource for our list
Resource head = bnode();
// convert our list and add it to a newly-created Model
Model aboutJohn = RDFCollections.asRDF(letters, head, new LinkedHashModel());
// set the ex:favoriteLetters property to link to the head of the list
aboutJohn.add(john, favoriteLetters, head);
Of course, we can also convert back:
Model aboutJohn = ... ; // our Model about John
// get the value of the ex:favoriteLetters property
Resource node = Models.objectResource(aboutJohn.filter(john, favoriteLetters, null)).orElse(null);
// Convert its collection back to an ArrayList of values
if(node != null) {
	 List<Value> values = RDFCollections.asValues(aboutJohn, node, new ArrayList<Value>());
	 // you may need to cast back to Literal.
	 Literal a = (Literal)values.get(0);
}
Extracting, copying, or deleting an RDF Collection
To extract an RDF Collection from the model which contains it, we can do the following:
Model aboutJohn = ...; // our model
// get the value of the ex:favoriteLetters property
Resource node = Models.objectResource(aboutJohn.filter(john, favoriteLetters, null)).orElse(null);
// get the RDF Collection in a separate model
if (node != null) {
	 Model rdfList = RDFCollections.getCollection(aboutJohn, node, new LinkedHashModel());
}
As you can see, instead of converting the RDF Collection to a Java List of values, we get back another Model object from this, containing a copy of the RDF statements that together form the RDF Collection. This is useful in cases where your original Model contains more data than just the RDF Collection, and you want to isolate the collection.
Once you have this copy of your Collection, you can use it to add it somewhere else, or to remove the collection from your Model:
// remove the collection from our model about John
aboutJohn.removeAll(rdfList);
// finally remove the triple that linked John to the collection
aboutJohn.remove(john, favoriteLetters, node);
Actually, deleting can be done more efficiently than this. Rather than first creating a completely new copy of the RDF Collection only to then delete it, we can use a streaming approach instead:
// extract the collection from our model in streaming fashion and remove each triple from the model
RDFCollections.extract(aboutJohn, node, st -> aboutJohn.remove(st));
// remove the statement that linked john to the collection
aboutJohn.remove(john, favoriteLetters, node);
Working with rdf:Alt, rdf:Bag, rdf:Seq
(new since 3.3.0)
The RDF container classes rdf:Alt, rdf:Bag, and rdf:Seq can also be used to model sets or lists of items in RDF. RDF containers look like this:
   urn:myBag -rdf:type--> rdf:Bag
     |
     +---rdf:_1--> "A"
     |
     +---rdf:_2--> "B"
     |
     +---rdf:_3--> "C"
RDF4J offers utility conversion functions very similar to the utilities for RDF Collections: the  RDFContainers
 class.
For example, to create the above RDF container, we can do this:
List<Literal> letters = Arrays.asList(new Literal[] { literal("A"), literal("B"), literal("C") });
IRI myBag = iri("urn:myBag");
Model letterBag = RDFContainers.toRDF(RDF.BAG, letters, myBag, new TreeModel());
and to convert back to a java collection:
List<Value> newList = RDFContainers.toValues(RDF.BAG, letterBag, myBag, new ArrayList<>());


  

     
      
        
          

  Table of Contents

  
  
    RDF Building Blocks: IRIs, literals, blank nodes and statements
      
        Creating new building blocks: the Values and Statements factory methods
        Using a ValueFactory
      
    
    The Model interface
    Building RDF Models with the ModelBuilder
    Quickly accessing data with the Models utility
      
        Property-centric access
      
    
    RDF Collections
      
        Converting to/from Java Collections
        Extracting, copying, or deleting an RDF Collection
      
    
    Working with rdf:Alt, rdf:Bag, rdf:Seq\n\n\n\nThe Repository API
    

  
  The Repository API is the central access point for RDF4J-compatible RDF databases (a.k.a. triplestores), as well as for SPARQL endpoints. This is what you use to execute SPARQL queries and update your data.
The interfaces for the Repository API can be found in package org.eclipse.rdf4j.repository. Several implementations for these interface exist in various sub-packages.
Creating a Repository object
The central interface of the Repository API is the Repository
 interface. There are several implementations available of this interface. The three main ones are:
SailRepository
 is a Repository that operates directly on top of a Sail
 - that is, a particular database. This is the class most commonly used when accessing/creating a local RDF4J repository. SailRepository operates on a (stack of) Sail object(s) for storage and retrieval of RDF data. An important thing to remember is that the behaviour of a repository is determined by the Sail(s) that it operates on; for example, the repository will only support RDF Schema or OWL semantics if the Sail stack includes support for this.
HTTPRepository
 is a Repository implementation that acts as a proxy to a repository available on a remote RDF4J Server, accessible through HTTP.
SPARQLRepository
 is a Repository implementation that acts as a proxy to any remote SPARQL endpoint (whether that endpoint is implemented using RDF4J or not).
Creating Repository objects can be done in multiple ways. We will first show an easy way to quickly create such an object ‘on the fly’. In the section about the RepositoryManager and RepositoryProvider, we show some more advanced patterns, which are particularly useful in larger applications which have to handle and share references to multiple repositories.
Main memory RDF Repository
One of the simplest configurations is a repository that just stores RDF data in main memory without applying any inferencing. This is also the fastest type of repository that can be used. The following code creates and initializes a non-inferencing main-memory repository:
import org.eclipse.rdf4j.repository.Repository;
import org.eclipse.rdf4j.repository.sail.SailRepository;
import org.eclipse.rdf4j.sail.memory.MemoryStore;
...
Repository repo = new SailRepository(new MemoryStore());
The constructor of the SailRepository class accepts any object of type Sail, so we pass it a new main-memory store object (which is a Sail implementation).
The repository that is created by the above code is volatile: its contents are lost when the object is garbage collected or when your Java program is shut down. This is fine for cases where, for example, the repository is used as a means for manipulating an RDF model in memory.
Different types of Sail objects take parameters in their constructor that change their behaviour. The MemoryStore takes a datadir parameter that specifies a data directory for persisent storage. If specified, the MemoryStore will write its contents to this directory so that it can restore it when it is re-initialized in a future session:
File dataDir = new File("C:\\temp\\myRepository\\");
Repository repo = new SailRepository( new MemoryStore(dataDir) );
We can fine-tune the configuration of our repository by passing parameters to the constructor of the Sail object. Some Sail types may offer additional configuration methods, all of which need to be called before the repository is initialized. The MemoryStore currently has one such method: setSyncDelay(long), which can be used to control the strategy that is used for writing to the data file. For example:
File dataDir = new File("C:\\temp\\myRepository\\");
MemoryStore memStore = new MemoryStore(dataDir);
memStore.setSyncDelay(1000L);
Repository repo = new SailRepository(memStore);
The MemoryStore is designed for datasets with fewer than 100,000 triples.
Native RDF Repository
The NativeStore saves data to disk in a binary format which is optimized for compact storage and fast retrieval. If there is sufficient physical memory, the Native store will act like the MemoryStore on most operating systems because the read/write commands will be cached by the OS.
It is therefore an efficient, scalable and fast solution for datasets with up to 100 million triples (and probably even more).
The code for creation of a Native RDF repository is almost identical to that of a main memory repository:
import org.eclipse.rdf4j.repository.Repository;
import org.eclipse.rdf4j.repository.sail.SailRepository;
import org.eclipse.rdf4j.sail.nativerdf.NativeStore;
...
File dataDir = new File("/path/to/datadir/");
Repository repo = new SailRepository(new NativeStore(dataDir));
By default, the Native store creates a set of two indexes. To configure which indexes it should create, we can either use the NativeStore.setTripleIndexes(String) method, or we can directly supply a index configuration string to the constructor:
import org.eclipse.rdf4j.repository.Repository;
import org.eclipse.rdf4j.repository.sail.SailRepository;
import org.eclipse.rdf4j.sail.nativerdf.NativeStore;
...
File dataDir = new File("/path/to/datadir/");
String indexes = "spoc,posc,cosp";
Repository repo = new SailRepository(new NativeStore(dataDir, indexes));
 New in RDF4J 3.7

If a data directory is not set, a temporary directory will be created when the native store is initialized and (contrary to the previous examples) subsequently deleted when the repository is shut down.
This is convenient when the repository is only used as a means to transform large RDF files.
import org.eclipse.rdf4j.repository.Repository;
import org.eclipse.rdf4j.repository.sail.SailRepository;
import org.eclipse.rdf4j.sail.nativerdf.NativeStore;
...
Repository repo = new SailRepository(new NativeStore());
In the unlikely event of corruption the system property org.eclipse.rdf4j.sail.nativerdf.softFailOnCorruptDataAndRepairIndexes can be set to true to
allow the NativeStore to output CorruptValue/CorruptIRI/CorruptIRIOrBNode/CorruptLiteral objects. Take a backup of all data before setting
this property as it allows the NativeStore to delete corrupt indexes in an attempt to recreate them. Consider this feature experimental and use with caution.
Elasticsearch RDF Repository
 New in RDF4J 3.1

 Experimental 

The ElasticsearchStore stores RDF data in Elasticsearch. Not to be confused with the ElasticsearchSail which uses Elasticsearch for enhanced search.
The ElasticsearchStore is experimental and future releases may be incompatible with the current version. Write-ahead-logging is not supported.
This means that a write operation can appear to have partially succeeded if the ElasticsearchStore looses its connection to Elasticsearch during a commit.
Note that, while RDF4J is licensed under the EDL, several ElasticSearch dependencies are licensed under the Elastic License or the SSPL,
which may have implications for some projects.
Please consult the ElasticSearch website and license FAQ for more information.
Transaction isolation is not as strong as for the other stores. The highest supported level is READ_COMMITTED, and even this
level is only guaranteed when all other transactions also use READ_COMMITTED.
Performance for the NativeStore is in most cases considerably better than for the ElasticsearchStore.
The read cache in the ElasticsearchStore makes workloads with repetitive reads fast. Storing small, infrequently updated, datasets such as a
reference library or an ontology is a good usecase for the ElasticsearchStore.
The code for creation of an ElasticsearchStore is almost identical to other repositories:
import org.eclipse.rdf4j.repository.Repository;
import org.eclipse.rdf4j.repository.sail.SailRepository;
import org.eclipse.rdf4j.sail.elasticsearchstore.ElasticsearchStore;
...
// ElasticsearchStore(String hostname, int port, String clusterName, String index)
Repository repo = new SailRepository(new ElasticsearchStore("localhost", 9300, "elasticsearch", "rdf4j_index"));
Remember to call repo.shutdown() when you are done with your ElasticsearchStore. This will close the underlying Elasticsearch Client.
RDF Schema inferencing
As we have seen, we can create Repository objects for any kind of back-end store by passing them a reference to the appropriate Sail object. We can pass any stack of Sails this way, allowing all kinds of repository configurations to be created. For example, to stack an RDF Schema inferencer on top of a memory store, we simply create a repository like so:
import org.eclipse.rdf4j.repository.Repository;
import org.eclipse.rdf4j.repository.sail.SailRepository;
import org.eclipse.rdf4j.sail.memory.MemoryStore;
import org.eclipse.rdf4j.sail.inferencer.fc.SchemaCachingRDFSInferencer;
...
Repository repo = new SailRepository(
			  new SchemaCachingRDFSInferencer(
			  new MemoryStore()));
Each layer in the Sail stack is created by a constructor that takes the underlying Sail as a parameter. Finally, we create the SailRepository object as a functional wrapper around the Sail stack.
The SchemaCachingRDFSInferencer
 that is used in this example is a generic RDF Schema inferencer; it can be used on top of any Sail that supports the methods it requires. Both MemoryStore and NativeStore support these methods. However, a word of warning: the RDF4J inferencers add a significant performance overhead when adding and removing data to a repository, an overhead that gets progressively worse as the total size of the repository increases. For small to medium-sized datasets it peforms fine, but for larger datasets you are advised not to use it and to switch to alternatives.
Custom Inferencing
The previous section showed how to use the built-in RDF schema inferencer. This section will show how to create a repository capable of performing inferences according to a custom rule that you provide.
import org.eclipse.rdf4j.query.QueryLanguage;
import org.eclipse.rdf4j.repository.Repository;
import org.eclipse.rdf4j.repository.sail.SailRepository;
import org.eclipse.rdf4j.sail.memory.MemoryStore;
import org.eclipse.rdf4j.sail.inferencer.fc.CustomGraphQueryInferencer;
...
String pre = "PREFIX : <http://foo.org/bar#>\n";
String rule = pre + "CONSTRUCT { ?p :relatesTo :Cryptography } WHERE " +
	      "{ { :Bob ?p :Alice } UNION { :Alice ?p :Bob } }";
String match = pre + "CONSTRUCT { ?p :relatesTo :Cryptography } " +
	       "WHERE { ?p :relatesTo :Cryptography }";
Repository repo = new SailRepository(new CustomGraphQueryInferencer(
		  new MemoryStore(), QueryLanguage.SPARQL, rule, match));
Here is a data sample (given in the Turtle format) that serves to illustrate this example:
@prefix : <http://foo.org/bar#> .
:Bob   :exchangesKeysWith :Alice .
:Alice :sendsMessageTo    :Bob .

If the above data is loaded into the repository, the repository will also automatically have the following inferred statements:
@prefix : <http://foo.org/bar#> .
:exchangesKeysWith :relatesTo :Cryptography .
:sendsMessageTo    :relatesTo :Cryptography .

The SPARQL graph query in rule defines a pattern to search on, and the inferred statements to add to the repository.
The graph query in match is needed to decide what inferred statements already exist that may need to be removed when the normal repository contents change. For example, if the first sample data statement was removed, then the inference layer will automatically remove the inferred statement regarding :exchangesKeysWith.
In simple rule cases, such as this one, an empty string could have been provided for match instead, and the correct matcher query would have been deduced.
The CustomGraphQueryInferencer used here is fairly limited: it effectively only allows a single inference rule. For more complex custom inferencing or validation needs, RDF4J offers the SPIN Sail or the SHACL Sail.
Access over HTTP
Server-side RDF4J repositories
Working with remote RDF4J repositories is just as easy as working with local ones. We use a different Repository object, the HTTPRepository, instead of the SailRepository class.
A requirement is that there is a RDF4J Server running on some remote system, which is accessible over HTTP. For example, suppose that at http://example.org/rdf4j-server/ a RDF4J Server is running, which has a repository with the identification example-db. We can access this repository in our code as follows:
import org.eclipse.rdf4j.repository.Repository;
import org.eclipse.rdf4j.repository.http.HTTPRepository;
...
String rdf4jServer = "http://example.org/rdf4j-server/";
String repositoryID = "example-db";
Repository repo = new HTTPRepository(rdf4jServer, repositoryID);
SPARQL endpoints
We can use the Repository interface to access any SPARQL endpoint as well. This is done as follows:
import org.eclipse.rdf4j.repository.Repository;
import org.eclipse.rdf4j.repository.sparql.SPARQLRepository;
...
String sparqlEndpoint = "http://example.org/sparql";
Repository repo = new SPARQLRepository(sparqlEndpoint);
After you have done this, you can query the SPARQL endpoint just as you would any other type of Repository.
Configuring the HTTP session thread pool
Both the HTTPRepository and the SPARQLRepository use the SPARQL Protocol over
HTTP under the hood (in the case of the HTTPRepository, it uses the extended
RDF4J REST API). The HTTP client session is managed by the HttpClientSessionManager
, which in turn depends
on the Apache HttpClient.
The session uses a caching thread pool executor to handle multithreaded
access to a remote endpoint, defined by default to use a thread pool with a
core size of 1.
To configure this to use a different core pool size, you can specify the
org.eclipse.rdf4j.client.executors.corePoolSize system property with a
different number.
The RepositoryManager and RepositoryProvider
Using what we’ve seen in the previous section, we can create and use various different types of repositories. However, when developing an application in which you have to keep track of several repositories, sharing references to these repositories between different parts of your code can become complex. The RepositoryManager
 and RepositoryProvider
 provide one central location where all information on the repositories in use (including id, type, directory for persistent data storage, etc.) is kept.
Using the RepositoryManager for handling repository creation and administration offers a number of advantages, including:

a single RepositoryManager object can be more easily shared throughout your application than a host of static references to individual repositories;
you can more easily create and manage repositories ‘on-the-fly’, for example if your application requires creation of new repositories on user input;
the RepositoryManager stores your configuration, including all repository data, in one central spot on the file system.

The RepositoryManager comes in two flavours: the LocalRepositoryManager and the RemoteRepositoryManager.
A LocalRepositoryManager manages repository handling for you locally, and is always created using a (local) directory. This directory is where all repositories handled by the manager store their data, and also where the LocalRepositoryManager itself stores its configuration data.
You create a new LocalRepositoryManager as follows:
import java.io.File;
import org.eclipse.rdf4j.repository.manager.LocalRepositoryManager;
File baseDir = new File("/path/to/storage/dir/");
LocalRepositoryManager manager = new LocalRepositoryManager(baseDir);
manager.init();
To use a LocalRepositoryManager to create and manage repositories is slightly different from what we’ve seen before about creating repositories. The LocalRepositoryManager works by providing it with RepositoryConfig
 objects, which are declarative specifications of the repository you want. You add a RepositoryConfig object for your new repository, and then request the actual Repository back from the LocalRepositoryManager:
import org.eclipse.rdf4j.repository.config.RepositoryConfig;

String repositoryId = "test-db";
RepositoryConfig repConfig = new RepositoryConfig(repositoryId, repositoryTypeSpec);
manager.addRepositoryConfig(repConfig);

Repository repository = manager.getRepository(repositoryId);
In the above bit of code, you may have noticed that we provide a variable called repositoryTypeSpec to the constructor of our RepositoryConfig. This variable is an instance of a class called RepositoryImplConfig, and this specifies the actual configuration of our new repository: what backends to use, whether or not to use inferencing, and so on.
Creating a RepositoryImplConfig object can be done in two ways: programmatically, or by reading a (RDF) config file. We will show the programmatic way first:
import org.eclipse.rdf4j.sail.memory.config.MemoryStoreConfig;
import org.eclipse.rdf4j.repository.config.RepositoryImplConfig;
import org.eclipse.rdf4j.repository.sail.config.SailRepositoryConfig;

// create a configuration for the SAIL stack
var storeConfig = new MemoryStoreConfig();

// create a configuration for the repository implementation
RepositoryImplConfig repositoryTypeSpec = new SailRepositoryConfig(storeConfig);
As you can see, we use a class called MemoryStoreConfig for specifying the type of storage backend we want. This class resides in a config sub-package of the memory store package (org.eclipse.rdf4j.sail.memory.config). Each particular type of SAIL in RDF4J has such a config class.
As a second example, we create a slightly more complex type of store: still in-memory, but this time we want it to use the memory store’s persistence option, and use standard query evaluation mode (instead of the default, which is strict). We also want to add RDFS inferencing on top. RDFS inferencing is provided by a separate SAIL implementation, which can be ‘stacked’ on top of another SAIL. We follow that pattern in the creation of our config object:
import org.eclipse.rdf4j.common.transaction.QueryEvaluationMode;
import org.eclipse.rdf4j.sail.inferencer.fc.config.SchemaCachingRDFSInferencerConfig;

// create a configuration for the SAIL stack
boolean persist = true;
var storeConfig = new MemoryStoreConfig(persist);
// tweak the store config to use standard query evaluation mode
storeConfig.setDefaultQueryEvaluationMode(QueryEvaluationMode.STANDARD);

// stack an inferencer config on top of our backend-config
var stackConfig = new SchemaCachingRDFSInferencerConfig(storeConfig);

// create a configuration for the repository implementation
SailRepositoryConfig repositoryTypeSpec = new SailRepositoryConfig(stackConfig);
The RemoteRepositoryManager
A useful feature of RDF4J is that most of its APIs are transparent with respect to whether you are working locally or remote. This is the case for the RDF4J repositories, but also for the RepositoryManager. In the above examples, we have used a LocalRepositoryManager, creating repositories for local use. However, it is also possible to use a RemoteRepositoryManager, using it to create and manage repositories residing on a remotely running RDF4J Server.
A RemoteRepositoryManager is initialized as follows:
import org.eclipse.rdf4j.repository.manager.RemoteRepositoryManager;

// URL of the remote RDF4J Server we want to access
String serverUrl = "http://localhost:8080/rdf4j-server";
RemoteRepositoryManager manager = new RemoteRepositoryManager(serverUrl);
manager.init();
Once initialized, the RemoteRepositoryManager can be used in the same fashion as the LocalRepositoryManager: creating new repositories, requesting references to existing repositories, and so on.
The RepositoryProvider
Finally, RDF4J also includes a RepositoryProvider class. This is a utility class that holds static references to RepositoryManagers, making it easy to share Managers (and the repositories they contain) across your application. In addition, the RepositoryProvider also has a built-in shutdown hook, which makes sure all repositories managed by it are shut down when the JVM exits.
To obtain a RepositoryManager from a RepositoryProvider you invoke it with the location you want a RepositoryManager for. If you provide a HTTP url, it will automatically return a RemoteRepositoryManager, and if you provide a local file URL, it will be a LocalRepositoryManager.
import org.eclipse.rdf4j.repository.manager.RepositoryProvider;
String url = "http://localhost:8080/rdf4j-server";
RepositoryManager manager  = RepositoryProvider.getRepositoryManager(url);
The RepositoryProvider creates and keeps a singleton instance of RepositoryManager for each distinct location you specify, which means that you invoke the above call in several places in your code without having to worry about creating duplicate manager objects.
Creating a Federation
RDF4J has the option to create a repository that acts as a federation of stores. For more information about this, see the FedX federation documentation.
Using a repository: RepositoryConnections
Now that we have created a Repository, we want to do something with it. In RDF4J, this is achieved through RepositoryConnection objects, which can be created by the Repository.
A RepositoryConnection
 represents a connection to the actual store. We can issue operations over this connection, and close it when we are done to make sure we are not keeping resources unnnecessarily occupied.
In the following sections, we will show some examples of basic operations.
Adding RDF to a repository
The Repository API offers various methods for adding data to a repository. Data can be added by specifying the location of a file that contains RDF data, and statements can be added individually or in collections.
We perform operations on a repository by requesting a RepositoryConnection from the repository. On this RepositoryConnection object we can perform various operations, such as query evaluation, getting, adding, or removing statements, etc.
The following example code adds two files, one local and one available through HTTP, to a repository:
import org.eclipse.RDF4J.RDF4JException;
import org.eclipse.rdf4j.repository.Repository;
import org.eclipse.rdf4j.repository.RepositoryConnection;
import org.eclipse.rdf4j.rio.RDFFormat;
import java.io.File;
import java.net.URL;
...
File file = new File("/path/to/example.rdf");
String baseURI = "http://example.org/example/local";
try {
   RepositoryConnection con = repo.getConnection();
   try {
      con.add(file, baseURI, RDFFormat.RDFXML);
      URL url = new URL("http://example.org/example/remote.rdf");
      con.add(url, url.toString(), RDFFormat.RDFXML);
   }
   finally {
      con.close();
   }
}
catch (RDF4JException e) {
   // handle exception
}
catch (java.io.IOEXception e) {
   // handle io exception
}
As you can see, the above code does very explicit exception handling and makes sure resources are properly closed when we are done. A lot of this can be simplified. RepositoryConnection implements AutoCloseable, so a first simple change is to use a try-with-resources construction for handling proper opening and closing of the RepositoryConnection:
File file = new File("/path/to/example.rdf");
String baseURI = "http://example.org/example/local";
try (RepositoryConnection con = repo.getConnection()) {
   con.add(file, baseURI, RDFFormat.RDFXML);
   URL url = new URL("http://example.org/example/remote.rdf");
   con.add(url, url.toString(), RDFFormat.RDFXML);
}
catch (RDF4JException e) {
   // handle exception. This catch-clause is
   // optional since RDF4JException is an unchecked exception
}
catch (java.io.IOEXception e) {
   // handle io exception
}
More information on other available methods can be found in the RepositoryConnection
 javadoc.
Querying a repository
The Repository API has a number of methods for creating and evaluating queries. Three types of queries are distinguished: tuple queries, graph queries and boolean queries. The query types differ in the type of results that they produce.
The result of a tuple query is a set of tuples (or variable bindings), where each tuple represents a solution of a query. This type of query is commonly used to get specific values (URIs, blank nodes, literals) from the stored RDF data. SPARQL SELECT queries are tuple queries.
The result of graph queries is an RDF graph (or set of statements). This type of query is very useful for extracting sub-graphs from the stored RDF data, which can then be queried further, serialized to an RDF document, etc. SPARQL CONSTRUCT and DESCRIBE queries are graph queries.
The result of boolean queries is a simple boolean value, i.e. true or false. This type of query can be used to check if a repository contains specific information. SPARQL ASK queries are boolean queries.
SELECT: tuple queries
To evaluate a (SELECT) tuple query we can do the following:
import java.util.List;
import org.eclipse.rdf4j.RDF4JException;
import org.eclipse.rdf4j.repository.RepositoryConnection;
import org.eclipse.rdf4j.query.TupleQuery;
import org.eclipse.rdf4j.query.TupleQueryResult;
import org.eclipse.rdf4j.query.BindingSet;
import org.eclipse.rdf4j.query.QueryLanguage;
...
try (RepositoryConnection conn = repo.getConnection()) {
   String queryString = "SELECT ?x ?y WHERE { ?x ?p ?y } ";
   TupleQuery tupleQuery = con.prepareTupleQuery(queryString);
   try (TupleQueryResult result = tupleQuery.evaluate()) {
      for (BindingSet bindingSet: result) {
         Value valueOfX = bindingSet.getValue("x");
         Value valueOfY = bindingSet.getValue("y");
         // do something interesting with the values here...
      }
   }
}
This evaluates a SPARQL SELECT query and returns a TupleQueryResult, which consists of a sequence of BindingSet objects. Each BindingSet contains a set of Binding objects. A binding is pair relating a variable name (as used in the query’s SELECT clause) with a value.
We can use the TupleQueryResult to iterate over all results and get each individual result for x and y. We retrieve values by name rather than by an index. The names used should be the names of variables as specified in your query (note that we leave out the ‘?’ or ‘$’ prefixes used in SPARQL). The TupleQueryResult.getBindingNames() method returns a list of binding names, in the order in which they were specified in the query. To process the bindings in each binding set in the order specified by the projection, you can do the following:
List<String> bindingNames = result.getBindingNames();
for (BindingSet bindingSet: result) {
   Value firstValue = bindingSet.getValue(bindingNames.get(0));
   Value secondValue = bindingSet.getValue(bindingNames.get(1));
   // do something interesting with the values here...
}
Finally, it is important to make sure that both the TupleQueryResult and the RepositoryConnection are properly closed after we are done with them. A TupleQueryResult evaluates lazily and keeps resources (such as connections to the underlying database) open. Closing the TupleQueryResult frees up these resources. You can either expliclty invoke close() in the finally clause, or use a try-with-resources construction (as shown in the above examples) to let Java itself handle proper closing for you. In the following code examples, we will use both ways to handle both result and connection closure interchangeably.
As said: a TupleQueryResult evaluates lazily, and keeps an open connection to the data source while being processed. If you wish to quickly materialize the full query result (for example, convert it to a Java List) and then close the TupleQueryResult, you can do something like this:
List<BindingSet> resultList;
try (TupleQueryResult result = tupleQuery.evaluate()) {
   resultList = QueryResults.asList(result);
}
A SPARQL SELECT query in a single line of code: the Repositories utility
RDF4J provides a convenience utility class org.eclipse.rdf4j.repository.util.Repositories, which allows us to significantly shorten our boilerplate code. In particular, the Repositories utility allows us to do away with opening/closing a RepositoryConnection completely. For example, to open a connection, create and evaluate a SPARQL SELECT query, and then put that query’s result in a list, we can do the following:
List<BindingSet> results = Repositories.tupleQuery(rep, "SELECT * WHERE {?s ?p ?o }", r -> QueryResults.asList(r));
We make use of Lambda expressions to process the result. In this particular example, the only processing we do is to convert the TupleQueryResult object into a List. However, you can supply any kind of function to this interface to fully customize the processing that you do on the result.
Using TupleQueryResultHandlers
You can also directly process the query result by supplying a TupleQueryResultHandler to the query’s evaluate() method. The main difference is that when using a return object, the caller has control over when the next answer is retrieved (namely, whenever next() is called), whereas with the use of a handler, the connection pushes answers to the handler object as soon as it has them available.
As an example we will use SPARQLResultsCSVWriter to directly write the query result to the console. SPARQLResultsCSVWriter is a TupleQueryResultHandler implementation that writes SPARQL Results as comma-separated values.
String queryString = "SELECT * WHERE {?x ?p ?y }";
con.prepareTupleQuery(queryString).evaluate(new SPARQLResultsCSVWriter(System.out));
RDF4J provides a number of standard implementations of TupleQueryResultHandler, and of course you can also supply your own application-specific implementation. Have a look in the Javadoc for more details.
CONSTRUCT/DESCRIBE: graph queries
The following code evaluates a graph query on a repository:
import org.eclipse.rdf4j.query.GraphQueryResult;
GraphQueryResult graphResult = con.prepareGraphQuery("CONSTRUCT { ?s ?p ?o } WHERE {?s ?p ?o }").evaluate();
A GraphQueryResult is similar to TupleQueryResult in that is an object that iterates over the query solutions. However, for graph queries the query solutions are RDF statements, so a GraphQueryResult iterates over Statement objects:
for (Statement st: graphResult) {
   // ... do something with the resulting statement here.
}
You can also quickly turn a GraphQueryResult into a Model (that is, a Java Collection of statements), by using the org.eclipse.rdf4j.query.QueryResults utility class:
Model resultModel = QueryResults.asModel(graphQueryResult);
Doing a graph query in a single line of code
Similarly to how we do this with SELECT queries, we can use the Repositories utility to obtain a result from a SPARQL CONSTRUCT (or DESCRIBE) query in a single line of Java code:
Model m = Repositories.graphQuery(rep, "CONSTRUCT WHERE {?s ?p ?o}", r -> QueryResults.asModel(r));
Using RDFHandlers
For graph queries, we can supply an org.eclipse.rdf4j.rio.RDFHandler to the evaluate() method. Again, this is a generic interface, each object implementing it can process the reported RDF statements in any way it wants.
All Rio writers (such as the RDFXmlWriter, TurtleWriter, TriXWriter, etc.) implement the RDFHandler interface. This allows them to be used in combination with querying quite easily. In the following example, we use a TurtleWriter to write the result of a SPARQL graph query to standard output in Turtle format:
import org.eclipse.rdf4j.rio.Rio;
import org.eclipse.rdf4j.rio.RDFFormat;
import org.eclipse.rdf4j.rio.RDFWriter;
try (RepositoryConnection conn = repo.getConnection()) {
   RDFWriter writer = Rio.createWriter(RDFFormat.TURTLE, System.out);
   conn.prepareGraphQuery(QueryLanguage.SPARQL,
       "CONSTRUCT {?s ?p ?o } WHERE {?s ?p ?o } ").evaluate(writer);
}
Note that in the above code we use the org.eclipse.rdf4j.rio.Rio utility to quickly create a writer of the desired format. The Rio utility offers a lot of useful functions to quickly create writers and parser for various formats.
Preparing and Reusing Queries
In the previous sections we have simply created a query from a string and immediately evaluated it. However, the prepareTupleQuery and prepareGraphQuery methods return objects of type Query, specifically TupleQuery and GraphQuery.
A Query object, once created, can be reused several times. For example, we can evaluate a Query object, then add some data to our repository, and evaluate the same query again.
The Query object also has a setBinding() method, which can be used to specify specific values for query variables. As a simple example, suppose we have a repository containing names and e-mail addresses of people, and we want to do a query for each person, retrieve his/her e-mail address, for example, but we want to do a separate query for each person. This can be achieved using the setBinding() functionality, as follows:
try (RepositoryConnection con = repo.getConnection()){
   // First, prepare a query that retrieves all names of persons
   TupleQuery nameQuery = con.prepareTupleQuery("SELECT ?name WHERE { ?person ex:name ?name . }");

   // Then, prepare another query that retrieves all e-mail addresses of persons:
   TupleQuery mailQuery = con.prepareTupleQuery("SELECT ?mail WHERE { ?person ex:mail ?mail ; ex:name ?name . }");

   // Evaluate the first query to get all names
   try (TupleQueryResult nameResult = nameQuery.evaluate()){
      // Loop over all names, and retrieve the corresponding e-mail address.
      for (BindingSet bindingSet: nameResult) {
         Value name = bindingSet.get("name");

         // Retrieve the matching mailbox, by setting the binding for
         // the variable 'name' to the retrieved value. Note that we
         // can set the same binding name again for each iteration, it will
         // overwrite the previous setting.
         mailQuery.setBinding("name", name);
         try ( TupleQueryResult mailResult = mailQuery.evaluate()) {
            // mailResult now contains the e-mail addresses for one particular person

            ....
         }
      }
   }
}
The values with which you perform the setBinding operation of course do not necessarily have to come from a previous query result (as they do in the above example). As also shown in The RDF Model API documentation, you can create your own value objects. You can use this functionality to, for example, query for a particular keyword that is given by user input:
import static org.eclipse.rdf4j.model.util.Values.literal;

// In this example, we specify the keyword string. Of course, this
// could just as easily be obtained by user input, or by reading from
// a file, or...
String keyword = "foobar";

// We prepare a query that retrieves all documents for a keyword.
// Notice that in this query the 'keyword' variable is not bound to
// any specific value yet.
TupleQuery keywordQuery = con.prepareTupleQuery("SELECT ?document WHERE { ?document ex:keyword ?keyword . }");

// Then we set the binding to a literal representation of our keyword.
// Evaluation of the query object will now effectively be the same as
// if we had specified the query as follows:
//   SELECT ?document WHERE { ?document ex:keyword "foobar". }
keywordQuery.setBinding("keyword", literal(keyword));

// We then evaluate the prepared query and can process the result:
TupleQueryResult keywordQueryResult = keywordQuery.evaluate();
Tweaking the query evaluation mode
 New in RDF4J 4.3

The SPARQL specification is by nature extensible, allowing query engines to add support for additional operators and operator functions (see section 17.3.1 “Operator Extensibility”). The SPARQL query engine in RDF4J has two configurable evaluation modes that regulate this: strict mode, and standard mode.

in strict mode, SPARQL queries are evaluated using the strictest form of minimal compliance to the recommendation. No additional operators have been added.
in standard mode, SPARQL queries are evaluated using a set of operator extensions that make practical sense, in a way that is still fully compliant with the W3C Recommendation. For example, in standard mode, comparisons and math operations between literals using calendar datatypes (xsd:DateTime, etc) are supported.

For historic reasons the default evaluation mode for RDF4J repositories is strict, but this can be changed in several ways. It’s possible to set a repository’s default evaluation mode to standard at Repository creation/configuration time. But you can also override the default behaviour per query, by wrapping the query in a transaction and using the QueryEvaluationMode
 transaction setting:
import org.eclipse.rdf4j.common.transaction.QueryEvaluationMode;
...

try (RepositoryConnection conn = repo.getConnection()) {
   String queryString = "SELECT ?x ?y WHERE { ?x ?p ?y } ";

   // set query evaluation mode to STANDARD in this transaction
   conn.begin(QueryEvaluationMode.STANDARD);
   TupleQuery tupleQuery = con.prepareTupleQuery(queryString);
   try (TupleQueryResult result = tupleQuery.evaluate()) {
      for (BindingSet bindingSet: result) {
         Value valueOfX = bindingSet.getValue("x");
         Value valueOfY = bindingSet.getValue("y");
         // do something interesting with the values here...
      }
   }
   conn.commit();
}
Explaining queries
SPARQL queries are translated to query plans and then run through an optimization pipeline before they get evaluated and
the results returned. The query explain feature gives a peek into what decisions are being made and how they affect
the performance of your query.
This feature is currently released as an experimental feature, which means that it may change, be moved or even removed in the future.
Explaining queries currently only works if you are using one of the built in stores directly in your Java code.
If you are connecting to a remote RDF4J Server, using the Workbench or connecting to a third party database then you will get an
UnsupportedException.
In RDF4J 3.2.0, queries have a new method explain(...) that returns an Explanation explaining how the query will be, or has been, evaluated.
try (SailRepositoryConnection connection = sailRepository.getConnection()) {
   TupleQuery query = connection.prepareTupleQuery("select * where .... ");
   String explanation = query.explain(Explanation.Level.Timed).toString();
   System.out.println(explanation);
}
There are 4 explanation levels to choose between:




Parsed
Optimized
Cost and Estimates
Fully evaluated
Real result sizes
Performance timing




Unoptimized
✓







Optimized
✓
✓
✓





Executed
✓
✓
✓
✓
✓



Timed
✓
✓
✓
✓
✓
✓



First try to use the Timed level, since this is the richest and gives the clearest understanding about
which part of the query is the slowest. Timed and Executed both fully evaluate the query and iterate
over all the result sets. Seeing as how this can be very time-consuming there is a default best-effort
timeout of 60 seconds. A different timeout can be set by changing the timeout for the query.
The lower levels Unoptimized and Optimized are useful for understanding how RDF4J reorders queries in
order to optimize them.
As an example, the following query intends to get everyone in Peter’s extended friend graph who is at least 18 years old
and return their node and optionally their name.
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?friend ?name WHERE
{
	BIND(<http://example.com/peter> as ?person)
	?person a foaf:Person ;
		(foaf:knows | ^foaf:knows)* ?friend.
	?friend foaf:age ?age
	OPTIONAL {
		?friend foaf:name ?name
	}
	FILTER(?age >= 18)
}
For our test data the query returns the following results:
[ friend=http://example.com/steve; name="Steve" ]
[ friend=http://example.com/mary ]
Our test data also contains other people, so the query has to evaluate a lot more data than the results lead us to believe.
Explaining the query at the Timed level gives us the following plan:
01 Projection (resultSizeActual=2, totalTimeActual=0.247ms, selfTimeActual=0.002ms)
02 ╠══ProjectionElemList
03 ║     ProjectionElem "friend"
04 ║     ProjectionElem "name"
05 ╚══LeftJoin (LeftJoinIterator) (resultSizeActual=2, totalTimeActual=0.245ms, selfTimeActual=0.005ms)
06    ├──Join (JoinIterator) (resultSizeActual=2, totalTimeActual=0.238ms, selfTimeActual=0.004ms)
07    │  ╠══Extension (resultSizeActual=1, totalTimeActual=0.002ms, selfTimeActual=0.001ms)
08    │  ║  ├──ExtensionElem (person)
09    │  ║  │     ValueConstant (value=http://example.com/peter)
10    │  ║  └──SingletonSet (resultSizeActual=1, totalTimeActual=0.0ms, selfTimeActual=0.0ms)
11    │  ╚══Join (JoinIterator) (resultSizeActual=2, totalTimeActual=0.231ms, selfTimeActual=0.009ms)
12    │     ├──Filter (resultSizeActual=4, totalTimeActual=0.023ms, selfTimeActual=0.014ms)
13    │     │  ╠══Compare (>=)
14    │     │  ║     Var (name=age)
15    │     │  ║     ValueConstant (value="18"^^<http://www.w3.org/2001/XMLSchema#integer>)
16    │     │  ╚══StatementPattern (costEstimate=4, resultSizeEstimate=12, resultSizeActual=12, totalTimeActual=0.009ms, selfTimeActual=0.009ms)
17    │     │        Var (name=friend)
18    │     │        Var (name=_const_8d89de74_uri, value=http://xmlns.com/foaf/0.1/age, anonymous)
19    │     │        Var (name=age)
20    │     └──Join (JoinIterator) (resultSizeActual=2, totalTimeActual=0.199ms, selfTimeActual=0.007ms)
31    │        ╠══ArbitraryLengthPath (costEstimate=24, resultSizeEstimate=2.2K, resultSizeActual=2, totalTimeActual=0.189ms, selfTimeActual=0.189ms)
32    │        ║     Var (name=person)
33    │        ║     Union
34    │        ║     ╠══StatementPattern (resultSizeEstimate=1.0K)
35    │        ║     ║     Var (name=person)
36    │        ║     ║     Var (name=_const_531c5f7d_uri, value=http://xmlns.com/foaf/0.1/knows, anonymous)
37    │        ║     ║     Var (name=friend)
38    │        ║     ╚══StatementPattern (resultSizeEstimate=1.0K)
39    │        ║           Var (name=friend)
40    │        ║           Var (name=_const_531c5f7d_uri, value=http://xmlns.com/foaf/0.1/knows, anonymous)
41    │        ║           Var (name=person)
42    │        ║     Var (name=friend)
43    │        ╚══StatementPattern (costEstimate=1, resultSizeEstimate=101, resultSizeActual=2, totalTimeActual=0.004ms, selfTimeActual=0.004ms)
44    │              Var (name=person)
45    │              Var (name=_const_f5e5585a_uri, value=http://www.w3.org/1999/02/22-rdf-syntax-ns#type, anonymous)
46    │              Var (name=_const_e1df31e0_uri, value=http://xmlns.com/foaf/0.1/Person, anonymous)
47    └──StatementPattern (resultSizeEstimate=5, resultSizeActual=1, totalTimeActual=0.003ms, selfTimeActual=0.003ms)
48          Var (name=friend)
49          Var (name=_const_23b7c3b6_uri, value=http://xmlns.com/foaf/0.1/name, anonymous)
50          Var (name=name)
We start by reading the query top to bottom. The first node we encounter is:
Projection (resultSizeActual=2, totalTimeActual=0.247ms, selfTimeActual=0.002ms)
The node name is “Projection”, which represents the SELECT keyword. The values in parentheses
are cost-estimates and actual measured output and timing. You may encounter:

costEstimate: an internal value that represents the cost for executing this node and is used for ordering the nodes
resultSizeEstimate: the cardinality estimate of a node, essentially how many results this node would return if it were executed alone
resultSizeActual: the actual number of results that this node produced
totalTimeActual: the total time this node took to return all its results, including the time for its children
selfTimeActual: the time this node took all on its own to produce its results

In the plan above we can see that ArbitraryLengthPath took most of our time by using 0.189ms (~75% of the overall time).
This node represents the (foaf:knows | ^foaf:knows)* ?friend part of our query.
Joins in RDF4J have a left, and a right node (the pipes in the query plan make it simpler to see which node is the left and which is the right).
The join algorithms will first retrieve a result from the left node before it gets a result from the right node. The left node is the first
node displayed under the join node. For the Join on line 06 we have the left node being line 07 and the right being line 11.Executed
and Timed plans will typically show the algorithm for all join and left join nodes. Our fastest algorithm is usually JoinIterator, it will
retrieve a result from the left node and use the results to “query” the right node for the next relevant result.
In our plan above we can see how Extension node and the Filter node can “inform” the ArbitraryLengthPath which values for
person and friend are relevant because the Extension node binds exactly one value for person and Filter node binds exactly
four values forfriend. This is why ArbitraryLengthPath has a resultSizeActual of two, meaning that it only produced two results.
The query above is a very efficient and nicely behaved query. Usually the reason to explain a query is because the query is slow or takes
up a lot of memory.
The following query is a typical example of a scoping issue, which is a very common cause of slow SPARQL queries.
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT * WHERE
{
    BIND(<http://example.com/peter> as ?person)
	?person a foaf:Person .
	{
		?person	(foaf:knows | ^foaf:knows)* ?friend.
	} UNION {
		?friend foaf:age ?age.
		FILTER(?age >= 18)
	}
}
The issue with this query is that each of the union clauses introduces a new scope. It’s quite easy to see in this example. Both unions define a new
variable ?friend, however the results should not be the intersection of common values but rather the union between “everyone that knows or is known by someone”
and “everyone 18 or older”. The only exception here is that ?person is used in the outer scope, so results from the inner union would be filtered to match
with bindings for ?person from the outer scope. SPARQL is designed with bottom-up semantics, which means that inner sections should be evaluated before
outer sections. This precisely so as to make scoping issues unambiguous.
The query plan for the query gives us a lot of hints about how this becomes problematic.
Projection (resultSizeActual=9, totalTimeActual=1.6s, selfTimeActual=0.074ms)
╠══ProjectionElemList
║     ProjectionElem "person"
║     ProjectionElem "friend"
║     ProjectionElem "age"
╚══Join (HashJoinIteration) (resultSizeActual=9, totalTimeActual=1.6s, selfTimeActual=5.45ms)
   ├──Extension (resultSizeActual=1, totalTimeActual=0.018ms, selfTimeActual=0.016ms)
   │  ╠══ExtensionElem (person)
   │  ║     ValueConstant (value=http://example.com/peter)
   │  ╚══SingletonSet (resultSizeActual=1, totalTimeActual=0.002ms, selfTimeActual=0.002ms)
   └──Union (new scope) (resultSizeActual=10.5K, totalTimeActual=1.6s, selfTimeActual=4.42ms)
      ╠══Join (HashJoinIteration) (resultSizeActual=10.1K, totalTimeActual=1.6s, selfTimeActual=50.0ms)
      ║  ├──StatementPattern (costEstimate=34, resultSizeEstimate=101, resultSizeActual=101, totalTimeActual=0.524ms, selfTimeActual=0.524ms)
      ║  │     Var (name=person)
      ║  │     Var (name=_const_f5e5585a_uri, value=http://www.w3.org/1999/02/22-rdf-syntax-ns#type, anonymous)
      ║  │     Var (name=_const_e1df31e0_uri, value=http://xmlns.com/foaf/0.1/Person, anonymous)
      ║  └──ArbitraryLengthPath (new scope) (costEstimate=47, resultSizeEstimate=2.2K, resultSizeActual=102.0K, totalTimeActual=1.5s, selfTimeActual=1.5s)
      ║        Var (name=person)
      ║        Union
      ║        ├──StatementPattern (resultSizeEstimate=1.0K)
      ║        │     Var (name=person)
      ║        │     Var (name=_const_531c5f7d_uri, value=http://xmlns.com/foaf/0.1/knows, anonymous)
      ║        │     Var (name=friend)
      ║        └──StatementPattern (resultSizeEstimate=1.0K)
      ║              Var (name=friend)
      ║              Var (name=_const_531c5f7d_uri, value=http://xmlns.com/foaf/0.1/knows, anonymous)
      ║              Var (name=person)
      ║        Var (name=friend)
      ╚══Join (JoinIterator) (resultSizeActual=404, totalTimeActual=0.533ms, selfTimeActual=0.145ms)
         ├──Filter (new scope) (costEstimate=12, resultSizeEstimate=12, resultSizeActual=4, totalTimeActual=0.087ms, selfTimeActual=0.073ms)
         │  ╠══Compare (>=)
         │  ║     Var (name=age)
         │  ║     ValueConstant (value="18"^^<http://www.w3.org/2001/XMLSchema#integer>)
         │  ╚══StatementPattern (resultSizeEstimate=12, resultSizeActual=12, totalTimeActual=0.014ms, selfTimeActual=0.014ms)
         │        Var (name=friend)
         │        Var (name=_const_8d89de74_uri, value=http://xmlns.com/foaf/0.1/age, anonymous)
         │        Var (name=age)
         └──StatementPattern (costEstimate=101, resultSizeEstimate=101, resultSizeActual=404, totalTimeActual=0.301ms, selfTimeActual=0.301ms)
               Var (name=person)
               Var (name=_const_f5e5585a_uri, value=http://www.w3.org/1999/02/22-rdf-syntax-ns#type, anonymous)
               Var (name=_const_e1df31e0_uri, value=http://xmlns.com/foaf/0.1/Person, anonymous)
The biggest time use and largest result size is produced at line:
ArbitraryLengthPath (new scope) (costEstimate=47, resultSizeEstimate=2.2K, resultSizeActual=102.0K, totalTimeActual=1.5s, selfTimeActual=1.5s)
This tells us that the query is probably producing all possible results for ?person (foaf:knows | ^foaf:knows)* ?friend.. In fact running this fragment in a new query
shows that it produces ~102,000 results.
Taking a look at the unoptimized plan we can see where the issue lies:
01 Projection
02 ╠══ProjectionElemList
03 ║     ProjectionElem "person"
04 ║     ProjectionElem "friend"
05 ║     ProjectionElem "age"
06 ╚══Join
07    ├──Join
08    │  ╠══Extension
09    │  ║  ├──ExtensionElem (person)
10    │  ║  │     ValueConstant (value=http://example.com/peter)
11    │  ║  └──SingletonSet
12    │  ╚══StatementPattern
13    │        Var (name=person)
14    │        Var (name=_const_f5e5585a_uri, value=http://www.w3.org/1999/02/22-rdf-syntax-ns#type, anonymous)
15    │        Var (name=_const_e1df31e0_uri, value=http://xmlns.com/foaf/0.1/Person, anonymous)
16    └──Union (new scope)
17       ╠══ArbitraryLengthPath (new scope)
18       ║     Var (name=person)
19       ║     Union
20       ║     ╠══StatementPattern
31       ║     ║     Var (name=person)
32       ║     ║     Var (name=_const_531c5f7d_uri, value=http://xmlns.com/foaf/0.1/knows, anonymous)
33       ║     ║     Var (name=friend)
34       ║     ╚══StatementPattern
35       ║           Var (name=friend)
36       ║           Var (name=_const_531c5f7d_uri, value=http://xmlns.com/foaf/0.1/knows, anonymous)
37       ║           Var (name=person)
38       ║     Var (name=friend)
39       ╚══Filter (new scope)
40          ├──Compare (>=)
41          │     Var (name=age)
42          │     ValueConstant (value="18"^^<http://www.w3.org/2001/XMLSchema#integer>)
43          └──StatementPattern
44                Var (name=friend)
45                Var (name=_const_8d89de74_uri, value=http://xmlns.com/foaf/0.1/age, anonymous)
46                Var (name=age)
The problem is that the Union on line 16 introduces a new scope. This means that the Join above it (line 6) can’t push its binding for ?person into the Union.
This is the reason that the execution of the query was done with the HashJoinIteration rather than with the JoinIterator.
One way to solve this issue is to copy the BIND into all relevant unions.
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?friend ?name WHERE
{
    BIND(<http://example.com/peter> as ?person)
	?person a foaf:Person .
	{
	    BIND(<http://example.com/peter> as ?person)
		?person	(foaf:knows | ^foaf:knows)* ?friend.
	} UNION {
		?friend foaf:age ?age.
		FILTER(?age >= 18)
	}
}
This forces the inner union to only consider ex:peter as ?person meaning we only need to find his friends and not everyone elses friends.
The query plan also agrees that this is better.
Projection (resultSizeActual=9, totalTimeActual=0.448ms, selfTimeActual=0.007ms)
╠══ProjectionElemList
║     ProjectionElem "person"
║     ProjectionElem "friend"
║     ProjectionElem "age"
╚══Union (new scope) (resultSizeActual=9, totalTimeActual=0.441ms, selfTimeActual=0.03ms)
   ├──Join (JoinIterator) (resultSizeActual=5, totalTimeActual=0.354ms, selfTimeActual=0.024ms)
   │  ╠══Join (JoinIterator) (resultSizeActual=1, totalTimeActual=0.042ms, selfTimeActual=0.002ms)
   │  ║  ├──Extension (resultSizeActual=1, totalTimeActual=0.037ms, selfTimeActual=0.03ms)
   │  ║  │  ╠══ExtensionElem (person)
   │  ║  │  ║     ValueConstant (value=http://example.com/peter)
   │  ║  │  ╚══SingletonSet (resultSizeActual=1, totalTimeActual=0.007ms, selfTimeActual=0.007ms)
   │  ║  └──Extension (resultSizeActual=1, totalTimeActual=0.003ms, selfTimeActual=0.002ms)
   │  ║     ╠══ExtensionElem (person)
   │  ║     ║     ValueConstant (value=http://example.com/peter)
   │  ║     ╚══SingletonSet (resultSizeActual=1, totalTimeActual=0.001ms, selfTimeActual=0.001ms)
   │  ╚══Join (JoinIterator) (resultSizeActual=5, totalTimeActual=0.289ms, selfTimeActual=0.112ms)
   │     ├──StatementPattern (costEstimate=34, resultSizeEstimate=101, resultSizeActual=1, totalTimeActual=0.013ms, selfTimeActual=0.013ms)
   │     │     Var (name=person)
   │     │     Var (name=_const_f5e5585a_uri, value=http://www.w3.org/1999/02/22-rdf-syntax-ns#type, anonymous)
   │     │     Var (name=_const_e1df31e0_uri, value=http://xmlns.com/foaf/0.1/Person, anonymous)
   │     └──ArbitraryLengthPath (costEstimate=47, resultSizeEstimate=2.2K, resultSizeActual=5, totalTimeActual=0.164ms, selfTimeActual=0.164ms)
   │           Var (name=person)
   │           Union
   │           ├──StatementPattern (resultSizeEstimate=1.0K)
   │           │     Var (name=person)
   │           │     Var (name=_const_531c5f7d_uri, value=http://xmlns.com/foaf/0.1/knows, anonymous)
   │           │     Var (name=friend)
   │           └──StatementPattern (resultSizeEstimate=1.0K)
   │                 Var (name=friend)
   │                 Var (name=_const_531c5f7d_uri, value=http://xmlns.com/foaf/0.1/knows, anonymous)
   │                 Var (name=person)
   │           Var (name=friend)
   └──Join (JoinIterator) (resultSizeActual=4, totalTimeActual=0.057ms, selfTimeActual=0.005ms)
      ╠══Extension (resultSizeActual=1, totalTimeActual=0.002ms, selfTimeActual=0.001ms)
      ║  ├──ExtensionElem (person)
      ║  │     ValueConstant (value=http://example.com/peter)
      ║  └──SingletonSet (resultSizeActual=1, totalTimeActual=0.001ms, selfTimeActual=0.001ms)
      ╚══Join (JoinIterator) (resultSizeActual=4, totalTimeActual=0.05ms, selfTimeActual=0.011ms)
         ├──Filter (new scope) (costEstimate=12, resultSizeEstimate=12, resultSizeActual=4, totalTimeActual=0.031ms, selfTimeActual=0.021ms)
         │  ╠══Compare (>=)
         │  ║     Var (name=age)
         │  ║     ValueConstant (value="18"^^<http://www.w3.org/2001/XMLSchema#integer>)
         │  ╚══StatementPattern (resultSizeEstimate=12, resultSizeActual=12, totalTimeActual=0.009ms, selfTimeActual=0.009ms)
         │        Var (name=friend)
         │        Var (name=_const_8d89de74_uri, value=http://xmlns.com/foaf/0.1/age, anonymous)
         │        Var (name=age)
         └──StatementPattern (costEstimate=101, resultSizeEstimate=101, resultSizeActual=4, totalTimeActual=0.008ms, selfTimeActual=0.008ms)
               Var (name=person)
               Var (name=_const_f5e5585a_uri, value=http://www.w3.org/1999/02/22-rdf-syntax-ns#type, anonymous)
               Var (name=_const_e1df31e0_uri, value=http://xmlns.com/foaf/0.1/Person, anonymous)
Notice that ArbitraryLengthPath produces 5 results and that the entire query runs in 0.164ms instead of 1.5s.
Another way to visualize the query plan is to use the Graphiz DOT format with query.explain(Explanation.Level.Timed).toDot().
This visualization makes it easier to see which part of the query is slowest by looking at the color coding.

Image produced by Dreampuf GraphvizOnline
If you want to practice with these examples, the code below produces these three plans.
public class QueryExplainExample {

	public static void main(String[] args) {

		SailRepository sailRepository = new SailRepository(new MemoryStore());

		try (SailRepositoryConnection connection = sailRepository.getConnection()) {

			ValueFactory vf = connection.getValueFactory();
			String ex = "http://example.com/";

			IRI peter = vf.createIRI(ex, "peter");
			IRI steve = vf.createIRI(ex, "steve");
			IRI mary = vf.createIRI(ex, "mary");
			IRI patricia = vf.createIRI(ex, "patricia");
			IRI linda = vf.createIRI(ex, "linda");

			connection.add(peter, RDF.TYPE, FOAF.PERSON);

			connection.add(peter, FOAF.KNOWS, patricia);
			connection.add(patricia, FOAF.KNOWS, linda);
			connection.add(patricia, FOAF.KNOWS, steve);
			connection.add(mary, FOAF.KNOWS, linda);

			connection.add(steve, FOAF.AGE, vf.createLiteral(18));
			connection.add(mary, FOAF.AGE, vf.createLiteral(18));

			connection.add(steve, FOAF.NAME, vf.createLiteral("Steve"));

			// Add some dummy data
			for (int i = 0; i < 100; i++) {
				connection.add(vf.createBNode(i + ""), RDF.TYPE, FOAF.PERSON);
			}

			for (int i = 0; i < 1000; i++) {
				connection.add(vf.createBNode(i % 150 + ""), FOAF.KNOWS, vf.createBNode(i + 10 + ""));
			}

			for (int i = 0; i < 10; i++) {
				connection.add(vf.createBNode(i + 3 + ""), FOAF.AGE, vf.createLiteral(i + 10));
			}

			for (int i = 0; i < 4; i++) {
				connection.add(vf.createBNode(i + ""), FOAF.NAME, vf.createLiteral("name" + i));
			}

		}

		try (SailRepositoryConnection connection = sailRepository.getConnection()) {
			TupleQuery query = connection.prepareTupleQuery(String.join("\n", "",
				"PREFIX foaf: <http://xmlns.com/foaf/0.1/>",
				"SELECT ?friend ?name WHERE ",
				"{",
				"	BIND(<http://example.com/peter> as ?person)",
				"	?person a foaf:Person ;",
				"		(foaf:knows | ^foaf:knows)* ?friend.",
				"	OPTIONAL {",
				"		?friend foaf:name ?name",
				"	}",
				"	?friend foaf:age ?age",
				"	FILTER(?age >= 18) ",

				"}"));

			Explanation explain = query.explain(Explanation.Level.Timed);
			System.out.println(explain);

		}

		System.out.println("\n\n");

		try (SailRepositoryConnection connection = sailRepository.getConnection()) {
			TupleQuery query = connection.prepareTupleQuery(String.join("\n", "",
				"PREFIX foaf: <http://xmlns.com/foaf/0.1/>",
				"SELECT * WHERE ",
				"{",
				"  BIND(<http://example.com/peter> as ?person)",
				"	?person a foaf:Person .",
				"	{",
				"		?person	(foaf:knows | ^foaf:knows)* ?friend.",
				"	} UNION {",
				"		?friend foaf:age ?age",
				"		FILTER(?age >= 18) ",
				"	}",
				"}"));

			Explanation explainUnoptimized = query.explain(Explanation.Level.Unoptimized);
			System.out.println(explainUnoptimized);
			System.out.println("\n\n");

			Explanation explain = query.explain(Explanation.Level.Timed);
			System.out.println(explain);

		}

		System.out.println("\n\n");

		try (SailRepositoryConnection connection = sailRepository.getConnection()) {
			TupleQuery query = connection.prepareTupleQuery(String.join("\n", "",
				"PREFIX foaf: <http://xmlns.com/foaf/0.1/>",
				"SELECT * WHERE ",
				"{",
				"  BIND(<http://example.com/peter> as ?person)",
				"	?person a foaf:Person .",
				"	{",
				"  	BIND(<http://example.com/peter> as ?person)",
				"		?person	(foaf:knows | ^foaf:knows)* ?friend.",
				"	} UNION {",
				"		?friend foaf:age ?age",
				"		FILTER(?age >= 18) ",
				"	}",
				"}"));

			Explanation explain = query.explain(Explanation.Level.Timed);
			System.out.println(explain);
			System.out.println(explain.toDot());

		}

		sailRepository.shutDown();

	}

}
Creating, retrieving, removing individual statements
The RepositoryConnection can also be used for adding, retrieving, removing or otherwise manipulating individual statements, or sets of statements.
To be able to add new statements, we can use either the Values
 factory methods or a ValueFactory to create the Values out of which the statements consist. For example, we want to add a few statements about two resources, Alice and Bob:
import org.eclipse.rdf4j.model.vocabulary.RDF;
import org.eclipse.rdf4j.model.vocabulary.RDFS;
...

// create some resources and literals to make statements out of
IRI alice = Values.iri("http://example.org/people/alice");
IRI bob = Values.iri("http://example.org/people/bob");
IRI name = Values.iri("http://example.org/ontology/name");
IRI person = Values.iri("http://example.org/ontology/Person");
Literal bobsName = Values.literal("Bob");
Literal alicesName = Values.literal("Alice");

try (RepositoryConnection con = myRepository.getConnection()) {
  // alice is a person
  conn.add(alice, RDF.TYPE, person);
  // alice's name is "Alice"
  conn.add(alice, name, alicesName);
  // bob is a person
  conn.add(bob, RDF.TYPE, person);
  // bob's name is "Bob"
  conn.add(bob, name, bobsName);
}
Of course, it will not always be necessary to create IRI objects. In practice, you will find that you quite often retrieve existing IRIs from the repository (for example, by evaluating a query) and then use those values to add new statements. Also, for several well-knowns vocabularies we can simply reuse the predefined constants found in the org.eclipse.rdf4j.model.vocabulary package, and using the ModelBuilder utility you can very quickly create collections of statements without ever touching a ValueFactory.
Retrieving statements works in a very similar way. One way of retrieving statements we have already seen actually: we can get a GraphQueryResult containing statements by evaluating a graph query. However, we can also use direct method calls to retrieve (sets of) statements. For example, to retrieve all statements about Alice, we could do:
RepositoryResult<Statement> statements = con.getStatements(alice, null, null);
Similarly to the TupleQueryResult object and other types of query results, the RepositoryResult is an iterator-like object that lazily retrieves each matching statement from the repository when its next() method is called. Note that, like is the case with QueryResult objects, iterating over a RepositoryResult may result in exceptions which you should catch to make sure that the RepositoryResult is always properly closed after use:
RepositoryResult<Statement> statements = con.getStatements(alice, null, null, true);
try {
   while (statements.hasNext()) {
      Statement st = statements.next();
      ... // do something with the statement
   }
}
finally {
   statements.close(); // make sure the result object is closed properly
}
Or alternatively, using try-with-resources and the fact that (since RDF4J 3.1.0) a RepositoryResult is an Iterable:
try (RepositoryResult<Statement> statements = con.getStatements(alice, null, null, true)) {
   for (Statement st: statements) {
      ... // do something with the statement
   }
}
In the above getStatements() invocation, we see four parameters being passed. The first three represent the subject, predicate and object of the RDF statements which should be retrieved. A null value indicates a wildcard, so the above method call retrieves all statements which have as their subject Alice, and have any kind of predicate and object. The optional fourth parameter indicates whether or not inferred statements should be included or not (you can leave this parameter out, in which case it defaults to true).
Removing statements again works in a very similar fashion. Suppose we want to retract the statement that the name of Alice is “Alice”):
con.remove(alice, name, alicesName);
Or, if we want to erase all statements about Alice completely, we can do:
con.remove(alice, null, null);
Using named graphs/context
RDF4J supports the notion of context, which you can think of as a way to group sets of statements together through a single group identifier (this identifier can be a blank node or a URI).
A very typical way to use context is tracking provenance of the statements in a repository, that is, which file these statements originate from. For example, consider an application where you add RDF data from different files to a repository, and then one of those files is updated. You would then like to replace the data from that single file in the repository, and to be able to do this you need a way to figure out which statements need to be removed. The context mechanism gives you a way to do that.
Another typical use case is to support named graphs: in the SPARQL query language, named graphs can be queried as subsets of the dataset over which the query is evaluated. In RDF4J, named graphs are implemented via the context mechanism. This means that if you put data in RDF4J in a context, you can query that context as a named graph in SPARQL.
We will start by showing some simple examples of using context in the API. In the following example, we add an RDF document from the Web to our repository, in a context. In the example, we make the context identifier equal to the Web location of the file being uploaded.
String location = "http://example.org/example/example.rdf";
String baseURI = location;
URL url = new URL(location);
IRI context = f.createIRI(location);
conn.add(url, baseURI, RDFFormat.RDFXML, context);
We can now use the context mechanism to specifically address these statements in the repository for retrieve and remove operations:
// Get all statements in the context
try (RepositoryResult<Statement> result = conn.getStatements(null, null, null, context)) {
   while (result.hasNext()) {
      Statement st = result.next();
      ... // do something interesting with the result
   }
}
// Export all statements in the context to System.out, in RDF/XML format
RDFHandler writer = Rio.createWriter(RDFFormat.RDFXML, System.out);
conn.export(context, writer);
// Remove all statements in the context from the repository
conn.clear(context);
In most methods in the Repository API, the context parameter is a vararg, meaning that you can specify an arbitrary number (zero, one, or more) of context identifiers. This way, you can combine different contexts together. For example, we can very easily retrieve statements that appear in either context1 or context2.
In the following example we add information about Bob and Alice again, but this time each has their own context. We also create a new property called creator that has as its value the name of the person who is the creator a particular context. The knowledge about creators of contexts we do not add to any particular context, however:
IRI context1 = f.createIRI("http://example.org/context1");
IRI context2 = f.createIRI("http://example.org/context2");
IRI creator = f.createIRI("http://example.org/ontology/creator");

// Add stuff about Alice to context1
conn.add(alice, RDF.TYPE, person, context1);
conn.add(alice, name, alicesName, context1);

// Alice is the creator of context1
conn.add(context1, creator, alicesName);

// Add stuff about Bob to context2
conn.add(bob, RDF.TYPE, person, context2);
conn.add(bob, name, bobsName, context2);

// Bob is the creator of context2
conn.add(context2, creator, bobsName);
Once we have this information in our repository, we can retrieve all statements about either Alice or Bob by using the context vararg:
// Get all statements in either context1 or context2
RepositoryResult<Statement> result = con.getStatements(null, null, null, context1, context2);
You should observe that the above RepositoryResult will not contain the information that context1 was created by Alice and context2 by Bob. This is because those statements were added without any context, thus they do not appear in context1 or context2, themselves.
To explicitly retrieve statements that do not have an associated context, we do the following:
// Get all statements that do not have an associated context
RepositoryResult<Statement> result = con.getStatements(null, null, null, (Resource)null);
This will give us only the statements about the creators of the contexts, because those are the only statements that do not have an associated context. Note that we have to explicitly cast the null argument to Resource, because otherwise it is ambiguous whether we are specifying a single value or an entire array that is null (a vararg is internally treated as an array). Simply invoking getStatements(s, p, o, null) without an explicit cast will result in an IllegalArgumentException.
We can also get everything that either has no context or is in context1:
// Get all statements that do not have an associated context, or that are in context1
RepositoryResult<Statement> result = con.getStatements(null, null, null, (Resource)null, context1);
So as you can see, you can freely combine contexts in this fashion.
Note:
getStatements(null, null, null);
is not the same as:
getStatements(null, null, null, (Resource)null);
The former (without any context id parameter) retrieves all statements in the repository, ignoring any context information. The latter, however, only retrieves statements that explicitly do not have any associated context.
Working with Models, Collections and Iterations
Most of these examples sofar have been on the level of individual statements. However, the Repository API offers several methods that work with Java Collections of statements, allowing more batch-like update operations.
For example, in the following bit of code, we first retrieve all statements about Alice, put them in a Model (which, as we have seen in the previous sections, is an implementation of java.util.Collection) and then remove them:
import org.eclipse.rdf4j.query.QueryResults;
import org.eclipse.rdf4j.model.Model;

// Retrieve all statements about Alice and put them in a Model
RepositoryResult<Statement> statements = con.getStatements(alice, null, null);
Model aboutAlice = QueryResults.asModel(statements);

// Then, remove them from the repository
con.remove(aboutAlice);
As you can see, the QueryResults class provides a convenient method that takes a CloseableIteration (of which RepositoryResult is a subclass) as input, and returns the Model with the contents of the iterator added to it. It also automatically closes the result object for you.
In the above code, you first retrieve all statements, put them in a Model, and then remove them. Although this works fine, it can be done in an easier fashion, by simply supplying the resulting object directly:
con.remove(con.getStatements(alice, null, null));
The RepositoryConnection interface has several variations of add, retrieve and remove operations. See the Javadoc for a full overview of the options.
RDF Collections and RepositoryConnections
In the Model API documentation we have already seen how we can use the RDFCollections utility on top of a Model. This makes it very easy to insert any RDF Collection into your Repository - after all a Model can simply be added as follows:
Model rdfList = ... ;
try (RepositoryConnection conn = repo.getConnection()) {
       conn.add(rdfList);
}
In addition to this the Repository API offers the Connections utility class, which contains some useful utility functions specifically for retrieving RDF Collections from a Repository.
For example, to retrieve all statements corresponding to an RDF Collection identified by the resource node from our Repository, we can do the following:
// retrieve all statements forming our RDF Collection from the Repository and put
// them in a Model
try(RepositoryConnection conn = rep.getConnection()) {
   Model rdfList = Connections.getRDFCollection(conn, node, new LinkedHashModel());
}
Or instead, you can retrieve them in streaming fashion as well:
try(RepositoryConnection conn = repo.getConnection()) {
    Connections.consumeRDFCollection(conn, node,
		 st -> { // ... do something with the triples forming the collection });
}
Transactions
So far, we have shown individual operations on repositories: adding statements, removing them, etc. By default, each operation on a RepositoryConnection is immediately sent to the store and committed.
The RepositoryConnection interface supports a full transactional mechanism that allows one to group modification operations together and treat them as a single update: before the transaction is committed, none of the operations in the transaction has taken effect, and after, they all take effect. If something goes wrong at any point during a transaction, it can be rolled back so that the state of the repository is the same as before the transaction started.
Bundling update operations in a single transaction often also improves update performance compared to multiple smaller transactions. This may not be noticeable when adding a few thousand statements, but it can make a big difference when loading millions of statements into a repository.
We can indicate that we want to begin a transaction by using the RepositoryConnection.begin() method. In the following example, we use a connection to bundle two file addition operations in a single transaction:
File inputFile1 = new File("/path/to/example1.rdf");
String baseURI1 = "http://example.org/example1/";
File inputFile2 = new File("/path/to/example2.rdf");
String baseURI2 = "http://example.org/example2/";

try (RepositoryConnection con = myRepository.getConnection()) {
   // start a transaction
   con.begin();
   try {
      // Add the first file
      con.add(inputFile1, baseURI1, RDFFormat.RDFXML);
      // Add the second file
      con.add(inputFile2, baseURI2, RDFFormat.RDFXML);
      // If everything went as planned, we can commit the result
      con.commit();
   }
   catch (RepositoryException e) {
      // Something went wrong during the transaction, so we roll it back
      con.rollback();
   }
}
In the above example, we use a transaction to add two files to the repository. Only if both files can be successfully added will the repository change. If one of the files can not be added (for example because it can not be read), then the entire transaction is cancelled and none of the files is added to the repository.
As you can see, we open a new try block after calling the begin() method (line 9 and further). The purpose of this is to catch any errors that happen during transaction execution, so that we can explicitly call rollback() on the transaction. If you prefer your code shorter, you can leave this out, and just do this:
try (RepositoryConnection con = myRepository.getConnection()) {
   // start a transaction
   con.begin();
   // Add the first file
   con.add(inputFile1, baseURI1, RDFFormat.RDFXML);
   // Add the second file
   con.add(inputFile2, baseURI2, RDFFormat.RDFXML);
   // If everything went as planned, we can commit the result
   con.commit();
}
The close() method, which is automatically invoked by Java when the try-with resources block ends, will also ensure that an unfinished transaction is rolled back (it will also log a warning about this).
A RepositoryConnection only supports one active transaction at a time. You can check at any time whether a transaction is active on your connection by using the isActive() method. If you need concurrent transactions, you will need to use several separate RepositoryConnections.
Transaction Isolation Levels
Any transaction operates according to a certain transaction isolation level. A transaction isolation level dictates who can ‘see’ the updates that are perfomed as part of the transaction while that transaction is active, as well as how concurrent transactions interact with each other.
The following transaction isolation levels are available:


NONE The lowest isolation level; transactions can see their own changes, but may not be able to roll them back, and no support isolation among transactions is guaranteed. This isolation level is typically used for things like bulk data upload operations.


READ_UNCOMMITTED Transactions can be rolled back, but are not necessarily isolated: concurrent transactions may be able to see other’s uncommitted data (so-called ‘dirty reads’).


READ_COMMITTED In this transaction isolation level, only data from concurrent transactions that has been committed can be seen by the current transaction. However, consecutive reads within the same transaction may see different results. This isolation level is typically used for long-lived operations.


SNAPSHOT_READ In addition to being READ_COMMITTED, query results in this isolation level will observe a consistent snapshot. Changes occurring to the data while a query is evaluated will not affect that query’s result. This isolation level is typically used in scenarios where there multiple concurrent transactions that do not conflict with each other.


SNAPSHOT In addition to being SNAPSHOT_READ, succesful transactions in this isolation level will operate against a particular dataset snapshot. Transactions in this isolation level will either see the complete effects of other transactions (consistently throughout) or not at all. This isolation level is typically used in scenarios where a write operation depends on the result of a previous read operation.


SERIALIZABLE In addition to SNAPSHOT, this isolation level requires that all other transactions must appear to occur either completely before or completely after a succesful serializable transaction. This isolation is typically used when multiple concurrent transactions are likely to conflict.


Which transaction isolation level is active is dependent on the actual store the action is performed upon. In addition, not all the transaction isolation levels listed above are by necessity supported by every store.
By default, both the memory store and the native store use the SNAPSHOT_READ transaction isolation level. In addition, both of them support the NONE, READ_COMMITTED, SNAPSHOT, and SERIALIZABLE levels.
The native and memory store use an optimistic locking scheme. This means that these stores allow multiple concurrent write operations, and set transaction locks ‘optimistically’, that is, they assume that no conflicts will occur. If a conflict does occur, an exception is thrown on commit, and the calling user has the option to replay the same transaction with the updated state of the store. This setup significantly reduces the risk of deadlocks, and makes a far greater degree of parallel processing possible, with the downside of having to deal with possible errors thrown to prevent inconsistencies. In cases where concurrent transactions are likely to conflict, the user is advised to use the SERIALIZABLE isolation level.
You can specify the transaction isolation level by means of an optional parameter on the begin() method. For example, to start a transaction that uses SERIALIZABLE isolation:
try (RepositoryConnection conn = rep.getConnection()) {
    conn.begin(IsolationLevels.SERIALIZABLE);
     ....
    conn.commit();
}
A transaction isolation level is a sort of contract, that is, a set of guarantees of what will minimally happen while the transaction is active. A store will make a best effort to honor the guarantees of the requested isolation level. If it does not support the specific isolation level being requested, it will attempt to use a level it does support that offers minimally the same guarantees.
Automated transaction handling
Although transactions are a convenient mechanism, having to always call begin() and commit() to explictly start and stop your transactions can be tedious. RDF4J offers a number of convenience utility functions to automate this part of transaction handling, using the Repositories utility class.
As an example, consider this bit of transactional code. It opens a connection, starts a transaction, adds two RDF statements, and then commits. It also makes sure that it rolls back the transaction if something went wrong, and it ensures that once we’re done, the connection is closed.
import static org.eclipse.rdf4j.model.util.Values.iri;
import static org.eclipse.rdf4j.model.util.Values.literal;

IRI bob = iri("urn:bob");
RepositoryConnection conn = myRepository.getConnection();
try {
   conn.begin();
   conn.add(bob, RDF.TYPE, FOAF.PERSON);
   conn.add(bob, FOAF.NAME, literal("Bob"));
   conn.commit();
}
catch (RepositoryException e) {
   conn.rollback();
}
finally {
   conn.close();
}
That’s an awful lot of code for just inserting two triples. The same thing can be achieved with far less boilerplate code, as follows:
import static org.eclipse.rdf4j.model.util.Values.iri;
import static org.eclipse.rdf4j.model.util.Values.literal;

IRI bob = iri("urn:bob");
Repositories.consume(myRepository, conn -> {
  conn.add(bob, RDF.TYPE, FOAF.PERSON);
  conn.add(bob, RDFS.LABEL, literal("Bob"));
});
As you can see, using Repositories.consume(), we do not explicitly begin or commit a transaction. We don’t even open and close a connection explicitly – this is all handled internally. The method also ensures that the transaction is rolled back if an exception occurs.
This pattern is useful for simple transactions, however as we’ve seen above, we sometimes do need to explicitly call begin(), especially if we want to modify the transaction isolation level.
Multithreaded Repository Access
The Repository API supports multithreaded access to a store: multiple concurrent threads can obtain connections to a Repository and query and performs operations on it simultaneously (though, depending on the transaction isolation level, access may occassionally block as a thread needs exclusive access).
The Repository object is thread-safe, and can be safely shared and reused across multiple threads (a good way to do this is via a RepositoryProvider).

    
    
RepositoryConnection is not thread-safe. This means that you should not try to share a single RepositoryConnection over multiple threads. Instead, ensure that each thread obtains its own RepositoryConnection from a shared Repository object. You can use transaction isolation levels to control visibility of concurrent updates between threads.




  

     
      
        
          

  Table of Contents

  
  
    Creating a Repository object
      
        Main memory RDF Repository
        Native RDF Repository
        Elasticsearch RDF Repository
        RDF Schema inferencing
        Custom Inferencing
        Access over HTTP
          
            Server-side RDF4J repositories
            SPARQL endpoints
            Configuring the HTTP session thread pool
          
        
        The RepositoryManager and RepositoryProvider
          
            The RemoteRepositoryManager
            The RepositoryProvider
          
        
        Creating a Federation
      
    
    Using a repository: RepositoryConnections
      
        Adding RDF to a repository
        Querying a repository
          
            SELECT: tuple queries
            A SPARQL SELECT query in a single line of code: the Repositories utility
            Using TupleQueryResultHandlers
            CONSTRUCT/DESCRIBE: graph queries
            Doing a graph query in a single line of code
            Using RDFHandlers
            Preparing and Reusing Queries
            Tweaking the query evaluation mode
            Explaining queries
          
        
        Creating, retrieving, removing individual statements
        Using named graphs/context
        Working with Models, Collections and Iterations
        RDF Collections and RepositoryConnections
      
    
    Transactions
      
        Transaction Isolation Levels
        Automated transaction handling
      
    
    Multithreaded Repository Access\n\n\n\nParsing and Writing RDF With Rio
    

  
  The RDF4J framework includes a set of parsers and writers for RDF called Rio. Rio (“RDF I/O”) is a toolkit that can be used independently from the rest of RDF4J.
In this chapter, we will take a look at various ways to use Rio to parse from or write to an RDF document. We will show how to do a simple parse and collect the results, how to count the number of triples in a file, how to convert a file from one syntax format to another, and how to dynamically create a parser for the correct syntax format.
If you use RDF4J via the Repository API, then typically you will not need to use the parsers directly: you simply supply the document (either via a URL, or as a File, InputStream or Reader object) to the RepositoryConnection and the parsing is all handled internally. However, sometimes you may want to parse an RDF document without immediately storing it in a triplestore. For those cases, you can use Rio directly.
Listening to the parser
The Rio parsers all work with a set of Listener interfaces that they report results to: ParseErrorListener
, ParseLocationListener
, and RDFHandler
. Of these three, RDFHandler is the most interesting one: this is the listener that receives parsed RDF triples. So we will concentrate on this interface here.
The RDFHandler interface contains five methods: startRDF, handleNamespace, handleComment, handleStatement, and endRDF. Rio also provides a number of default implementations of RDFHandler, such as StatementCollector
, which stores all received RDF triples in a Java Collection. Depending on what you want to do with parsed statements, you can either reuse one of the existing RDFHandlers, or, if you have a specific task in mind, you can simply write your own implementation of RDFHandler. Here, I will show you some simple examples of things you can do with RDFHandlers.
Parsing a file and collecting all triples
As a simple example of how to use Rio, we parse an RDF document and collect all the parsed statements in a Java Collection object (specifically, in a Model
 object).
Let’s say we have a Turtle file, available at http://example.org/example.ttl:
java.net.URL documentUrl = new URL("http://example.org/example.ttl");
InputStream inputStream = documentUrl.openStream();
We now have an open InputStream to our RDF file. Now we need a RDFParser
 object that reads this InputStream and creates RDF statements out of it. Since we are reading a Turtle file, we create a RDFParser object for the RDFFormat.TURTLE
 syntax format:
RDFParser rdfParser = Rio.createParser(RDFFormat.TURTLE);
Note that all Rio classes and interfaces are in package org.eclipse.rdf4j.rio or one of its subpackages.
We also need an RDFHandler which can receive RDF statements from the parser.
Since we just want to create a collection of Statements for now, we’ll just use
Rio’s StatementCollector:
Model model = new LinkedHashModel();
rdfParser.setRDFHandler(new StatementCollector(model));
Note, by the way, that you can use any standard Java Collection class (such as
java.util.ArrayList or java.util.HashSet) in place of the Model object, if you
prefer.
Finally, we need to set the parser to work:
try {
   rdfParser.parse(inputStream, documentURL.toString());
}
catch (IOException e) {
  // handle IO problems (e.g. the file could not be read)
}
catch (RDFParseException e) {
  // handle unrecoverable parse error
}
catch (RDFHandlerException e) {
  // handle a problem encountered by the RDFHandler
}
finally {
  inputStream.close();
}
After the parse() method has executed (and provided no exception has occurred), the collection model will be filled by the StatementCollector. As an aside: you do not have to provide the StatementCollector with a list in advance, you can also use an empty constructor and then just get the collection, using StatementCollector.getStatements().
The Rio
 utility class provides additional helper methods, to make parsing to a Model a single API call:
Model results = Rio.parse(inputStream, documentUrl.toString(), RDFFormat.TURTLE);
Iterating through all the triples in a file
RDF files can also be parsed in a background thread and iterated through as a query result. This allows files that are too big to fit into memory (at once) to be parsed sequentially using a familiar API (specifically, the GraphQueryResult
 interface).
Using the same Turtle file from above:
java.net.URL documentUrl = new URL("http://example.org/example.ttl");
InputStream inputStream = documentUrl.openStream();
We now have an open InputStream to our RDF file. Instead of using a parser directly, we can use the QueryResults.parseGraphBackground()
 function:
String baseURI = documentUrl.toString();
RDFFormat format = RDFFormat.TURTLE;
try (GraphQueryResult res = QueryResults.parseGraphBackground(inputStream, baseURI, format)) {
  while (res.hasNext()) {
    Statement st = res.next();
    // ... do something with the resulting statement here.
  }
}
catch (RDF4JException e) {
  // handle unrecoverable error
}
finally {
  inputStream.close();
}
Using your own RDFHandler: counting statements
Suppose you want to count the number of triples in an RDF file. You could of
course parse the file, add all triples to a Collection, and then check the size
of that Collection. However, this will get you into trouble when you are
parsing very large RDF files: you might run out of memory. And in any case:
creating and storing all these Statement objects just to be able to count them
seems a bit of a waste. So instead, we will create our own RDFHandler
implementation, which just counts the parsed RDF statements and then
immediately throws them away.
To create your own handler, you can of course create a class that implements
the RDFHandler interface, but a useful shortcut is to instead create a subclass
of AbstractRDFHandler
. This is a base class that provides dummy implementations
of all interface methods. The advantage is that you only have to override the
methods in which you need to do something. Since what we want to do is just
count statements, we only need to override the handleStatement method.
Additionaly, we of course need a way to get back the total number of statements
found by our counter:
class StatementCounter extends AbstractRDFHandler {

  private int countedStatements = 0;

  @Override
  public void handleStatement(Statement st) {
     countedStatements++;
  }

 public int getCountedStatements() {
   return countedStatements;
 }
}
Once we have our custom RDFHandler class, we can supply that to the parser
instead of the StatementCollector we saw earlier:
StatementCounter myCounter = new StatementCounter();
rdfParser.setRDFHandler(myCounter);
try {
   rdfParser.parse(inputStream, documentURL.toString());
}
catch (Exception e) {
  // oh no!
}
finally {
  inputStream.close();
}
int numberOfStatements = myCounter.getCountedStatements();
Detecting the file format
In the examples sofar, we have always assumed that you know what the syntax
format of your input file is: we assumed Turtle syntax and created a new parser
using RDFFormat.TURTLE
. However, you may not always know in advance what
exact format the RDF file is in. What then? Fortunately, Rio has a couple of useful features to help you.
The Rio
 utility class has a couple of methods for guessing the correct format,
given either a filename or a MIME-type. For example, to get back the RDF format
for our Turtle file, we could do the following:
RDFFormat format = Rio.getParserFormatForFileName(documentURL.toString()).orElse(RDFFormat.RDFXML);
This will guess, based on the name of the file, that it is a Turtle file and
return the correct format. We can then use that with the Rio class to create
the correct parser dynamically.
Note the .orElse(RDFFormat.RDFXML) bit at the end: if Rio can not guess the
parser format based on the file name, it will simply return RDFFormat.RDFXML as
a default value. Of course if setting a default value makes no sense, you could
also choose to return null or even to throw an exception - that’s up to you.
Once we have the format determined, we can create a parser for it like so:
RDFParser rdfParser = Rio.createParser(format);
As you can see, we still have the same result: we have created an RDFParser
object which we can use to parse our file, but now we have not made the
explicit assumption that the input file is in Turtle format: if we would later
use the same code with a different file (say, a .owl file – which is in RDF/XML
format), our program would be able to detect the format at runtime and create
the correct parser for it.
Writing RDF
Sofar, we’ve seen how to read RDF, but Rio of course also allows you to write
RDF, using RDFWriter
s, which are a subclass of RDFHandler that is intended for
writing RDF in a specific syntax format.
As an example, we start with a Model containing several RDF statements, and we
want to write these statements to a file. In this example, we’ll write our
statements to a file in RDF/XML syntax:
Model model; // a collection of several RDF statements
FileOutputStream out = new FileOutputStream("/path/to/file.rdf");
RDFWriter writer = Rio.createWriter(RDFFormat.RDFXML, out);
try {
  writer.startRDF();
  for (Statement st: model) {
    writer.handleStatement(st);
  }
  writer.endRDF();
}
catch (RDFHandlerException e) {
 // oh no, do something!
}
finally {
  out.close();
}
Again, the Rio helper class provides convenience methods which you can use to
make this a one step process. If the collection is a Model and the desired
output format supports namespaces, then the namespaces from the model will also
be serialised.
Model model; // a collection of several RDF statements
FileOutputStream out = new FileOutputStream("/path/to/file.rdf")
try {
  Rio.write(model, out, RDFFormat.RDFXML);
}
finally {
  out.close();
}
Since we have now seen how to read RDF using a parser and how to write using a
writer, we can now convert RDF files from one syntax to another, simply by
using a parser for the input syntax, collecting the statements, and then
writing them again using a writer for the intended output syntax. However, you
may notice that this approach may be problematic for very large files: we are
collecting all statements into main memory (in a Model object).
Fortunately, there is a shortcut. We can eliminate the need for using a Model
altogether. If you’ve paid attention, you might have spotted it already:
RDFWriters are also RDFHandlers. So instead of first using a StatementCollector
to collect our RDF data and then writing that to our RDFWriter, we can
use the RDFWriter directly. So if we want to convert our input RDF file from
Turtle syntax to RDF/XML syntax, we can do that, like so:
// open our input document
java.net.URL documentUrl = new URL(“http://example.org/example.ttl”);
InputStream inputStream = documentUrl.openStream();
// create a parser for Turtle and a writer for RDF/XML
RDFParser rdfParser = Rio.createParser(RDFFormat.TURTLE);
RDFWriter rdfWriter = Rio.createWriter(RDFFormat.RDFXML,
			   new FileOutputStream("/path/to/example-output.rdf");

// link our parser to our writer...
rdfParser.setRDFHandler(rdfWriter);
// ...and start the conversion!
try {
   rdfParser.parse(inputStream, documentURL.toString());
}
catch (IOException e) {
  // handle IO problems (e.g. the file could not be read)
}
catch (RDFParseException e) {
  // handle unrecoverable parse error
}
catch (RDFHandlerException e) {
  // handle a problem encountered by the RDFHandler
}
finally {
  inputStream.close();
}
Configuring the parser / writer
The Rio parsers and writers have several configuration options, allowing you to
tweak their behavior. The configuration of a Rio parser/writer can be modified
in two ways: programmatically, or by specifying Java system properties
(typically done by passing -D command line flag).
The available configuration options are available via several helper classes,
listed in the Javadoc documentation:

BasicParserSettings

and BasicWriterSettings

contains various parser/writers settings that can be used with most Rio parsers/writers. This includes things
such a IRI syntax validation, datatype verification/normalisation, and
various other general options;
JSONSettings
 has configuration options related to JSON-based formats such as JSON-LD and RDF/JSON;
JSONLDSettings
 has configuration options specific to JSON-LD writing;
NTriplesParserSettings
 and NTriplesWriterSettings
 have additional settings specific to the N-Triples and N-Quads formats;
TurtleParserSettings
 and TurtleWriterSettings
 have additional settings
specific for the Turtle and TriG formats;
XMLParserSettings
 and XMLWriterSettings
 have configuration options specific to XML-based parsing and writing.

The Javadoc documentation shows which settings are available, and what their system property keys and their default values are.
Programmatic configuration
The Rio parser/writer configuration can be retrieved and modified via RDFParser.getParserConfig() / RDFWriter.getWriterConfig(). This returns a RioConfig
 object, which is a collection for the various supported parser/writer settings. Each configuration option can be added to this object with a value of choice.
Each Rio parser/writer can be queried about which settings it supports, using RDFParser.getSupportedSettings() / RDFWriter.getSupportedSettings().
Lastly, the RioConfig also allows you to mark certain types of error as “non-fatal”. Non-fatal errors are still reported by Rio, but will not abort file processing.
Some examples follow:
Example: IRI syntax validation
By default the Rio parsers validate IRI syntax and produce a fatal error if an IRI can not be parsed. You can disable this as follows:
RDFParser rdfParser = Rio.createParser(RDFFormat.TURTLE);
rdfParser.getParserConfig().set(BasicParserSettings.VERIFY_URI_SYNTAX, false);
If you want to make Rio still report syntax errors, but continue processing the file, you can do so as follows:
RDFParser rdfParser = Rio.createParser(RDFFormat.TURTLE);
rdfParser.getParserConfig().addNonFatalError(BasicParserSettings.VERIFY_URI_SYNTAX);
Example: blank node preservation
If you want to preserve blank node identifiers as found in the source file (by default the parser creates new identifiers to ensure uniqueness across multiple files), you can reconfigure the parser as follows:
RDFParser rdfParser = Rio.createParser(RDFFormat.TURTLE);
rdfParser.getParserConfig().set(BasicParserSettings.PRESERVE_BNODE_IDS, true);
Configuration via command line switches
To allow reconfiguring a Rio parser/writer in a runtime deployment (for example in an Rdf4j Server), it is also possible to set certain configuration options through Java system properties. You can specify these by passing -D commandline switches to the JRE in which the application runs.
The Javadoc for each parser/writer setting documents the system property name by which it can be reconfigured. For example, BasicParserSettings.VERIFY_LANGUAGE_TAGS (which determines if Rio verifies that language tags are standards-compliant) can be disabled by using the following command line switch:
-Dorg.eclipse.rdf4j.rio.verify_language_tags=false

Some notes on parsing RDF/XML and JAXP limits
The Rio RDF/XML parser uses the Java API for XML Processing
(JAXP) to process
XML data. Check the documentation on
limit definitions and
using the jaxp.properties file if you get one of the following errors:

JAXP00010001: The parser has encountered more than “64000” entity expansions in this document
JAXP00010004: The accumulated size of entities is … that exceeded the “50,000,000” limit

To disable these limits, you can pass -Djdk.xml.totalEntitySizeLimit=0 -Djdk.xml.entityExpansionLimit=0 to the JVM.
If you have Apache Xerces on the classpath, it replaces the default JDK XML
parser. Unfortunately, Apache Xerces at the time of writing does not fully
implement the JAXP limit definitions, and the above fix will not work with
Xerces. If you run into this issue, we recommend removing Xerces from the
runtime classpath and relying on the default JDK XML processor instead.

  

     
      
        
          

  Table of Contents

  
  
    Listening to the parser
      
        Parsing a file and collecting all triples
        Iterating through all the triples in a file
        Using your own RDFHandler: counting statements
      
    
    Detecting the file format
    Writing RDF
    Configuring the parser / writer
      
        Programmatic configuration
          
            Example: IRI syntax validation
            Example: blank node preservation
          
        
        Configuration via command line switches
        Some notes on parsing RDF/XML and JAXP limits\n\n\n\nThe LMDB Store
    

  
  New in RDF4J 4.0

Experimental

The RDF4J LMDB Store is a new SAIL database, using the Symas Lightning
Memory-Mapped Database: a fast embeddable
key-value database using memory-mapped IO for great performance and stability.
The LMDB Store can be used in any RDF4J project that requires persistent
storage that is fast, scalable and reliable.
Dependencies for the LMDB Store and native extensions
To make use of the LMDB Store, you’ll need to include the following Maven dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-lmdb</artifactId>
</dependency>
Alternatively you can also rely on the rdf4j-storage pom dependency (see (“Which maven artifact?"), which includes the LMDB Store.
Because the LMDB Store relies on a third party embedded database (LMDB) that is
not itself a Java library, you’ll need two additional runtime dependencies for
native extensions, provided by LWJGL. These dependencies
have an OS-specific classifier that is based on the platform OS you wish to run
on. For example, to run the LMDB Store on a Linux machine, you’ll need to
include the following:
<dependency>
  <groupId>org.lwjgl</groupId>
  <artifactId>lwjgl</artifactId>
  <classifier>natives-linux</classifier>
  <scope>runtime</scope>
</dependency>
<dependency>
  <groupId>org.lwjgl</groupId>
  <artifactId>lwjgl-lmdb</artifactId>
  <classifier>natives-linux</classifier>
  <scope>runtime</scope>
</dependency>
The required versions of the native extensions are in the RDF4J Bill Of
Materials.
Available extensions for different OS platforms:



Operating System
native extension classifier




Linux
natives-linux


MS Windows
natives-windows


Mac OS
natives-macos


Mac OS (ARM64)
natives-macos-arm64



Create RDF Repository
The code for creation of an LMDB-based RDF repository is similar to that of the MemoryStore or NativeStore:
import org.eclipse.rdf4j.repository.Repository;
import org.eclipse.rdf4j.repository.sail.SailRepository;
import org.eclipse.rdf4j.sail.lmdb.LmdbStore;
import org.eclipse.rdf4j.sail.lmdb.config.LmdbStoreConfig;
...
File dataDir = new File("/path/to/datadir/");
Repository repo = new SailRepository(new LmdbStore(dataDir));
The above code initializes a new or loads an existing repository at the location specified by dataDir.
import org.eclipse.rdf4j.repository.Repository;
import org.eclipse.rdf4j.repository.sail.SailRepository;
import org.eclipse.rdf4j.sail.lmdb.LmdbStore;
...
File dataDir = new File("/path/to/datadir/");
Repository repo = new SailRepository(new LmdbStore(dataDir));
By default, the store uses the two indexes: spoc and posc.
To configure the indexes and other options an instance of LmdbStoreConfig can be used.
import org.eclipse.rdf4j.repository.Repository;
import org.eclipse.rdf4j.repository.sail.SailRepository;
import org.eclipse.rdf4j.sail.lmdb.LmdbStore;
import org.eclipse.rdf4j.sail.lmdb.config.LmdbStoreConfig;
...
File dataDir = new File("/path/to/datadir/");

LmdbStoreConfig config = new LmdbStoreConfig();
// set triple indexes
config.setTripleIndexes("spoc,ospc,psoc");
// always sync to disk, disabled by default
config.setForceSync(true);
// disable autogrow, enabled by default
config.setAutoGrow(false);
// set maximum size of value db to 1 GiB

config.setValueDBSize(1_073_741_824L);
// set maximum size of triple db to 1 GiB
config.setTripleDBSize(1_073_741_824L);

Repository repo = new SailRepository(new LmdbStore(dataDir), config);
Required storage space, RAM size and disk performance
You can expect a footprint of around 120 - 130 bytes per quad when using the LMDB store
with 3 indexes (like spoc, ospc and psoc).
Therefore 120 - 130 GB storage space per 1 billion quads are required.
Please note that the actual footprint also depends largely on the size of IRIs and literals.
Some basic information about LMDB database and RAM sizes can be found in the
OpenLDAP & LMDB Sizing Guide.
The bottom line is that more RAM is better. The best is to have enough RAM to accommodate the
entire database or at least the database’s working set.
Another factor is the speed of your disks. You should use SSDs for larger databases.
More up-to-date information about LMDB can be found at: https://www.symas.com/symas-lmdb-tech-info
Especially the SSD-benchmarks may be of interest.
Backup and restore
LMDB provides a set of command line tools that can be used
to backup and restore the value and triple databases.
Those tools can typically be used while the databases are in use as LMDB permits concurrent use
through multiple processes. Please note that it may happen that the backups of the value and triple
databases may get out of sync if the LMDB store has active writes as each uses its own transaction.
Control database file size
LMDB uses memory-mapped files
for data storage. The size of the memory map needs to be configured and is also the maximum size
of the database. As shown above the map size can be controlled individually for the value and
triple databases via LmdbStoreConfig.setValueDBSize(...) and
LmdbStoreConfig.setTripleDBSize(...). The size is automatically aligned
to the system’s page size as suggested by the
LMDB documentation.
The database sizes can be increased when re-opening the LmdbStore.
Usually these sizes can be set to large values that must be smaller or equal
to the system’s address space. This fact needs to be especially considered on 32-bit systems.
On Linux-based systems the file size grows dynamically according to the
actual size of the used memory pages for the data.
On Windows the file size is entirely allocated and care should be taken when
choosing the value and triple db sizes.
Autogrow feature
RDF4J implements an autogrow feature to simplify the management of memory map sizes.
If it is enabled (which is the default) then RDF4J monitors the actual used pages and
automatically increases the map size if required.
This monitoring only has a very minimal overhead.
The only downsides are:


Some kind of stop the world approach is required to set the new map sizes where all running
transactions are suspended for a short time.


A running write transaction may lead to a temporary overflow of data to disk if the
current map size needs to be increased. This may be an issue with large transactions that
can get slowed down.



  

     
      
        
          

  Table of Contents

  
  
    Dependencies for the LMDB Store and native extensions
    Create RDF Repository
    Required storage space, RAM size and disk performance
    Backup and restore
    Control database file size
    Autogrow feature\n\n\n\nFull-Text Indexing With the Lucene SAIL
    

  
  The LuceneSail enables you to add full text search of RDF literals to find subject resources to any Sail stack.
It provides querying support for the following statement patterns:
PREFIX search: <http://www.openrdf.org/contrib/lucenesail#>

?subj search:matches [
	      search:query "search terms...";
	      search:property my:property;
	      search:score ?score;
	      search:snippet ?snippet ] .
The ‘virtual’ properties in the search: namespace have the following meaning:

search:matches – links the resource to be found with the following query statements (required)
search:query – specifies the Lucene query (required)
search:property – specifies the property to search. If omitted all properties are searched (optional)
search:score – specifies a variable for the score (optional)
search:snippet – specifies a variable for a highlighted snippet (optional)

Configuration
The LuceneSail is a stacked Sail: to use it, simply wrap your base SAIL with it:
Sail baseSail = new NativeStore(new File("."));
LuceneSail lucenesail = new LuceneSail();
// set any parameters, this one stores the Lucene index files into memory
lucenesail.setParameter(LuceneSail.LUCENE_RAMDIR_KEY, "true");
...
// wrap base sail
lucenesail.setBaseSail(baseSail);
Language filtering
You can add a filter to only index literals with particular languages, for example:
// this sail will now will only index French literals
lucenesail.setParameter(LuceneSail.INDEXEDLANG, "fr");
To use multiple languages, split them with spaces, for example:
// this sail will now only index French and English literals
lucenesail.setParameter(LuceneSail.INDEXEDLANG, "fr en");
Type filtering
You can add a filter to only index literals of subject with particular type, for example with the subject/literals
@prefix my: <http://example.org/> .

my:subject1 my:oftype my:type1 ;
            my:prop   "text"   .

my:subject2 my:oftype my:type2 ;
            my:prop   "text"   .
To only index the literals of the subjects with the type my:type1, you can use the type filter parameter:
// this sail will now only index literals of subjects ?s with the triple (?s ex:oftype ex:type1).
lucenesail.setParameter(LuceneSail.INDEXEDTYPES, "http\\://example.org/oftype=http\\://example.org/type1");
You can specify multiple types for the same type predicate by splitting them with spaces, you can specify multiple type predicates by splitting them with new lines, example:
// this sail will now only index literals of subjects ?s with the triple:
// (?s ex:oftype1 ex:type11), (?s ex:oftype1 ex:type12), (?s ex:oftype2 ex:type21) 
// or (?s ex:oftype2 ex:type22).
lucenesail.setParameter(LuceneSail.INDEXEDTYPES, 
		"http\\://example.org/oftype1=http\\://example.org/type11 http\\://example.org/type12\n"
		"http\\://example.org/oftype2=http\\://example.org/type21 http\\://example.org/type22"
);
You can use the special predicate a instead of rdf:type.
You can also reduce the usage of the base sail to set the type of backtracking:

TypeBacktraceMode.COMPLETE: (default) will check every triples with ?s and try to add or remove them in the Lucene Index.
TypeBacktraceMode.PARTIAL: won’t check previous triples in the store, assume that the user would add new elements to the index after and with the add of a type triple and would remove elements to the index with the remove of type.

// the sail won't search for the type a triple if the type isn't in the UPDATE request
lucenesail.setIndexBacktraceMode(TypeBacktraceMode.PARTIAL);
Full text search
Search is case-insensitive, wildcards and other modifiers can be used to broaden the search. For example, search all literals containing words starting with “alic” (e.g. persons named “Alice”):
....
Repository repo = new SailRepository(lucenesail);

// Get the subjects and a highlighted snippet
String qry = "PREFIX search: <http://www.openrdf.org/contrib/lucenesail#> " +
			"SELECT ?subj ?text " +
			"WHERE { ?subj search:matches [" +
					" search:query ?term ; " +
					" search:snippet ?text ] } ";

List<BindingSet> results;
try (RepositoryConnection con = repo.getConnection()) {
	ValueFactory fac = con.getValueFactory();

	TupleQuery tq = con.prepareTupleQuery(QueryLanguage.SPARQL, qry);
	// add wildcard '*' to perform wildcard search
	tq.setBinding("term", fac.createLiteral("alic" + "*"));

	// copy the results and processs them after the connection is closed
	results = QueryResults.asList(tq.evaluate());
}

results.forEach(res -> {
		System.out.println(res.getValue("subj").stringValue());
		System.out.println(res.getValue("text").stringValue());
});
Complex query (Field boosting and per-field search)
This feature might no be available for your implementation, see SearchIndex implementations.
During the search, it might be important to boost the value of a single field while using multiple fields, to do that, you can use complex query:
PREFIX search: <http://www.openrdf.org/contrib/lucenesail#>

?subj search:matches [
          search:query 
          [
              search:query "search terms over my:property1...";
              search:property my:property1;
              search:boost 0.8;
              search:snippet ?snippet1;
          ] ,
          [
              search:query "search terms over my:property2...";
              search:property my:property2;
              search:boost 0.2;
              search:snippet ?snippet2;
          ];
          search:score ?score
] .
The ‘virtual’ properties in the search: namespace have the following meaning:

search:matches – links the resource to be found with the following query statements (required)
search:query – specifies the Lucene query object(s), you can put as much query object as you want (required)

search:query – specifies the Lucene query (required)
search:property – specifies the property to search. If omitted all properties are searched (optional)
search:boost – (float number) specifies the boost for the property to search. If omitted, no boost is applied (optional)
search:snippet – specifies a variable for a highlighted snippet (optional)


search:score – specifies a variable for the score (optional)

You can use complex queries with a literal query!
SearchIndex implementations
The LuceneSail can currently be used with three SearchIndex implementations:




SearchIndex implementation
Maven module
Complex query support ?




Apache Lucene
org.eclipse.rdf4j.sail.lucene.impl.LuceneIndex
rdf4j-sail-lucene
yes


ElasticSearch
org.eclipse.rdf4j.sail.elasticsearch.ElasticsearchIndex
rdf4j-sail-elasticsearch
no


Apache Solr
org.eclipse.rdf4j.sail.solr.SolrIndex
rdf4j-sail-solr
no



Each SearchIndex implementation can easily be extended if you need to add extra features or store/access data with a different schema.
The following example uses a local Solr instance running on the default port 8983. Make sure that both the Apache httpcore and commons-logging jars are in the classpath, and that the Solr core uses an appropriate schema (an example can be found in RDF4J’s embedded solr source code on GitHub).
import org.eclipse.rdf4j.sail.solr.SolrIndex;
....
LuceneSail luceneSail = new LuceneSail();
luceneSail.setParameter(LuceneSail.INDEX_CLASS_KEY, SolrIndex.class.getName());
luceneSail.setParameter(SolrIndex.SERVER_KEY, "http://localhost:8983/solr/rdf4j");
If needed, the Solr Client can be accessed via:
SolrIndex index = (SolrIndex) luceneSail.getLuceneIndex();
SolrClient client = index.getClient();


  

     
      
        
          

  Table of Contents

  
  
    Configuration
      
        Language filtering
        Type filtering
      
    
    Full text search
    Complex query (Field boosting and per-field search)
    SearchIndex implementations\n\n\n\nReasoning and Validation With SPIN
    

  
  The SPARQL Inferencing Notation (SPIN) is a way to represent a wide range of business rules on top of an RDF dataset. These rules can be anything from constraint validation to inferred property value calculation.

    
    
The use of SPIN is no longer recommended. The SpinSail is not actively maintained and has performance and scalability issues. If you are considering using SPIN for validation, we recommend looking at SHACL and the ShaclSail instead. Should you still want to use SPIN for inference, we recommend disabling the validation step: spinSail.setValidateConstraints(false). SPIN was never designed to work in a transactional environment, which means that you should expect odd scenarios where you update your data without new data being inferred or old inferred data still sticking around.




The SpinSail is a StackedSail component that adds a forward-chaining SPIN rule engine on top of any store. In its most basic form it can be used directly on top of a Sail:
// create a basic Sail Stack with a simple Memory Store and SPIN inferencing support
SpinSail spinSail = new SpinSail();
spinSail.setBaseSail(new MemoryStore());
// create a repository with the Sail stack:
Repository rep = new SailRepository(spinSail);
rep.init();
Alternatively, a SpinSail can be configured via the RepositoryManager:
// create the config for the sail stack
SailImplConfig spinSailConfig = new SpinSailConfig(new MemoryStoreConfig());
RepositoryImplConfig repositoryTypeSpec = new SailRepositoryConfig(spinSailConfig);
// create the config for the actual repository
String repositoryId = "spin-test";
RepositoryConfig repConfig = new RepositoryConfig(repositoryId, repositoryTypeSpec);
manager.addRepositoryConfig(repConfig);

// get the Repository from the manager
Repository repository = manager.getRepository(repositoryId);
While this configuration already allows you to do many useful things, it does not do complete SPIN reasoning: the SpinSail relies on basic RDFS inferencing to be supplied by the underlying Sail stack. This means that for use cases where you need to rely on things like transitivity of rdfs:subClassOf relations, you should configure a Sail stack that includes the SchemaCachingRDFSInferencer. In addition, a DedupingInferencer is supplied which is a small optimization for both reasoners: it takes care to filter out potential duplicate results – though at the cost of an increase in memory usage. The full configuration with both additional inferencers looks like this:
// create a basic Sail Stack with a simple Memory Store, full RDFS reasoning,
// and SPIN inferencing support
SpinSail spinSail = new SpinSail();
spinSail.setBaseSail(
        new SchemaCachingRDFSInferencer(
               new DedupingInferencr(new MemoryStore())
        )
);
// create a repository with the Sail stack:
Repository rep = new SailRepository(spinSail);
rep.init();
or using configuration via the RepositoryManager:
// create the config for the sail stack
SailImplConfig spinSailConfig = new SpinSailConfig(
           new SchemaCachingRDFSInferencerConfig(
                 new DedupingInferencerConfig(new MemoryStoreConfig())
           )
);
RepositoryImplConfig repositoryTypeSpec = new SailRepositoryConfig(spinSailConfig);
// create the config for the actual repository
String repositoryId = "spin-test";
RepositoryConfig repConfig = new RepositoryConfig(repositoryId, repositoryTypeSpec);
manager.addRepositoryConfig(repConfig);

// get the Repository from the manager
Repository repository = manager.getRepository(repositoryId);
Adding rules
Once your repository is set up with SPIN support, you can add rules by simply uploading an RDF document contain SPIN rules (which are expressed in RDF using the SPIN vocabulary). The SpinSail will automatically execute these rules on the data.
As an example, consider the following data:
@prefix ex: <http://example.org/>.

ex:John a ex:Father ;
	ex:parentOf ex:Lucy .

ex:Lucy a ex:Person .
Now assume we wish to introduce a rule that defines persons who are the object of the ex:parentOf relation to be subject of an ex:childOf relation (in other words, we want to infer the inverse relationship for the parent-child relation). In SPIN, this could be done with the following rule:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix sp: <http://spinrdf.org/sp#>.
@prefix spin: <http://spinrdf.org/spin#>.
@prefix ex: <http://example.org/>.

## every person who has a parent is a child of that parent.
ex:Person a rdfs:Class ;
	spin:rule [
		a sp:Construct ;
	sp:text """PREFIX ex: <http://example.org/>
		   CONSTRUCT { ?this ex:childOf ?parent . }
		   WHERE { ?parent ex:parentOf ?this . }"""
] .
To get the SpinSail to execute this rule, all you need to do is upload both above RDF datasets to the Repository. The relation will be automatically inferred at data upload time, so the query:
    SELECT ?child WHERE { ?child ex:childOf ?parent }
will give this result:



child




ex:Lucy



Limitations
The SpinSail attempts to only run relevant rules by detecting if data related to the rule has changed. This is only done by checking if any of the subjects in the added data have the type required by the rule. There is no analysis of the query, so if your query contains more than a simple ?a ex:pred ?b then you will run into incomplete inference in the face of updates.
An example of a rule that will lead to incomplete inference results in the face of updates:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix sp: <http://spinrdf.org/sp#>.
@prefix spin: <http://spinrdf.org/spin#>.
@prefix ex: <http://example.org/>.

## if you are the parent of a parent of a child, that child is your grandchild.
ex:Person a rdfs:Class ;
	spin:rule [
		a sp:Construct ;
	sp:text """PREFIX ex: <http://example.org/>
		   CONSTRUCT { ?this ex:grandchildOf ?grandparent . }
		   WHERE { ?grandparent ex:parentOf/ex:parentOf ?this . }"""
] .
If ex:Peter ex:parentOf ex:PeterJr and ex:PeterJr a ex:Person, then adding ex:Selma ex:parentOf ex:Peter will not lead to ex:PeterJr ex:grandchildOf ex:Selma being true because the SpinSail does not understand that adding ex:Selma ex:parentOf ex:Peter should trigger the rule for ex:PeterJr.
Removal of already infered data is only done when a statement is deleted. This means that the use of aggregation, negation og subselects (and possibly other cases) will lead to incorrect inference where old inferred statements will still remain in your data.
An example of a rule with negation that will lead to incorrect (stale) inference:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix sp: <http://spinrdf.org/sp#>.
@prefix spin: <http://spinrdf.org/spin#>.
@prefix ex: <http://example.org/>.

## A child is an Only Child if their parent's have no other children
ex:Person a rdfs:Class ;
	spin:rule [
		a sp:Construct ;
	sp:text """PREFIX ex: <http://example.org/>
		   CONSTRUCT { ?this a ex:OnlyChild . }
		   WHERE {
		    ?parent ex:parentOf ?this .
		    FILTER( NOT EXISTS {?parent ex:parentOf ?otherChild. FILTER(?this != ?otherChild)} )

		   }"""
] .
Adding ex:Peter ex:parentOf ex:PeterJr and ex:PeterJr a ex:Person will lead to ex:PeterJr a ex:OnlyChild being true. This is correct. Adding ex:Peter ex:parentOf ex:Caroline means that ex:PeterJr should not be an Only Child anymore (according to the rule). ex:PeterJr a ex:OnlyChild will still be true even after adding ex:Peter ex:parentOf ex:Caroline because the SpinSail does not refresh already inferred data when there are no user-initiated deletions.
Performance
Performance is largely dictated by how complex your rules and constraints are.
Removing a statement will force all inferred data to be removed and reinferred. In the best case this will take a second or two on modern hardware, because even an empty SpinSail contains a number of default SPIN rules and constraints. Adding your own rules, constraints and data will only make this slower.
Disabling constraint validation will improve performance: spinSail.setValidateConstraints(false)
Further reading
Here are some useful links to learn more about SPIN:

SPIN Primer
Getting Started with SPIN (by Topquadrant)


  

     
      
        
          

  Table of Contents

  
  
    Adding rules
    Limitations
    Performance
    Further reading\n\n\n\nValidation With SHACL
    

  
  The SHapes Constraint Language (SHACL) is a language for validating RDF graphs.
This documentation is for RDF4J 2.5 and onwards, as the SHACL Engine was experimental until this release.
Understanding the RDF4J SHACL Engine
The SHACL Engine works by analyzing the changes made in a transaction and creating a set of validation
plans (similar to query plans) and executing these as part of the transaction commit() call.
The SHACL Engine can often validate your changes based solely on the changes themselves. However,
there are times when your alterations may have an impact on, or be influenced by, pre-existing data in
the database. In these cases, the engine retrieves the related data from the database.
Let’s consider an example to understand how the current data in the database can influence the validation plans:
ex:PersonShape
    a sh:NodeShape  ;
    sh:targetClass ex:Person ;
    sh:property [
        sh:path ex:age ;
        sh:datatype xsd:int ;
        sh:message "A person's age must be an integer (xsd:int)." ;
    ] .
Here is the initial data in the database:
ex:pete a ex:Person.
Next, a transaction adds the following data:
ex:pete ex:age "eighteen".
In this scenario, the SHACL Engine will match the predicate ex:age with ex:PersonShape and realize that ex:pete may be already defined as an ex:Person in the database. As a result, the validation plan will include a check to confirm if ex:pete is indeed categorized as ex:Person.
How to load and update SHACL shapes
By default, the ShaclSail uses a reserved graph (http://rdf4j.org/schema/rdf4j#SHACLShapeGraph) for storing the SHACL shapes.
Utilize a normal connection to load your shapes into this graph. SPARQL is not supported.
The Shapes Graph section explains how to store your shapes in a different named graph.
ShaclSail shaclSail = new ShaclSail(new MemoryStore());
Repository repo = new SailRepository(shaclSail);

try (RepositoryConnection connection = repo.getConnection()) {

    Reader shaclRules = ....

    // add shapes
    connection.begin();
    connection.add(shaclRules, "", RDFFormat.TURTLE, RDF4J.SHACL_SHAPE_GRAPH);
    connection.commit();
    
    // clear existing shapes and add new ones in single transaction (eg. update shapes)
    connection.begin();
    connection.clear(RDF4J.SHACL_SHAPE_GRAPH);
    connection.add(shaclRules, "", RDFFormat.TURTLE, RDF4J.SHACL_SHAPE_GRAPH);
    connection.commit();

    // clear all shapes
    connection.begin();
    connection.clear(RDF4J.SHACL_SHAPE_GRAPH);
    connection.commit();
    
    // connection.clear(); will not clear the Shapes graph!


}
You can at any point update your shapes. Updating shapes will cause your data to be re-validated. The transaction
will fail if the data is not valid according to the changed shapes.
The simplest way to update your shapes is within a transaction, drop the shapes graph and load your updated shapes.
The ShaclSail will only apply the actual changes. Modifying shapes in parallel should not be a problem, but may cause
slowdowns.
Remember that a full validation is run for all the affect shapes. Depending on the amount of
data already in your store this may take a significant amount of time. If you are sure that your
data will not violate the shapes, or otherwise need to skip validation then you can read the section on Performance.
Do not use SPARQL to update your shapes!
Supported SHACL features
The SHACL W3C Recommendation defines the SHACL features that should be supported and RDF4J is working hard to
support them all. At the moment a fairly large subset of these features are supported by the ShaclSail, however quite a
few are not yet implemented.
An always-up-to-date list of features can be found by calling the static method ShaclSail.getSupportedShaclPredicates().
As of writing this documentation the following features are supported.


sh:targetClass


sh:targetNode


sh:targetSubjectsOf


sh:targetObjectsOf


sh:target for use with DASH targets and sh:SPARQLTarget


sh:path, sh:inversePath, sh:alternativePath and sequence paths


sh:node


sh:property


sh:and


sh:or


sh:not


sh:minCount


sh:maxCount


sh:qualifiedMinCount


sh:qualifiedMaxCount


sh:qualifiedValueShape


sh:nodeKind


sh:minLength


sh:maxLength


sh:pattern and sh:flags


sh:languageIn


sh:datatype


sh:class


sh:hasValue


sh:in


sh:minExclusive


sh:minInclusive


sh:maxExclusive


sh:maxInclusive


sh:uniqueLang


sh:sparql


sh:select


sh:prefixes


sh:prefix


sh:namespace


sh:declare


sh:deactivated


sh:severity


sh:message


sh:shapesGraph


dash:hasValueIn


rsx:targetShape


DASH and RSX features need to be explicitly enabled, for instance with setDashDataShapes(true) and
setEclipseRdf4jShaclExtensions(true). These are currently experimental features. For more information
about the RSX features, see the RSX section of this document.
Implicit sh:targetClass is supported for nodes that are rdfs:Class and either of sh:PropertyShape or sh:NodeShape. Validation for all nodes,
equivalent to owl:Thing or rdfs:Resource in an environment with a reasoner, can be enabled by setting setUndefinedTargetValidatesAllSubjects(true).
sh:pathsupports single predicate paths, e.g. ex:age, inverse paths, sequence paths and alternate paths.
Validation results
On commit() the ShaclSail will validate your changes and throw an exception if there are violations. The exception contains a validation report and can be retrieved like this:
try {
    connection.commit();
} catch (RepositoryException exception) {
    Throwable cause = exception.getCause();
    if (cause instanceof ValidationException) {
        Model validationReportModel = ((ValidationException) cause).validationReportAsModel();
        // use validationReportModel to understand validation violations

        Rio.write(validationReportModel, System.out, RDFFormat.TURTLE);
    }
    throw exception;
}
The validationReportModel follows the report format specified by the W3C SHACL recommendation. It does not provide all the information specified in the recommendation. Example report:
[]
    a sh:ValidationReport ;
    sh:conforms false ;
    rdf4j:truncated false;
    sh:result [
        a sh:ValidationResult ;
        sh:value "eighteen";
        sh:focusNode ex:pete ;
        sh:resultPath ex:age ;
        sh:sourceConstraintComponent sh:DatatypeConstraintComponent ;
        sh:resultSeverity sh:Violation;
        sh:resultMessage "A person's age must be a number (xsd:int).";
        sh:sourceShape [ a sh:PropertyShape;
            sh:message "A persons age must be a number (xsd:int).";
            sh:path ex:age;
            sh:datatype xsd:int
        ]
    ] .
The ValidationReport class provides the same information as the validationReportModel, but as a Java object with getters for accessing the report data.
Limiting the validation report
Large validation reports take time to generate and can use large amounts of memory.
Limiting the size of the report can be useful to speed up validation and to reduce the number of similar violations.
Limitations can either be configured directly in the ShaclSail or through the configuration files.

setValidationResultsLimitTotal(1000) limits the total number of validation results per report to 1000. (1 000 000 by default)

<http://rdf4j.org/config/sail/shacl#validationResultsLimitTotal>


setValidationResultsLimitPerConstraint(10) limits the number of validation results per constraint component to 10. (1000 by default)

<http://rdf4j.org/config/sail/shacl#validationResultsLimitPerConstraint>



Use -1 to remove a limit and 0 to validate but return an empty validation report.
A truncated validation report will have isTruncated() return true and the model will have rdf4j:truncated true.
Retrieving violated shapes
The validation report contains the portions of the source shape responsible for the violation. As the SHACL
shapes graph stores all shapes, you can obtain the entire violated shape from the ShaclSail when a transaction fails.
try {
    connection.commit();
} catch (RepositoryException exception) {
    Throwable cause = exception.getCause();
    if (cause instanceof ValidationException) {
        Model validationReportModel = ((ValidationException) cause).validationReportAsModel();

        validationReportModel
            .filter(null, SHACL.SOURCE_SHAPE, null);
            .forEach(s -> {
                Value object = s.getObject();

                try (SailRepositoryConnection connection = shaclSail.getConnection()) {

                    try (Stream<Statement> stream = connection.getStatements((Resource) object, null, null, RDF4J.SHACL_SHAPE_GRAPH).stream()) {
                        List<Statement> collect = stream.collect(Collectors.toList());

                        // collect contains the shape!
                    }

                }

            });
    }
    throw exception;
}
Transactional support
When running multiple transactions in parallel, the results from two transactions may jointly cause the validation to fail.
A good example is as follows:
ex:PersonShape
    a sh:NodeShape  ;
    sh:targetClass ex:Person ;
    sh:property [
        sh:path ex:age ;
        sh:datatype xsd:int ;
    ] .
One transaction adds:
ex:pete a ex:Person.
In parallel another transaction adds:
ex:pete ex:age "eighteen".
Neither of these transactions will by themselves cause the validation to fail, but together they will.
Typically, in order to handle this scenario a user would need to use SERIALIZABLE transactions, which are slow and
prone to failure. The ShaclSail instead uses locking to run transactions one-after-the-other if the isolation level is set to
SNAPSHOT. This is typically 2-4x faster than using SERIALIZABLE.
Locking only affects transactions that write to the ShaclSail, and locking is only applied when calling commit().
It is possible to disable this type of validation with setSerializableValidation(false).
Performance
The ShaclSail is built for performance. Each transaction is analyzed so that only the minimal set of shapes need to be
validated, and for each of those shapes only the least amount of data is retrieved in order to perform the validation.
Parallel validation further increases performance and is enabled by default. This can be disabled with setParallelValidation(false).
Some workloads will not fit in memory and need to be validated while stored on disk. This can be achieved by using a
NativeStore and specifying the Bulk transaction setting.

ShaclSail.TransactionSettings.ValidationApproach.Auto: Let the ShaclSail choose the best approach.
ShaclSail.TransactionSettings.ValidationApproach.Bulk: Optimized for large transactions, disables caching and parallel validation and runs a full validation step at the end of the transaction.
ShaclSail.TransactionSettings.ValidationApproach.Disabled: Disable validation.

Disabling validation for a transaction may leave your data in an invalid state. Running a transaction with bulk validation will force a full validation.
This is a useful approach if you need to use multiple transactions to bulk load your data. You can always skip this if you are certain that your changes are valid.
There are also a set of experimental transaction settings for hinting about performance aspects of the validation.

ShaclSail.TransactionSettings.PerformanceHint.CacheEnabled: Enable the cache that stores intermediate results so these only need to be computed once.
ShaclSail.TransactionSettings.PerformanceHint.CacheDisabled: Disable the cache.
ShaclSail.TransactionSettings.PerformanceHint.ParallelValidation: Run validation in parallel (multithreaded).
ShaclSail.TransactionSettings.PerformanceHint.SerialValidation:  Run validation in serial (single threaded).

ShaclSail shaclSail = new ShaclSail(new NativeStore(new File(...), "spoc,ospc,psoc"));
SailRepository sailRepository = new SailRepository(shaclSail);

try (SailRepositoryConnection connection = sailRepository.getConnection()) {

      connection.begin(
          IsolationLevels.NONE, 
          ShaclSail.TransactionSettings.ValidationApproach.Bulk, 
          ShaclSail.TransactionSettings.PerformanceHint.CacheDisabled, 
          ShaclSail.TransactionSettings.PerformanceHint.SerialValidation
      );    
        
//    You can enable parallel validation and the intermediate cache for better performance if you have sufficient memory 
//    connection.begin(
//        IsolationLevels.NONE, 
//        ShaclSail.TransactionSettings.ValidationApproach.Bulk, 
//        ShaclSail.TransactionSettings.PerformanceHint.CacheEnabled, 
//        ShaclSail.TransactionSettings.PerformanceHint.ParallelValidation
//    );    
    
    // load shapes
    try (InputStream inputStream = new FileInputStream("shacl.ttl")) {
        connection.add(inputStream, "", RDFFormat.TURTLE, RDF4J.SHACL_SHAPE_GRAPH);
    }

    // load data
    try (InputStream inputStream = new BufferedInputStream(new FileInputStream("data.ttl"))) {
        connection.add(inputStream, "", RDFFormat.TURTLE);
    }

    // commit transaction and catch any exception
    try {
        connection.commit();
    } catch (RepositoryException e){
        if(e.getCause() instanceof ValidationException){
            Model model = ((ValidationException) e.getCause()).validationReportAsModel();
            Rio.write(model, System.out, RDFFormat.TURTLE);
        }
    }

}

sailRepository.shutDown();
Automatic bulk validation
Large transactions will take up significant amounts of memory because the transactional validation needs to track and analyze the transactional
changes in order to decide what needs to be validated. Very large transactions could exceed the amount of memory available and cause the
JVM to crash.
As of 4.0.0 transactions will automatically be switched to bulk validation if they exceed a set limit.

setTransactionalValidationLimit(1000) will make transactions switch to bulk validation if the transaction size is more than 1000 statements. Default is 500 000.

<http://rdf4j.org/config/sail/shacl#transactionalValidationLimit>



Automatic bulk validation is not compatible with serializable validation.
Reasoning
By default, the ShaclSail supports the simple rdfs:subClassOf reasoning required by the SHACL W3C recommendation. There is no
support for sh:entailment, however the entire reasoner can be disabled with setRdfsSubClassReasoning(false).
Shapes graph
When shapes are used to validate the contents of a database it makes sense to consider them part of the database schema and as such
keep them seperated from each other. This separation is achieved by storing shapes in a reserved graph
(http://rdf4j.org/schema/rdf4j#SHACLShapeGraph) which is hidden unless specifically named when loading or removing data.
As of 4.0.0 the ShaclSail can be configured to load shapes from other graphs (both named graphs and the default graph).

setShapesGraphs(Set.of(RDF4J.SHACL_SHAPE_GRAPH, Values.iri("http://example.org/myShapeGraph")) will read shapes from both the normal reserved graph and the named graph http://example.org/myShapeGraph.

<http://rdf4j.org/config/sail/shacl#shapesGraph>



Shapes stored in the reserved graph (http://rdf4j.org/schema/rdf4j#SHACLShapeGraph) are used to validate the union of all triples
in the default graph and any other named graph. The ShaclSail relies on sh:shapesGraph statements to understand how shapes stored
in other graphs should be used for validation.
SHACL uses the term shapes graph to refer to an RDF graph where the shapes are defined, and the term data graph to refer to an RDF
graph where the data to be validated is stored. Data graphs and shapes graphs are linked together using sh:shapesGraph statements
which are used by the ShaclSail to decide which shapes should be used to validate which graphs.
For security and performance the ShaclSail ignores sh:shapesGraph statements that are not in a graph that has been configured for
shapes, as explained above. This means that you can always trust data you load into your database to not tamper with your shapes or
with which shapes are used for validation, as long as you limit which graphs your load the data into.
Example
ex:shapesGraph1 {
    ex:PersonShape
        a sh:NodeShape  ;
        sh:targetClass ex:Person ;       
}  


ex:shapesGraph2 {
    ex:PersonShape       
        sh:property [
            sh:path ex:age ;
            sh:datatype xsd:int ;
        ] .

    ex:dataGraph sh:shapesGraph ex:shapesGraph1, ex:shapesGraph2.         
}


ex:shapesGraph3 {
    ex:PersonShape       
        sh:property [
            sh:path ex:age ;
            sh:minCount 1;
        ] .
}
The above shapes will result in all the data in ex:dataGraph being validated against the shapes defined in the union
of both ex:shapesGraph1 and ex:shapesGraph2. The shape defined in ex:shapesGraph3 is effectively ignored.
The following data is valid.
ex:dataGraph {
    ex:steve a ex:Person.
    
    ex:jane a ex:Person;
        ex:age 40.
}  
While the following data is invalid.
ex:dataGraph {    
    ex:john a ex:Person;
        ex:age "seventy two".
}  
It is also possible to validate the data in the default (unnamed) graph. Use rdf4j:nil to refer to the default graph.
To validate all the data in your database, e.g. the union of the default graph and all named graphs, you simply need to put your shapes in the rdf4j:SHACLShapeGraph.
RSX - Eclipse RDF4J SHACL Extensions
RDF4J has seen a need to develop its own extension the W3C SHACL Recommendation in order to support new
and innovative features. We always strive to collaborate with the community when developing these features.
RSX currently contains rsx:targetShape which will allow a Shape to be the target for your constraints. This means
that it will be possible to model more complex use-cases like “all norwegian companies with 10 or more employees,
a revenue greater than or equal to 6 million NOK or valued at 23 million or above are required to have a registered
accountant”. This also allows for considerably faster implementations than what is currently possible with SPARQL Targets.
The RSX specification will be published soon together with the limited support for rsx:targetShape in 3.3.0 (sh:or, sh:and, sh:hasValue, dash:hasValueIn, sh:path, sh:property).
Logging and debugging
By default, there is no logging enabled in the ShaclSail. There are four methods for enabling logging:

shaclSail.setLogValidationPlans(true);
shaclSail.setGlobalLogValidationExecution(true);
shaclSail.setLogValidationViolations(true);
shaclSail.setPerformanceLogging(true);

All these will log as INFO.
First step to debugging and understanding an unexpected violation is to enable shaclSail.setLogValidationViolations(true);.
Log validation plans
Validation plans are logged as Graphviz DOT. Validations plans are a form of query plan.
Here is the validation plan for the example above: Link
The structure of this log and its contents may change in the future, without warning.
Log validation execution
The execution of the validation plan shows what data was requested during the execution and how that data was joined together and filtered.
Enabling this logging will enable it for all ShaclSails on all threads.
Enabling this logging will make your validation considerably slower and take up considerably more memory.
Following on from the example above
01. [main] INFO   Select.next():  Tuple{line=[http://example.com/ns#pete, "eighteen"]}
02. [main] INFO    DatatypeFilter;falseNode:  Tuple{line=[http://example.com/ns#pete, "eighteen"]}
03. [main] INFO     DirectTupleFromFilter.next():  Tuple{line=[http://example.com/ns#pete, "eighteen"]}
04. [main] INFO      InnerJoin;discardedRight:  Tuple{line=[http://example.com/ns#pete, "eighteen"]}
05. [main] INFO       BufferedTupleFromFilter.next():  Tuple{line=[http://example.com/ns#pete, "eighteen"]}
06. [main] INFO         ExternalTypeFilterNode.next():  Tuple{line=[http://example.com/ns#pete, "eighteen"]}
07. [main] INFO           UnionNode.next():  Tuple{line=[http://example.com/ns#pete, "eighteen"]}
08. [main] INFO             UnionNode.next():  Tuple{line=[http://example.com/ns#pete, "eighteen"]}
09. [main] INFO              DatatypeFilter;falseNode:  Tuple{line=[http://example.com/ns#pete, "eighteen"]}
10. [main] INFO               DirectTupleFromFilter.next():  Tuple{line=[http://example.com/ns#pete, "eighteen"]}

The log is best read in conjunction with the validation plan. By taking the bottom log line (10.) as the bottom node in the plan and for each indentation following the plan upwards. Multiple lines at a given indentation mean that that node produced multiple tuples.
Line 6 shows a query to the underlying database. Line 1 is the query for everything matching the path (ex:age) against the added data in the transaction.
The indentation is best-effort.
The structure of this log and its contents may change in the future, without warning.
Log validation violations
As the commit() call iterates over the shapes it can log the results (tuples) from the execution of each validation plan.
Following on from the example above:
1. [main] INFO  SHACL not valid. The following experimental debug results were produced:
2.     NodeShape: http://example.com/ns#PersonShape
3.         Tuple{line=[http://example.com/ns#pete, "eighteen"], propertyShapes= DatatypePropertyShape <_:node1d285h2ktx1>} -cause->  [ Tuple{line=[http://example.com/ns#pete, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.com/ns#Person]} ]

Line 2 shows the shape that triggered this violation. Line 3 shows the ultimate tuple produced and which PropertyShape produced the exception followed by a cause listing other tuples that caused the violation. In this case the existing type statement.
The structure of this log and its contents may change in the future, without warning.
Full working example
import ch.qos.logback.classic.Level;
import ch.qos.logback.classic.Logger;
import org.eclipse.rdf4j.common.exception.ValidationException;
import org.eclipse.rdf4j.model.Model;
import org.eclipse.rdf4j.model.vocabulary.RDF4J;
import org.eclipse.rdf4j.repository.RepositoryException;
import org.eclipse.rdf4j.repository.sail.SailRepository;
import org.eclipse.rdf4j.repository.sail.SailRepositoryConnection;
import org.eclipse.rdf4j.rio.RDFFormat;
import org.eclipse.rdf4j.rio.Rio;
import org.eclipse.rdf4j.rio.WriterConfig;
import org.eclipse.rdf4j.rio.helpers.BasicWriterSettings;
import org.eclipse.rdf4j.sail.memory.MemoryStore;
import org.eclipse.rdf4j.sail.shacl.ShaclSail;
import org.eclipse.rdf4j.sail.shacl.results.ValidationReport;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.StringReader;

public class ShaclSampleCode {

    public static void main(String[] args) throws IOException {

        ShaclSail shaclSail = new ShaclSail(new MemoryStore());

//        Logger root = (Logger) LoggerFactory.getLogger(ShaclSail.class.getName());
//        root.setLevel(Level.INFO);
//
//        shaclSail.setPerformanceLogging(true);


        SailRepository sailRepository = new SailRepository(shaclSail);
        sailRepository.init();

        try (SailRepositoryConnection connection = sailRepository.getConnection()) {

            connection.begin();

            StringReader shaclRules = new StringReader(
                String.join("\n", "",
                    "@prefix ex: <http://example.com/ns#> .",
                    "@prefix sh: <http://www.w3.org/ns/shacl#> .",
                    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .",
                    "@prefix foaf: <http://xmlns.com/foaf/0.1/>.",

                    "ex:PersonShape",
                    "  a sh:NodeShape  ;",
                    "  sh:targetClass foaf:Person ;",
                    "  sh:property ex:PersonAgeIntShape, ex:PersonMustHaveAge, ex:PersonCanNotHaveMultipleAge  .",

                    "ex:PersonAgeIntShape ",
                    "  sh:path foaf:age ;",
                    "  sh:message \"A person's age must be a number (xsd:int).\" ;",
                    "  sh:datatype xsd:int .",

                    "ex:PersonMustHaveAge ",
                    "  sh:path foaf:age ;",
                    "  sh:message \"A must have an age.\" ;",
                    "  sh:minCount 1 .",

                    "ex:PersonCanNotHaveMultipleAge ",
                    "  sh:path foaf:age ;",
                    "  sh:message \"A person can only have one age.\" ;",
                    "  sh:maxCount 1 ."

                ));

            connection.add(shaclRules, "", RDFFormat.TURTLE, RDF4J.SHACL_SHAPE_GRAPH);
            connection.commit();

            connection.begin();

            StringReader invalidSampleData = new StringReader(
                String.join("\n", "",
                    "@prefix ex: <http://example.com/ns#> .",
                    "@prefix foaf: <http://xmlns.com/foaf/0.1/>.",
                    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .",

                    "ex:peter a foaf:Person ;",
                    "  foaf:age 20, \"30\"^^xsd:int  ."

                ));

            connection.add(invalidSampleData, "", RDFFormat.TURTLE);
            try {
                connection.commit();
            } catch (RepositoryException exception) {
                Throwable cause = exception.getCause();
                if (cause instanceof ValidationException) {
                    Model validationReportModel = ((ValidationException) cause).validationReportAsModel();

                    WriterConfig writerConfig = new WriterConfig()
                        .set(BasicWriterSettings.INLINE_BLANK_NODES, true)
                        .set(BasicWriterSettings.XSD_STRING_TO_PLAIN_LITERAL, true)
                        .set(BasicWriterSettings.PRETTY_PRINT, true);

                    Rio.write(validationReportModel, System.out, RDFFormat.TURTLE, writerConfig);
                }
                throw exception;
            }
        }
    }
}
Further reading
Here are some useful links to learn more about SHACL:

W3C SHACL specification
Validating RDF Data (various authors)


  

     
      
        
          

  Table of Contents

  
  
    Understanding the RDF4J SHACL Engine
    How to load and update SHACL shapes
    Supported SHACL features
    Validation results
      
        Limiting the validation report
        Retrieving violated shapes
      
    
    Transactional support
    Performance
      
        Automatic bulk validation
      
    
    Reasoning
    Shapes graph
      
        Example
      
    
    RSX - Eclipse RDF4J SHACL Extensions
    Logging and debugging
      
        Log validation plans
        Log validation execution
        Log validation violations
      
    
    Full working example
    Further reading\n\n\n\nFederation With FedX
    

  
  FedX provides transparent federation of multiple SPARQL endpoints under a single virtual endpoint.
As an example, a knowledge graph such as Wikidata can be queried in a federation with endpoints that are linked to Wikidata as an integration hub. In a federated SPARQL query in FedX, one no longer needs to explicitly address specific endpoints using SERVICE clauses. Instead, FedX automatically selects relevant sources, sends statement patterns to these sources for evaluation, and joins the individual results. FedX seamlessly integrates into RDF4J using the Repository API and can be used as a drop-in component in existing applications including the RDF4J Workbench.
Core Features

Virtual Integration of heterogeneous Linked Data sources (e.g. as SPARQL endpoints)
Transparent access to data sources through a federation
Efficient query processing in federated environments
On-demand federation setup at query time
Fast and effective query execution due to new optimization techniques for federated setups
Practical applicability & easy integration as a RDF4J Repository

Getting Started
Below we present examples for getting started in using FedX.
The examples query data from http://dbpedia.org/ and join it with data from https://www.wikidata.org/. It turns out that these endpoints are currently the most reliable ones that are publicly accessible.
Example query
Retrieve the European Union countries from DBpedia and join it with the GDP data coming from Wikidata
SELECT * WHERE {
  ?country a yago:WikicatMemberStatesOfTheEuropeanUnion .
  ?country owl:sameAs ?countrySameAs .
  ?countrySameAs wdt:P2131 ?gdp .
}
Note that the query is a bit artificial, however, it illustrates quite well the powers of federating different data sources.
Using a Java program
The following Java code can be used to execute our example query against the federation.
Repository repository = FedXFactory.newFederation()
	.withSparqlEndpoint("http://dbpedia.org/sparql")
	.withSparqlEndpoint("https://query.wikidata.org/sparql")
	.create();

try (RepositoryConnection conn = repository.getConnection()) {

	String query =
		"PREFIX wd: <http://www.wikidata.org/entity/> "
		+ "PREFIX wdt: <http://www.wikidata.org/prop/direct/> "
		+ "SELECT * WHERE { "
		+ " ?country a <http://dbpedia.org/class/yago/WikicatMemberStatesOfTheEuropeanUnion> ."
		+ " ?country <http://www.w3.org/2002/07/owl#sameAs> ?countrySameAs . "
		+ " ?countrySameAs wdt:P2131 ?gdp ."
		+ "}";

	TupleQuery tq = conn.prepareTupleQuery(query);
	try (TupleQueryResult tqRes = tq.evaluate()) {

		int count = 0;
		while (tqRes.hasNext()) {
			BindingSet b = tqRes.next();
			System.out.println(b);
			count++;
		}

		System.out.println("Results: " + count);
	}
}

repository.shutDown();
The full code is also available as source in the “demos” package of the test source folder.
Instead of defining the federation via code, it is also possible to use data configurations. See the following sections for further examples.
FedX in the RDF4J Workbench
FedX is integrated into the RDF4J server and RDF4J workbench. This allows creation of a managed federation repository and exposing it as a SPARQL repository. In the following provide some details on how this can be achieved.
Using the workbench UI
The RDF4J workbench offers a UI for creating a federation:

Navigate to the workbench UI
Create some repositories (the federation members) and prepare them with data
Create a new repository and select federation as type
Pick the federation members from the list of managed repositories
Save and explore the federation

See RDF4J Workbench Federation for further information.
Advanced federation using a repository config template
Repositories in the workbench can also be created using Repository configuration templates.
Also a FedX federation can be configured using such template and deployed in the RDF4J Server.
The following snippet depicts an example repository configuration that defines a federation over the repositories my-repository-1 and my-repository-2. The actual repositories of the federation members are managed by the RDF4J server.
#
# RDF4J configuration template for a FedX Repository
#
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.
@prefix fedx: <http://rdf4j.org/config/federation#>.

[] a config:Repository ;
   config:rep.impl [
      config:rep.type "fedx:FedXRepository" ;
      fedx:member [
         fedx:store "ResolvableRepository" ;
         fedx:repositoryName "my-repository-1"
      ] ,
      [
         fedx:store "ResolvableRepository" ;
         fedx:repositoryName "my-repository-2"
      ]
   ];
   config:rep.id "my-federation" ;
   rdfs:label "FedX Federation" .
In order to deploy a FedX configuration the repository configuration template needs to be placed in a config.ttl file in the RDF4J application dir. The full location is [Rdf4j_DATA]/server/repositories/[REPOSITORY_ID]/config.ttl, see here for further details.
FedX in Java Applications
FedX is implemented as a RDF4J Repository. To initialize FedX and the underlying federation SAIL, we provide the FedXFactory class, which provides various methods for intuitive configuration. In the following, we present various Java code snippets that illustrate how FedX can be used in an application.
Basically, FedX can be used and accessed using the SAIL architecture (see the RDF4J SAIL documentation for details). The Repository can be obtained from any FedXFactory initialization method. Besides using the Repository interface for creating queries, we also provide a QueryManager class to conveniently create queries. The advantage of the QueryManager over using the RepositoryConnection to create queries, is that preconfigured PREFIX declarations are added automatically to the query, i.e. the user can use common prefixes (such as rdf, foaf, etc.) without the need to specify them in the prologue of the query. See PREFIX Declarations for a detailed documentation.
Example 1: Using a simple SPARQL Federation as a Repository
In the following example, we configure a federation with the publicly available DBpedia and SemanticWebDogFood SPARQL endpoints. Please refer to FedX configuration for details.
Repository repo = FedXFactory.createSparqlFederation(Arrays.asList(
			"http://dbpedia.org/sparql",
			"http://data.semanticweb.org/sparql"));

String q = "PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\n"
	+ "PREFIX dbpedia-owl: &lt;http://dbpedia.org/ontology/&gt;\n"
	+ "SELECT ?President ?Party WHERE {\n"
	+ "?President rdf:type dbpedia-owl:President .\n"
	+ "?President dbpedia-owl:party ?Party . }";

try (RepositoryConnection conn = repo.getConnection()) {
	TupleQuery query = conn.prepareTupleQuery(QueryLanguage.SPARQL, q);
	try (TupleQueryResult res = query.evaluate()) {

		while (res.hasNext()) {
			System.out.println(res.next());
		}
	}
}

repo.shutDown();
System.out.println("Done.");
System.exit(0);
Example 2: Using a data configuration file
In this example we use a data configuration file to set up the federation members (see section on member configuration below for more details). Note that in this example we use an initialized Repository to create the query, as well as the connection.
File dataConfig = new File("local/dataSourceConfig.ttl");
Repository repo = FedXFactory.createFederation(dataConfig);

String q = "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n"
	+ "PREFIX dbpedia-owl: <http://dbpedia.org/ontology/>\n"
	+ "SELECT ?President ?Party WHERE {\n"
	+ "?President rdf:type dbpedia-owl:President .\n"
	+ "?President dbpedia-owl:party ?Party . }";

try (RepositoryConnection conn = repo.getConnection()) {
	TupleQuery query = conn.prepareTupleQuery(QueryLanguage.SPARQL, q);
	try (TupleQueryResult res = query.evaluate()) {

		while (res.hasNext()) {
			System.out.println(res.next());
		}
	}
}

repo.shutDown();
System.out.println("Done.");
System.exit(0);
Example 3: Setting up FedX using the Endpoint utilities
This example shows how to setup FedX using a mechanism to include dynamic endpoints.
List<Endpoint> endpoints = new ArrayList<>();
endpoints.add( EndpointFactory.loadSPARQLEndpoint("dbpedia", "http://dbpedia.org/sparql"));
endpoints.add( EndpointFactory.loadSPARQLEndpoint("swdf", "http://data.semanticweb.org/sparql"));

Repository repo = FedXFactory.createFederation(endpoints);

String q = "PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\n"
	+ "PREFIX dbpedia-owl: &lt;http://dbpedia.org/ontology/&gt;\n"
	+ "SELECT ?President ?Party WHERE {\n"
	+ "?President rdf:type dbpedia-owl:President .\n"
	+ "?President dbpedia-owl:party ?Party . }";

TupleQuery query = QueryManager.prepareTupleQuery(q);
try (TupleQueryResult res = query.evaluate()) {
	while (res.hasNext()) {
		System.out.println(res.next());
	}
}

repo.shutDown();
System.out.println("Done.");
System.exit(0);
Example 4: Using repositories from a remote RDF4J server as federation members
This example shows how repositories from a remote RDF4J server can be easily used as federation members. The main idea is to instruct FedX to use the RemoteRepositoryManager as a resolver for the resolvable endpoints.
Similarly of course a LocalRepositoryManager or any other construct implementing the RepositoryResolver interface can be used.
// connection URL of a RDF4J server which manages the repositories
String serverUrl = "http://localhost:8080/rdf4j-server";
RepositoryManager repoManager = new RemoteRepositoryManager(serverUrl);

// assumes that remote repositories exist
Repository repo = FedXFactory.newFederation()
		.withRepositoryResolver(repoManager)
		.withResolvableEndpoint("my-repository-1")
		.withResolvableEndpoint("my-repository-2")
		.create();

try (RepositoryConnection conn = repo.getConnection()) {
	try (RepositoryResult<Statement> repoResult = conn.getStatements(null, RDF.TYPE, FOAF.PERSON)) {
		repoResult.forEach(st -> System.out.println(st));
	}
}

repo.shutDown();
repoManager.shutDown();
Federation Management
FedX federations can be managed both at initialization and at runtime. This is possible since FedX is capable of on-demand federation setup, meaning that we do not require any prior knowledge about data sources.
The federation can be controlled at runtime using the FederationManager. This class provides all means for interacting with the federation at runtime, e.g. adding or removing federation members.
Endpoints can be added to the federation using the methods addEndpoint(Endpoint) and removed with removeEndpoint(endpoint). Note that new endpoints can be initialized using the endpoint Management facilities.
Endpoint Management
In FedX any federation member is mapped to an Endpoint. The endpoint maintains all relevant information for a particular endpoint, e.g. how triples can be retrieved from the endpoint. Endpoints can be added to the federation at initialization time or at runtime.
In FedX we provide support methods to create Endpoints for SPARQL endpoints, RDF4J NativeStores as well as Resolvable Endpoints (e.g. managed by an RDF4J RepositoryManager). The methods can be used to create endpoints easily.
Example: Using the endpoint Manager to create endpoints
Config.initialize();
List<Endpoint> endpoints = new ArrayList<>();

// initializing a SPARQL endpoint (with explicit name)
endpoints.add( EndpointFactory.loadSPARQLEndpoint("http://dbpedia", "http://dbpedia.org/sparql"));

// another SPARQL endpoint (name is constructed from url)
endpoints.add( EndpointFactory.loadSPARQLEndpoint("http://data.semanticweb.org/sparql"));

// load a RDF4J NativeStore (path either absolute or relative to Config#getBaseDir)
endpoints.add( EndpointFactory.loadNativeEndpoint("http://mystore", "path/to/myNativeStore"));

FedXFactory.initializeFederation(endpoints);
For details about the methods please refer to the javadoc help of the class EndpointFactory
Note: With the Endpoint mechanism it is basically possible to support any kind of Repository of SAIL implementation as federation member. For documentation consider the javadoc, in particular EndpointFactory and  EndpointProvider.
FedX configuration
FedX provides various means for configuration. Configuration settings can be defined using the FedXConfig facility, which can be passed at initialization time. Note that certain settings can also be changed during runtime, please refer to the API documentation for details.
Available Properties



Property
Description




prefixDeclarations
Path to prefix declarations file, see PREFIX Declarations


sourceSelectionCacheSpec
Cache specification for the SourceSelectionMemoryCache, default maximumSize=1000,expireAfterWrite=6h


joinWorkerThreads
The number of join worker threads for parallelization, default 20


unionWorkerThreads
The number of union worker threads for parallelization, default 20


leftJoinWorkerThreads
The number of left join worker threads for parallelization, default 10


boundJoinBlockSize
Block size for bound joins, default 25


enforceMaxQueryTime
Max query time in seconds, 0 to disable, default 30


enableServiceAsBoundJoin
Flag for evaluating a SERVICE expression (contacting non-federation members) using vectored evaluation, default true.


enableOptionalAsBindJoin
Flag for evaluating an OPTIONAL expression using bind join, default true.


includeInferredDefault
whether include inferred statements should be considered, default true


consumingIterationMax
the max number of results to be consumed by ConsumingIteration, default 1000


debugQueryPlan
Print the optimized query execution plan to stdout, default false


enableMonitoring
Flag to enable/disable monitoring features, default false


logQueryPlan
Flag to enable/disable query plan logging via Java class QueryPlanLog, default false


logQueries
Flag to enable/disable query logging via QueryLog, default false. The QueryLog facility allows to log all queries to a file



Overriding via configuration template
The aforementioned properties can also be set using a configuration template, via the fedx:config property, e.g.:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix rep: <http://www.openrdf.org/config/repository#>.
@prefix config: <tag:rdf4j.org,2023:config/>.
@prefix fedx: <http://rdf4j.org/config/federation#>.

[] a rep:Repository ;
 rep:repositoryImpl [
   rep:repositoryType "fedx:FedXRepository" ;
   fedx:member [
      fedx:store "ResolvableRepository" ;
      fedx:repositoryName "endpoint1"
   ],
   [
      fedx:store "ResolvableRepository" ;
      fedx:repositoryName "endpoint2"
   ]
   fedx:config [
      fedx:sourceSelectionCacheSpec "maximumSize=0" ;
      fedx:enforceMaxQueryTime 30 ;
   ]
 ];
 rep:repositoryID "fedx" ;
 rdfs:label "FedX Federation" .
Query timeouts
FedX supports to define the maximum execution time for a query. This can be set on query level Query#setMaxExecutionTimeor globally using the FedX config setting enforceMaxQueryTime.
Note that the query engine attempts to abort any running evaluation of a subquery when the maximum execution time has reached.
If a query timeout occurs, a QueryInterruptedException is thrown.
Prefix declarations
FedX allows to (optionally) define commonly used prefixes (e.g. rdf, foaf, etc.) in a
configuration file. These configured prefixes are then automatically inserted into a query,
meaning that the user does not have to specify full URIs nor the PREFIX declaration in the
query.
The prefixes can be either specified in a configuration file as key-value pairs or directly
configured via Java code (see examples below). When using a configuration file, this can be
configured via the prefixDeclarations property.
Example: Prefix configuration via configuration file
# this file contains a set of prefix declarations
=http://example.org/
foaf=http://xmlns.com/foaf/0.1/
rdf=http://www.w3.org/1999/02/22-rdf-syntax-ns#
dbpedia=http://dbpedia.org/ontology/
Example: Setting prefixes at runtime
The QueryManager can be used to define additional prefixes at runtime.
QueryManager qm = repo.getQueryManager();
qm.addPrefixDeclaration("rdf", "http://www.w3.org/1999/02/22-rdf-syntax-ns#");
qm.addPrefixDeclaration("dbpedia", "http://dbpedia.org/ontology/");
Member configuration
Federation members can be added to a federation either directly as a list of endpoints, or using a data configuration file (see section FedX in Java applications). In a data configuration the federation members are specified using turtle syntax.
Example 1: SPARQL Federation:
@prefix sd: <http://www.w3.org/ns/sparql-service-description#> .
@prefix fedx: <http://rdf4j.org/config/federation#> .

<http://DBpedia> a sd:Service ;
	fedx:store "SPARQLEndpoint";
	sd:endpoint "http://dbpedia.org/sparql";
	fedx:supportsASKQueries false .

<http://SWDF> a sd:Service ;
	fedx:store "SPARQLEndpoint" ;
	sd:endpoint "http://data.semanticweb.org/sparql".

<http://LinkedMDB> a sd:Service ;
	fedx:store "SPARQLEndpoint";
	sd:endpoint "http://data.linkedmdb.org/sparql".
Note: if a SPARQL endpoint does not support ASK queries, the endpoint can be configured to use SELECT queries instead using fedx:supportsASKQueries false. This is for instance useful for Virtuoso based endpoints like
DBpedia. Moreover note that for convenience the public DBpedia endpoint is automatically configured to use SELECT queries.
Example 2: SPARQL Federation with RDF4J remote repositories
@prefix sd: <http://www.w3.org/ns/sparql-service-description#> .
@prefix fedx: <http://rdf4j.org/config/federation#> .

<http://dbpedia> a sd:Service ;
	fedx:store "RemoteRepository";
	fedx:repositoryServer "http://host/rdf4j-server" ;
	fedx:repositoryName "repoName" .
Example 3: Local Federation (NativeStore):
@prefix sd: <http://www.w3.org/ns/sparql-service-description#> .
@prefix fedx: <http://rdf4j.org/config/federation#> .

<http://DBpedia> a sd:Service ;
	fedx:store "NativeStore";
	fedx:repositoryLocation "repositories\\native-storage.dbpedia36".

<http://NYTimes> a sd:Service ;
	fedx:store "NativeStore";
	fedx:repositoryLocation "repositories\\native-storage.nytimes".
Example 4: Federation with resolvable endpoints:
FedX supports to use resolvable endpoints as federation members. These resolvable repositories are not managed by FedX, but are resolved using a provided RepositoryResolver. An example use case is to reference a repository managed by the RDF4J Server (i.e. from within the RDF4J workbench). Alternatively, any custom resolver can be provided to FedX during the initialization using the FedXFactory, e.g. a LocalRepositoryManager.
@prefix sd: <http://www.w3.org/ns/sparql-service-description#> .
@prefix fedx: <http://rdf4j.org/config/federation#> .

<http://myNativeStore> a sd:Service ;
	fedx:store "ResolvableRepository" ;
	fedx:repositoryName "myNativeStore" .
Note that also hybrid combinations are possible.
Example 5: Federation with writable endpoint:
(new in RDF4J 3.2.0)
FedX supports nominating a single federation member as being able to receive updates. If enabled, any statement add/remove operations, including SPARQL updates, will be forwarded on top of the nominated member:
@prefix sd: <http://www.w3.org/ns/sparql-service-description#> .
@prefix fedx: <http://rdf4j.org/config/federation#> .

<http://myNativeStore> a sd:Service ;
	fedx:store "NativeStore";
	fedx:repositoryLocation "repositories\\my-native-store" ;
	fedx:writable true .

<http://DBpedia> a sd:Service ;
	fedx:store "SPARQLEndpoint";
	sd:endpoint "http://dbpedia.org/sparql";
	fedx:supportsASKQueries false .
Notes:

If more than one endpoint is configured to be writable, FedX will select any at random for write operations
Any type of endpoint can be configured to be writable. For production settings it is best practice to use external repositories accessed as ResolvableRepository or SPARQLEndpoint.

Monitoring & Logging
FedX does not rely on a specific logging backend implementation at runtime. To integrate with any logging backends it is possible to use any of the SLF4J adapters.
FedX brings certain facilities to monitor the application state. These facilities are described in the following.
Note: for the following features enableMonitoring must be set in the FedX configuration.
Logging queries
By setting logQueries=true in the FedX configuration, all incoming queries are traced
to a logger with the name QueryLogger. If a corresponding configuration is added to the logging backend, the queries can for instance be traced to a file.
Logging the query plan
There are two ways of seeing the optimized query plan:
a) by setting debugQueryPlan=true, the query plan is printed to stdout (which is handy in the CLI or for debugging).
b) by setting logQueryPlan=true the optimized query plan is written to a variable local to the executing thread.The optimized query plan can be retrieved via the QueryPlanLog service, as illustrated in the following abstract snippet.
FedXConfig config = new FedXConfig().withEnableMonitoring(true).withLogQueryPlan(true);
Repository repo = FedXFactory.newFederation()
		.withSparqlEndpoint("http://dbpedia.org/sparql")
		.withSparqlEndpoint("https://query.wikidata.org/sparql")
		.withConfig(config)
		.create();

TupleQuery query = repo.getConnection().prepareTupleQuery(QueryLanguage.SPARQL, <SOME_QUERY>);
.. evaluate query ..

System.out.println("# Optimized Query Plan:");
System.out.println(QueryPlanLog.getQueryPlan());
Monitoring the number of requests
If monitoring is enabled, the number of requests sent to each individual federation member are monitored. All
available information can be retrieved by the MonitoringService, which can be retrieved via
MonitoringUtil.getMonitoringService()
The following snippet illustrates a monitoring utility that prints all monitoring information to stdout.
FedXConfig config = new FedXConfig().withEnableMonitoring(true).withLogQueryPlan(true);
FedXRepository repo = FedXFactory.newFederation()
		.withSparqlEndpoint("http://dbpedia.org/sparql")
		.withSparqlEndpoint("https://query.wikidata.org/sparql")
		.withConfig(config)
		.create();

TupleQuery query = ...

.. evaluate queries ..

MonitoringUtil.printMonitoringInformation(repo.getFederationContext());


  

     
      
        
          

  Table of Contents

  
  
    Core Features
    Getting Started
      
        Using a Java program
      
    
    FedX in the RDF4J Workbench
      
        Using the workbench UI
        Advanced federation using a repository config template
      
    
    FedX in Java Applications
    Federation Management
    Endpoint Management
    FedX configuration
      
        Available Properties
          
            Overriding via configuration template
          
        
        Query timeouts
        Prefix declarations
      
    
    Member configuration
      
        Example 1: SPARQL Federation:
        Example 2: SPARQL Federation with RDF4J remote repositories
        Example 3: Local Federation (NativeStore):
        Example 4: Federation with resolvable endpoints:
        Example 5: Federation with writable endpoint:
      
    
    Monitoring & Logging
      
        Logging queries
        Logging the query plan
        Monitoring the number of requests\n\n\n\nIntegration With Spring
    

  
  The rdf4j-spring
 module allows for using an RDF4J repository as the data backend of a spring application.
A self-contained demo application can be found at rdf4j-spring-demo

Getting Started
To use RDF as the data backend of a spring application built with maven, use these dependencies:
    <dependency>
        <groupId>org.eclipse.rdf4j</groupId>
        <artifactId>rdf4j-spring</artifactId>
        <version>${rdf4j.version}</version>
    </dependency>
… setting the property rdf4j.version is set to the RDF4J version you want (minimum 4.0.0).
In order for the application to run, a repository has to be configured:
To configure the application to access an existing repository, set the following configuration properties, e.g. in application.properties:
 rdf4j.spring.repository.remote.manager-url=http://localhost:7200
 rdf4j.spring.repository.remote.name=myrepo

 # Optional with username and password
 rdf4j.spring.repository.remote.username=admin
 rdf4j.spring.repository.remote.password=1234
To use an in-memory repository (for example, for unit tests), use
rdf4j.spring.repository.inmemory.enabled=true
Programming with RDF4J-Spring
The main purpose of rdf4j-spring is to support accessing an RDF4J repository using the DAO pattern.
DAOs are subclasses of RDF4JDao
 and use
the RDF4JTemplate
 for accessing
the RDF4J repository configured for the application.
RDF4JTemplate
The RDF4JTemplate is the class used to access a Repository in rdf4j-spring. A bean of this type is configured
at start up and available for wiring into beans. The RDF4JTemplate accesses the Repository through a RepositoryConnection
that it obtains from a RepositoryConnectionFactory
.
This indirection allows for using a connection pool, connect RDF4J to spring’s transaction management, and provide
query logging to a file or exposing query statistics via JMX. These features can be enabled/disabled using
configuration properties (see Configuration)
Wiring into a spring bean
To use the RDF4JTemplate in a bean, define that bean in the spring application’s configuration and wire the RDF4JTemplate in:
@Configuration
@Import(RDF4JConfig.class)
public class MyAppConfig {
	@Bean
	public MyBeanClass getMyBean(@Autowired RDF4JTemplate template){
        return new MyBeanClass(template);
    }    
}
public class MyBeanClass {

	private RDF4JTemplate rdf4JTemplate;
    
    public MyBeanClass(RDF4JTemplate template){
        this.rdf4jTemplate = template;
    }
}
Evaluating queries and executing updates
The RDF4JTemplate offers various ways to access the repository.
For example, to evaluate a TupleQuery using the RDF4JTemplate (in this case, counting all triples):
int count = rdf4JTemplate
				.tupleQuery("SELECT (count(?a) as ?cnt) WHERE { ?a ?b ?c }")
				.evaluateAndConvert()
				.toSingleton(bs -> TypeMappingUtils.toInt(QueryResultUtils.getValue(bs, "cnt")));
The query, provided through the tupleQuery method, is executed with the call to evaluateAndConvert(), which returns
a TupleQueryResultConverter
.
The latter provides methods for converting the TupleQueryResult of the query into an object, an Optional, a Map,
Set, List, or Stream. In the example, we are just interested in the count as an int - one single object - so we use
the toSingleton() method and convert the value of the projection variable to an int. The conversion is done
using TypeMappingUtils
; the
extraction of the variable’s value from the BindingSet bs is done using
QueryResultUtils
.
Pre-binding variables
For binding variables before executing a query or update, use the OperationBuilder

returned by the tupleQuery(), graphQuery, or update methods. It provides various withBinding() methods following
the builder pattern, allowing for binding variables, as illustrated in the following example.
Set<IRI> artists = rdf4JTemplate
                    .tupleQuery("PREFIX ex: <http://example.org/>"
				            + "SELECT distinct ?artist "
				            + "WHERE { ?artist a ?type }")
				    .withBinding("type", EX.Artist)
				    .evaluateAndConvert()
				    .toSet(bs -> QueryResultUtils.getIRI(bs, "artist"));
Using the RepositoryConnection directly
For using the RepostoryConnection
 directly,
without the need to generate a result, the consumeConnection() method is used:
rdf4JTemplate.consumeConnection(con -> con.remove(EX.Picasso, RDF.TYPE, EX.Artist);
Alternatively, to generate a result, the applyToConnection() method is used:
boolean isPresent = rdf4JTemplate.applyToConnection(
                        con -> con.hasStatement(EX.Picasso, RDF.TYPE, EX.Artist, true);
Using SPARQL queries/updates from external files
For running queries or updates from external resources, the [(tupleQuery|graphQuery|update)FromResource] methods can be used.
For example, the sparql/construct-artists.rq file on the classpath might contain this query:
PREFIX ex: <http://example.org/>
CONSTRUCT {?artist ?p ?o } WHERE { ?artist a ex:Artist; ?p ?o }
and could be evaluated using
Model model = rdf4JTemplate.graphQueryFromResource(
                        getClass(), 
                        "classpath:sparql/construct-artists.rq")
				        .evaluateAndConvert()
				        .toModel();
The resource to be read is resolved by spring’s ResourceLoader,
which supports fully qualified URLs (e.g., file:// URLs, relative paths and classpath: pseudo-URLs.)
Implementing a DAO
Any spring bean that uses the RDF4JTemplate can be seen as a DAO and participates in transactionality, query logging,
caching, etc. However, rdf4j-spring provides a few base classes that provide frequently used functionality.
RDF4JDao
RDF4JDao
 is a suitable base class for a general-purpose DAO. It provides
two functionalities to subclasses:


The RDF4JTemplate is automatically wired into the bean and it is available through getRDF4JTemplate()


It provides a simple management facility for SPARQL query/update strings. This allows for SPARQL queries being
generated only once (by String concatenation, read from a file, or built with the SparqlBuilder).
The queries are prepared in the template method prepareNamedSparqlSuppliers():


In the following example, we

create a DAO, extending RDF4JDao
annotate it with @Component
so it gets auto-detected during Spring’s component scan
create an inner class, QUERY_KEYS, as a container for String constants we use for query keys
implement the prepareNamedSparqlSuppliers
 method and add one query
use the prepared query in a DAO method (getArtistsWithoutPaintings()). We access the prepared query with getNamedTupleQuery(String)
, passing the constant we defined in QUERY_KEYS.


@Component // make the DAO a spring component so it's auto-detected in the classpath scan
public class ArtistDao extends RDF4JDao {
    
    // constructor, other methods etc

    // recommended: encapsulate the keys for queries in an object
    // so it's easier to find them when you need them
	static abstract class QUERY_KEYS {
		public static final String ARTISTS_WITHOUT_PAINTINGS = "artists-without-paintings";
	}

    // prepare the named queries, assigning each one of the keys
	@Override
	protected NamedSparqlSupplierPreparer prepareNamedSparqlSuppliers(NamedSparqlSupplierPreparer preparer) {
		return preparer
            .forKey(QUERY_KEYS.ARTISTS_WITHOUT_PAINTINGS)
            .supplySparql(Queries.SELECT(
                            ARTIST_ID)
                            .where(
                                ARTIST_ID.isA(iri(EX.Artist))
                                .and(ARTIST_ID.has(iri(EX.creatorOf), Painting.PAINTING_ID).optional())
                                .filter(not(bound(Painting.PAINTING_ID)))).getQueryString()
            );
	}

    // use the named query with getNamedTupleQuery(String)
	public Set<Artist> getArtistsWithoutPaintings(){
		return getNamedTupleQuery(QUERY_KEYS.ARTISTS_WITHOUT_PAINTINGS)
						.evaluateAndConvert()
						.toStream()
						.map(bs -> QueryResultUtils.getIRI(bs, ARTIST_ID))
						.map(iri -> getById(iri))
						.collect(Collectors.toSet());
	}
    
    // ...

}
SimpleRDF4JCRUDDao
The SimpleRDF4JCRUDDao
 is a suitable base class for a DAO for
creating, reading, updating, and deleting one class of entities. It requires two type parameters, ENTITY and ID.
It provides create, read, update, and delete functionality for the ENTITY class, using the ID class wherever the
entity’s identifier is required.
Subclasses of SimpleRDF4JCRUDDao must implement a couple of template methods in order to customize the generic
behaviour for the specific entity and id classes.
In the following, we use the entity Artist
 (as used in the demo
application) as an example. Note that we define public constants of type Variable
,
one corresponding to each of the entity’s fields.
public class Artist {
    // recommended pattern: use a public Variable constant for each of the entities fields 
    // for use in queries and result processing. 
	public static final Variable ARTIST_ID = SparqlBuilder.var("artist_id"); 
	public static final Variable ARTIST_FIRST_NAME = SparqlBuilder.var("artist_firstName");
	public static final Variable ARTIST_LAST_NAME = SparqlBuilder.var("artist_lastName");
	private IRI id;
	private String firstName;
	private String lastName;
    // getter, setter, constructor, ...
    // be sure to implement equals() and hashCode() for proper behaviour of collections!
}
The ArtistDao
 is shown in the following code snippets.
We recommend to use @Component for auto-detection. Implementing the constructor is required.
@Component // again, make it a component (see above) 
public class ArtistDao extends SimpleRDF4JCRUDDao<Artist, IRI> {

	public ArtistDao(RDF4JTemplate rdf4JTemplate) {
		super(rdf4JTemplate);
	}
The populateIdBindings method is called by the superclass to bind the id to variable(s) in a SPARQL query.
	@Override
	protected void populateIdBindings(MutableBindings bindingsBuilder, IRI iri) {
		bindingsBuilder.add(ARTIST_ID, iri);
	}
The populateBindingsForUpdate method is called by the superclass to bind all non-id variables when performing an update.
	@Override
	protected void populateBindingsForUpdate(MutableBindings bindingsBuilder, Artist artist) {
		bindingsBuilder
				.add(ARTIST_FIRST_NAME, artist.getFirstName())
				.add(ARTIST_LAST_NAME, artist.getLastName());
	}
The mapSolution method converts a query solution, i.e., a BindingSet
, to an instance of the entity.
	@Override
	protected Artist mapSolution(BindingSet querySolution) {
		Artist artist = new Artist();
		artist.setId(QueryResultUtils.getIRI(querySolution, ARTIST_ID));
		artist.setFirstName(QueryResultUtils.getString(querySolution, ARTIST_FIRST_NAME));
		artist.setLastName(QueryResultUtils.getString(querySolution, ARTIST_LAST_NAME));
		return artist;
	}
The getReadQuery method provides the SPARQL string used to read one entity. Note that the variable names must be the
same ones used in mapSolution(BindingSet). It may be cleaner to use the SparqlBuilder for generating this string.
	@Override
	protected String getReadQuery() {
		return "prefix foaf: <http://xmlns.com/foaf/0.1/> "
				+ "prefix ex: <http://example.org/> "
				+ "SELECT ?artist_id ?artist_firstName ?artist_lastName where {"
				+ "?artist_id a ex:Artist; "
				+ "    foaf:firstName ?artist_firstName; "
				+ "    foaf:surname ?artist_lastName ."
				+ " } ";
	}
The getInsertSparql(ENTITY) method provides the SPARQL string for inserting a new instance. This SPARQL operation
will also be used for updates. If updates require a different operation from inserts, it must be provided by implementing
getUpdateSparql(ENTITY).
	@Override
	protected NamedSparqlSupplier getInsertSparql(Artist artist) {
		return NamedSparqlSupplier.of("insert", () -> Queries.INSERT(ARTIST_ID.isA(iri(EX.Artist))
				.andHas(iri(FOAF.FIRST_NAME), ARTIST_FIRST_NAME)
				.andHas(iri(FOAF.SURNAME), ARTIST_LAST_NAME))
				.getQueryString());
	}
The getInputId(ENTITY) method is used to generate the id of an entity to be inserted. Here, we use the id of the
specified artist object; if it is null we generate a new IRI using getRdf4JTemplate().getNewUUID().
	@Override
	protected IRI getInputId(Artist artist) {
		if (artist.getId() == null) {
			return getRdf4JTemplate().getNewUUID();
		}
		return artist.getId();
	}
}
Composite Keys
If the entity uses a composite key, a class implementing CompositeKey

must be used for the ID type parameter. For a key consisting of two components, the CompositeKey2

class is available. If more components are needed, the key class can be modeled after that one.
RelationMapBuilder
It is not uncommon for an application to read a relation present in the repository data into a Map. For example, we
might want to group painting ids by artist id. The RelationMapBuilder

provides the necesary functionality for such cases:
RelationMapBuilder b = new RelationMapBuilder(getRDF4JTemplate(), EX.creatorOf);
Map<IRI, Set<IRI>> paintingsByArtists = b.buildOneToMany();
Additional Functionality:

The constraints(GraphPattern) method restricts the relation
The relationIsOptional() method allows for the object to be missing, in which case an empty set is generated for the subject key.
The useRelationObjectAsKey() method flips the map such that the objects of the relation are used as keys and the subjects are aggregated.
The buildOneToOne() method returns a one to one mapping, which dies horribly if the data is not 1:1

RDF4JCRUDDao
The RDF4JCRUDDao
 is essentially the same as the SimpleRDF4JCRUDDao,
with the one difference that it has three type parameters, ENTITY, INPUT, and ID. The class thus allows different
classes for input and output: creation and updates use INPUT, e.g. save(INPUT), reading methods use ENTITY, e.g.
ENTITY getById(ID).
Service Layer
Usually, the functionality offered by DAOs is rather narrow, e.g. CRUD methods for one entity class. They
are combined to provide a wider range of functionality in the servcie layer. The only thing one
needs to know when implementing the service layer with rdf4j-spring DAOs is that its methods need to participate
in spring’s transaction management. The most straightforward way to do this is to use the @Transactional
method annotation, causing the service object to be wrapped with a proxy that takes care of transactionality.
The following code snippet, taken from the demo’s ArtService
 class,
shows part of a simple service.
@Component
public class ArtService {
	
    @Autowired
	private ArtistDao artistDao;

	@Autowired
	private PaintingDao paintingDao;

	@Transactional
	public Artist createArtist(String firstName, String lastName) {
		Artist artist = new Artist();
		artist.setFirstName(firstName);
		artist.setLastName(lastName);
		return artistDao.save(artist);
	}

	@Transactional
	public Painting createPainting(String title, String technique, IRI artist) {
		Painting painting = new Painting();
		painting.setTitle(title);
		painting.setTechnique(technique);
		painting.setArtistId(artist);
		return paintingDao.save(painting);
	}

	@Transactional
	public List<Painting> getPaintings() {
		return paintingDao.list();
	}

	@Transactional
	public List<Artist> getArtists() {
		return artistDao.list();
	}

	@Transactional
	public Set<Artist> getArtistsWithoutPaintings(){
		return artistDao.getArtistsWithoutPaintings();
	}
    
    // ...

}
Testing with Junit 5
Testing an application built with rdf4j-spring can be done at the DAO layer as well as on the service layer. Generally,
applications will have more than one test classes.
The common approach is to have a configuration for tests that is shared by all tests, and this configuration prepares
the spring context with all the required facilities. A minimal, shared test configuration is the following. Note that
it imports RDF4JTestConfig
:
@TestConfiguration
@EnableTransactionManagement
@Import(RDF4JTestConfig.class)
@ComponentScan("com.example.myapp.dao")
public class TestConfig {

    @Bean
    DataInserter getDataInserter() {
        return new DataInserter();
    }
    
}
With this configuration, a test class can use the dataInserter bean to insert data into an inmemory repository before
each test:
@ExtendWith(SpringExtension.class)
@Transactional
@ContextConfiguration(classes = { TestConfig.class })
@TestPropertySource("classpath:application.properties")
@TestPropertySource(
		properties = {
				"rdf4j.spring.repository.inmemory.enabled=true",
				"rdf4j.spring.repository.inmemory.use-shacl-sail=true",
				"rdf4j.spring.tx.enabled=true",
				"rdf4j.spring.resultcache.enabled=false",
				"rdf4j.spring.operationcache.enabled=false",
				"rdf4j.spring.pool.enabled=true",
				"rdf4j.spring.pool.max-connections=2"
		})
@DirtiesContext
public class ArtistDaoTests {

    @Autowired
    private ArtistDao artistDao;       

	@BeforeAll
	public static void insertTestData(
			@Autowired DataInserter dataInserter,
			@Value("classpath:/data/my-testdata.ttl") Resource dataFile) {
		dataInserter.insertData(dataFile);
	}
    
    @Test
    public void testReadArtist(){
         // ...          
    }      

}
Testing against a local database
The inmemory repository is likely to behave differently from any database used in production in some edge cases. It
is recommended to test against a local installation of the database that is used in production in addition to testing
against the inmemory repository.
With rdf4j-spring this is quite straightforward:

install the database locally and create a repository for the tests
provide a property file on the classpath with the necessary properties to connect to that repository
(rdf4j.spring.repository.remote.* properties)
Create a subclass of your test class and provide the properties file through the @TestPropertySource annotation
Use the @Tag annotation, so you can easily switch the test on or off using the configuration of your test environment
(most likely the Maven Surefire Plugin), as the local database installation will not be present in many build environments.

Example:
@Tag("requires-local-database")
@TestPropertySource("classpath:/repository-localdb.properties")
public class ArtistDaoDbTests extends ArtistDaoTests {
// no code needed, the class is just created to run your ArtistDaoTests with a different configuration     
}
Debugging
In addition to query logging, if you need to get a close look at what’s happening inside the rdf4j-spring code,
set the loglevel for org.eclipse.rdf4j.spring to DEBUG. Sometimes it may be required to look into what spring is doing.
In this case, set org.springframework to DEBUG or even TRACE.
One way to do this is to provide a logback.xml file on the classpath, as can be found in the source at rdf4j-spring/src/test/resources/logback.xml.
Another way to set the loglevel is to provide an application property starting with logging.level., e.g.
logging.level.org.eclipse.rdf4j.spring=DEBUG
which can be provided in an application.properties (for details and other ways to do that, have a look at the documentation on Externalied Configuration in Spring).
Configuration
rdf4j-spring makes use of the auto-configuration feature in Spring (configured in the source file rdf4j-spring/META-INF/spring.factories).
That means that bean creation at start up is governed by configuration properties, all of which are prefixed by rdf4j.spring.
The following table shows all subsystems with their property prefixes, the packages they reside in, and the class holding their properties.



Subsystem
property prefix
package (links to reference)
Properties class




Repository
rdf4j.spring.repository.
org.eclipse.rdf4j.spring.repository

RemoteRepositoryProperties
 and InMemoryRepositoryProperties



Transaction management
rdf4j.spring.tx.
org.eclipse.rdf4j.spring.tx

TxProperties



Connection Pooling
rdf4j.spring.pool.
org.eclipse.rdf4j.spring.pool

PoolProperties



Operation caching
rdf4j.spring.operationcache.
org.eclipse.rdf4j.spring.operationcache

OperationCacheProperties



Operation logging
rdf4j.spring.operationlog.
org.eclipse.rdf4j.spring.operationlog

OperationLogProperties
 and OperationLogJmxProperties



Query result caching
rdf4j.spring.resultcache.
org.eclipse.rdf4j.spring.resultcache

ResultCacheProperties



UUIDSource
rdf4j.spring.uuidsource.
org.eclipse.rdf4j.spring.uuidsource

SimpleUUIDSourceProperties
, NoveltyCheckingUUIDSourceProperties
, UUIDSequenceProperties
, and PredictableUUIDSourceProperties




These subsystems and their configuration are described in more detail below.
Repository
As stated in the Getting Started section, to configure the application
to access an existing repository, set the following configuration properties, e.g. in application.properties:
 rdf4j.spring.repository.remote.manager-url=[manager-url]
 rdf4j.spring.repository.remote.name=[name]
To use an in-memory repository (for example, for unit tests), use
rdf4j.spring.repository.inmemory.enabled=true
Transaction management
By default, rdf4j-spring connects with Spring’s PlatformTransactionManager. To disable this connection, use
rdf4j.spring.tx.enabled=false
Connection Pooling
Creating a RepositoryConnection has a certain overhead that many applications wish to avoid. rdf4j-spring allows for
pooling of such connections. Several configuration options, such as the maximum number of connections, are available
(see PoolProperties
).
To enable, use
rdf4j.spring.pool.enabled=true
Operation caching
SPARQL operations (queries and updates) require some computation time to prepare from the SPARQL string they are based on.
In rdf4j-spring, this process is hidden from clients and happens in the RDF4JTemplate. By default, operations
are not cached, and the same operation executed multiple times always has the overhead of parsing the SPARQL string and
generating the operation. If this feature is enabled, operations are cached per connection.
Note: If connection pooling is enabled, it is possible that operations created in different threads will use different connections and will therefore
all generate their own instance of the SPARQL operation, thus reducing the speedup incurred by operation caching.
To enable, use
rdf4j.spring.operationcache.enabled=true
Operation logging
(aka Query logging)
Two options are available for logging operations (queries and updates) sent to the repository:
Operation logging via SLF4J
Each operation is written to the logger org.eclipse.rdf4j.spring.operationlog.log.slf4 with loglevel DEBUG.
To enable, use
rdf4j.spring.operationlog.enabled=true
Operation logging via JMX
Each operation is recorded (if identical operations are executed, statistics are aggregated) and exposed via JMX.
To enable, use
rdf4j.spring.operationlog.jmx.enabled=true
Query result caching
Applications that frequently execute the same queries might profit from result caching. If enabled, query results are
cached on a per-connection basis. By default, this cache is cleared at the end of the ongoing transaction. The performance
impact of result caching is application-specific and is not unlikely to be negative. Measure carefully!
However, if the application is the only one using the repository, and therefore no updates are possible that the
application does not know about, the property rdf4j.spring.resultcache.assumeNoOtherRepositoryClients=true can be set.
In this case, results are copied to a global cache that all connections have access to, and which is only cleared when
the application executes an update.
To enable result caching, use
rdf4j.spring.resultcache.enabled=true
UUIDSource
Using UUIDs as identifiers for entities is a common strategy for applications using an RDF store as their backend. Doing
this requires a source of new, previously unused UUIDs for new entities created by the application. Conversely, in
unit tests, it is sometimes required that the UUIDs are generated in a predictable manner, so that actual results
can be compared with expected results containing generated UUIDs.
The UUIDSource subsystem provides different implementations of the UUIDSource

interface. The configuration of this subsystem determines which implementation is wired into the RDF4JTemplate at
start up and gets used by the application.
In our opinion, the default implementation, DefaultUUIDSource

is sufficient for generating previously unused UUIDs. Collisions are possible but sufficiently unlikely, so using
any one of noveltychecking, sequence, and simple subsystems should not be necessary.
For using the predictable UUIDSource, which always produces the same sequence of UUIDs, use
rdf4j.spring.uuidsource.predictable.enabled=true
Acknowledgments
The RDF4J-Spring module, the RDF4J-Spring-Demo, and this documentation have been developed in the project
‘BIM-Interoperables Merkmalservice’, funded by the Austrian Research Promotion Agency and Österreichische Bautechnik
Veranstaltungs GmbH.

  

     
      
        
          

  Table of Contents

  
  
    Getting Started
    Programming with RDF4J-Spring
      
        RDF4JTemplate
          
            Wiring into a spring bean
            Evaluating queries and executing updates
            Pre-binding variables
            Using the RepositoryConnection directly
            Using SPARQL queries/updates from external files
          
        
        Implementing a DAO
          
            RDF4JDao
            SimpleRDF4JCRUDDao
            RDF4JCRUDDao
          
        
        Service Layer
      
    
    Testing with Junit 5
      
        Testing against a local database
      
    
    Debugging
    Configuration
      
        Repository
        Transaction management
        Connection Pooling
        Operation caching
        Operation logging
          
            Operation logging via SLF4J
            Operation logging via JMX
          
        
        Query result caching
        UUIDSource
      
    
    Acknowledgments\n\n\n\nGeoSPARQL
    

  
  RDF4J offers an extended algebra for partial GeoSPARQL support. When enabled, this offers additional geospatial functionality as part of the SPARQL engine, on top of any RDF4J repository, using the well-known Spatial4J and JTS libraries for geospatial reasoning.
To enable GeoSPARQL support, all you need to do is include the rdf4j-queryalgebra-geosparql Maven module in your project:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-queryalgebra-geosparql</artifactId>
</dependency>

Adding geospatial data to the Repository
By default, RDF4J only supports GeoSPARQL functions on top of geospatial data represented as so-called Well-Known Text (WKT) strings.
For example, to model the coordinates of landmarks like the Eiffel Tower in Paris or the Tower Bridge in London, you could do something like this:
@prefix geo: <http://www.opengis.net/ont/geosparql#> .
@prefix sf: <http://www.opengis.net/ont/sf> .
@prefix ex: <http://example.org/> .

ex:eiffelTower a ex:Landmark ;
          geo:hasGeometry ex:coordinates-et.
ex:coordinates-et a sf:Point;
        geo:asWKT "POINT(2.2945 48.8584)"^^geo:wktLiteral .
ex:towerBridge a ex:Landmark ;
          geo:hasGeometry ex:coordinates-tb.
ex:coordinates-tb a sf:Point;
        geo:asWKT "POINT(-0.0754 51.5055)"^^geo:wktLiteral .

After adding this data to a repository, we can use a GeoSPARQL query to get, for example, the distance between the two landmarks, like so:
PREFIX geo: <http://www.opengis.net/ont/geosparql#>
PREFIX geof: <http://www.opengis.net/def/function/geosparql/>
PREFIX uom: <http://www.opengis.net/def/uom/OGC/1.0/>
PREFIX ex: <http://example.org/>
SELECT *
WHERE {
  ?lmA a ex:Landmark ;
       geo:hasGeometry [ geo:asWKT ?coord1 ].
  ?lmB a ex:Landmark ;
       geo:hasGeometry [ geo:asWKT ?coord2 ].
  BIND((geof:distance(?coord1, ?coord2, uom:metre)/1000) as ?dist) .
  FILTER (str(?lmA) < str(?lmB))
}

Supported GeoSPARQL functions
In this section we briefly enumerate the GeoSPARQL extension functions supported by RDF4J. For more information on the precise meaning and use of these functions we refer you to to the GeoSPARQL specification.
Non-topological and common query functions
RDF4J supports the following non-topological geospatial functions as defined in the GeoSPARQL specs: geof:distance, geof:boundary, geof:buffer, geof:convexHull, geof:difference, geof:envelope, geof:intersection, geof:getSRID, geof:symDifference, geof:union, and geof:relate.
Simple Feature functions
RDF4J supports the following Simple Feature (sf) functions: geof:sfEquals, geof:sfDisjoint, geof:sfIntersects, geof:sfTouches, geof:sfCrosses, geof:sfWithin, geof:sfContains, and geof:sfOverlaps.
Egenhofer functions
RDF4J supports the following Egenhofer (eh) functions: geof:ehEquals, geof:ehDisjoint, geof:ehMeet, geof:ehOverlap, geof:ehCovers, geof:ehCoveredBy , geof:ehInside, and geof:ehContains.
RCC8 functions
RDF4J supports the following RCC8 functions: geof:rcc8eq, geof:rcc8dc, geof:rcc8ec, geof:rcc8po, geof:rcc8tppi, geof:rcc8tpp, geof:rcc8ntpp, and geof:rcc8ntppi.
Improved performance through Lucene
Although RDF4J supports GeoSPARQL querying on any type of store, the Lucene SAIL and its derivates (the SolrSail and the ElasticSearchSail) have built-in optimizations that make geospatial querying on large datasets more efficient. By default, the Lucene SAIL only spatially indexes http://www.opengis.net/ont/geosparql#asWKT fields. This can be changed using the LuceneSail.WKT_FIELDS parameter.
Reading and writing WKT Literals
RDF4J 2.4 uses Spatial4J 0.7, which has full support for converting between Shape objects and WKT strings. See the Spatial4J Format documentation for more details.
Further reading
Here are some useful links:

Spatial4J website
OGC GeoSPARQL specification
Wikipedia article on WKT


  

     
      
        
          

  Table of Contents

  
  
    Adding geospatial data to the Repository
    Supported GeoSPARQL functions
      
        Non-topological and common query functions
        Simple Feature functions
        Egenhofer functions
        RCC8 functions
        Improved performance through Lucene
      
    
    Reading and writing WKT Literals
    Further reading\n\n\n\nRDF-Star and SPARQL-Star
    

  
  RDF4J has (experimental) support for RDF-star and SPARQL-star.
RDF-star and its companion SPARQL-star are proposed extensions to the RDF and
SPARQL standards (see RDF-star W3C Community
Group) to provide a more convenient way to
annotate RDF statements and to query such annotations. In essence, RDF-star
attempts to bridge the gap between the RDF world and the Property Graph world.
RDF4J support for these extensions currently covers:

reading and writing RDF-star data in a variety of syntax formats (including Turtle-star and TriG-star)
converting between an RDF-star Model using annotations and a regular RDF Model (translating the annotations to regular RDF reification)
persisting RDF-star data in the Memory Memory Store and querying with SPARQL-star, regular SPARQL and / or API calls
adding extension hooks for third-party triplestores that implement the SAIL API to allow persistence and querying of RDF-star annotations

Note: these features are currently in the experimental/beta stage. While we’ll do our best to not make breaking changes in future releases unless necessary, we make no guarantees.
The RDF-star data model in RDF4J
To support RDF-star annotations, the core RDF model in RDF4J has been extended with a new type of Resource: the Triple
 (not to be confused with the pre-existing Statement
, which is the representation of a “regular” RDF statement).
You can create Triple objects using a ValueFactory or through the static Values factory methods, and then use them as the subject (or object) of another statement, for example:
IRI bob = Values.iri("http://example.org/bob");
Triple bobsAge = Values.triple(bob, FOAF.AGE, Values.literal(23));

IRI certainty = Values.iri("http://example.org/certainty");
Statement aboutBobsAge = Statements.statement(bobsAge, certainty, Values.literal(0.9), null);
The Statements
 and Values
   utility classes offers several functions to easily transform Statements into Triples, vice versa:
IRI bob = Values.iri("http://example.org/bob");
Triple bobsAge = Values.triple(bob, FOAF.AGE, valueFactory.createLiteral(23));

Statement ageStatement = Statements.toStatement(bobsAge);
Triple backToTriple = Values.triple(ageStatement);
Reading and writing RDF-star data
RDF4J currently provides several Rio parser/writers for RDF-star enabled syntax formats: the Turtle-star format, and the TriG-star format. As the names suggest, both are extended versions of existing RDF format (Turtle and TriG, respectively). In addition, RDF4J’s binary RDF format parser has also been extended to be able to read and write RDF-star data.
Reading / writing a Turtle-star file
A Turtle-star file that contains an annotation with a certainty score, on a statement saying “Bob’s age is 23”, would look like this:
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix ex: <http://example.org/> .

<<ex:bob foaf:age 23>> ex:certainty 0.9 .
If we wish to read this data into an RDF4J Model object, we can do so using the Rio Turtle-star parser:
Model model = Rio.parse(new FileInputStream("/path/to/file.ttls"), "", RDFFormat.TURTLESTAR);
Similarly, Rio can be used to write RDF-star models to file:
Rio.write(model, new FileOutputStream("/path/to/file.ttls"), "", RDFFormat.TURTLESTAR);
Storing and retrieving RDF-star in a Repository
Note: not every store can handle RDF-star data. Attempting to upload an RDF-star model directly to a Repository that does not support it will result in errors.
The RDF4J MemoryStore accepts RDF-star data. You can add the RDF-star model we created above directly to an in-memory Repository:
try(RepositoryConnection conn = repo.getConnection()) {
    conn.add(model);
}
You can query this data via the Repository API, like any “normal” RDF data. For example:
try(RepositoryConnection conn = repo.getConnection()) {
   RepositoryResult<Statement> result = conn.getStatements(null, null, null);
   result.forEach(System.out::println);
}
will output:
<<<http://example.org/bob> <http://xmlns.com/foaf/0.1/age> 23>> <http://example.org/certainty> 0.9
and of course the subject triple can be inspected in code as well:
try(RepositoryConnection conn = repo.getConnection()) {
   RepositoryResult<Statement> result = conn.getStatements(null, null, null);
   Statement st = result.next();
   Triple rdfStarTriple = (Triple)st.getSubject();
   System.out.println(rdfStarTriple.getSubject()); // will output http://example.org/bob
}
SPARQL query results containing RDF-star
When querying a store that contains RDF-star data, even regular SPARQL queries may include RDF-star triples as a result. To make it possible to serialize such query results (e.g. for network transfer), the SPARQL/JSON, SPARQL/XML, Binary and TSV query result formats have all been extended to handle RDF-star triples as possible value bindings.
Extended SPARQL JSON format
The default SPARQL 1.1 Query Results JSON format has been extended as in the following example:
{
  "head" : {
    "vars" : [
      "a",
      "b",
      "c"
    ]
  },
  "results" : {
    "bindings": [
      { "a" : {
          "type" : "triple",
          "value" : {
            "subject" : {
              "type" : "uri",
              "value" : "http://example.org/bob"
            },
            "predicate" : {
              "type" : "uri",
              "value" : "http://xmlns.com/foaf/0.1/age"
            },
            "object" : {
              "datatype" : "http://www.w3.org/2001/XMLSchema#integer",
              "type" : "literal",
              "value" : "23"
            }
          }
        },
        "b": {
          "type": "uri",
          "value": "http://example.org/certainty"
        },
        "c" : {
          "datatype" : "http://www.w3.org/2001/XMLSchema#decimal",
          "type" : "literal",
          "value" : "0.9"
        }
      }
    ]
  }
}
RDF4J introduces a new, custom content type application/x-sparqlstar-results+json to specifically request this extended content. By default, if a client requests the standard content type (application/sparql-results+json or application/json), the response serializer will instead “compress” the RDF-star triple into a single IRI, using Base64 encoding:
{
  "head" : {
    "vars" : [
      "a",
      "b",
      "c"
    ]
  },
  "results" : {
    "bindings": [
      { "a" : {
          "type" : "uri",
          "value" : "urn:rdf4j:triple:PDw8aHR0cDovL2V4YW1wbGUub3JnL2JvYj4gPGh0dHA6Ly94bWxucy5jb20vZm9hZi8wLjEvYWdlPiAiMjMiXl48aHR0cDovL3d3dy53My5vcmcvMjAwMS9YTUxTY2hlbWEjaW50ZWdlcj4-Pg=="
        },
        "b": {
          "type": "uri",
          "value": "http://example.org/certainty"
        },
        "c" : {
          "datatype" : "http://www.w3.org/2001/XMLSchema#decimal",
          "type" : "literal",
          "value" : "0.9"
        }
      }
    ]
  }
}
This ensures that by default, an RDF4J-based endpoint will always use standards-compliant serialization to avoid breaking clients that have not yet been updated.
It is possible to override this behavior by explicitly configuring the writer to not encode RDF-star triples. Setting the BasicWriterSetting ENCODE_RDF_STAR
 to false will ensure that even when requesting the standard content type, the serializer will use the extended syntax.
Extended SPARQL XML format
The default SPARQL Query Results XML format has been extended as in the following example:
<?xml version='1.0' encoding='UTF-8'?>
<sparql xmlns='http://www.w3.org/2005/sparql-results#'>
  <head>
    <variable name='a'/>
    <variable name='b'/>
    <variable name='c'/>
  </head>
  <results>
    <result>
      <binding name='a'>
        <triple>
          <subject>
            <uri>http://example.org/bob</uri>
          </subject>
          <predicate>
            <uri>http://xmlns.com/foaf/0.1/age</uri>
          </predicate>
          <object>
            <literal datatype='http://www.w3.org/2001/XMLSchema#integer'>23</literal>
          </object>
        </triple>
      </binding>
      <binding name='b'>
        <uri>http://example.org/certainty</uri>
      </binding>
      <binding name='c'>
        <literal datatype='http://www.w3.org/2001/XMLSchema#decimal'>0.9</literal>
      </binding>
    </result>
  </results>
</sparql>
As with the extended SPARQL Results JSON format, this format is sent by default only when specifically requesting a custom content type: application/x-sparqlstar-results+xml. And as with the JSON format, the XML writer can be reconfigured to also send the extended format on requesting the regular SPARQL/XML content type, by setting the BasicWriterSetting ENCODE_RDF_STAR
 to false.
Extended TSV format
The SPARQL 1.1 Query Results TSV format has been extended as follows:
?a	?b	?c
<<<http://example.org/bob> <http://xmlns.com/foaf/0.1/age> 23>>	<http://example.org/certainty>	0.9
SPARQL-star queries
The SPARQL engine in RDF4J has been extended to allow for SPARQL-star queries. Executing a SPARQL-star query relies on the underlying store supporting RDF-star data storage.
SPARQL-star allows accessing the RDF-star triple patterns directly in the query. For example, after you have uploaded the above simple RDF-star model to a MemoryStore, you can execute a query like this:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
PREFIX ex: <http://example.org/>
SELECT ?p ?a ?c WHERE {
   <<?p foaf:age ?a>> ex:certainty ?c .
}
The result will be:
?p      ?a     ?c
ex:bob  23     0.9

Converting RDF-star to regular RDF and back
RDF4J offers functions to convert between RDF-star data and regular RDF. In the converted regular RDF, the RDF-star triple is replaced with a reified statement using the RDF Reification vocabulary. For example:
<<ex:bob foaf:age 23>> ex:certainty 0.9 .
becomes:
_:node1 a rdf:Statement;
        rdf:subject ex:bob ;
        rdf:predicate foaf:age ;
        rdf:object 23 ;
        ex:certainty 0.9 .
You can find the the conversion functions in the Models
 utility class. There are several variants, but the simplest form just takes a Model containing RDF-star data and creates a new Model containing the same data, but with all RDF-star annotation converted to reified statements:
Model rdfStarModel; // model containing RDF-star annotations
Model convertedModel = Models.convertRDFStarToReification(rdfStarModel);
Likewise, you can convert back:
Model reificiationModel; // model containing RDF reification statements
Model rdfStarModel = Models.convertReificiationtoRDFStar(reificiationModel);
Note: since the RDF-star functionality is currently in experimental stage, it is possible that the names or precise signatures of these functions will be changed in a future release.

  

     
      
        
          

  Table of Contents

  
  
    The RDF-star data model in RDF4J
    Reading and writing RDF-star data
      
        Reading / writing a Turtle-star file
      
    
    Storing and retrieving RDF-star in a Repository
      
        SPARQL query results containing RDF-star
          
            Extended SPARQL JSON format
            Extended SPARQL XML format
            Extended TSV format
          
        
        SPARQL-star queries
      
    
    Converting RDF-star to regular RDF and back\n\n\n\nTools
    

  
  
    
        
          
          RDF4J ConsoleThe RDF4J Console is a text console application for interacting with RDF4J. It can be used to create and use local RDF databases, or to connect to a running RDF4J Server.
          
          RDF4J Server and WorkbenchIn this chapter, we explain how you can install RDF4J Server (the actual database server and SPARQL endpoint service) and RDF4J Workbench (a web-based client UI for managing databases and executing queries).
          
          Application directory configurationIn this section we explain how to configure the application directory of the various RDF4J tools.
          
          Repository configuration templatesIn RDF4J Server, repository configurations with all their parameters are modeled in RDF. The syntax is documented in more detail in Repository and SAIL Configuration. In order to create a new repository, the Console needs to create such an RDF document and submit it to the repository manager. The Console uses configuration templates to accomplish this.
          
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nRDF4J Console
    

  
  The RDF4J Console is a text console application for interacting with RDF4J. It can be used to create and use local RDF databases, or to connect to a running RDF4J Server.
Getting started
Rdf4j Console can be started using the console.bat/.sh scripts in the bin directory of the Rdf4j SDK. By default, the console will connect to the “default data directory”, which contains the console’s own set of repositories.
The console is operated by typing commands. For example, to get an overview of the available commands, type:
help
To get help for a specific command, type ‘help’ followed by the command name, e.g.:
help connect
History
The Console has a built-in history, use the Up and Down arrows to cycle through the history of commands.
By default all commands will be saved to the history.txt file in the Console’s application directory, and this file will be loaded when the Console is started again.
To prevent newly entered commands from being saved to file, set the savehistory setting to false:
set savehistory=false
To re-enable, simply set savehistory to true.
Connecting to a set of repositories
As indicated in the previous section, the Console connects to its own set of repositories by default. Using the connect command you can make the console connect to a Rdf4j Server or to a set of repositories on your file system. For example, to connect to a Rdf4j Server that is listening to port 8080 on localhost, enter the following command:
connect http://localhost:8080/rdf4j-server

To connect to the default set of repositories, enter:
connect default

When connecting to a remote server, a user name and password can be provided as well:
connect http://example.rdf4j.org/rdfj-server myname mypassword

Not surprisingly, the disconnect command disconnects the console from the set of repository.
Showing the list of repositories
To get an overview of the repositories that are available in the set that your console is connected to, use the show command:
 show repositories

Creating a new repository
The create command creates a new repository in the set the console is connected to. This command expects the name of a template describing the repository’s configuration. Several templates are available, including:

memory — a memory based RDF repository
memory-rdfs — a main-memory repository with RDF Schema inferencing
memory-rdfs-dt — a main-memory repository with RDF Schema and direct type hierarchy inferencing
native — a repository that uses on-disk data structure
native-rdfs — a native repository with RDF Schema inferencing
native-rdfs-dt — a native repository with RDF Schema and direct type hierarchy inferencing
remote — a repository that serves as a proxy for a repository on a Rdf4j Server
sparql — a repository that serves as a proxy for a SPARQL endpoint

When the create command is executed, the console will ask you to fill in a number of parameters for the type of repository that you chose. For example, to create a native repository, you execute the following command:
 create native

The console will ask you to provide an ID and title for the repository, as well as the triple indexes that need to be created for this kind of store. The values between square brackets indicate default values which you can select by simply hitting enter. The output of this dialogue looks something like this:
 Please specify values for the following variables:
 Repository ID [native]: myRepo
 Repository title [Native store]: My repository
 Triple indexes [spoc,posc]:
 Repository created

Opening and closing a repository
The open command opens a specific repository. For example, to open the myrepo repository, enter:
open myrepo

The close command closes the connection.
Verifying a file
The verify command verifies the validity of an RDF file. Several formats (serializations) are supported, including JSON-LD, Turtle, N-Triples and RDF/XML. The console will select the format based upon the extension of the file name. For example, to verify a JSON-LD file:
verify data.jsonld

On a MS-Windows system, forward slashes or double backward slashes are to be used when specifying the file path, for example:
verify C:\\data\\rdf\\data.jsonld

or:
verify C:/data/rdf/data.jsonld

Validating the file against a set of shapes and constraints in a SHACL file, and storing the validation report to a file, is equally straightforward:
verify data.jsonld shacl-file.ttl validation-report.ttl
Loading a file into a repository
The load command loads a file into the opened repository.  Several formats (serializations) are supported, including JSON-LD, Turtle, N-Triples and RDF/XML. The console will select the format based upon the extension of the file name.
load import.nt

Specifying a base IRI for resolving relative IRIs:
load import.nt from http://example.org

Exporting a repository to a file
The export command exports statements from a repository to a file. Either the entire repository can be exported, or a (list of) named graphs / contexts.
export export.nt

Executing a SPARQL query
The sparql command executes a sparql query.
sparql

Multiple lines can be entered. To terminate the input, enter a new line containing only a single dot .
select ?s ?p ?o
where { ?s ?p ?o }
.

###= reading queries from and exporting results to a file
Queries can be read from an existing file:
sparql infile="file.qr"

Results can be saved to an output file. The file type extension is used to determine the output format, but the exact list of available file formats depends on the type of the query.
Graph queries (construct) can be saved as JSON-LD, RDF/XML, N-Triples or Turtle, by using the respective extensions .jsonld, .xml, .nt or .ttl.
Tuple queries (select) can be saved as SPARQL Results CSV, TSV, JSON or XML, by using the respective extensions .csv, .tsv, .srj or .srx.
For example:
sparql outfile="result.srj" select ?s where { ?s ?p ?o }

Or:
sparql outfile="result.nt" construct { ?s ?p ?o } where { ?s ?p ?o }

Combining input file for reading a query and an output for writing the result is also possible:
sparql infile="query.txt" outfile="result.tsv"

When relative paths are used, files are read from or saved to the working directory, which can be changed using the following command:
set workdir=/path/to/working/dir
Setting namespace prefixes
Using prefixes for namespaces (e.g. dcterms: instead of http://purl.org/dc/terms/) makes queries and results easier to read, and queries less error-prone to write.
By default a few well-known prefixes are available, including dcterms, foaf, rdfs and skos.
For a complete list, see:
set prefixes

Adding and clearing a namespace prefix is quite straightforward:
set prefixes=ex http://example.com
set prefixes=ex <none>

Enter the following command to remove all namespace prefixes:
set prefixes=<none>

Going back to the built-in list of well-know prefixes is easy, even when the list of prefixes was cleared:
set prefixes=<default>

In addition, it is possible to toggle between using / showing the short prefix or using / showing the full namespace URI, without actually changing the prefixes:
set queryprefix=true
set showprefix=true

Other commands
Please check the documentation that is provided by the console itself for help on how to use the other commands. Most commands should be self explanatory.

  

     
      
        
          

  Table of Contents

  
  
    Getting started
      
        History
        Connecting to a set of repositories
        Showing the list of repositories
        Creating a new repository
        Opening and closing a repository
        Verifying a file
        Loading a file into a repository
        Exporting a repository to a file
        Executing a SPARQL query
        Setting namespace prefixes
        Other commands\n\n\n\nRDF4J Server and Workbench
    

  
  In this chapter, we explain how you can install RDF4J Server (the actual database server and SPARQL endpoint service) and RDF4J Workbench (a web-based client UI for managing databases and executing queries).
Required software
RDF4J Server and RDF4J Workbench requires the following software:

Java 11 or newer
A Java Servlet Container that supports Java Servlet API 3.1 and Java Server Pages (JSP) 2.2, or newer.

We recommend using a recent, stable version of Apache Tomcat (version 9.0) or Jetty (version 9.4)
Deploying Server and Workbench
RDF4J Server is a database management application: it provides HTTP access to RDF4J repositories, exposing them as SPARQL endpoints. RDF4J Server is meant to be accessed by other applications. Apart from some functionality to view the server’s log messages, it doesn’t provide any user oriented functionality. Instead, the user oriented functionality is part of RDF4J Workbench. The Workbench provides a web interface for querying, updating and exploring the repositories of an RDF4J Server.
If you have not done so already, you will first need to download the RDF4J SDK. Both RDF4J Server and RDF4J Workbench can be found in the war directory of the SDK. The war-files in this directory need to be deployed in a Java Servlet Container. The deployment process is container-specific, please consult the documentation for your container on how to deploy a web application. For Apache Tomcat, we recommend using the Tomcat Manager to make deployment easier.
For Jetty, it’s just a matter of copying the war-files to $JETTY_BASE\webapps
After you have deployed the RDF4J Workbench webapp, you should be able to access it, by default, at path http://localhost:8080/rdf4j-workbench. You can point your browser at this location to verify that the deployment succeeded.
Configuring RDF4J Workbench for UTF-8 Support
UTF-8 in the Request URI (GET)
There is a known issue affecting the proper exploring of resources that use an extended character set. Workbench client-side code generates URI’s assuming an ISO-8859-1 character encoding, and often Tomcat comes pre-configured to expect UTF-8 encoded URI’s. It will be necessary to change the HTTP Connector configuration, or to add a separate HTTP Connector that uses ISO-8859-1. For details, see the Tomcat 8.5 or Tomcat 9 documentation.
UTF-8 in the Request Body (POST)
To resolve issues where the request body is not getting properly interpreted as UTF-8, it is necessary to configure Tomcat to use its built-in SetCharacterEncodingFilter. More details are available at the Tomcat wiki. Un-commenting the  and  elements for setCharacterEncodingFilter in $CATALINA_BASE/conf/web.xml, and restarting the server, should be the only necessary steps.
Application directory configuration
The RDF4J Server and Workbench store configuration files and repository data in a single directory (with subdirectories). On Windows machines, this directory is %APPDATA%\RDF4J\ by default, where %APPDATA% is the application data directory of the user that runs the application. For example, in case the application runs under the ‘LocalService’ user account on Windows XP, the directory is C:\Documents and Settings\LocalService\Application Data\RDF4J\. On Linux/UNIX, the default location is $HOME/.RDF4J/, for example /home/tomcat/.rdf4j/. We will refer to this data directory as [RDF4J_DATA] in the rest of this manual.
The location of this data directory can be reconfigured using the Java system property org.eclipse.rdf4j.appdata.basedir. When you are using Tomcat as the servlet container then you can set this property using the JAVA_OPTS parameter, for example:
set JAVA_OPTS=-Dorg.eclipse.rdf4j.appdata.basedir=\path\to\other\dir\ (on Windows)
export JAVA_OPTS='-Dorg.eclipse.rdf4j.appdata.basedir=/path/to/other/dir/' (on Linux/UNIX)

One easy way to find out what the directory is in a running instance of the RDF4J Server, is to go to http://localhost:8080/rdf4j-server/home/overview.view in your browser and click on ‘System’ in the navigation menu on the left. The data directory will be listed as one of the configuration settings of the current server.
Tomcat as a Windows service
If you are using Apache Tomcat as a Windows Service you should use the Windows Services configuration tool to set this property. Other users can either edit the Tomcat startup script or set the property some other way.
Tomcat as a systemd service
On Linux systems, systemd may impose additional restrictions on the location of the data directory. If the server seems to have started successfully, but the [RDF4J-DATA] directory remains empty - i.e. no server nor workbench subdirectories are being created - the tomcat service configuration needs to be adapted.
For example: Tomcat 9 on Debian with /var/rdf4j/ as the data directory requires an additional ReadWritePaths in the /etc/systemd/system/tomcat9.service file,
followed by a systemctl daemon-reload and systemctl restart tomcat9.service to apply this change.
 [Service]

ReadWritePaths=/var/rdf4j/
Repository Configuration
Each repository in RDF4J Server stores both its configuration and the actual persisted data in the application dir. The location is [RDF4J_DATA]/server/repositories/[REPOSITORY_ID]. The configuration is stored as a file config.ttl in that directory. The other files in this directory represent stored data, indexes and other files that the database needs to persist its data (in other words: best don’t touch).
The easiest way to create and manage repositories on an RDF4J Server to use the RDF4J Console or RDF4J Workbench. Both offer commands to quickly create a new repository and guide you through the various configuration options.
However, you can also directly edit the config.ttl of your repository to change its configuration. For example, you can use this to change the repository name as it is shown in the Workbench, or perhaps to change configuration parameters, or change the repository type. However, proceed with caution: if you make a mistake, your repository may become unreadable until after you’ve rectified the mistake. Also note that if you change the actual store type (e.g. switching from a memory store to a native store), it won’t migrate your existing data to the new store configuration!
The different types of repository are:

SPARQLRepository
HTTPRepository
SailRepository
DatasetRepository
ContextAwareRepository

More information can be found in the Repository configuration and templates section of the documentation.
Logging Configuration
Both RDF4J Server and RDF4J Workbench use the Logback logging framework. In its default configuration, all RDF4J Server log messages are sent to the log file [RDF4J_DATA]/Server/logs/main.log (and log messages for the Workbench to the same file in [RDF4J_DATA]/Workbench ).
The default log level is INFO, indicating that only important status messages, warnings and errors are logged. The log level and -behaviour can be adjusted by modifying the [RDF4J_DATA]/Server/conf/logback.xml file. This file will be generated when the server is first run. Please consult the logback manual for configuration instructions.
Access Rights and Security
It is possible to set up your RDF4J Server to authenticate named users and restrict their permissions.  RDF4J Server is a servlet-based Web application deployed to any standard servlet container (for the remainder of this section it is assumed that Tomcat is being used).
The RDF4J Server exposes its functionality using a REST API that is an extension of the SPARQL protocol for RDF. This protocol defines exactly what operations can be achieved using specific URL patterns and HTTP methods (GET, POST, PUT, DELETE). Each combination of URL pattern and HTTP method can be associated with a set of user roles, thus giving very fine-grained control.
In general, read operations are effected using GET and write operations using PUT, POST and DELETE. The exception to this is that POST is allowed for SPARQL queries. This is for practical reasons, because some HTTP servers have limits on the length of the parameter values for GET requests.
Security constraints and roles
The association between operations and security roles is specified using security constraints in RDF4J Server’s deployment descriptor - a file called web.xml that can be found in the .../webapps/rdf4j-server/WEB-INF directory. web.xml becomes available immediately after the installation without any security roles defined.
Warning: When redeployed, the web.xml file gets overwritten with the default version. Therefore, if you change it, make sure you create a backup. In particular, do not edit web.xml while Tomcat is running.
The deployment descriptor defines:

authentication mechanism/configuration;
security constraints in terms of operations (URL pattern plus HTTP method);
security roles associated with security constraints.

To enable authentication, add the following XML element to web.xml inside the <web-app> element:
    <login-config>
        <auth-method>BASIC</auth-method>
        <realm-name>rdf4j</realm-name>
    </login-config>
Security constraints associate operations (URL pattern plus HTTP method) with security roles. Both security constraints and security roles are nested in the <web-app> element.
A security constraint minimally consists of a collection of web resources (defined in terms of URL patterns and HTTP methods) and an authorisation constraint (the role name that has access to the resource collection). Some example security constraints are shown below:
<security-constraint>
    <web-resource-collection>
        <web-resource-name>SPARQL query access to the 'test' repository</web-resource-name>
        <url-pattern>/repositories/test</url-pattern>
        <http-method>GET</http-method>
        <http-method>POST</http-method>
    </web-resource-collection>
    <auth-constraint>
        <role-name>viewer</role-name>
        <role-name>editor</role-name>
    </auth-constraint>
</security-constraint>

<security-constraint>
    <web-resource-collection>
        <web-resource-name>
        Read access to 'test' repository's namespaces, size, contexts, etc
        </web-resource-name>
        <url-pattern>/repositories/test/*</url-pattern>
        <http-method>GET</http-method>
</web-resource-collection>
    <auth-constraint>
        <role-name>viewer</role-name>
        <role-name>editor</role-name>
    </auth-constraint>
</security-constraint>

<security-constraint>
    <web-resource-collection>
        <web-resource-name>Write access</web-resource-name>
        <url-pattern>/repositories/test/*</url-pattern>
        <http-method>POST</http-method>
        <http-method>PUT</http-method>
        <http-method>DELETE</http-method>
    </web-resource-collection>
    <auth-constraint>
        <role-name>editor</role-name>
    </auth-constraint>
</security-constraint>
The ability to create and delete repositories requires access to the SYSTEM repository. An administrator security constraint for this looks like the following:
<security-constraint>
    <web-resource-collection>
        <web-resource-name>Administrator access to SYSTEM</web-resource-name>
        <url-pattern>/repositories/SYSTEM/</url-pattern>
        <url-pattern>/repositories/SYSTEM/*/</url-pattern>
        <http-method>GET</http-method>
        <http-method>POST</http-method>
        <http-method>PUT</http-method>
        <http-method>DELETE</http-method>
    </web-resource-collection>
    <auth-constraint>
        <role-name>administrator</role-name>
    </auth-constraint>
</security-constraint>
Also nested inside the <web-app> element are definitions of security roles. The format is shown by the example:
<security-role>
    <description>
        Read only access to repository data
    </description>
    <role-name>viewer</role-name>
</security-role>

<security-role>
    <description>
        Read/write access to repository data
    </description>
    <role-name>editor</role-name>
</security-role>

<security-role>
    <description>
        Full control over the repository, as well as creating/deleting repositories
    </description>
    <role-name>administrator</role-name>
</security-role>
User accounts
Tomcat has a number of ways to manage user accounts. The different techniques are called ‘realms’ and the default one is called ‘UserDatabaseRealm’. This is the simplest one to manage, but also the least secure, because usernames and passwords are stored in plain text.
For the default security realm, usernames and passwords are stored in the file tomcat-users.xml in the Tomcat configuration directory, usually /etc/tomcat/tomcat-users.xml on Linux systems. To add user accounts, add <user> elements inside the <tomcat-users> element, for example:
<user username="adam" password="secret" roles="viewer" />
<user username="eve" password="password" roles="viewer,editor,administrator" />
Programmatic authentication
To use a remote repository where authentication has been enabled, it is necessary to provide the username and password to the RDF4J API. Remote repositories are usually accessed via the RemoteRepositoryManager
 class. Tell the repository manager what the security credentials are using the following method:
void setUsernameAndPassword(String username, String password)
Alternatively, they can be passed in the factory method:
static RemoteRepositoryManager getInstance(String serverURL, String username, String password)
RDF4J Workbench
This chapter describes the RDF4J Workbench, a web application for interacting with RDF4J and/or other SPARQL endpoints.
This chapter will refer to URLs on a local server served from port 8080, which is possibly the most common “out-of-the-box” configuration. That is, Workbench URLs will start with http://localhost:8080/.
Getting Started
To start using Workbench for the first time, point your browser to http://localhost:8080/rdf4j-workbench. Your browser will be automatically redirected to http://localhost:8080/rdf4j-workbench/repositories/NONE/repositories. This page will display all repositories in the default server, as indicated by the “default-server” property in WEB-INF/web.xml. Normally this is set to /rdf4j-server. That is, the default server for Workbench is usually the RDF4J Server instance at the path /rdf4j-server on the same web server. To view information about the RDF4J Server instance, click on “RDF4J Server” at the top of the side menu.
Setting the Server, Repository and User Credentials
A “current selection” section sits at the top right in the Workbench, informing you of the URL of the server you are using, the repository you are currently using, and the user name used when accessing the server. Each of these items can be changed by clicking the “change” link immediately to the right of them. Since the Workbench is generally used for prototyping and exploration, “user” is commonly set to “none”. In this case, the Workbench is connecting to the RDF4J Server without authenticating, and below we refer to the user in this mode as the anonymous user.
Setting the Server and User Credentials
There are two ways to reach the “Change Server” page, which allows you to enter a URL for the server and, optionally, user credentials:

Clicking on “change” for either the server or the user.
Clicking on “RDF4J Server” on the sidebar menu.

A full URL is expected in the “Change Server” field. You may enter a file:/// URL to access a local repository on the Workbench server, but need to be sure that the Workbench server process has permission to access the given folder.
Important Security Consideration
Workbench stores user name and password credentials in plain-text cookies in the browser. You will need to configure your Workbench server for HTTPS to properly
protect these credentials. See the Tomcat 8.5 or Tomcat 9 SSL/TLS documentation for more information.
Setting the Repository
There are two ways to change the current repsository:

Clicking on “change” for the current repository in the “current selection” section.
Clicking on “Repositories” in the sidebar menu.

You will be presented with a table listing of all the repositories available on the current server, with the following columns:

Readable
Writable
Id
Description
Location

“Location” is the URL of the repository, useful for accessing it via the RDF4J REST API. “Id” is presented as a clickable hyperlink that will open that repository in the Workbench, bringing the user to a summary page for the repository.
Creating a Repository
Click on “New repository” in the sidebar menu. This brings up the “New Repository” page. You are presented with a simple form that provides a “Type:” selector with several repository types:

1-4 In Memory Stores: Simple, RDF Schema, RDF Schema and Direct Type Inferencing, or Custom Graph Query Inference
5-8 Native Stores: Simple, RDF Schema, RDF Schema and Direct Type Inferencing, or Custom Graph Query Inference
11 Remote RDF Store:	References a RDF4J repository external to the present server.
12 SPARQL Endpoint Proxy: References a SPARQL Endpoint (See SPARQL 1.1 Protocol).
1| Federation Store: Presents other stores referenced on the present server as a single federation store for querying purposes.

The “ID:” and “Title:” fields are optional in this form. Clicking “Next” brings up a form with more fields specific to the repository type selected. On that form, it will be necessary to enter something in the “ID:” field before the “Create” button may be clicked. If creation is successful, the new repository is also opened and its “Summary” page is presented.
Modifying the Data Contents of a Repository
Data may be added to or removed from current repository using any of the sidebar menu items under “Modify”. After all successful operations, the user is presented with the repository “Summary” page.
Add
The “Add” page allows you to specify a URL with RDF data, a local file on on your client system, or to enter serialized RDF data into its text area for loading into the present repository. It is also possible to specify the Base URI and a Context for the triples. Think of the Context as a 4th element of each RDF statement, specifying a graph within the repository. You may specify one of eight serialization formats, or select “auto-detect” to let the server do a best guess at the format.
Remove
The “Remove Statements” page presents you with a form where you may enter values for subject, predicate, object or context. Clicking on “Remove” then removes all statements from the repository which match the given values. Leaving an item blank means that any value will match. If all values are left blank, clicking “Remove” will not do anything except present a warning message.
Clear
The “Clear Repository” page is powerful. Leaving the lone “Context:” field blank and clicking “Clear Context(s)” will remove all statements from all graphs in the repository. It is also possible to enter a resource value corresponding to a context that exists in the repository, and the statements for that graph only will be removed.
SPARQL Update
The “Execute SPARQL Update on Repository” page gives a text area where you enter a SPARQL 1.1 Update command. SPARQL Update is an extension to the SPARQL query language that provides full CRUD (Create Read Update Delete) capabilities. For more information see the W3C Recommendation for SPARQL 1.1 Update. Clicking “Execute” executes the specified SPARQL Update operation.
Exploring a Repository
Summary Page
Click on “Summary” on the sidebar menu. A simple summary is displayed with the repository’s id, description, URL for remote access and the associated server’s URL for remote access. Many operations when repositories are created and updated display this page afterwards.
Namespaces Page
Namespace-prefix pairings can be defined within a repository, so that URIs can be displayed in shorthand form as a qualified name. To edit them, click on “Namespaces” on the sidebar menu. A page is displayed with a table of all presently defined pairs. Existing namespaces may be edited by selecting them in the drop-down list, which populates the text fields. The text fields may then be edited, and the “Update” button will make the change on the repository. The “Delete” button will remove whichever pair has been selected.
Contexts Page
“Context” is the RDF4J construct for implementing RDF Named Graphs, which allow a repository to group data into separately addressable graphs. The Explore page always displays the context (always a URI or blank node) with each triple, the combination of which is often referred to as a quad.
To view all the contexts for the present repository, click on “Contexts” on the sidebar menu. Each context is clickable, bringing you to the “Explore” page for that context value.
Types Page
Click on “Types” on the sidebar menu. A list of types is displayed. These types are the resulting output from this SPARQL query:
SELECT DISTINCT ?type WHERE { ?subj a ?type }
Explore Page
Click on “Explore” in the sidebar menu. You are presented with an “Explore” page. Type a resource value into the empty “Resource” field, and hit Enter. You will be presented with a table listing all triples where your given resource is a part of the statement, or is the context (graph) name. Currently allowable resource values are:

URI’s enclosed in angle brackets, e.g., <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
Qualified Names (qnames), e.g. rdf:type, where the prefix “rdf” is associated with the namespace “http://www.w3.org/1999/02/22-rdf-syntax-ns#” in the repository.
Literal values with an explicit datatype or language tag, e.g., “dog”@en or “hund”@de or “1”^^xsd:integer or “9.99”^^<http://www.w3.org/2001/XMLSchema#decimal>

Data types expressed with qnames also need to have their namespace defined in the repository.
By using the “Results per page” setting and the “Previous …” and “Next …” buttons, you may page through a long set of results, or display all of the results at once. There is also a “Show data types & language tags” checkbox which, when un-checked, allows a less verbose table view.
Querying a Repository
Clicking on “Query” on the sidebar menu brings you to Workbench’s querying interface. Here, you may enter queries in the SPARQL language, save them for future access, and execute them against your repository.
If you have executed queries previously, the query text area will show the most recently executed query. If not, it will be pre-populated with a prefix header containing all the defined namespaces for the repository. The “Clear” button below the text area gives you the option to restore this pre-populated state for the currently selected query language.
The two other action buttons are “Save Query” and “Execute”:

“Save Query” is only enabled when a name has been entered into the adjacent text field. Once clicked, your query is saved under the given name. An option to back out or overwrite is given if the name already exists. Saved queries are associated with the current repository and user name. If the “Save privately (do not share)” option is checked, then the saved query will only be visible to the current user.
“Execute” attempts to execute the given query text, and then you are presented with a query results page. Values are clickable, and clicking on a value brings you to its “Explore” page. Similar display options are presented as the “Explore” page, as well.

Working with Saved Queries
Clicking “Saved queries” on the sidebar menu brings you to the Workbench’s interface for working with previously saved queries. All saved queries accessible to the current user are listed in alphabetical order by

the user that saved them, then
the query name

The query name is displayed as a clickable link that will execute the query, followed by 3 buttons:

Show:	Toggles the display of the query metadata and query text. When the “Save Queries” page loads, this information is not showing to conserve screen real estate.
Edit: 	Brings you to the query entry page, pre-populated with the query text.
Delete...: Deletes the saved query, with a confirmation dialog provided for safety. Users may only delete their own queries or queries that were saved anonymously.

The query metadata fields, aside from query name and user, are:

Query Language: SPARQL
Include Inferred Statements: whether to use any inferencing defined on the repository to expand the result set
Rows per page: How many results to display per page at first
Shared: whether this query is visible to users other than the one that saved it, restricted to always be true for the “anonymous” user

Note that it is only possible to save queries as the present user. If you edit another user’s query and save it with the same query name, a new saved query will be created associated with your user name.
Viewing all Triples and Exporting the Data
The “Export” link on the sidebar menu is convenient for bringing up a paged view of all quads in your triple store. As with other result views, resources are displayed as clickable links that bring you to that resource’s “Explore” page. In addition, it is possible to select from a number of serialization formats to download the entire contents of the triple store in:

TriG
BinaryRDF
TriX
N-Triples
N-Quads
N3
RDF/XML
RDF/JSON
Turtle

SHACL
NOTE: new in RDF4J 3.0
Workbench supports two SHACL wrapped stores.

memory-shacl
native-shacl

Loading shapes
Shapes need to be loaded into the following context:
<http://rdf4j.org/schema/rdf4j#SHACLShapeGraph>


This context is a hidden context that is only available through the following commands:

Add
Remove
Clear

To update your shapes you should first clear the shapes context and then add your shapes again.
Retrieving shapes
There is no functionality in the user interface to retrieve the loaded shapes. Instead the loaded
shapes can be retrieved through the REST interface.
Assuming you are running the server and workbench locally, and the repository ID you
want to use is 1, you can use the following URL to download your shapes as RDF-XML:
http://localhost:8080/rdf4j-server/repositories/1/statements?context=%3Chttp%3A%2F%2Frdf4j.org%2Fschema%2Frdf4j%23SHACLShapeGraph%3E
Validation
All transactions are validated before being committed. A validation error when uploading data in
the workbench looks like this:

Your data will only be committed if it passes validation.
Supported features and more info
For a list of supported features and more info on how to use SHACL - see Programming with SHACL.
Federation
NOTE: since RDF4J 3.1
RDF4J integrates federation support using the FedX engine.
A federation can be configured from the RDF4J Workbench UI by picking the federation members. Once configured it can be used to explore or query the data of the federation members as a virtually integrated graph.

See Federation with FedX for more information.

  

     
      
        
          

  Table of Contents

  
  
    Required software
    Deploying Server and Workbench
      
        Configuring RDF4J Workbench for UTF-8 Support
          
            UTF-8 in the Request URI (GET)
            UTF-8 in the Request Body (POST)
          
        
        Application directory configuration
          
            Tomcat as a Windows service
            Tomcat as a systemd service
          
        
        Repository Configuration
        Logging Configuration
      
    
    Access Rights and Security
      
        Security constraints and roles
        User accounts
        Programmatic authentication
      
    
    RDF4J Workbench
    Getting Started
      
        Setting the Server, Repository and User Credentials
        Setting the Server and User Credentials
        Important Security Consideration
        Setting the Repository
        Creating a Repository
        Modifying the Data Contents of a Repository
        Add
          
            Remove
            Clear
            SPARQL Update
          
        
        Exploring a Repository
          
            Summary Page
            Namespaces Page
            Contexts Page
            Types Page
            Explore Page
          
        
        Querying a Repository
        Working with Saved Queries
        Viewing all Triples and Exporting the Data
      
    
    SHACL
      
        Loading shapes
        Retrieving shapes
        Validation
        Supported features and more info
      
    
    Federation\n\n\n\nApplication Directory Configuration
    

  
  In this section we explain how to configure the application directory of the various RDF4J tools.
RDF4J Server, Workbench and Console all store configuration files and repository data in a single directory (with subdirectories). On Windows machines, this directory is %APPDATA%\Rdf4j\ by default, where %APPDATA% is the application data directory of the user that runs the application. For example, in case the application runs under the ‘LocalService’ user account on Windows XP, the directory is C:\Documents and Settings\LocalService\Application Data\Rdf4j\. On Linux/UNIX, the default location is $HOME/.Rdf4j/, for example /home/tomcat/.rdf4j/. We will refer to this data directory as [RDF4J_DATA] in the rest of this manual.
The location of this data directory can be reconfigured using the Java system property org.eclipse.rdf4j.appdata.basedir. When you are using Tomcat as the servlet container then you can set this property using the JAVA_OPTS parameter, for example:
set JAVA_OPTS=-Dorg.eclipse.rdf4j.appdata.basedir=\path\to\other\dir\ (on Windows)
export JAVA_OPTS='-Dorg.eclipse.rdf4j.appdata.basedir=/path/to/other/dir/' (on Linux/UNIX)

If you are using Apache Tomcat as a Windows Service you should use the Windows Services configuration tool to set this property. Other users can either edit the Tomcat startup script or set the property some other way.
One easy way to find out what the directory is in a running instance of the Rdf4j Server, is to go to http://localhost:8080/rdf4j-server/home/overview.view in your browser and click on ‘System’ in the navigation menu on the left. The data directory will be listed as one of the configuration settings of the current server.

  

     
      
        
          

  Table of Contents\n\n\n\nRepository Configuration Templates
    

  
  In RDF4J Server, repository configurations with all their parameters are modeled in RDF. The syntax is documented in more detail in Repository and SAIL Configuration. In order to create a new repository, the Console needs to create such an RDF document and submit it to the repository manager. The Console uses configuration templates to accomplish this.
Repository configuration templates are simple Turtle RDF files that describe a repository configuration, where some of the parameters are replaced with variables. The Console parses these templates and asks the user to supply values for the variables. The variables are then substituted with the specified values, which produces the required configuration data.
The RDF4J Console comes with a number of default templates. The Console tries to resolve the parameter specified with the ‘create’ command (e.g. “memory”) to a template file with the same name (e.g. “memory.ttl”). The default templates are included in Console library, but the Console also looks in the templates subdirectory of [Rdf4j_DATA]. You can define your own templates by placing template files in this directory.
To create your own templates, it’s easiest to start with an existing template and modify that to your needs. The default “memory.ttl” template looks like this:
#
# RDF4J configuration template for a main-memory repository
#
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.

[] a config:Repository ;
   config:rep.id "{%Repository ID|memory%}" ;
   rdfs:label "{%Repository title|Memory store%}" ;
   config:rep.impl [
      config:rep.type "openrdf:SailRepository" ;
      config:sail.impl [
         config:sail.type "openrdf:MemoryStore" ;
         config:mem.persist {%Persist|true|false%} ;
         config:mem.syncDelay {%Sync delay|0%}
      ]
   ].

Template variables are written down as {%var name%} and can specify zero or more values, seperated by vertical bars (“|”). If one value is specified then this value is interpreted as the default value for the variable. The Console will use this default value when the user simply hits the Enter key. If multiple variable values are specified, e.g. {%Persist|true|false%}, then this is interpreted as set of all possible values. If the user enters an unspecified value then that is considered to be an error. The value that is specified first is used as the default value.
The URIs that are used in the templates are terms defined in the RDF4J Config vocabulary. The relevant namespace and URIs can be found in the CONFIG
 vocabulary javadoc, or in Repository and Sail Configuration.

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nReference
    

  
  
RDF4J REST API
RDF4J API Javadoc


    
        
          
          The SAIL APIThe SAIL (Storage And Inference Layer) API is a collection of interfaces designed for low-level transactional access to RDF data.
          
          RDF4J Binary RDF FormatRDF4J supports reading and writing a custom binary RDF serialization format. Its main features are reduced parsing overhead and minimal memory requirements (for handling really long literals, amongst other things).
          
          Repository and SAIL configurationRDF4J repositories and SAIL configuration can be set up and changed by means of a configuration file in Turtle syntax. Here, we document the way the various components (repositories and SAILs) work together and how a full database configuration can be defined by “stacking” SAILs and wrapping in a Repository implementation.
          
          Sesame to Eclipse RDF4J migrationEclipse RDF4J is the successor of the OpenRDF Sesame project. The RDF4J framework and tools offer the same functionality, and will continue to be maintained and improved by the same team of developers as Sesame was, under Eclipse stewardship. For any users who wish to migrate their existing projects from Sesame to RDF4J (and we certainly urge you to do so quickly), here’s an overview of what has changed.
          
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\n\n\nEclipse RDF4J version 5.1.3 API specification


All PackagesRDF Model APIRepository APIRio: RDF Parsers/WritersQuery Result Parsers and WritersSAIL APIOther Packages


Package
Description
org.eclipse.rdf4j
 
org.eclipse.rdf4j.benchmark.rio
 
org.eclipse.rdf4j.benchmark.rio.impl
 
org.eclipse.rdf4j.benchmark.rio.util
 
org.eclipse.rdf4j.collection.factory.api
 
org.eclipse.rdf4j.collection.factory.impl
 
org.eclipse.rdf4j.collection.factory.mapdb
 
org.eclipse.rdf4j.common.annotation
 
org.eclipse.rdf4j.common.app
 
org.eclipse.rdf4j.common.app.config
 
org.eclipse.rdf4j.common.app.logging
 
org.eclipse.rdf4j.common.app.logging.base
 
org.eclipse.rdf4j.common.app.logging.logback
 
org.eclipse.rdf4j.common.app.net
 
org.eclipse.rdf4j.common.app.util
 
org.eclipse.rdf4j.common.concurrent.locks

Package offering various locking scheme implementations.

org.eclipse.rdf4j.common.concurrent.locks.diagnostics
 
org.eclipse.rdf4j.common.exception
 
org.eclipse.rdf4j.common.io
 
org.eclipse.rdf4j.common.iteration
 
org.eclipse.rdf4j.common.iterator
 
org.eclipse.rdf4j.common.lang
 
org.eclipse.rdf4j.common.lang.service
 
org.eclipse.rdf4j.common.logging
 
org.eclipse.rdf4j.common.logging.base
 
org.eclipse.rdf4j.common.logging.file.logback
 
org.eclipse.rdf4j.common.net
 
org.eclipse.rdf4j.common.order
 
org.eclipse.rdf4j.common.platform
 
org.eclipse.rdf4j.common.platform.support
 
org.eclipse.rdf4j.common.text
 
org.eclipse.rdf4j.common.transaction

Common classes and interfaces for transaction settings

org.eclipse.rdf4j.common.webapp
 
org.eclipse.rdf4j.common.webapp.filters
 
org.eclipse.rdf4j.common.webapp.navigation
 
org.eclipse.rdf4j.common.webapp.navigation.functions
 
org.eclipse.rdf4j.common.webapp.system
 
org.eclipse.rdf4j.common.webapp.system.logging
 
org.eclipse.rdf4j.common.webapp.system.proxy
 
org.eclipse.rdf4j.common.webapp.util
 
org.eclipse.rdf4j.common.webapp.views
 
org.eclipse.rdf4j.common.xml
 
org.eclipse.rdf4j.console


A command line console tool for querying and updating local or remote RDF4J repositories.

org.eclipse.rdf4j.console.command
 
org.eclipse.rdf4j.console.setting
 
org.eclipse.rdf4j.console.util
 
org.eclipse.rdf4j.examples.function
 
org.eclipse.rdf4j.examples.model
 
org.eclipse.rdf4j.examples.model.vocabulary
 
org.eclipse.rdf4j.examples.repository
 
org.eclipse.rdf4j.federated
 
org.eclipse.rdf4j.federated.algebra
 
org.eclipse.rdf4j.federated.api
 
org.eclipse.rdf4j.federated.cache
 
org.eclipse.rdf4j.federated.endpoint
 
org.eclipse.rdf4j.federated.endpoint.provider
 
org.eclipse.rdf4j.federated.evaluation
 
org.eclipse.rdf4j.federated.evaluation.concurrent
 
org.eclipse.rdf4j.federated.evaluation.iterator
 
org.eclipse.rdf4j.federated.evaluation.join
 
org.eclipse.rdf4j.federated.evaluation.union
 
org.eclipse.rdf4j.federated.exception
 
org.eclipse.rdf4j.federated.monitoring
 
org.eclipse.rdf4j.federated.optimizer
 
org.eclipse.rdf4j.federated.repository
 
org.eclipse.rdf4j.federated.structures
 
org.eclipse.rdf4j.federated.util
 
org.eclipse.rdf4j.federated.write
 
org.eclipse.rdf4j.http.client
 
org.eclipse.rdf4j.http.client.query
 
org.eclipse.rdf4j.http.client.shacl
 
org.eclipse.rdf4j.http.client.util
 
org.eclipse.rdf4j.http.protocol
 
org.eclipse.rdf4j.http.protocol.error
 
org.eclipse.rdf4j.http.protocol.transaction
 
org.eclipse.rdf4j.http.protocol.transaction.operations
 
org.eclipse.rdf4j.http.server
 
org.eclipse.rdf4j.http.server.protocol
 
org.eclipse.rdf4j.http.server.readonly
 
org.eclipse.rdf4j.http.server.readonly.sparql
 
org.eclipse.rdf4j.http.server.repository
 
org.eclipse.rdf4j.http.server.repository.config
 
org.eclipse.rdf4j.http.server.repository.contexts
 
org.eclipse.rdf4j.http.server.repository.graph
 
org.eclipse.rdf4j.http.server.repository.handler
 
org.eclipse.rdf4j.http.server.repository.namespaces
 
org.eclipse.rdf4j.http.server.repository.resolver
 
org.eclipse.rdf4j.http.server.repository.size
 
org.eclipse.rdf4j.http.server.repository.statements
 
org.eclipse.rdf4j.http.server.repository.transaction
 
org.eclipse.rdf4j.model

The RDF Model API

org.eclipse.rdf4j.model.base

Abstract base classes for RDF Model API interfaces.

org.eclipse.rdf4j.model.datatypes

Utility classes for handling datatypes.

org.eclipse.rdf4j.model.impl

Default implementations of the RDF model interfaces

org.eclipse.rdf4j.model.util

Helper classes for working with RDF models.

org.eclipse.rdf4j.model.vocabulary

Re-usable constants for various well-known RDF vocabularies.

org.eclipse.rdf4j.query

Interfaces and classes for handling queries and query results.

org.eclipse.rdf4j.query.algebra

Abstract Query Algebra model.

org.eclipse.rdf4j.query.algebra.evaluation
 
org.eclipse.rdf4j.query.algebra.evaluation.federation
 
org.eclipse.rdf4j.query.algebra.evaluation.function
 
org.eclipse.rdf4j.query.algebra.evaluation.function.aggregate
 
org.eclipse.rdf4j.query.algebra.evaluation.function.aggregate.stdev
 
org.eclipse.rdf4j.query.algebra.evaluation.function.aggregate.variance
 
org.eclipse.rdf4j.query.algebra.evaluation.function.datetime
 
org.eclipse.rdf4j.query.algebra.evaluation.function.geosparql
 
org.eclipse.rdf4j.query.algebra.evaluation.function.hash
 
org.eclipse.rdf4j.query.algebra.evaluation.function.numeric
 
org.eclipse.rdf4j.query.algebra.evaluation.function.rdfterm
 
org.eclipse.rdf4j.query.algebra.evaluation.function.string
 
org.eclipse.rdf4j.query.algebra.evaluation.function.triple
 
org.eclipse.rdf4j.query.algebra.evaluation.function.xsd

Functions for casting values to various XML Schema datatypes

org.eclipse.rdf4j.query.algebra.evaluation.impl
 
org.eclipse.rdf4j.query.algebra.evaluation.impl.evaluationsteps
 
org.eclipse.rdf4j.query.algebra.evaluation.impl.evaluationsteps.values
 
org.eclipse.rdf4j.query.algebra.evaluation.iterator

Implementations of 

invalid reference
Iteration

 relevant to query evaluation.

org.eclipse.rdf4j.query.algebra.evaluation.optimizer
 
org.eclipse.rdf4j.query.algebra.evaluation.util
 
org.eclipse.rdf4j.query.algebra.helpers
 
org.eclipse.rdf4j.query.algebra.helpers.collectors
 
org.eclipse.rdf4j.query.dawg

Functionality to convert tuple query results to and from the 
        Data Access Working Group Test Result Set RDF Vocabulary

org.eclipse.rdf4j.query.explanation
 
org.eclipse.rdf4j.query.impl
 
org.eclipse.rdf4j.query.parser
 
org.eclipse.rdf4j.query.parser.impl
 
org.eclipse.rdf4j.query.parser.sparql

The rdf4j SPARQL 1.1 parser.

org.eclipse.rdf4j.query.parser.sparql.aggregate
 
org.eclipse.rdf4j.query.resultio
 
org.eclipse.rdf4j.query.resultio.binary
 
org.eclipse.rdf4j.query.resultio.helpers
 
org.eclipse.rdf4j.query.resultio.sparqljson

A writer for the 
        SPARQL Query Results JSON Format

org.eclipse.rdf4j.query.resultio.sparqlxml

Parsers and writers for the 
        SPARQL Query Results XML Format

org.eclipse.rdf4j.query.resultio.text
 
org.eclipse.rdf4j.query.resultio.text.csv
 
org.eclipse.rdf4j.query.resultio.text.tsv
 
org.eclipse.rdf4j.queryrender

This package contains classes for working with RDF4J query objects.

org.eclipse.rdf4j.queryrender.sparql
 
org.eclipse.rdf4j.queryrender.sparql.experimental

This package contains classes for rendering RDF4J query objects as SPARQL queries.

org.eclipse.rdf4j.repository

The Repository API: the main API for accessing rdf databases and SPARQL endpoints.

org.eclipse.rdf4j.repository.base

Abstract base classes and wrappers for the main Repository API interfaces.

org.eclipse.rdf4j.repository.config

Repository configuration interfaces.

org.eclipse.rdf4j.repository.contextaware

A repository wrapper with convenience functions for handling contexts.

org.eclipse.rdf4j.repository.contextaware.config
 
org.eclipse.rdf4j.repository.dataset

A repository wrapper which supports auto-loading of datasets specified in a query.

org.eclipse.rdf4j.repository.dataset.config
 
org.eclipse.rdf4j.repository.evaluation
 
org.eclipse.rdf4j.repository.event

Interfaces for notification/interception of events happening on Repositories and RepositoryConnections

org.eclipse.rdf4j.repository.event.base

Wrapper/adapter base implementations.

org.eclipse.rdf4j.repository.event.util
 
org.eclipse.rdf4j.repository.filters
 
org.eclipse.rdf4j.repository.http

A repository that serves as a proxy client for a remote repository on an RDF4J Server.

org.eclipse.rdf4j.repository.http.config
 
org.eclipse.rdf4j.repository.http.helpers
 
org.eclipse.rdf4j.repository.manager

Functionality for Repository lifecycle management and sharing.

org.eclipse.rdf4j.repository.manager.util
 
org.eclipse.rdf4j.repository.sail

Repository implementation for local RDF databases that implement the SAIL SPI.

org.eclipse.rdf4j.repository.sail.config
 
org.eclipse.rdf4j.repository.sail.helpers

Helper and utility classes for the SailRepository

org.eclipse.rdf4j.repository.sparql

A Repository that serves as a SPARQL endpoint client.

org.eclipse.rdf4j.repository.sparql.config
 
org.eclipse.rdf4j.repository.sparql.federation
 
org.eclipse.rdf4j.repository.sparql.query
 
org.eclipse.rdf4j.repository.util

Helper classes for working with Repositories.

org.eclipse.rdf4j.rio

Rio: The RDF4J parser/writer API.

org.eclipse.rdf4j.rio.binary

Parser/writer for the RDF4J binary RDF format.

org.eclipse.rdf4j.rio.datatypes

Various DatatypeHandler implementations.

org.eclipse.rdf4j.rio.hdt

Parser/writer for the HDT v1.0 format.

org.eclipse.rdf4j.rio.helpers

Provides helpers classes for Rio.

org.eclipse.rdf4j.rio.jsonld

Parser/writer for the JSON-LD 1.1 format.

org.eclipse.rdf4j.rio.jsonld.legacy

Parser/writer for the JSON-LD 1.0 format.

org.eclipse.rdf4j.rio.languages

Various LanguageHandler implementations for processing language-tags.

org.eclipse.rdf4j.rio.n3

Writer for the Notation-3 (N3) format.

org.eclipse.rdf4j.rio.ndjsonld
 
org.eclipse.rdf4j.rio.ndjsonld.legacy
 
org.eclipse.rdf4j.rio.nquads

Parser/writer for the N-Quads format.

org.eclipse.rdf4j.rio.ntriples

Parser/writer for the N-Triples format.

org.eclipse.rdf4j.rio.rdfjson

Parser/writer for the RDF/JSON format.

org.eclipse.rdf4j.rio.rdfxml

Parser/writer for the RDF/XML format.

org.eclipse.rdf4j.rio.rdfxml.util

Pretty-printing for the RDF/XML format.

org.eclipse.rdf4j.rio.trig

Parser/writer for the TriG format.

org.eclipse.rdf4j.rio.trigstar
 
org.eclipse.rdf4j.rio.trix

Parser/writer for the TriX format.

org.eclipse.rdf4j.rio.turtle

Parser/writer for the Turtle format.

org.eclipse.rdf4j.rio.turtlestar
 
org.eclipse.rdf4j.sail

RDF Storage And Inference Layer (RDF Sail): a set of interfaces defining an SPI for RDF databases.

org.eclipse.rdf4j.sail.base

Base functionality for Sail implementations that require multi-versioned concurrency control (MVCC).

org.eclipse.rdf4j.sail.base.config
 
org.eclipse.rdf4j.sail.config
 
org.eclipse.rdf4j.sail.elasticsearch

ElasticSearch index for the LuceneSail.

org.eclipse.rdf4j.sail.elasticsearch.config
 
org.eclipse.rdf4j.sail.elasticsearchstore

Elasticsearch store for string triples.

org.eclipse.rdf4j.sail.elasticsearchstore.config
 
org.eclipse.rdf4j.sail.evaluation
 
org.eclipse.rdf4j.sail.extensiblestore

Elasticsearch store for string triples

org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics
 
org.eclipse.rdf4j.sail.extensiblestore.valuefactory
 
org.eclipse.rdf4j.sail.features
 
org.eclipse.rdf4j.sail.helpers

Abstract base implementation and internal helper classes for Sail implementations.

org.eclipse.rdf4j.sail.inferencer
 
org.eclipse.rdf4j.sail.inferencer.fc

Forward-chaining inferencers, implemented as StackableSails.

org.eclipse.rdf4j.sail.inferencer.fc.config
 
org.eclipse.rdf4j.sail.inferencer.util
 
org.eclipse.rdf4j.sail.lmdb

The LMDB based Store.

org.eclipse.rdf4j.sail.lmdb.config
 
org.eclipse.rdf4j.sail.lmdb.model

Lmdb implementations of the RDF Model interfaces.

org.eclipse.rdf4j.sail.lucene

A Sail implementation that supports full-text indexing via the Lucene API.

org.eclipse.rdf4j.sail.lucene.config
 
org.eclipse.rdf4j.sail.lucene.impl
 
org.eclipse.rdf4j.sail.lucene.impl.config
 
org.eclipse.rdf4j.sail.lucene.util
 
org.eclipse.rdf4j.sail.memory

An implementation of the RDF SAIL API that uses main memory for storage.

org.eclipse.rdf4j.sail.memory.config
 
org.eclipse.rdf4j.sail.memory.model

MemoryStore-specific implementations of the core RDF model objects.

org.eclipse.rdf4j.sail.model
 
org.eclipse.rdf4j.sail.nativerdf

The Native Store.

org.eclipse.rdf4j.sail.nativerdf.btree

B-Tree on disk implementation.

org.eclipse.rdf4j.sail.nativerdf.config
 
org.eclipse.rdf4j.sail.nativerdf.datastore

File and data storage functionality.

org.eclipse.rdf4j.sail.nativerdf.model

Native implementations of the RDF Model interfaces.

org.eclipse.rdf4j.sail.shacl

A Sail implementation for SHACL constraint checking.

org.eclipse.rdf4j.sail.shacl.ast.constraintcomponents
 
org.eclipse.rdf4j.sail.shacl.ast.paths
 
org.eclipse.rdf4j.sail.shacl.ast.planNodes
 
org.eclipse.rdf4j.sail.shacl.ast.targets
 
org.eclipse.rdf4j.sail.shacl.config
 
org.eclipse.rdf4j.sail.shacl.results
 
org.eclipse.rdf4j.sail.shacl.results.lazy
 
org.eclipse.rdf4j.sail.shacl.wrapper
 
org.eclipse.rdf4j.sail.shacl.wrapper.data
 
org.eclipse.rdf4j.sail.shacl.wrapper.shape
 
org.eclipse.rdf4j.sail.solr
 
org.eclipse.rdf4j.sail.solr.client.cloud
 
org.eclipse.rdf4j.sail.solr.client.embedded
 
org.eclipse.rdf4j.sail.solr.client.http
 
org.eclipse.rdf4j.sail.solr.config
 
org.eclipse.rdf4j.sparqlbuilder.constraint
 
org.eclipse.rdf4j.sparqlbuilder.constraint.propertypath
 
org.eclipse.rdf4j.sparqlbuilder.constraint.propertypath.builder
 
org.eclipse.rdf4j.sparqlbuilder.core

Core classes and interfaces for the SPARQLBuilder.

org.eclipse.rdf4j.sparqlbuilder.core.query
 
org.eclipse.rdf4j.sparqlbuilder.graphpattern
 
org.eclipse.rdf4j.sparqlbuilder.rdf
 
org.eclipse.rdf4j.sparqlbuilder.util
 
org.eclipse.rdf4j.spin


 See discussion at https://github.com/eclipse/rdf4j/issues/1262

org.eclipse.rdf4j.spin.function

Core functions required for SPIN.

org.eclipse.rdf4j.spin.function.apf
 
org.eclipse.rdf4j.spin.function.list
 
org.eclipse.rdf4j.spin.function.spif
 
org.eclipse.rdf4j.spring

Rdf4J-Spring

org.eclipse.rdf4j.spring.dao

Rdf4j-Spring DAO

org.eclipse.rdf4j.spring.dao.exception
 
org.eclipse.rdf4j.spring.dao.exception.mapper
 
org.eclipse.rdf4j.spring.dao.support
 
org.eclipse.rdf4j.spring.dao.support.bindingsBuilder
 
org.eclipse.rdf4j.spring.dao.support.key
 
org.eclipse.rdf4j.spring.dao.support.opbuilder
 
org.eclipse.rdf4j.spring.dao.support.operation
 
org.eclipse.rdf4j.spring.dao.support.sparql
 
org.eclipse.rdf4j.spring.demo
 
org.eclipse.rdf4j.spring.demo.dao
 
org.eclipse.rdf4j.spring.demo.model
 
org.eclipse.rdf4j.spring.demo.service
 
org.eclipse.rdf4j.spring.demo.support
 
org.eclipse.rdf4j.spring.operationcache

Rdf4j-Spring OperationCache

org.eclipse.rdf4j.spring.operationlog

Rdf4j-Spring OperationLog

org.eclipse.rdf4j.spring.operationlog.log
 
org.eclipse.rdf4j.spring.operationlog.log.jmx
 
org.eclipse.rdf4j.spring.operationlog.log.slf4j
 
org.eclipse.rdf4j.spring.pool

Rdf4j-Spring Pool

org.eclipse.rdf4j.spring.repository

Rdf4j-Spring Repository

org.eclipse.rdf4j.spring.repository.inmemory
 
org.eclipse.rdf4j.spring.repository.remote
 
org.eclipse.rdf4j.spring.resultcache

Rdf4j-Spring ResultCache

org.eclipse.rdf4j.spring.support
 
org.eclipse.rdf4j.spring.support.connectionfactory
 
org.eclipse.rdf4j.spring.support.query
 
org.eclipse.rdf4j.spring.test
 
org.eclipse.rdf4j.spring.tx

Rdf4J-Spring Tx

org.eclipse.rdf4j.spring.tx.exception
 
org.eclipse.rdf4j.spring.util
 
org.eclipse.rdf4j.spring.uuidsource

This package contains different approaches for generating UUIDs.

org.eclipse.rdf4j.spring.uuidsource.noveltychecking
 
org.eclipse.rdf4j.spring.uuidsource.predictable
 
org.eclipse.rdf4j.spring.uuidsource.sequence
 
org.eclipse.rdf4j.spring.uuidsource.simple
 
org.eclipse.rdf4j.testsuite.model
 
org.eclipse.rdf4j.testsuite.query.algebra.geosparql
 
org.eclipse.rdf4j.testsuite.query.parser.sparql
 
org.eclipse.rdf4j.testsuite.query.parser.sparql.manifest
 
org.eclipse.rdf4j.testsuite.query.resultio
 
org.eclipse.rdf4j.testsuite.repository
 
org.eclipse.rdf4j.testsuite.repository.optimistic
 
org.eclipse.rdf4j.testsuite.rio
 
org.eclipse.rdf4j.testsuite.rio.n3
 
org.eclipse.rdf4j.testsuite.rio.nquads
 
org.eclipse.rdf4j.testsuite.rio.ntriples
 
org.eclipse.rdf4j.testsuite.rio.rdfjson
 
org.eclipse.rdf4j.testsuite.rio.rdfxml
 
org.eclipse.rdf4j.testsuite.rio.trig
 
org.eclipse.rdf4j.testsuite.rio.turtle
 
org.eclipse.rdf4j.testsuite.sail
 
org.eclipse.rdf4j.testsuite.sparql

A SPARQL test suite for RDF4J repositories

org.eclipse.rdf4j.testsuite.sparql.tests
 
org.eclipse.rdf4j.testsuite.sparql.vocabulary
 
org.eclipse.rdf4j.workbench
 
org.eclipse.rdf4j.workbench.base
 
org.eclipse.rdf4j.workbench.commands
 
org.eclipse.rdf4j.workbench.exceptions
 
org.eclipse.rdf4j.workbench.proxy
 
org.eclipse.rdf4j.workbench.util
 
org.eclipse.testsuite.rdf4j.sail.lucene
 





Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nThe SAIL API
    

  
  The SAIL (Storage And Inference Layer) API is a collection of interfaces designed for low-level transactional access to RDF data.
It functions as a decoupling point between specific database implementations and the functional modules (parsers, query engines, end-user API access, etc) of the rdf4j framework.
Here, we document the design of the API and explain the roles and rationale behind the various interfaces. We also explain how various abstract base classes provided as part of the API can be reused by third-party implementors, in order to make implementing a SAIL-compatible database easier.
WARNING: this document is currently in draft, and incomplete. Feedback and suggestions for change are welcome, either on our GitHub repo, or on the rdf4j Users Group.



  
    SAIL Main interfaces



In the above diagram we see an overview of the two main interfaces: Sail, and SailConnection. The Sail interface is the main access point for RDF storage. Roughly speaking, this is “the database”. Each Sail object is composed of zero or more SailConnection objects. This is where all the actual database access functionality is concentrated. SailConnection provides methods to execute queries, retrieve and modify triples, and manage transactions.
AbstractSail and AbstractSailConnection
rdf4j provides default (abstract) implementations for most of the SAIL functionality, which can be reused (and of course overridden) by any concrete implementation.



  
    Abstract base implementations



The AbstractSail class  provides base implementations of all methods of the Sail interface. It provides the following benefits to concrete Sail implementations:
. implementations of all required basic getter/setter methods
. store shutdown management, including grace periods for active connections and eventual forced closure of active connections on store shutdown.
. thread-safety: take care of basic concurrency issues around opening multiple connections.
. ongoing compatibility: future rdf4j releases that introduce new functionality in Sail provide default implementations in AbstractSail.
Similarly, the AbstractSailConnection provides base implementations of all methods of the SailConnection interface. It provides the following benefits to concrete SailConnection implementations:
. handles all basic concurrency issues around starting / executing transactions
. (configurable) buffering of active changes in any transaction
. ongoing compatibility: future rdf4j releases that introduce new functionality in SailConnection provide default implementations in AbstractSailConnection.
The abstract base classes use the naming convention methodname**Internal** to indicate the methods that concrete subclasses should concentrate on implementing. The rationale is that the public method implementations in the abstract class implement basic concurrency handling and other book-keeping, and their corresponding (protected) ...Internal methods can be implemented by the concrete subclass to provide the actual business logic of the method.
For example, the query method AbstractSailConnection.getStatements() provides a lot of book keeping: it ensures pending updates are flushed, acquires a read lock on the connection, verifies the connection is still open, and takes care of internally registering the resulting Iteration from the query for resource management and concurrency purposes. In between all of this, it calls getStatementsInternal. The only job of this method is to answer the query by retrieving the relevant data from the data source.
NotifyingSail and AbstractNotifyingSail
The NotifyingSail and NotifyingSailConnection interfaces provide basic event handling for SAIL implementations. The main goal of these interfaces is to provide a messaging mechanism for closely-linked SAIL implementations (for example, a “Sail stack” where a reasoner is to be kept informed of changes to the underlying database).



  
    NotifyingSail interfaces



As can be seen in this diagram, the NotifyingSail interface provides the option of registering one or more SailChangedListener implementations. When registered, the listener will be messaged via the sailChanged method. The contents of the message is a SailChangedEvent that provides basic info on what has been changed.
More fine-grained event data is available at the Connection level. The NotifyingSailConnection allows registering a SailConnectionListener, which receives a message for each individual statement added or removed on the connection.
Rdf4j also provides base implementation classes for these two interfaces. These classes - AbstractNotifyingSail and AbstractNotifyingSailConnection - are extensions of the AbstractSail(Connection) classes that add
default implementations of the methods defined in the NotifyingSail(Connection) interfaces.
Stacking SAILs



  
    Stackable SAIL interface



The SAIL API provides the StackableSail interface to allow SAIL implementations to “stack” on top of each other, providing a chain of responsility: each SAIL implementation in the stack implements a specific feature (reasoning, access control, data filtering, query expansion, etc. etc.). The last SAIL implementation in the stack is expected to not implement StackableSail, and this Sail is responsible for the actual persistence of the data. rdf4j’s NativeStore and MemoryStore are implementations of such a persistence SAIL, while the SchemaCachingRDFSInferencer (responsible for RDFS inferencing) and LuceneSail (responsible for full-text indexing) are examples of StackableSail implementations.
Querying
The SAIL API has no knowledge of SPARQL queries. Instead, it operates on a query algebra, that is, an object representation of a (SPARQL) query as provided by the SPARQL query parser.
SailConnection has a single evaluate() method, which accepts a org.eclipse.rdf4j.queryalgebra.model.TupleExpr object. This is the object representation of the query as produced by the query parser.
Transactions
TODO

  

     
      
        
          

  Table of Contents

  
  
    
      
        AbstractSail and AbstractSailConnection
        NotifyingSail and AbstractNotifyingSail
        Stacking SAILs
      
    
    Querying
    Transactions\n\n\n\nRDF4J Binary RDF Format
    

  
  RDF4J supports reading and writing a custom binary RDF serialization format. Its main features are reduced parsing overhead and minimal memory requirements (for handling really long literals, amongst other things).
MIME Content Type
RDF4J assigns the content type application/x-binary-rdf to its format.
Overall design
Results encoded in the RDF4J Binary RDF format consist of a header followed by zero or more records, and closes with an END_OF_DATA marker (see below). Values are stored in network order (Big-Endian).
All string values use UTF-16 encoding. Reference ids are assigned to recurring values to avoid having to repeat long strings.
Header
The header is 8 bytes long:

Bytes 0-3 contain a magic number, namely the ASCII codes for the string “BRDF”, which stands for Binary RDF.
Bytes 4-7 specify the format version (a 4-byte signed integer).

For example, a header for a result in format version 1 will look like this:
  byte: 0  1  2  3 |  4  5  6  7 |
-------------------+-------------+
 value: B  R  D  F |  0  0  0  1 |

Content records
Zero or more records follow after the header. Each record can be a namespace declaration, a comment, a value reference declaration, or a statement.
Each record starts with a record type marker (a single byte). The following record types are defined in the current format:

NAMESPACE_DECL (byte value: 0):
This indicates a namespace declaration record.
STATEMENT (byte value: 1):
This indicates an RDF statement record.
COMMENT (byte value: 2):
This indicates a comment record.
VALUE_DECL (byte value: 3):
This indicates a value declaration.
END_OF_DATA (byte value: 127):
This indicates the end of the data stream has been reached.

Strings
All strings are encoded as UTF-16 encoded byte arrays. A String is preceeded by a 4-byte signed integer that encodes the length of the string (specifically, it records the number of Unicode code units). For example, the string ‘foo’ will be encoded as follows:
 byte: 0 1 2 3 | 4 6 8 |
---------------+-------+
value: 0 0 0 3 | f o o |

RDF Values
Each RDF value type has its own specific 1-byte record type marker:

NULL_VALUE (byte value: 0)
marks an empty RDF value (this is used, for example, in encoding of context in statements)
URI_VALUE (byte value: 1)
marks a URI value
BNODE_VALUE (byte value: 2)
marks a blank node value
PLAIN_LITERAL_VALUE (byte value: 3)
marks a plain literal value
LANG_LITERAL_VALUE (byte value: 4)
marks a language-tagged literal value
DATATYPE_LITERAL_VALUE (byte value: 5)
marks a datatyped literal value
TRIPLE_VALUE (byte value: 7)
marks an RDF* triple value

URIs
URIs are recorded by the URI_VALUE marker followed by the URI encoded as a string.
Blank nodes
Blank nodes are recorded by the BNODE_VALUE marker followed by the id of the blank node encoded as a string.
Literals
Depending on the specific literal type (plain, language-tagged, datatyped), a literal is recorded by one of the markers PLAIN_LITERAL_VALUE, LANG_LITERAL_VALUE or DATATYPE_LITERAL_VALUE. This is followed by the lexical label of the literal as a string, optionally followed by either a language tag encoded as a string value or a datatype encoded as a string.
RDF* triples
RDF* triples are recorded by the TRIPLE_VALUE marker, followed by value markers and values for the triple’s subject, predicate, and object, in order.
Value reference declaration records
To enable further compression of the byte stream, the Binary RDF format enables encoding of reference-identifiers for often-repeated RDF values. A value reference declaration starts with a VALUE_DECL record marker (1 byte, value 3), followed by a 4-byte signed integer that encodes the reference id. This is followed by the actual value, encoded as an RDF value (see above).
For example, a declaration that assigns id 42 to the URI ‘http://example.org/HHGTTG’ will look like this:
  byte: 0 | 1 2 3 4 | 5 | 6 7 8 9 | 10 12 14 16 18 (etc) |
----------+---------+---+---------+----------------------+
 value: 3 | 0 0 0 42| 1 | 0 0 0 25| h  t  t  p  :  (etc) |

Explanation: byte 0 marks the record as a VALUE_DECL, bytes 1-4 encode the reference id, byte 5 encodes the value type (URI_VALUE), bytes 6-9 encode the length of the string value, bytes 10 and further encode the actual string value as an UTF-16 encoded byte array.
Note that the format allows the same reference id to be assigned more than once. When a second value declaration occurs, it effectively overwrites a previous declaration, reassigning the id to a new value for all following statements.
Namespace records
A namespace declaration is recorded by the NAMESPACE_DECL marker. Next follows the namespace prefix, as a string, then followed by the namespace URI, as a string.
For example, a namespace declaration record for prefix ‘ex’ and namespace uri ‘http://example.org/’ will look like this:
  byte: 0 | 1 2 3 4 | 5 6 | 7 8 9 10 | 11 13 15 17 19 (etc) |
----------+---------+-----+----------+----------------------+
 value: 0 | 0 0 0 2 | e x | 0 0 0 19 | h  t  t  p  :  (etc) |

Comment records
A comment is recorded by the COMMENT marker, followed by the comment text encoded as a string.
For example, a record for the comment ‘example’ will look like this:
  byte: 0 | 1 2 3 4 | 5 7 9 11 13 15 17 |
----------+---------+-------------------+
 value: 2 | 0 0 0 7 | e x a m  p  l  e  |

Statement records
Each statement record starts with a STATEMENT marker (1 byte, value 1). For the encoding of the statement’s subject, predicate, object and context, either the RDF value is encoded directly, or a previously assigned value reference (see section 2.3) is reused. A Value references is recorded with the VALUE_REF marker (1 byte, value 6), followed by the reference id as a 4-byte signed integer.
An example statement
Consider the following RDF statement:
<http://example.org/George> <http://example.org/name> "George" .

Assume that the subject and predicate previously been assigned reference ids,
(42 and 43 respecively). The object value has not been assigned a reference id.
This statement would then be recorded as follows:
 byte: 0 | 1 | 2 3 4 5 | 6 | 7 8 9 10| 11 | 12 13 14 15 | 16 18 20 22 24 26 | 28 |
---------+---+---------+---+---------+----+-------------+-------------------+----+
value: 1 | 6 | 0 0 0 42| 6 | 0 0 0 43|  3 |  0  0  0  5 |  G  e  o  r  g  e |  0 |

Explanation: byte 0 marks the record as a STATEMENT. Byte 1 marks the subject of the statement as a VALUE_REF. Bytes 2-5 encode the reference id of the subject. Byte 6 marks the predicate of the statement as a VALUE_REF. Byte 7-10 encode the reference id of the predicate. Byte 11 marks the obect of the statement as a PLAIN_LITERAL value, bytes 12-15 encode the length of the lexical value of the literal, and bytes 16-26 encode the literal’s lexical value as a UTF-16 encoded byte array. Finally, byte 28 marks the context field of the statement as a NULL_VALUE.
Buffering and value reference handling
The binary RDF format enables declaration of value references for more compressed representation of often-repeated values.
A binary RDF producer may choose to introduce a reference for every RDF value. This is a simple approach, but it produces a suboptimal compression (because for values which occur only once, direct encoding of the value uses fewer bytes than introducing a reference for it).
Another approach is to introduce a buffered writing strategy: statements to be serialized are put on a queue with a certain capacity, and for each RDF value in these queued statements the number of occurrences in the queue is determined. As the queue is emptied and each statement is serialized, all values that occur more than once in the queue are assigned a reference id. This is, in fact, the strategy employed by the Rio Writer.
It is also important to note that reference ids are not necessarily global over the entire document: ids are assigned on the basis of number of occurrences of a value in the current statement queue. If that number drops to zero, the reference id for that value can be ‘recycled’, that is, reassigned to another value. This ensures that we never run out of reference ids, even for very large datasets.

  

     
      
        
          

  Table of Contents

  
  
    MIME Content Type
    Overall design
    Header
    Content records
      
        Strings
        RDF Values
          
            URIs
            Blank nodes
            Literals
            RDF* triples
          
        
        Value reference declaration records
        Namespace records
        Comment records
        Statement records
          
            An example statement
          
        
      
    
    Buffering and value reference handling\n\n\n\nRepository and SAIL Configuration
    

  
  RDF4J repositories and SAIL configuration can be set up and changed by means of a configuration file in Turtle syntax. Here, we document the way the various components (repositories and SAILs) work together and how a full database configuration can be defined by “stacking” SAILs and wrapping in a Repository implementation.

    
    
Since RDF4J 4.3.0, the configuration vocabulary for repositories and Sail implementations has been unified into a single namespace: tag:rdf4j.org,2023:config/. While currently RDF4J can still read configurations using the legacy vocabulary (using various namespaces starting with http://www.openrdf.org/config/), we strongly urge you to update existing configuration files to use the new vocabulary. See the section on Migrating old configurations for some pointers.



Repository configuration
A Repository configuration consists of a single RDF subject of type config:Repository. Typically this subject is a blank node ([] in Turtle syntax), and is assigned its configuration parameters through use of RDF properties.
The configuration namespace is tag:rdf4j.org,2023:config/, commonly abbreviated to config.
A Repository takes a following configuration parameters:

config:rep.id (String): the repository identifier (required). This must be unique within the running system.
rdfs:label (String): a human-readable name or short description of the repository (optional).
config:rep.impl: this specifies and configures the specific repository implementation (required). This is typically supplied as a nested blank node, which in turns has the implementation-specific configuration parameter. Every repository implemenation must specify a config:rep.type.

@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.


[] a config:Repository ;
   config:rep.id "example" ;
   rdfs:label "SPARQL endpoint at http://example.org/" ;
   config:rep.impl [ config:rep.type "example:repositoryType";
                        # your implementation config here
   ].
SPARQLRepository
The SPARQLRepository repository implementation is a client interface for a remote SPARQL endpoint.
Its config.rep.type value is "openrdf:SPARQLRepository".
It takes the following configuration parameters:

config:sparql.queryEndpoint (IRI): the SPARQL query endpoint URL (required).
config:sparql.updateEndpoint (IRI): the SPARQL update endpoint URL (optional). Only needs to be defined if different from the query endpoint.

Example configuration
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.

[] a config:Repository ;
   config:rep.id "example" ;
   rdfs:label "SPARQL endpoint at http://example.org/" .
   config:rep.impl [
      config:repositoryType "openrdf:SPARQLRepository" ;
      config:sparql.queryEndpoint <http://example.org/sparql> ;
      config:sparql.updateEndpoint <http://example.org/sparql/update> ;
   ];
HTTPRepository
The HTTPRepository repository implementation is a client interface for a
store on a (remote) RDF4J Server. It differs from SPARQLRepository in that it
implements several RDF4J-specific  extensions to the standard SPARQL Protocol
which enhance transactional support and general performance.  Its
config:rep.type value is "openrdf:HTTPRepository".
It takes the following configuration parameters:

config:http.url (IRI): the location of the repository on an RDF4J Server (required).
config:http.username (string): username for basic authentication (optional).
config:http.password (string): password for basic authentication (optional).

Example configuration
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.

[] a config:Repository ;
   config:rep.id "example" ;
   rdfs:label "example repository on locally running RDF4J Server" .
   config:rep.impl [
      config:rep.type "openrdf:HTTPRepository" ;
      config:http.url <http://localhost:8080/rdf4j-server/repositories/test>
   ];
SailRepository
The SailRepository
 repository implementation is the main implementation for direct access to a local RDF database (a SAIL implementation). Its config:rep.type is openrdf:SailRepository.
It takes the following configuration parameters:

config:sail.impl: this specifies and configures the specific SAIL implementation (required). This is typically supplied as a nested blank node, which in turns has the SAIL-specific configuration parameters. Every SAIL implementation must specify a config:sail.type property.

Example configuration
In this example we configure a simple SailRepository using a MemoryStore SAIL. See the section SAIL Configuration for more details.
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.

[] a config:Repository ;
   config:rep.id "example" ;
   rdfs:label "Example Memory store" ;
   config:rep.impl [
      config:rep.type "openrdf:SailRepository" ;
      config:sail.impl [
         config:sail.type "openrdf:MemoryStore" ;
         config:sail.iterationCacheSyncThreshold "10000";
         config:mem.persist true;
         config:mem.syncDelay 0;
         config:sail.defaultQueryEvaluationMode "STANDARD"
      ]
   ].
DatasetRepository
The DatasetRepository
 is a wrapper around a SailRepository that dynamically loads datasets specified in the FROM and FROM NAMED clauses in SPARQL queries.
Example configuration
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.

[] a config:Repository ;
   config:rep.id "example" ;
   rdfs:label "Example Dataset Repository" ;
   config:rep.impl [
      config:rep.type "openrdf:DatasetRepository" ;
      config:delegate [
        config:rep.type "openrdf:SailRepository" ;
        config:sail.impl [
          ...
        ]
      ]
   ].
ContextAwareRepository
The ContextAwareRepository
 is a wrapper around any other Repository. It can be used to configure the default context(s) on which query, update, or delete operations operate per default.
It takes the following parameters:

config:ca.readContext. Specifies the context(s) used per default when executing read operations (including SPARQL queries). Can be specified with multiple values to include more than one context;
config:ca.insertContext. Specifies the context used for insertion operations.
config:ca.removeContext. Specifies the context(s) used for delete operations. Can be specified with multiple values to include more than one context.

Example configuration
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.
@prefix ex: <http://example.org/>.

[] a config:Repository ;
   config:rep.id "example" ;
   rdfs:label "Example Context-aware repository" ;
   config:rep.impl [
      config:rep.type "openrdf:ContextAwareRepository" ;
      config:ca.readContext ex:namedGraph1, ex:namedGraph2;
      config:delegate [
        config:rep.type "openrdf:HTTPRepository" ;
      ]
   ].
Sail Configuration
Sail implementations come in two basic types: base Sails, and Sail wrappers / adapters (sometimes also referred to as “Stackable Sails”).
Each Sail configuration identifies one base Sail, which typically functions as the actual database layer. This base Sail configuration can optionally be wrapped in one or more Stackable Sail configurations. The entire “Sail stack” is then usually wrapped as part of a SailRepository configuration.
Every Sail in a Sail configuration has at least one required parameter:

config:sail.type - this identifies the type of Sail being configured.
config:sail.iterationCacheSyncThreshold (integer). Specifies the size of the internal cache for query result iterations before on-disk overflow is enabled.

Base Sails
Base Sail implementations are responsible for persistence of data and handling of queries, transactions and update operations on that data. A base Sail is typically a database.
Base Sails commonly take the following parameter:

config:sail.defaultQueryEvaluationMode (string). Specifies the default query evaluation mode used for SPARQL queries on this store. Expected values are STRICT or STANDARD.

Memory Store
A Memory Store is an RDF4J database that keeps all data in main memory, with optional persistence on disk. Its config:sail.type value is "openrdf:MemoryStore".
It takes the following configuration options:

config:mem.persist (boolean). Specifies if the store persists its data to disk (required). Persistent memory stores write their data to disk before being shut down and read this data back in the next time they are initialized. Non-persistent memory stores are always empty upon initialization.
config:mem.syncDelay (integer). Specifies the amount of time (in milliseconds) between an update operation completing and the store syncing its contents to disk (optional). By default, the memory store persistence mechanism synchronizes the disk backup directly upon any change to the contents of the store. Setting a delay on this synchronization can be useful if your application performs several transactions in sequence and you want to prevent disk synchronization in the middle of this sequence to improve update performance.

Example configuration
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.

[] a config:Repository ;
   config:rep.id "example" ;
   rdfs:label "Example Memory store" ;
   config:rep.impl [
      config:rep.type "openrdf:SailRepository" ;
      config:sail.impl [
         config:sail.type "openrdf:MemoryStore" ;
         config:sail.iterationCacheSyncThreshold "10000";
         config:mem.persist true;
         config:mem.syncDelay 0;
         config:sail.defaultQueryEvaluationMode "STANDARD"
      ]
   ].
Native Store
A Native Store is an RDF4J database that persists all data directly on disk in an indexed binary format. Its config:sail.type value is "openrdf:NativeStore".
It takes the following configuration options:

config:native.tripleIndex (string).  Specifices a comma-separated list of indexes for the store to use (optional).
config:native.forceSync (boolean). Specifies if an OS-level force sync should be executed after every update (optional).
config:native.valueCacheSize (integer). Specifies the size of the value cache (optional).
config:native.valueIDCacheSize (integer). Specifices the size of the value ID cache (optional).
config:native.namespaceCacheSize (integer). Specifies the size of the namespace cache (optional).
config:native.namespaceIDCacheSize (integer). Specifies the size of the namespace ID cache (optional).

Native store indexes
The native store uses on-disk indexes to speed up querying. It uses B-Trees for indexing statements, where the index key consists of four fields: subject (s), predicate (p), object (o) and context (c). The order in which each of these fields is used in the key determines the usability of an index on a specify statement query pattern: searching statements with a specific subject in an index that has the subject as the first field is significantly faster than searching these same statements in an index where the subject field is second or third. In the worst case, the ‘wrong’ statement pattern will result in a sequential scan over the entire set of statements.
By default, the native repository only uses two indexes, one with a subject-predicate-object-context (spoc) key pattern and one with a predicate-object-subject-context (posc) key pattern. However, it is possible to define more or other indexes for the native repository, using the config:native.tripleIndex parameter. This can be used to optimize performance for query patterns that occur frequently.
The subject, predicate, object and context fields are represented by the characters ‘s’, ‘p’, ‘o’ and ‘c’ respectively. Indexes can be specified by creating 4-letter words from these four characters. Multiple indexes can be specified by separating these words with commas, spaces and/or tabs. For example, the string “spoc, posc” specifies two indexes; a subject-predicate-object-context index and a predicate-object-subject-context index.
Creating more indexes potentially speeds up querying (a lot), but also adds overhead for maintaining the indexes. Also, every added index takes up additional disk space.
The native store automatically creates/drops indexes upon (re)initialization, so the parameter can be adjusted and upon the first refresh of the configuration the native store will change its indexing strategy, without loss of data.
Example configuration
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.

[] a config:Repository ;
   config:rep.id "example" ;
   rdfs:label "Example Native store" ;
   config:rep.impl [
      config:rep.type "openrdf:SailRepository" ;
      config:sail.impl [
        config:sail.type "openrdf:NativeStore" ;
         config:sail.iterationCacheSyncThreshold "10000";
         config:sail.defaultQueryEvaluationMode "STANDARD";
         config:native.tripleIndexes "spoc,posc"
      ]
   ].
Elasticsearch Store
The Elasticsearch Store is an RDF4J database that persists all data directly in Elasticsearch (not to be confused with the Elasticsearch Fulltext Search Sail, which is an adapter Sail implementation to provided full-text search indexing on top of other RDF databases). Its config:sail.type value is "rdf4j:ElasticsearchStore".
The ElasticsearchStore takes the following configuration options:

config:ess.hostname (string). Specifies the hostname to use for connecting to Elasticsearch (required).
config:ess.port (int). Specifies the port number to use for connecting to Elasticsearch (optional).
config:ess.clusterName (string). Specifies the Elasticsearch cluster name (optional).
config:ess.index (string). Specifies the index name to use for storage and  retrieval of data (optional).

Example configuration
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.

[] a config:Repository ;
   config:rep.id "example" ;
   rdfs:label "Example Elasticsearch store" ;
   config:rep.impl [
      config:rep.type "openrdf:SailRepository" ;
      config:sail.impl [
         config:sail.type "rdf4j:ElasticsearchStore" ;
         config:sail.iterationCacheSyncThreshold "10000";
         config:ess.hostname "localhost";
         config:ess.port 9200;
         config:ess.clusterName "myCluster";
         config:ess.index "myIndex";
         config:sail.defaultQueryEvaluationMode "STANDARD"
      ]
   ].
Sail Adapter implementations
Sail Adapters, or “Stackable Sails”, are Sail implementations that implement intercepting behavior for operations on RDF databases before delegating down to the wrapped Sail implementation (ultimately a base Sail, though of course multiple Stackable Sails can be stacked on top of each other as well).
Every Sail Adapter has at least one required parameter:

config:delegate. Specifies and configures the wrapped Sail implementation (required). This is typically supplied as a nested blank node, which in turns has the implementation-specific configuration parameter.

RDF Schema inferencer
The RDF Schema inferencer is an Sail Adapter that performs rule-based entailment as defined in the RDF 1.1 Semantics recommendation. Reasoning happens in a forward-chaining manner on update operations, and the entailed statements are sent to be persisted by the wrapped Sail. The inferencer also caches RDF Schema statements to speed up entailment and retrieval operations.
The config:sail.type value for the RDF Schema inferencer is "rdf4j:SchemaCachingRDFSInferencer".
Example configuration
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.

[] a config:Repository ;
   config:rep.id "example" ;
   rdfs:label "Example memory store with RDF Schema entailment" ;
   config:rep.impl [
      config:rep.type "openrdf:SailRepository" ;
      config:sail.impl [
         config:sail.type "rdf4j:SchemaCachingRDFSInferencer";
         config:delegate [
             config:sail.type "openrdf:MemoryStore" ;
             config:sail.iterationCacheSyncThreshold "10000";
             config:mem.persist true;
             config:mem.syncDelay 0;
             config:sail.defaultQueryEvaluationMode "STANDARD"
         ];
      ]
   ].
Direct Type inferencer
The Direct Type inferencer is an Sail Adapter that performs entailment on the
the class and instance inheritance hierarchy, asserting specific “shortcut”
properties (sesame:directType, sesame:directSubClassOf and
sesame:directSubPropertyOf) that allow querying for the direct type or
subclass of a resource.  Reasoning happens in a forward-chaining manner on
update operations, and the entailed statements are sent to be persisted by the
wrapped Sail.
The config:sail.type value is "openrdf:DirectTypeHierarchyInferencer".
Example configuration
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.

[] a config:Repository ;
   config:rep.id "example" ;
   rdfs:label "Example Memory store with direct type entailment" ;
   config:rep.impl [
      config:rep.type "openrdf:SailRepository" ;
      config:sail.impl [
         config:sail.type "openrdf:DirectTypeHierarchyInferencer";
         config:delegate [
             config:sail.type "openrdf:MemoryStore" ;
             ...
         ];
      ]
   ].
SHACL Sail
The SHACL Sail is a Sail Adapter that performs transaction validation using the Shapes Constraint Language SHACL. More information about the use of the SHACL Sail can be found in Validation with SHACL.
The config:sail.type value of the SHACL Sail is "rdf4j:ShaclSail". It takes the following configuration options:

config:shacl.parallelValidation (boolean): Enables parallel validation (optional).
config:shacl.undefinedTargetValidatesAllSubjects (boolean):
Specifies if an undefined target in a shape leads to validating all subjects (optional) deprecated
 .
config:shacl.logValidationPlans (boolean): Specifies if validation plans are sent to the logging framework (optional).
config:shacl.logValidationViolations (boolean): Specifies if shape violations are sent to the logging framework (optional).
config:shacl.ignoreNoShapesLoadedException (boolean): Specifies if the “no shapes loaded” error is ignored (optional) deprecated
.
config:shacl.validationEnabled (boolean): Specifies if transaction valudation is enabled by default (optional).
config:shacl.cacheSelectNodes (boolean): Specifies if select nodes are cached (optional).
config:shacl.globalLogValidationExecution (boolean): Specifies if validation execution details are sent to the logging framework (optional).
config:shacl.rdfsSubClassReasoning (boolean): Enables RDF Schema Subclass entailment (optional).
config:shacl.performanceLogging (boolean): Enables performance logging (optional).
config:shacl.serializableValidation (boolean): When enabled, combined with transactions using isolation level SNAPSHOT the validation guarantee is equivalent to using SERIALIZABLE without the performance impact.
config:shacl.eclipseRdf4jShaclExtensions (boolean): Enables RDF4J-specific SHACL extensions  (optional).
config:shacl.dashDataShapes (boolean): Enables use of DASH data shapes (optional).
config:shacl.validationResultsLimitTotal (integer): Specifies a limit on the total number of validation results sent in a validation report (optional). A values of -1 indicates no limit.
config:shacl.validationResultsLimitPerConstraint (integer): Specifies a limit on the number of validation results sent per constraint in a validation report (optional). A values of -1 indicates no limit.

Example configuration
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.

[] a config:Repository ;
   config:rep.id "example" ;
   rdfs:label "Example Memory store with SHACL validation" ;
   config:rep.impl [
      config:rep.id "openrdf:SailRepository" ;
      sr:sail.impl [
         config:sail.type "rdf4j:ShaclSail";
         config:shacl.parallelValidation true;
         config:shacl.validationResultsLimitTotal 300;
         config:delegate [
             config:sail.type "openrdf:MemoryStore" ;
             ...
         ];
      ]
   ].
Migrating old configurations
Since RDF4J 4.3.0, the configuration vocabulary has been updated and simplified:

we have removed all references to the (now defunct) openrdf.org domain, replacing it with rdf4j.org
we have unified the vocabularies for the various modules and components into a single namespace.

To illustrate what changes are required, we first show a legacy configuration for a repository using a native store backend:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix rep: <http://www.openrdf.org/config/repository#>.
@prefix sr: <http://www.openrdf.org/config/repository/sail#>.
@prefix sail: <http://www.openrdf.org/config/sail#>.
@prefix ns: <http://www.openrdf.org/config/sail/native#>.
@prefix sb: <http://www.openrdf.org/config/sail/base#>.

[] a rep:Repository ;
    rep:repositoryID "mystore" ;
    rdfs:label "my native store" ;
    rep:repositoryImpl [
        rep:repositoryType "openrdf:SailRepository" ;
        sr:sailImpl [
            sail:sailType "openrdf:NativeStore" ;
            sail:iterationCacheSyncThreshold "10000";
            ns:tripleIndexes "spoc,posc" ;
            sb:defaultQueryEvaluationMode "STANDARD"
        ]
    ].
To migrate this config to the new vocabulary, we first replace all custom namespace prefixes with a single new one:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.
Each attribute and vocabulary value in the actual data needs to be rewritten. The old rep, sr, sail, ns and sb namespace prefixes all need to be replaced with the config prefix. Additionally, the local names of each attribute need to be prepended with the shortname of the component to which they belong - and we have taken the opportunity to shorten/clean up the actual local names as well. For example, rep:repositoryImpl has become config:rep.impl, and ns:tripleIndexes now is config:native.tripleIndexes.
The fully rewritten configuration looks like this:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix config: <tag:rdf4j.org,2023:config/>.

[] a config:Repository ;
    config:rep.id "mystore" ;
    rdfs:label "my native store" ;
    config:rep.impl [
        config:rep.type "openrdf:SailRepository" ;
        config:sail.impl [
            config:sail.type "openrdf:NativeStore" ;
            config:sail.iterationCacheSyncThreshold "10000";
            config:native.tripleIndexes "spoc,posc" ;
            config:sail.defaultQueryEvaluationMode "STANDARD"
        ]
    ].
Note that we have not (yet) renamed the type identifier literals openrdf:SailRepository and openrdf:NativeStore. For more details we refer you to the CONFIG javadoc
.

  

     
      
        
          

  Table of Contents

  
  
    Repository configuration
      
        SPARQLRepository
          
            Example configuration
          
        
        HTTPRepository
          
            Example configuration
          
        
        SailRepository
          
            Example configuration
          
        
        DatasetRepository
          
            Example configuration
          
        
        ContextAwareRepository
          
            Example configuration
          
        
      
    
    Sail Configuration
      
        Base Sails
          
            Memory Store
            Native Store
            Elasticsearch Store
          
        
        Sail Adapter implementations
          
            RDF Schema inferencer
            Direct Type inferencer
            SHACL Sail
          
        
      
    
    Migrating old configurations\n\n\n\nSesame to Eclipse RDF4J Migration
    

  
  Eclipse RDF4J is the successor of the OpenRDF Sesame project. The RDF4J framework and tools offer the same functionality, and will continue to be maintained and improved by the same team of developers as Sesame was, under Eclipse stewardship. For any users who wish to migrate their existing projects from Sesame to RDF4J (and we certainly urge you to do so quickly), here’s an overview of what has changed.
Migrating from Sesame 4
The RDF4J 2.0 code is based off of the latest stable Sesame release, 4.1.2. This means that migrating from Sesame 4 to RDF4J 2 is fairly straightforward.
Changes for Java Programmers
Maven artifacts
All RDF4J Maven artifacts have a new groupId as well as a new artifactId. Fortunately, the rename is very simple:

the groupId for all artifacts has changed from org.openrdf.sesame to org.eclipse.rdf4j;
the artifactIds all have a different name prefix: sesame-<...> has become rdf4j-<...>.

For example, the Maven artifact for the Repository API in Sesame was:
<dependency>
   <groupId>org.openrdf.sesame</groupId>
   <artifactId>sesame-repository-api</artifactId>
</dependency>
In RDF4J, this same artifact is identified as follows:
<dependency>
   <groupId>org.eclipse.rdf4j</groupId>
   <artifactId>rdf4j-repository-api</artifactId>
</dependency>
Package renaming
Although Java class and method names have (almost) all remained the same, the package names of all RDF4J code has been changed. To upgrade your Java code to use these new package names, you will need to do the following replacements:

org.openrdf.* becomes org.eclipse.rdf4j.*
info.aduna.* becomes org.eclipse.rdf4j.common.*

If you are using the Eclipse IDE, a simple way to achieve this is to make sure your project has the new RDF4J libraries on the build path (and the old Sesame libraries are removed), and then use “Source” -> “Organize imports” (Ctrl+Shift+O) to replace old package names with new ones.
Note that in some cases Eclipse may need you to decide between two or more possibilities. In this case a dialog will pop up, and you can pick the correct candidate.
Alternatively, you can of course just run a global search-and-replace over your code to update your import statements.
Upgrading Sesame Server to RDF4J Server
Sesame Server is now called RDF4J Server. When upgrading Sesame Server to RDF4J Server, you need to take the following into account.
Server URL change
The default server URL for the RDF4J Server is http://localhost:8080/RDF4J-server. This is different from the default server URL for Sesame Server, which was http://localhost:8080/openrdf-sesame.
Migrating your data: Server Data Directory change
Sesame Server by default stores its data (configuration, log files, as well as the actual databases) in a directory %APPDATA%\Aduna\OpenRDF Sesame (on Windows), $HOME/.aduna/openrdf-sesame (on Linux), or $HOME/Library/Application Support/Aduna/OpenRDF Sesame (on Mac OSX).
RDF4J server stores its data in a different location: %APPDATA%\RDF4J\Server (Windows), $HOME/.RDF4J/Server (Linux), or $HOME/Library/Application Support/RDF4J/Server (on Mac OSX). Please note that RDF4J Server will not automatically detect existing data from an old Sesame Server installation.
This means that if you wish to migrate your data, you will have to manually copy over the data. This can be done quite easily as follows:

Start your new RDF4J Server instance, make sure it creates its initial data directory, then stop it again;
Stop your old Sesame Server;
Delete the RDF4J/Server/repositories subdirectory that was just created by RDF4J Server;
Copy the repositories directory from your old Sesame Server installation to replace the directory you deleted in the previous step;
Restart RDF4J Server.

Upgrading to RDF4J Workbench
Like RDF4J Server, RDF4J Workbench also uses a different data directory from its predecessor. The only thing RDF4J Workbench stores in its data directory is saved queries, so if you haven’t used this functionality you can safely skip this.
To migrate your saved queries from Sesame Workbench to RDF4J Workbench, do the following:

Start your new RDF4J Workbench instance, make sure it creates its initial data directory, then stop it again;
Stop your old Sesame Workbench;
Delete the RDF4J/Workbench/queries subdirectory that was just created by RDF4J Workbench;
Copy the queries directory from your old Sesame Workbench installation to replace the directory you deleted in the previous step;
Restart RDF4J Workbench.

Migrating from Sesame 2
RDF4J is based on the Sesame 4 code base, which is a major new release of the framework with significant changes in its core APIs, compared to Sesame 2. Here we give a quick overview of the major changes that you will need to take into account (in addition to the changes documented in the previous sections) when upgrading your project from Sesame 2 to RDF4J.
RDF4J 2.0 requires Java 8
If you use Sesame components in your own Java application and you wish to upgrade, you will have to make sure you are using a Java 8 compiler and runtime environment.
If you are using the Sesame Server and/or Sesame Workbench applications, you will need to upgrade the Java Runtime Environment (JRE) to Java 8 before upgrading to the new versions.
If you, for whatever reason, really can not upgrade to Java 8, you can use RDF4J 1.0, instead of 2.0. RDF4J 1.0 is a backport of the RDF4J code compatible with Java 7. However, to be clear: RDF4J 1.0 is not identical to Sesame 2! Many of the changes described here for RDF4J 2.0 also hold for RDF4J 1.0 – we merely adapted the code to take out Java 8-specific features such as Optionals and lambda expressions.
Statement equals method now includes context
In Sesame 2, the method Statement.equals() was defined to consider two statements equal if their subject, predicate, and object were identical, disregarding any context information. In RDF4J, this was changed: the context field is now included in the equality check.  As an example of what this means, see the following code:
  Statement st1 = vf.createStatement(s1, p1, o1);
  Statement st2 = vf.createStatement(s1, p1, o1, c1);
  System.out.println(st1.equals(st2));
In Sesame 2, the above code would print out true. In RDF4J, it prints out false.
Use of java.util.Optional in Model API
In several places in the Model API, RDF4J (2.0) uses java.util.Optional return types, where it previously returned either some value or null.
As one example of this, Literal.getLanguage(), which return the language tag of an RDF literal (if any is defined) now has Optional<String> as its return type, instead of just String. As a consequence, if you previously had code that retrieved the language tag like this:
  String languageTag = literal.getLanguage();
  if (languageTag != null) {
        System.out.println("literal language tag is " + languageTag);
  }
You will need to modify slightly, for example, like this:
  String languageTag = literal.getLanguage().orElse(null);
  if (languageTag != null) {
        System.out.println("literal language tag is " + languageTag);
  }
Or more drastically:
  literal.getLanguage().ifPresent(tag -> System.out.println("literal language tag is " + tag));
For more information about how to effectively make use of Optional, see this article by Oracle.
RDBMS Sail removed
The RDBMS Sail (that is, Sesame storage support for PostgreSQL and MySQL), which was deprecated since Sesame release 2.7.0, has been completely removed in RDF4J. If you were still using this storage backend as part of your project, you will need to switch to a different database type before upgrading or look into third-party implementations that may still support those databases.
Deprecation, Deprecation, Deprecation
Since Sesame 2, we have cleaned up interfaces, renamed methods and classes, and just generally streamlined a lot of stuff. In most cases we have done this in a way that is backward-compatible: old class/method/interface names have been preserved and can still be used, so you will not immediately need to completely change your code.
However, they have been marked deprecated. This means that we intend to drop support for these older names in future RDF4J releases. In every case, however, we have extensively documented what you should do in the API Javadoc: each deprecated method or class points to the new alternative that you should be using. Upgrade at your leisure, just remember: better sooner than later.
API Changes in RDF4J compared to Sesame 2
Compared to Sesame 2, RDF4J is a significantly improved version of the framework. Several of the core APIs and interfaces have been improved to make them easier to use, and to make full use of Java 7/8 features. Here, we outline some of these improvements in more detail.
Unchecked Exceptions
All exceptions thrown by RDF4J are unchecked: they all inherit from java.lang.RuntimeException. This means that you as a developer you are now longer forced to catch (or throw) any exceptions that inherit from org.eclipse.rdf4j.RDF4JException, such as RepositoryException, QueryEvaluationException, or RDFParseException .
Of course, you still can catch these exceptions if you want, but it means that in cases where you as a programmer are certain that an exception would never occur in practice, you can just ignore the exception, instead of having to write verbose try-catch-finally blocks everywhere. In other words: we are shifting the responsibility to you as a programmer to take care you write robust code. Be assured that all exceptions that can be thrown will still be properly documented in the Javadoc.
AutoCloseable results and connections
In RDF4J, all instances of both CloseableIteration (the root interface of all query result types, including resource and statement iterators, SPARQL query results) and RepositoryConnection (the interface for communicating with a Repository) inherit from java.lang.AutoCloseable.
This means that instead of explicitly having to call the close() method on these resources when you’re done with them, you can now use the so-called try-with-resources construction. For example, instead of:
  RepositoryConnection conn = repo.getConnection();
  try {
      // do something with the connection here
  }
  finally {
      conn.close();
  }
You can now do:
  try (RepositoryConnection conn = repo.getConnection()) {
          // do something with the connection
  }
Use of long instead of int for parser line and column references
The ParseErrorListener and ParseLocationListener parser interfaces, and the QueryResultParseException and RDFParseException exception classes, now accept and return long instead of int. This enables the parsing of much larger files without being concerned about numeric overflow in tracking.
Fluent APIs for RDFParser and QueryResultParser
The RDFParser and QueryResultParser APIs have been enhanced to provide fluent construction and configuration:
  Model rdfStatements = new LinkedHashModel();
  ParseErrorCollector errorCollector = new ParseErrorCollector();
  RDFParser aParser = Rio.createParser(RDFFormat.TURTLE)
                                 .setRDFHandler(new StatementCollector(rdfStatements))
                                 .setParseErrorListener(errorCollector);
  try {
      aParser.parse(myInputStream);
  } catch (RDFParseException e) {
      // log or rethrow RDFParseException
  } finally {
      // Examine any parse errors that occurred before moving on
      System.out.println("There were "
              + errorCollector.getWarnings().size()
              + " warnings during parsing");
      System.out.println("There were "
              + errorCollector.getErrors().size()
              + " errors during parsing");
      System.out.println("There were "
              + errorCollector.getFatalErrors().size()
              + " fatal errors during parsing");
  }

  QueryResultCollector handler = new QueryResultCollector();
  ParseErrorCollector errorCollector = new ParseErrorCollector();
  QueryResultParser aParser = QueryResultIO.createTupleParser(TupleQueryResultFormat.SPARQL)
                                         .setQueryResultHandler(handler)
                                         .setParseErrorListener(errorCollector);
  try {
      aParser.parseQueryResult(myInputStream);
  } catch (QueryResultParseException e) {
      // log or rethrow QueryResultParseException
  } finally {
      // Optionally examine any parse errors that occurred before moving on
      System.out.println("There were "
              + errorCollector.getWarnings().size()
              + " warnings during parsing");
      System.out.println("There were "
              + errorCollector.getErrors().size()
              + " errors during parsing");
      System.out.println("There were "
              + errorCollector.getFatalErrors().size()
              + " fatal errors during parsing");
  }
The QueryResultParser has also been updated to allow many of the operations that RDFParser allows including methods such as setParseErrorListener and setParseLocationListener, and the set method that is a shortcut for getParserConfig().set. Although RDFParser and QueryResultParser are still distinct at this point, in the future they may either be merged or have a common ancestor and these changes are aimed at making that process smoother for both developers and users.
Lambdas and the Stream API (RDF4J 2.0 only)
Java 8 offers two powerful new features in the core language: lambda expressions, and the Stream API. RDF4J 2.0 offers a number of improvements and new utilities that allow you to leverage these features. In this section we show a few simple usage examples of these new utilities.
Stream-based processing of results
In RDF4J,  results from queries (or from any Repository API retrieval operation) are returned in a lazily-evaluating iterator-like object named a CloseableIteration. It has specific subclasses (such as RepositoryResult, GraphQueryResult, and TupleQueryResult), but the basis for them all is identical. RDF4J 2.0 offers a number of ways to more easily interact with such results, by converting the source iteration to a Stream object.
For example, when post-processing the result of a SPARQL CONSTRUCT query (for example to filter out all triples with ‘a’ as the subject), the classic way to do it would be something like this:
  GraphQuery gq = conn.prepareGraphQuery(QueryLanguage.SPARQL, "CONSTRUCT ... ");
  GraphQueryResult gqr = gq.evaluate();
  List<Statement> aboutA = new ArrayList<Statement>();
  try {
    while (gqr.hasNext()) {
        Statement st = grq.next();
        if (st.getSubject().equals(a)) {
              aboutA.add(st);
        }
    }
  } finally {
     gqr.close();
  }
In RDF4J, we can do this shorter, and more elegantly, using the new QueryResults.stream() method and its support for lambda-expressions:
  GraphQuery gq = conn.prepareGraphQuery("CONSTRUCT ... ");
  GraphQueryResult gqr = gq.evaluate();
  List<Statement> aboutA = QueryResults.stream(gqr)
                     .filter(s -> s.getSubject().equals(a))
                     .collect(Collectors.toList());
Note that by using QueryResults.stream() instead of manually iterating over our result, we no longer have to worry about closing the query result when we’re done with it, either: the provided Stream automatically takes care of this when it is either fully exhausted or when some exception occurs.
The same trick also works for results of SELECT queries:
  TupleQuery query = conn.prepareTupleQuery("SELECT ?x ?c WHERE { ... } ");
  TupleQueryResult result = query.evaluate();
  // only get those results where c is equal to foaf:Person
  List<BindingSet> filteredResults = QueryResults.stream(result)
                        .filter(bs -> bs.getValue("c").equals(FOAF.PERSON))
                        .collect(Collectors.toList());
Stream-based querying and transaction handling
In addition to additional Stream-based utilities for results, RDF4J also offers various utilities for more convenient handling of queries and transactions, by means of the Repositories utility class.
As a simple example, suppose we want to open a connection, fire off a SPARQL CONSTRUCT query, collect the results in a Model, and then close the connection. The ‘classic’ way to do this is as follows:
  RepositoryConnection conn = rep.getConnection();
  try {
     GraphQuery query = conn.prepareGraphQuery(QueryLanguage.SPARQL, "CONSTRUCT WHERE {?s ?p ?o }");
     Model results = QueryResults.asModel(query.evaluate());
  }
  finally {
     conn.close();
  }
Using RDF4J lambda/Stream support, we can now do all of this in a single line of code:
  Model results = Repositories.graphQuery(rep, "CONSTRUCT WHERE {?s ?p ?o} ",
                   gqr -> QueryResults.asModel(gqr));
Note that although in this example we only do some very basic processing on the result (we convert the result of the query to a Model object), we can write an arbitrarily complex function here to fully customize how the result is processed, and fully control the type of the returned object as well.
The Repositories utility really comes into its own when used for update transactions. It takes care of opening a connection, starting a transaction, and properly committing (or rolling back if an error occurs).
As an example, this is the ‘classic’ way to add two new statements to the repository, using a single transaction:
  RepositoryConnection conn = rep.getConnection();
  try {
     conn.begin();
     conn.add(st1);
     conn.add(st2);
     conn.commit();
  }
  catch (RepositoryException e) {
     conn.rollback();
     throw e;
  }
  finally {
     conn.close();
  }
And this is the new lambda-based equivalent:
  Repositories.consume(rep, conn -> {
      conn.add(st1);
      conn.add(st2);
  });


  

     
      
        
          

  Table of Contents

  
  
    Migrating from Sesame 4
      
        Changes for Java Programmers
          
            Maven artifacts
            Package renaming
          
        
        Upgrading Sesame Server to RDF4J Server
          
            Server URL change
            Migrating your data: Server Data Directory change
          
        
        Upgrading to RDF4J Workbench
      
    
    Migrating from Sesame 2
      
        RDF4J 2.0 requires Java 8
        Statement equals method now includes context
        Use of java.util.Optional in Model API
        RDBMS Sail removed
        Deprecation, Deprecation, Deprecation
        API Changes in RDF4J compared to Sesame 2
          
            Unchecked Exceptions
            AutoCloseable results and connections
            Use of long instead of int for parser line and column references
            Fluent APIs for RDFParser and QueryResultParser
            Lambdas and the Stream API (RDF4J 2.0 only)\n\n\n\nInfo for RDF4J Developers
    

  
  
    
        
          
          Developer workflow and project managementIn this document the Eclipse RDF4J project workflow and developer best practices are explained. It contains information on how to create branches, tag releases, manage pull requests, create and schedule issues, and so on.
          
          RDF4J merge strategyRDF4J values a clean but accurate commit history on our main branches, where
commits are meaningfully described and linked back to the issue that they
address. To achieve this, we merge everything using merge-commits, and we may
ask you to squash your pull request branch before we
merge it.
          
          Release managementThis document outlines how to create a new release of RDF4J.
          
          Squashing CommitsWhen submitting a pull request to RDF4J, we sometimes ask that you squash your commits, to clean up the commit history a bit. Here we explain a simple way to do that.
          
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nDeveloper Workflow and Project Management
    

  
  In this document the Eclipse RDF4J project workflow and developer best practices are explained. It contains information on how to create branches, tag releases, manage pull requests, create and schedule issues, and so on.
Some of this information is targeted specifically at the project lead(s), other information is relevant to every committer.
Semantic Versioning
RDF4J strives to apply Semantic Versioning principles to its development:

We use a MAJOR.MINOR.PATCH versioning template.
A PATCH release (2.2.1, 2.2.2, etc.) is a release that contains only bug fixes that are binary compatible and source compatible.
A MINOR release (2.0.0, 2.1.0, 2.2.0, etc.) is a release that can contain improvements and new features but makes no binary-incompatible changes to existing functionality.
A MAJOR release (1.0.0, 2.0.0, 3.0.0, etc) is a release that can contain changes to the public API that are not compatible.

We allow changes to public or protected methods/classes/interfaces in minor releases under the following conditions:

any renamed interface is declared an extension of the old interface. The old interface is marked deprecated with Javadoc containing a reference to the new name;
any renamed class is declared a superclass of the old class. The old class is marked deprecated with Javadoc containing a reference to the new name;
any renamed member is added next to the old member name. The old member is declared deprecated with Javadoc containing a reference to the new name.
any class, interface or method that is annotated with @Experimental or @InternalUseOnly is not considered part of the public API, and may be changed in a minor release.

For patch releases we never allow changes in the public API, unless the change is specifically to fix a bug that aligns the actual behavior of the code with the publicly documented behavior.
The main branches (main and develop) use a SNAPSHOT version number to indicate that they are snapshots on the road to the next version. The main version always has the same major and minor number as the latest release, with the patch version incremented by one: for example if the latest release was 3.1.0, the main version will be 3.1.1-SNAPSHOT. The develop version uses the next expected major/minor release number, for example 3.2.0-SNAPSHOT.
Workflow
Every issue, no matter how small, gets its own issue
ticket, and its own branch while
under development. The milestone label of the issue is set to the planned
release version for the issue, but that could change by the time a PR is
merged. Issue branch names are always prefixed with GH-<issuenumber>-,
followed by one or two dash-separated keywords for the issue.
For example: GH-1664-transformation-servlet is the branch for a fix for issue
GH-1664, which has to do with the transformation servlet.
RDF4J uses a git branching model where collaborative feature development takes
place on branches from the develop branch. This is where all development for
the next (minor or major) release happens. The main branch is reserved for
small bug fixes (to be released in patch/service releases) only.
Once a issue is complete and tested, a Pull Request (PR) should be created
for peer review. Like the feature branch to which it corresponds, a Pull
Request should be a self-contained change, that is it fixes a single issue.
Don’t be tempted to fix several unrelated issues in a single PR please.
The Pull Request description should start with a link to the
issue that is addressed by the PR. If the issue is a new feature or improvment,
the PR should target the develop branch. If the issue is a bug fix, the PR
should be branched from (and target) the main branch.
Tip: when starting work on an issue, and you are unsure if it will be a new
feature or “just” a bug fix, start by branching from the main branch. It
will always be possible to merge your issue branch into develop later if
necessary. However, if you start from develop, merging into main will not
be possible, and you’re therefore committed to the next minor/major release.
RDF4J uses ‘merge-commits’ as its pull request merge strategy. We aim to
achieve a clean but accurate history. Read more about our strategy and the
motivation for it in this article: RDF4J merge
strategy.
For step-by-step instructions on how to create contributions, see the contributor guidelines.
Further reading
Some generic sources of information about projects hosted by Eclipse:

The Eclipse Project Handbook
The Eclipse Development Process
Committer Cheat Sheet


  

     
      
        
          

  Table of Contents

  
  
    Semantic Versioning
    Workflow
    Further reading\n\n\n\nRDF4J Merge Strategy
    

  
  RDF4J values a clean but accurate commit history on our main branches, where
commits are meaningfully described and linked back to the issue that they
address. To achieve this, we merge everything using merge-commits, and we may
ask you to squash your pull request branch before we
merge it.
We use merge-commits exclusively to merge pull requests into our main branches
as this preserves the history of changes and who made those changes accurately.
You as a contributor are completely free to use rebasing, squashing or merging
on your own branches as you see fit, of course - as long as you make sure that
history of your branch is clean when it’s time to merge your PR.
Note: we previously experimented with using ‘squash-and-merge’ as our Pull
Request strategy. The advantage of this was that it kept the main branch
history nicely linear, and automatically squashes all changes in a Pull Request
into a single commit. However, squash-and-merge sometimes overwrites the Author
field of a commit, which introduces problems in terms of IP provenance. For
that reason, we have decided to switch back to a simpler ‘merge-commit’
strategy. See also Github issue 2011.
Self-contained changes, pull requests, and commits
We define a self-contained change as a change that addresses a single issue,
addresses it completely, and does not also address other issues.
We expect every pull request to be a self-contained change. Note that that does
not mean that a pull request can only contain a single commit: it can have
several commits that together form a self-contained change.
We do, however, prefer fewer commits before merging, so if you have created a
long list of commits on our branch, we may ask you to squash it first.
Commit messages
We prefer every commit message to be descriptive: it should start with the
github issue number to which it relates, then have a short one line description
that details the specific change.
Examples of good commit messages:

“GH-1234 else condition no longer hits NPE in MyFancyClass”
“GH-666 added test for corner case where user inputs negative number”

Examples of poor commit messages:

“typo”
“GH-666 typo”
“GH-1234 fixed the problem”

We prefer meaningful commits because:

it makes reviewing the pull request easier;
after your PR has been merged, your commit messages become part of the main branch’s history, and having each commit linked to an issue and meaningfully described makes it easier to figure what got changed where and why.

That said, if occassionally a less “perfect” commit message slips through, that’s
fine. We’re all human.

  

     
      
        
          

  Table of Contents

  
  
    Self-contained changes, pull requests, and commits
      
        Commit messages\n\n\n\nRelease Management
    

  
  This document outlines how to create a new release of RDF4J.
The simple way: using the release script
The script scripts/release.sh is a shell-script that (almost) fully automates the handling of releases. It creates branches, sets correct version numbers, builds and uploads artifacts, etc. It gives you several prompts along the way to guide you through the process.
The release script should always be run from the main branch.
If for whatever reason, you wish to manually create a release instead, the following sections detail the manual process.
Patch releases
Patch releases are created by branching the main branch into a release branch, and
when complete, tagging this release branch with the version number before
release deployment. Once release deployment is complete, the release branch
is deleted.
IMPORTANT: the main branch is always in a release ready state (build
passes, no new features, docs are up to date), so a patch release from the
main branch can be done in an ad-hoc fashion, without the need for a formal
review.
Plans to do a patch release are announced by the project lead on the
rdf4j-dev@eclipse.org mailinglist,
usually about a week in advance, with an open invitation for contributors to
propose additional fixes to include, which are done as Pull Requests to the
main branch.
Creating a patch release branch
Any fixes to be included in a patch release must be merged into the main
branch first.  A patch release branch should differ from the main branch,
at the time of release, only by the version number - a patch release branch has
a patch number version, while the main branch has a SNAPSHOT version.  To
create a patch release branch, follow these steps:


Check out the main branch.
E.g. if we’re preparing a release 2.2.1, the main branch will have the version 2.2.1-SNAPSHOT:
git checkout main


Create a new release branch, named releases/<version>:
git checkout -b releases/2.2.1


Fix maven version numbers in the release branch. We need to set the project version to 2.2.1 by using:
mvn versions:set

This will ask for the new version number. Enter 2.2.1 to indicate that this is the 2.2.1 release.
After this is done, execute
mvn versions:commit

which will remove backup files.
Finally, commit the version number changes:
git commit -s -a -m "release 2.2.1"


Tag the version and push tag and branch upstream:
git tag 2.2.1
git push -u origin releases/2.2.1
git push origin 2.2.1


Prepare the release branch for the next iteration:
mvn versions:set and enter 2.2.2-SNAPSHOT as the new snapshot number
mvn versions:commit
git commit -s -a -m "next development iteration"
git push


Finally, create a pull request to merge the release branch back into main. Like branch sync PRs, this PR will be merged by means of a merge-commit, rather than the default ‘squash and merge’, so as not to lose the version-tagged commit.


Hotfix releases
Hotfix release are patch releases that target a prior minor version (not the
latest stable release). These are needed when a critical bug was found in a
production deployment using an earlier version.
A hotfix release use a preceding release as its basis. This means we need to create a release branch not by simply branching from the current main branch, but by branching from a specific release tag. To create a patch release branch, follow these steps:


Check out the tag of the previous release. E.g. if we’re preparing a release 2.1.6, the preceding release is 2.1.5, so we do:
git checkout 2.1.5


Create a new release branch, named releases/<version>:
git branch releases/2.1.6


Fix maven version numbers in the release branch. We need to set the project version to 2.1.6-SNAPSHOT by using:
mvn versions:set

This will ask for the new version number. Enter 2.1.6-SNAPSHOT to indicate that this is the development branch for the upcoming 2.1.6 release. After this is done, execute
mvn versions:commit

which will remove backup files. Finally, commit the version number changes:
git commit -s -a -m "release branch for 2.1.6"


Push the newly created branch, as follows:
git push origin releases/2.1.6


Bug fixes are typically added to a hotfix release branch by cherry-picking the relevant commits from the main branch.
This works as follows:


Check out the patch release branch.


In the git commit history, identify the commit for the fix you wish to add to the release. You can usually easily find this by looking at git commit message, which should start with the issue nmber.


Add this fix to the release branch by executing the following command:
git cherry-pick <commit-SHA>


Once all fixes are applied to the release branch, and the build is stable (NB verify by executing mvn clean verify), we can tag and finalize the release:


Set the maven pom version numbers. We need to set the project version to from 2.1.6-SNAPSHOT to 2.1.6 by using:
mvn versions:set

This will ask for the new version number. Enter 2.1.6 to indicate that this is the actual code for 2.1.6 release. After this is done, execute
mvn versions:commit

which will remove backup files. Finally, commit the changes and push:
git commit -s -a -m "patch release 2.1.6"
git push


Tag the version and push the tag upstream:
git tag 2.1.6
git push origin 2.1.6


Once the release is complete, the hotfix branch can to be deleted. Although this can of course be done from the command line, it is cumbersome, and we recommend using a decent Git client (like SourceTree) that can do this for you.
Note that, although the branch is deleted, the release tag is still in place, for future use of further hotfix releases.
Release distribution deployment
RDF4J has two separate distributions:

the SDK and onejar archives, downloadable via http://www.rdf4j.org/download .
the Maven artifacts, available via The Central Repository.

RDF4J has four separate announcements:

the github releases tab,
the RDF4J download page,
the RDF4J.org website, and
the rdf4j-user mailing list.

Building and uploading the release
We use the Eclipse RDF4J Jenkins CI instance to build and deploy new releases to the Central Repository, as well as building and deploying the SDK archive and the onejar to the Eclipse download mirrors.
To do this, log in to Jenkins, and start the job named rdf4j-deploy-release-sdk.
The job will ask for the github release tag as an input parameters, e.g. ‘2.2.1’. It will automatically check out the release tag, build the project, and upload the SDK zip file and the onejar to the Eclipse download mirrors.
After successful completion, it will kick off a second job:
rdf4j-deploy-release-ossrh. This job will build all Maven artifacts and
upload them to OSS Sonatype.  After successful
upload, it will also automatically invoke synchronization with the Central
Repository.  Note that after successful completion, the artifacts may not be
available on the Central Repository for several hours.
Eclipse release reviews
At least once a year, the Eclipse Foundation requires a formal release review. We typically try to use a major or minor release for such a review.
We plan a reviewed release about 8 weeks in advance. At this stage, the final feature set is not etched in stone but a number of priority features/improvements is identified (via discussion on the mailinglist and/or via issue tracker comments and PRs) and scheduled. A first draft of a release plan is created by the project lead on the Eclipse RDF4J project site, and the necessary milestones are created in the issue tracker.
Review planning and application
Eclipse release review are announced in regular cycles, and always complete on the first or third Wednesday of each month. For this reason, we schedule our reviewed releases to happen on a first or third Thursday.
A release review runs for a week. Although mostly a formality, it does need some careful preparation and planning. It needs to be formally applied for, and this application in turn requires that several pieces of documentation are in order:

The project’s IP log needs to be filed and approved by the Eclipse legal team;
The IP log can be automatically generated and submitted to the legal team. Obtaining approval may require several days, so it’s good practice to submit this at least two weeks before the planned release date.
The project’s review documentation, part of the application, needs to be in order.

Typical review documentation can be a simple reiteration of the most important new features, a link to the issue tracker/release notes and documentation, and a remark about compatibility (if applicable). Once the review documentation is up, a mail needs to be sent to technology-pmc@eclipse.org to ask for approval. Here’s an example of such a message, which was to get approval for the RDF4J 2.2 release:
Dear PMC members,

Can I get your approval for RDF4J release 2.2, scheduled for February 2.

Release review info: https://projects.eclipse.org/projects/technology.rdf4j/reviews/2.2-release-review

Issue tracking the release: https://bugs.eclipse.org/bugs/show_bug.cgi?id=510577

Kind regards,

Jeen Broekstra

When IP log approval and review approval have been given, the review can be scheduled. To do this, emo@eclipse.org needs to be notified. This can happen through the eclipse project governance page (accessible through the project site), which will show a link at the top of the page for the planned release.
For more detailed information about the release review process, see the Eclipse Project Handbook.
Branching minor releases
Prior to a minor release, the develop branch is merged into the main branch
(along with the develop branch’s version). This will increment the main version to the latest major/minor SNAPSHOT version.
IMPORTANT: It is important that only features and fixes that have already been scheduled
for release (via PR milestone labels) be merged into the develop branch, so
that there is no confusion as to what will be included in the next minor release.
Once a minor release is published the develop minor version should be incremented to the next SNAPSHOT version and any approved features that are scheduled for this next minor
version should be merged into develop branch.
Optional: publish a docker image
The docker images on hub.docker.com are stored as part of the Eclipse organizational account.
Since this account is managed separately by the Eclipse Foundation,
only a limited number of committers will be granted access by the EMO.
Method 1: using the build script and docker push
Build the SDK ZIP file and docker image using the docker/build.sh script.
Both the Workbench and the server will be part of the same image.
Log into hub.docker.com:
docker login --username=yourhubusername
Push the image:
docker push eclipse/rdf4j-workbench:VERSION_TAG
VERSION_TAG is the version (tag) you want to push, e.g. 4.3.0
Note that hub.docker.com does not update the latest tag automatically,
the newly created image has also to be tagged latest and pushed to hub.docker.com.
Method 2: multi-platform docker image using buildx
Since the base image being used is available for multiple architectures,
it is quite easy to build a multi-platform image.
Currently the Workbench/Server image is made available for 64-bit AMD/Intel and ARM v8.
Check if Docker Buildx is installed on your system.
Build the SDK ZIP file using the docker/build.sh script mentioned above,
or download the SDK from https://rdf4j.org/download/ and store the ZIP as ignore/rdf4j.zip.
Log into hub.docker.com:
docker login --username=yourhubusername
Build and push the image (note the . at the end of the command):
docker buildx build --push --platform linux/arm64/v8,linux/amd64 --tag eclipse/rdf4j-workbench:VERSION_TAG .
VERSION_TAG is the version (tag) you want to push, e.g. 4.3.0

  

     
      
        
          

  Table of Contents

  
  
    The simple way: using the release script
    Patch releases
      
        Creating a patch release branch
      
    
    Hotfix releases
    Release distribution deployment
      
        Building and uploading the release
      
    
    Eclipse release reviews
      
        Review planning and application
        Branching minor releases
      
    
    Optional: publish a docker image
      
        Method 1: using the build script and docker push
        Method 2: multi-platform docker image using buildx\n\n\n\nSquashing Commits
    

  
  When submitting a pull request to RDF4J, we sometimes ask that you squash your commits, to clean up the commit history a bit. Here we explain a simple way to do that.
Squashing in five steps
On the command line, a relatively simple way to squash commits is as follows:

Make sure your local main and develop branches are up to date with the upstream.
Check out your pull request branch.
Run git rebase -i main (or git rebase -i develop if your branch started from the develop branch).

You should see a list of commits, each commit starting with the word pick.
Make sure the first commit says “pick” and change the rest from “pick” to “squash”.


Save and close the editor.
It will give you the opportunity to change the commit message. This is where you make sure the message is meaningful and starts with the issue number.
Save and close the editor again.
Then you have to force-push the final, squashed commit: git push --force-with-lease origin.

Example
Let’s say you have been working on an improvement, and you have a list of five
commits on your branch, which you are now ready to merge into the
develop branch. The commit log of your branch looks as follows:
* 43d2565 (HEAD -> GH-1234-my-feature-branch) fixed typo
* ce064f9 adjusted related class FooBar to be more efficient
* b135e03 oops forgot one
* 73e58a1 GH-1234 added feature: tests now succeed
* a0178bd GH-1234 added tests for my feature
You have some commits that have descriptions like “fixed typo” or “oops”. Some messages miss a reference to the issue number. These are the commits that you will want to adjust by squashing, and editing the commit message.
Before you start squashing, first make sure that your local main or develop branch (the branch that you want to merge your changes into) is up-to-date with the upstream:
git checkout develop
git pull

    
    
If you are working on a forked copy of the eclipse/rdf4j Github repository, you will need to make sure that your fork's main/develop branch is up to date with the original as well. See this Stackoverflow article for tips on how to handle this.




You can now switch back to your feature branch:
git checkout GH-1234-my-feature-branch
The second step is starting an “interactive rebase”, using the following command:
git rebase -i develop
In this command, develop identifies the branch against which we
want to rebase our current branch, the -i flag indicates that we want to
rebase interactively (that is, being asked what commits to keep, which ones to
squash, etc), and finally the --signoff flag is there to make sure that the
new “squashed” commits are correctly signed off.
When you execute this, git will open an editor with the following contents:
pick 43d2565 fixed typo in FooBar
pick ce064f9 adjusted related class FooBar to be more efficient
pick b135e03 oops forgot one
pick 73e58a1 GH-1234 added feature: tests now succeed
pick a0178bd GH-1234 added tests for my feature

# Rebase 43d2565..a0178bd onto c0fa78d (5 command(s))
#
# Commands:
# p, pick = use commit
# r, reword = use commit, but edit the commit message
# e, edit = use commit, but stop for amending
# s, squash = use commit, but meld into previous commit
# f, fixup = like "squash", but discard this commit's log message
# x, exec = run command (the rest of the line) using shell
# d, drop = remove commit
#
# These lines can be re-ordered; they are executed from top to bottom.
#
# If you remove a line here THAT COMMIT WILL BE LOST.
#
# However, if you remove everything, the rebase will be aborted.
#
# Note that empty commits are commented out
Notice the first five lines: these identify your commits, and the word ‘pick’ in front of each indicates that you want to keep that commit as-is. To define your squash operation, you edit these lines, changing the word ‘pick’ into something else (usually ‘squash’, or ‘fixup’), and then save and close the editor.
Squashing everything down to one commit
The simplest way to squash is just to stick everything into one commit. You do this by changing the word “pick” in every line except the first one to “squash”:
pick 43d2565 fixed typo in FooBar
squash ce064f9 adjusted related class FooBar to be more efficient
squash b135e03 oops forgot one
squash 73e58a1 GH-1234 added feature: tests now succeed
squash a0178bd GH-1234 added tests for my feature
After you save this change and close the editor, git will immediately open a new editor where you can make adjustments to the commit message for the new “squashed” commit. It will look like something like this:
# This is a combination of 5 commits.
# This is the 1st commit message:

fixed typo in FooBar

# This is the commit message #2:

adjusted related class FooBar to be more efficient

# This is the commit message #3:

oops forgot one

# This is the commit message #4:

GH-1234 added feature: tests now succeed

# This is the commit message #5:

GH-1234 added tests for my feature

# Please enter the commit message for your changes. Lines starting
# with '#' will be ignored, and an empty message aborts the commit.
#
# Date:      Tue Oct 13 10:01:24 2020 +1100
#
# interactive rebase in progress; onto dc0fa78d
As you can see, it has preserved all the original commit messages and put them in this single, new commit message, each on its own line. You can now edit this commit message if you wish.
One thing you will definitely need to do is edit the first line of the commit message. This will become the commit that encapsulates the entire change you made. It should therefore contain the issue number, and also have a description of the entire fix, not just the fact that it fixes a typo. You can edit that line, getting something like this:
# This is a combination of 5 commits.
# This is the 1st commit message:

GH-1234 added new feature ABC and adjusted FooBar

# This is the commit message #2:

adjusted related class FooBar to be more efficient

# This is the commit message #3:

oops forgot one

# This is the commit message #4:

GH-1234 added feature: tests now succeed

# This is the commit message #5:

GH-1234 added tests for my feature

# Please enter the commit message for your changes. Lines starting
# with '#' will be ignored, and an empty message aborts the commit.
#
# Date:      Tue Oct 13 10:01:24 2020 +1100
#
# interactive rebase in progress; onto dc0fa78
You can choose to keep the other commit messages in place or just remove them if
you think they add no value. In this case, they’re a bit messy, so you can clean it up
a little further, removing the lines about typos and small mistakes, and making
it a bit easier to read, ending up with something like this:
# This is a combination of 5 commits.
# This is the 1st commit message:

GH-1234 added new feature ABC and adjusted FooBar

- added feature and tests
- adjusted related class FooBar to be more efficient

# Please enter the commit message for your changes. Lines starting
# with '#' will be ignored, and an empty message aborts the commit.
#
# Date:      Tue Oct 13 10:01:24 2020 +1100
#
# interactive rebase in progress; onto dc0fa78
After you save and close, the cleaned up git log of our feature branch will look like this:
* d01376a (HEAD -> GH-1234-my-feature-branch) GH-1234 addded new feature ABC and adjusted FooBar
So your local commit history now looks clean, but you still need to push these
changes to your upstream branch. Because doing a rebase like this changes the
history (you have modified existing commits, after all), you will need to
“force-push” your changes:
git push --force-with-lease origin
Preserving more than one commit
Sometimes, you want to preserve more than one commit separately on your
feature branch. Let’s say that instead of sticking everything in one commit, you
want to keep the changes to the FooBar class and the actual new feature + test
code separate. You can do this by leaving multiple “pick” options in place.
For example:
pick 43d2565 fixed typo in FooBar
squash ce064f9 adjusted related class FooBar to be more efficient
pick b135e03 oops forgot one
squash 73e58a1 GH-1234 added feature: tests now succeed
squash a0178bd GH-1234 added tests for my feature
The first two commits (relating to the class FooBar) will be squashed into one new commit, and then the further three commits will all be squashed into one as well. Note that if you do this, git will open a new commit editor twice, one for each new squashed commit you’re adding, and you can adjust each new commit’s message accordingly. You can end up with something like this:
* d01376a (HEAD -> GH-1234-my-feature-branch) GH-1234 adjusted FooBar for better performance
* 30467d2  GH-1234 addded new feature ABC
Keeping your feature branch up to date: rebase vs merge
While you are working on a feature in your own branch, in parallel other changes
can be made on the main/develop branch, by other developers. It is sometimes
necessary that you bring your feature branch up-to-date with those changes, so
that you can reuse their work in your feature, or for example when you’ve both
been working on the same part of the code and there are conflicts to be
resolved.
Although in RDF4J we merge all feature branches into the main branches using
merge-commits, we recommend that you use git rebase instead of git merge to
bring your feature branch up-to-date. There are two large advantages to this:

it makes your feature branch “shorter” (the starting point of your branch
moves up), which makes the git history easier to read once your feature branch
has been merged;
it makes doing squashing a lot easier. When merging in changes through
merge-commits, especially when those changes involve resolving conflicts,
squashing later on becomes really difficult, as git will repeatedly ask you
to resolve the same merge conflict multiple times (this is caused by the way
rebase works - it “replays” commits one by one, and stops at each step if it
detects a conflict, even if you have in fact already resolved that conflict
in a later commit).

A potential disadvantage of rebasing is that, if you are working together with
another developer on the same branch, you need to inform them whenever you do a
rebase and a force-push, so that they can update their local copy. A rebase
“rewrites history”, which can cause problems if your co-developer is still
working with the “unrevised history” and adds their own commits on top of that,
instead of on top of your rewritten history. The trick is to communicate with
each other.
Phew, all of this is a lot of work, can’t you make this easier?
Unfortunately, at the end of the day, we really can’t: RDF4J is a large project and to be able to keep track of what changed when, and for what reason, we require a git history that is descriptive, and links commits back to the issue they tried to fix.
However, you can make things easier for yourself, with a few of these tips:

make sure that most of the commits you do are already descriptive in the first place, and perhaps squashing more than once if you are working on a large branch, to keep things “tidy as you go”.
Do not immediately push every commit that you do, so that you can amend your latest commit if you discover a typo or other small change, rather than adding a new separate commit.
If you need to sync your feature branch with the main or develop branch, consider using ‘git rebase’ instead of ‘git merge’, so that squashing later on does not become too difficult.


    
    
Practicing clean git commits is like brushing your teeth: yes, it's a bit of a chore, and you could use the time to do something more fun, but it's a lot nicer for the people around you (think coffee breath), and also better for you (think having to figure out why something was changed a year from now). And once you're used to doing it, it quickly becomes a habit.



  

     
      
        
          

  Table of Contents

  
  
    Squashing in five steps
    Example
      
        Squashing everything down to one commit
        Preserving more than one commit
      
    
    Keeping your feature branch up to date: rebase vs merge
    Phew, all of this is a lot of work, can’t you make this easier?\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nDocumentation
    

  
  
    
      
      
      Tutorials
      
        
            
              
              Getting started with RDF4J
              
              Starting a new Maven project in Eclipse
              
              Creating custom SPARQL functions
              
              Creating SPARQL Queries with the SparqlBuilder
              
          
      
      
      
      
      Programming with RDF4J
      
        
            
              
              Setting up your development environment
              
              The RDF Model API
              
              The Repository API
              
              Parsing and Writing RDF with Rio
              
              The LMDB Store
              
              Full-text indexing with the Lucene SAIL
              
              Reasoning and Validation with SPIN
              
              Validation with SHACL
              
              Federation with FedX
              
              Integration with Spring
              
              GeoSPARQL
              
              RDF-star and SPARQL-star
              
          
      
      
      
      
      Tools
      
        
            
              
              RDF4J Console
              
              RDF4J Server and Workbench
              
              Application directory configuration
              
              Repository configuration templates
              
          
      
      
      
      
      Reference
      
        
RDF4J REST API
RDF4J API Javadoc


            
              
              The SAIL API
              
              RDF4J Binary RDF Format
              
              Repository and SAIL configuration
              
              Sesame to Eclipse RDF4J migration
              
          
      
      
      
      
      Info for RDF4J Developers
      
        
            
              
              Developer workflow and project management
              
              RDF4J merge strategy
              
              Release management
              
              Squashing Commits
              
          
      
      
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nDocumentation
    

  
  
    
      
      
      Tutorials
      
        
            
              
              Getting started with RDF4J
              
              Starting a new Maven project in Eclipse
              
              Creating custom SPARQL functions
              
              Creating SPARQL Queries with the SparqlBuilder
              
          
      
      
      
      
      Programming with RDF4J
      
        
            
              
              Setting up your development environment
              
              The RDF Model API
              
              The Repository API
              
              Parsing and Writing RDF with Rio
              
              The LMDB Store
              
              Full-text indexing with the Lucene SAIL
              
              Reasoning and Validation with SPIN
              
              Validation with SHACL
              
              Federation with FedX
              
              Integration with Spring
              
              GeoSPARQL
              
              RDF-star and SPARQL-star
              
          
      
      
      
      
      Tools
      
        
            
              
              RDF4J Console
              
              RDF4J Server and Workbench
              
              Application directory configuration
              
              Repository configuration templates
              
          
      
      
      
      
      Reference
      
        
RDF4J REST API
RDF4J API Javadoc


            
              
              The SAIL API
              
              RDF4J Binary RDF Format
              
              Repository and SAIL configuration
              
              Sesame to Eclipse RDF4J migration
              
          
      
      
      
      
      Info for RDF4J Developers
      
        
            
              
              Developer workflow and project management
              
              RDF4J merge strategy
              
              Release management
              
              Squashing Commits
              
          
      
      
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nWhat's new
  
  
    
      
  
  
    
      Wed, Apr 16, 2025
      
        RDF4J 4.3.16 released 
      
      
        RDF4J 4.3.16 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.

        
      
    
  
  
  
    
      Tue, Apr 15, 2025
      
        RDF4J 5.1.3 released
      
      
        RDF4J 5.1.3 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
    
      Tue, Feb 11, 2025
      
        RDF4J 5.1.2 released
      
      
        RDF4J 5.1.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
View all news


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.
  
  
  


        
      
    
  

  What's new
  
  
    
      
  
  
    
      Wed, Apr 16, 2025
      
        RDF4J 4.3.16 released 
      
      
        RDF4J 4.3.16 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.

        
      
    
  
  
  
    
      Tue, Apr 15, 2025
      
        RDF4J 5.1.3 released
      
      
        RDF4J 5.1.3 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
    
      Tue, Feb 11, 2025
      
        RDF4J 5.1.2 released
      
      
        RDF4J 5.1.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
View all news\n\n\n\nRDF4J 4.3.16 Released
    

  
  
  Wed, Apr 16, 2025
  RDF4J 4.3.16 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\n4.3.16
    

  
  RDF4J 4.3.16 is a patch release that fixes 2 issues backported from RDF4J 5.
For a complete overview, see all issues fixed in 4.3.16.


  

     
      
        
          

  Table of Contents\n\n\n\nRDF4J 5.1.3 Released
    

  
  
  Tue, Apr 15, 2025
  RDF4J 5.1.3 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\n5.1.3
    

  
  RDF4J 5.1.3 is a patch release that fixes 6 issues.
For a complete overview, see all issues fixed in 5.1.3.
Acknowledgements
This release was made possible by contributions from Håvard M. Ottestad, Manuel Fiorelli, Jerven Bolleman, Piotr Sowiński, Matthew Nguyen, Wolfgang Schell and Ken Wenzel.


  

     
      
        
          

  Table of Contents

  
  
    
      
        Acknowledgements\n\n\n\nRDF4J 5.1.2 Released
    

  
  
  Tue, Feb 11, 2025
  RDF4J 5.1.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\n5.1.2
    

  
  RDF4J 5.1.2 is a patch release that fixes 1 issues.
For a complete overview, see all issues fixed in 5.1.2.


  

     
      
        
          

  Table of Contents\n\n\n\nNews
    

  
   
  
  
    
      
    
      Wed, Apr 16, 2025
      
        RDF4J 4.3.16 released
      
      
        RDF4J 4.3.16 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Apr 15, 2025
      
        RDF4J 5.1.3 released
      
      
        RDF4J 5.1.3 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Feb 11, 2025
      
        RDF4J 5.1.2 released
      
      
        RDF4J 5.1.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Jan 28, 2025
      
        RDF4J 5.1.1 released
      
      
        RDF4J 5.1.1 is now available. This is a patch release fixing 9 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Thu, Nov 21, 2024
      
        RDF4J 5.1.0 released
      
      
        RDF4J 5.1.0 is now available.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Nov 10, 2024
      
        RDF4J 5.0.3 released
      
      
        RDF4J 5.0.3 is now available. This is a patch release fixing 11 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Nov 10, 2024
      
        RDF4J 5.1.0 Milestone 1
      
      
        Milestone number 1 of the upcoming 5.1.0 release of RDF4J is now available for download.
RDF4J 5.1.0 is a minor release focusing on … .
This milestone build is not yet feature-complete, but we are putting it out to receive early feedback on all the improvements we have put in.
        
      
      
        [read more]
      
    
    

    
      
    
      Thu, Nov 7, 2024
      
        RDF4J 4.3.15 released
      
      
        RDF4J 4.3.15 is now available. This is a patch release fixing 4 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Wed, Oct 2, 2024
      
        RDF4J 4.3.14 released
      
      
        RDF4J 4.3.14 is now available. This is a patch release fixing 1 bug.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Fri, Aug 2, 2024
      
        RDF4J 5.0.2 released
      
      
        RDF4J 5.0.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
    
    
      
        ««
      
      
        «
      
      
        1
      
      
        2
      
      
        3
      
      
        4
      
      
        5
      
      
        »
      
      
        »»
      
    
  

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nWhat's new
  
  
    
      
  
  
    
      Wed, Apr 16, 2025
      
        RDF4J 4.3.16 released 
      
      
        RDF4J 4.3.16 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.

        
      
    
  
  
  
    
      Tue, Apr 15, 2025
      
        RDF4J 5.1.3 released
      
      
        RDF4J 5.1.3 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
    
      Tue, Feb 11, 2025
      
        RDF4J 5.1.2 released
      
      
        RDF4J 5.1.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
View all news


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.
  
  
  


        
      
    
  

  What's new
  
  
    
      
  
  
    
      Wed, Apr 16, 2025
      
        RDF4J 4.3.16 released 
      
      
        RDF4J 4.3.16 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.

        
      
    
  
  
  
    
      Tue, Apr 15, 2025
      
        RDF4J 5.1.3 released
      
      
        RDF4J 5.1.3 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
    
      Tue, Feb 11, 2025
      
        RDF4J 5.1.2 released
      
      
        RDF4J 5.1.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
View all news\n\n\n\nWhat's new
  
  
    
      
  
  
    
      Wed, Apr 16, 2025
      
        RDF4J 4.3.16 released 
      
      
        RDF4J 4.3.16 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.

        
      
    
  
  
  
    
      Tue, Apr 15, 2025
      
        RDF4J 5.1.3 released
      
      
        RDF4J 5.1.3 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
    
      Tue, Feb 11, 2025
      
        RDF4J 5.1.2 released
      
      
        RDF4J 5.1.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
View all news


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.
  
  
  


        
      
    
  

  What's new
  
  
    
      
  
  
    
      Wed, Apr 16, 2025
      
        RDF4J 4.3.16 released 
      
      
        RDF4J 4.3.16 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.

        
      
    
  
  
  
    
      Tue, Apr 15, 2025
      
        RDF4J 5.1.3 released
      
      
        RDF4J 5.1.3 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
    
      Tue, Feb 11, 2025
      
        RDF4J 5.1.2 released
      
      
        RDF4J 5.1.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
      
    
  
View all news\n\n\n\nNews
    

  
   
  
  
    
      
    
      Wed, Apr 16, 2025
      
        RDF4J 4.3.16 released
      
      
        RDF4J 4.3.16 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Apr 15, 2025
      
        RDF4J 5.1.3 released
      
      
        RDF4J 5.1.3 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Feb 11, 2025
      
        RDF4J 5.1.2 released
      
      
        RDF4J 5.1.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Jan 28, 2025
      
        RDF4J 5.1.1 released
      
      
        RDF4J 5.1.1 is now available. This is a patch release fixing 9 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Thu, Nov 21, 2024
      
        RDF4J 5.1.0 released
      
      
        RDF4J 5.1.0 is now available.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Nov 10, 2024
      
        RDF4J 5.0.3 released
      
      
        RDF4J 5.0.3 is now available. This is a patch release fixing 11 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Nov 10, 2024
      
        RDF4J 5.1.0 Milestone 1
      
      
        Milestone number 1 of the upcoming 5.1.0 release of RDF4J is now available for download.
RDF4J 5.1.0 is a minor release focusing on … .
This milestone build is not yet feature-complete, but we are putting it out to receive early feedback on all the improvements we have put in.
        
      
      
        [read more]
      
    
    

    
      
    
      Thu, Nov 7, 2024
      
        RDF4J 4.3.15 released
      
      
        RDF4J 4.3.15 is now available. This is a patch release fixing 4 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Wed, Oct 2, 2024
      
        RDF4J 4.3.14 released
      
      
        RDF4J 4.3.14 is now available. This is a patch release fixing 1 bug.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Fri, Aug 2, 2024
      
        RDF4J 5.0.2 released
      
      
        RDF4J 5.0.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
    
    
      
        ««
      
      
        «
      
      
        1
      
      
        2
      
      
        3
      
      
        4
      
      
        5
      
      
        »
      
      
        »»
      
    
  

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nRDF4J 5.1.1 Released
    

  
  
  Tue, Jan 28, 2025
  RDF4J 5.1.1 is now available. This is a patch release fixing 9 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\n5.1.1
    

  
  RDF4J 5.1.1 is a patch release that fixes 9 issues.
For a complete overview, see all issues fixed in 5.1.1.
Acknowledgements
This release was made possible by contributions from Håvard M. Ottestad, Andreas Schwarte and Benji Herber.


  

     
      
        
          

  Table of Contents

  
  
    
      
        Acknowledgements\n\n\n\nRDF4J 5.1.0 Released
    

  
  
  Thu, Nov 21, 2024
  RDF4J 5.1.0 is now available.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\n5.1.0
    

  
  RDF4J 5.1.0 is a minor release of the Eclipse RDF4J framework. Some highlights:

Stability and performance improvements in the FedX federation engine, including refinements to the bind join implementation for regular and OPTIONAL joins
Support for specifying the lucene document threshold at query time
VALUES for SparqlBuilder
Enable UpdateWithModelBuilder to delete triples
HttpClient timeout and pooling options
DOCUMENT_LOADER support for JSON-LD 1.1 parser
Upgrade to Tomcat 9 in docker image
Add support for Jetty 9 in docker image
Improve performance of SPARQLConnection#size() method
Adjust how SHACL validation of minCount and maxCount is handled for union graphs with duplicates
Support for DCAT v3

For a complete overview, see all issues fixed in 5.1.0.
Acknowledgements
This release was made possible by contributions from Bart Hanssens, Antoine Willerval, Håvard M. Ottestad, Florian Kleedorfer, Andreas Schwarte and Jerven Bolleman.


  

     
      
        
          

  Table of Contents

  
  
    
      
        Acknowledgements\n\n\n\nRDF4J 5.0.3 Released
    

  
  
  Sun, Nov 10, 2024
  RDF4J 5.0.3 is now available. This is a patch release fixing 11 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\n5.0.3
    

  
  RDF4J 5.0.3 is a patch release that fixes 11 issues.
For a complete overview, see all issues fixed in 5.0.3.
Acknowledgements
This release was made possible by contributions from Hannes Ebner, Håvard M. Ottestad, Frens Jan Rumph and Andreas Schwarte.


  

     
      
        
          

  Table of Contents

  
  
    
      
        Acknowledgements\n\n\n\nRDF4J 5.1.0 Milestone 1
    

  
  
  Sun, Nov 10, 2024
  Milestone number 1 of the upcoming 5.1.0 release of RDF4J is now available for download.
RDF4J 5.1.0 is a minor release focusing on … .
This milestone build is not yet feature-complete, but we are putting it out to receive early feedback on all the improvements we have put in.

issues fixed in 5.1.0 Milestone 1
issues planned for 5.1.0

Links

Download RDF4J


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nRDF4J 4.3.15 Released
    

  
  
  Thu, Nov 7, 2024
  RDF4J 4.3.15 is now available. This is a patch release fixing 4 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\n4.3.15
    

  
  RDF4J 4.3.15 is a patch release that fixes 4 issues.
For a complete overview, see all issues fixed in 4.3.15.


  

     
      
        
          

  Table of Contents\n\n\n\nRDF4J 4.3.14 Released
    

  
  
  Wed, Oct 2, 2024
  RDF4J 4.3.14 is now available. This is a patch release fixing 1 bug.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\n4.3.14
    

  
  RDF4J 4.3.14 is a patch release that fixes 1 issue.
For a complete overview, see all issues fixed in 4.3.14.


  

     
      
        
          

  Table of Contents\n\n\n\nRDF4J 5.0.2 Released
    

  
  
  Fri, Aug 2, 2024
  RDF4J 5.0.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
Links

Download RDF4J
release notes.


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\n5.0.2
    

  
  RDF4J 5.0.2 is a patch release that fixes 1 issue.
For a complete overview, see all issues fixed in 5.0.2.


  

     
      
        
          

  Table of Contents\n\n\n\nNews
    

  
   
  
  
    
      
    
      Wed, Jul 24, 2024
      
        RDF4J 4.3.13 released
      
      
        RDF4J 4.3.13 is now available. This is a patch release with backported fixes and improvements from 5.0.0 and 5.0.1.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Jul 9, 2024
      
        RDF4J 5.0.1 released
      
      
        RDF4J 5.0.1 is now available. This is a patch release fixing 8 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Fri, Jun 21, 2024
      
        RDF4J 5.0.0 released
      
      
        We are very excited to announce the release of RDF4J 5.0.0! RDF4J 5.0.0 is a major release of the RDF4J framework with many new features and improvements.
Highlights include:
 JSON-LD 1.1 support Many improvements to FedX Improved SHACL validation with support for sh:closed and pairwise validation Stability and performance improvements to the LmdbStore Upgrade of MapDB  More queries with intermediary results are no longer limited by RAM/java heap but disk space available    For more details, including instruction on how to upgrade, see the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Wed, Jun 5, 2024
      
        RDF4J 5.0.0 Milestone 3
      
      
        Milestone number 3 of the upcoming 5.0.0 release of RDF4J is now available for download.
This milestone build is now feature-complete and we will only be fixing bugs from now on. The final release is planned for the end of June.
 issues fixed in 5.0.0 Milestone 3  Links  Download RDF4J  
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Jun 4, 2024
      
        RDF4J 4.3.12 released
      
      
        RDF4J 4.3.12 is now available. This is a patch release fixing 5 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Mon, Apr 1, 2024
      
        RDF4J 4.3.11 released
      
      
        RDF4J 4.3.11 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Wed, Mar 6, 2024
      
        RDF4J 4.3.10 released
      
      
        RDF4J 4.3.10 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Jan 21, 2024
      
        RDF4J 4.3.9 released
      
      
        RDF4J 4.3.9 is now available. This is a patch release fixing 7 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Nov 7, 2023
      
        RDF4J 4.3.8 released
      
      
        RDF4J 4.3.8 is now available. This is a patch release fixing 7 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Oct 17, 2023
      
        RDF4J 5.0.0 Milestone 2
      
      
        Milestone number 2 of the upcoming 5.0.0 release of RDF4J is now available for download.
Notable changes since the previous milestone build:

We have switched to the new CONFIG vocabulary. If you ware interacting with the config files directly then take care to use the new vocabulary and if need be use the Configuration class to help with handling use cases where the new and old config vocabularies are mixed.
The refactoring and optimizations of the various iterations is nearing completion and there should be very few iterations that are still marked as deprecated.
More deprecated code has been removed, if you find that you were depending on deprecated code that is now removed then please let us know.

This milestone build is not yet feature-complete, but we are putting it out to receive early feedback on all the improvements we have put in.
        
      
      
        [read more]
      
    
    

    
    
    
      
        ««
      
      
        «
      
      
        1
      
      
        2
      
      
        3
      
      
        4
      
      
        5
      
      
        »
      
      
        »»
      
    
  

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nNews
    

  
   
  
  
    
      
    
      Fri, Oct 6, 2023
      
        RDF4J 4.3.7 released
      
      
        RDF4J 4.3.7 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Aug 27, 2023
      
        RDF4J 4.3.6 released
      
      
        RDF4J 4.3.6 is now available. This is a patch release fixing 4 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Aug 8, 2023
      
        RDF4J 5.0.0 Milestone 1
      
      
        Milestone number 1 of the upcoming 5.0.0 release of RDF4J is now available for download.
This first milestone focuses on removing deprecated code with the largest change being the removal of the Iteration interface in favour of using the CloseableIteration interface instead. The CloseableIteration has also been simplified by removing support for specifying checked exceptions using Java generics.
This milestone build is not yet feature-complete, but we are putting it out to receive early feedback on all the improvements we have put in.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Aug 6, 2023
      
        RDF4J 4.3.5 released
      
      
        RDF4J 4.3.5 is now available. This is a patch release fixing 5 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sat, Jul 22, 2023
      
        RDF4J 4.3.4 released
      
      
        RDF4J 4.3.4 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Wed, Jul 5, 2023
      
        RDF4J 4.3.3 released
      
      
        RDF4J 4.3.3 is now available. This is a patch release fixing 5 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Wed, Jun 7, 2023
      
        RDF4J 4.3.2 released
      
      
        RDF4J 4.3.2 is now available. This is a patch release fixing 4 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Fri, May 19, 2023
      
        RDF4J 4.3 released
      
      
        RDF4J 4.3 is now available. This is a minor release fixing 34 issues.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sat, Apr 22, 2023
      
        RDF4J 4.2.4 released
      
      
        RDF4J 4.2.4 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sat, Feb 11, 2023
      
        RDF4J 4.2.3 released
      
      
        RDF4J 4.2.3 is now available. This is a patch release fixing 9 issues.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
    
    
      
        ««
      
      
        «
      
      
        1
      
      
        2
      
      
        3
      
      
        4
      
      
        5
      
      
        »
      
      
        »»
      
    
  

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nNews
    

  
   
  
  
    
      
    
      Thu, Dec 1, 2022
      
        RDF4J 4.2.2 released
      
      
        RDF4J 4.2.2 is now available. This is a patch release fixing 3 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Nov 8, 2022
      
        RDF4J 4.2.1 released
      
      
        RDF4J 4.2.1 is now available. This is a patch release fixing 11 bugs, 3 of which are security fixes.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Fri, Sep 16, 2022
      
        RDF4J 4.2.0 released
      
      
        RDF4J 4.2.0 is now available. This is a minor release with performance improvements, bug fixes and a new feature to allow for custom SPARQL aggregate functions together with implementations for standard deviation and variance.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Wed, Sep 14, 2022
      
        RDF4J 4.1.3 released
      
      
        RDF4J 4.1.3 is now available. This is a patch release fixing 5 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Sep 11, 2022
      
        RDF4J 4.1.2 released
      
      
        RDF4J 4.1.2 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Thu, Sep 1, 2022
      
        RDF4J 4.1.1 and 4.0.5 released
      
      
        RDF4J 4.1.1 and 4.0.5 are now available. RDF4J 4.1.1 is a patch release
fixing 13 bugs and 1 security vulnerability (CVE-2020-36518 in FasterXML
Jackson Databind). Two of the issues fixed in 4.1.1, including the security vulnerability, have been backported and released as 4.0.5.
For more details, have a look at the 4.1.1 release notes and 4.0.5 release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Jul 31, 2022
      
        RDF4J 4.1.0 released
      
      
        RDF4J 4.1.0 is now available.
A few notable features:

Major performance improvements[1]

3x higher throughput for concurrent queries with the Memory Store
25x faster deletion with the Native Store
6x faster evaluation of SPARQL queries using MINUS with the Memory Store
2-3x faster evaluation of most SPARQL queries with the Memory Store
Faster SPARQL query parsing
Faster SHACL validation
Passing the RDFS Reasoner Challenge posted by Justin


LMDB Store is available in the server and workbench
Fuzzy prefix support in the Lucene Sail
Improved handling of low memory in the Memory Store
Improvements to the Spring Components

For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Thu, Jul 7, 2022
      
        RDF4J 4.0.4 released
      
      
        RDF4J 4.0.4 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sat, Jul 2, 2022
      
        RDF4J 4.1.0 Milestone 1
      
      
        Milestone number 1 of the upcoming 4.1.0 minor release of RDF4J is now available for download.
A few notable features:

Major performance improvements[1]

3x higher throughput for concurrent queries with the Memory Store
25x faster deletion with the Native Store
6x faster evaluation of SPARQL queries using MINUS with the Memory Store
2-3x faster evaluation of most SPARQL queries with the Memory Store
Faster SPARQL query parsing
Faster SHACL validation
Passing the RDFS Reasoner Challenge posted by Justin


LMDB Store is available in the server and workbench
Fuzzy prefix support in the Lucene Sail
Improved handling of low memory in the Memory Store
Improvements to the Spring Components

This milestone build is not yet feature-complete, but we are publishing it in order to receive early feedback on all the changes and improvements. Feedback on the performance and integrity of the revamped internals of the Memory Store is especially welcome, as is feedback on the various deprecations related to query evaluation.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Jun 28, 2022
      
        RDF4J 4.0.3 released
      
      
        RDF4J 4.0.3 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
    
    
      
        ««
      
      
        «
      
      
        2
      
      
        3
      
      
        4
      
      
        5
      
      
        6
      
      
        »
      
      
        »»
      
    
  

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nNews
    

  
   
  
  
    
      
    
      Mon, Jun 6, 2022
      
        RDF4J 4.0.2 released
      
      
        RDF4J 4.0.2 is now available. This is a patch release fixing 8 issues.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, May 15, 2022
      
        RDF4J 4.0.1 released
      
      
        RDF4J 4.0.1 is now available. This is a patch release fixing 8 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Apr 24, 2022
      
        RDF4J 4.0.0 released
      
      
        
We are very excited to announce the release of RDF4J 4.0!
RDF4J 4.0.0 is a major release of the RDF4J framework, focusing on improved
performance and improved maintainability. We’ve done a massive clean up of the
project structure, which will make it easier to deliver new features in this
and future releases. In total, we addressed over 100 issues. Highlights
include:
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Apr 12, 2022
      
        RDF4J 4.0.0 Milestone 3
      
      
        Milestone number 3 of the upcoming 4.0.0 release of RDF4J is now available for download.
RDF4J 4.0.0 is a major release of the RDF4J framework, focusing on improved performance and improved maintainability. We’ve done a massive clean up of the project structure, which is intended to make it easier to deliver new features in this and future releases.
Some of the highlights covered in this third milestone:

improved normalization of BCP47 language tags
type/language-based filtering in Lucene full-text search
support for concurrent reads on a single connection
storing SHACL Shapes in user specified graphs docs
various performance improvements in Native/LMDB/Memory stores

For more details see the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sat, Apr 9, 2022
      
        RDF4J 3.7.7 released
      
      
        RDF4J 3.7.7 is now available. This is a patch release fixing 2 bugs, including a potential security issue in our Spring dependencies.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Mar 27, 2022
      
        RDF4J 3.7.6 released
      
      
        RDF4J 3.7.6 is now available. This is a patch release fixing 4 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Mar 6, 2022
      
        RDF4J 3.7.5 released
      
      
        RDF4J 3.7.5 is now available. This is a patch release fixing 14 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Jan 23, 2022
      
        RDF4J 4.0.0 Milestone 2
      
      
        Milestone number 2 of the upcoming 4.0.0 release of RDF4J is now available for download.
RDF4J 4.0.0 is a major release of the RDF4J framework, focusing on improved performance and improved maintainability. We’ve done a massive clean up of the project structure, which is intended to make it easier to deliver new features in this and future releases.
Some of the highlights covered in this second milestone:

the LMDB Store, a new persistent embedded RDF Store based on the Lightning Memory-Mapped Database (LMDB) - see the documentation for details
massive improvements in (concurrent) performance of various aspects of the framework, including the memory store, the SPARQL and SHACL engine, and more
more cleanups of deprecated code

For more details see the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Nov 28, 2021
      
        RDF4J 4.0.0 Milestone 1
      
      
        Milestone number 1 of the upcoming 4.0.0 release of RDF4J is now available for download.
RDF4J 4.0.0 is a major release of the RDF4J framework, focusing on improved performance and improved maintainability. We’ve done a massive clean up of the project structure, which is intended to make it easier to deliver new features in this and future releases.
Some of the highlights covered in this first milestone:

update to Java 11 as the minimally-required version of Java
improvements in the SparqlBuilder
performance improvements in the SPARQL query engine
performance improvements in the SHACL engine
improved support for working with RDF4J in Spring and Spring Boot applications
SeRQL and SPIN are no longer supported
removal of several long-deprecated packages and interfaces

This milestone build is not yet feature-complete, but we are putting it out to receive early feedback on all the improvements we have put in.
        
      
      
        [read more]
      
    
    

    
      
    
      Sat, Nov 13, 2021
      
        RDF4J 3.7.4 released
      
      
        RDF4J 3.7.4 is now available. This is a patch release fixing 5 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
    
    
      
        ««
      
      
        «
      
      
        3
      
      
        4
      
      
        5
      
      
        6
      
      
        7
      
      
        »
      
      
        »»
      
    
  

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nNews
    

  
   
  
  
    
      
    
      Sun, May 26, 2019
      
        new project website
      
      
        As you can see, we’ve given our website a fresh coat of paint. We’ve updated our site to use the Eclipse Solstice theme, and have unified our general website and documentation site in a single project. This makes updating, maintaining, and improving the site and the project documentation a lot easier.
        
      
      
        [read more]
      
    
    

    
      
    
      Sat, May 25, 2019
      
        rdf4j 2.5.2 released
      
      
        Rdf4j 2.5.2 is a patch release containing several bug fixes, including:
        
      
      
        [read more]
      
    
    

    
      
    
      Sat, Apr 6, 2019
      
        rdf4j 2.5.1 released
      
      
        Rdf4j 2.5.1 is a patch release containing bug fixes and a library update, including:
        
      
      
        [read more]
      
    
    

    
      
    
      Thu, Mar 7, 2019
      
        rdf4j 2.5.0 released
      
      
        We are very pleased to announce the release of Eclipse rdf4j 2.5. This release addresses over 70 issues, and includes several major new features and improvements, including:
        
      
      
        [read more]
      
    
    

    
      
    
      Sat, Mar 2, 2019
      
        rdf4j 2.4.6 released
      
      
        Release 2.4.6 is a patch release containing one bug fix.
        
      
      
        [read more]
      
    
    

    
    
    
      
        ««
      
      
        «
      
      
        7
      
      
        8
      
      
        9
      
      
        10
      
      
        11
      
      
        »
      
      
        »»
      
    
  

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nNews
    

  
   
  
  
    
      
    
      Wed, Apr 16, 2025
      
        RDF4J 4.3.16 released
      
      
        RDF4J 4.3.16 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Apr 15, 2025
      
        RDF4J 5.1.3 released
      
      
        RDF4J 5.1.3 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Feb 11, 2025
      
        RDF4J 5.1.2 released
      
      
        RDF4J 5.1.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Jan 28, 2025
      
        RDF4J 5.1.1 released
      
      
        RDF4J 5.1.1 is now available. This is a patch release fixing 9 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Thu, Nov 21, 2024
      
        RDF4J 5.1.0 released
      
      
        RDF4J 5.1.0 is now available.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Nov 10, 2024
      
        RDF4J 5.0.3 released
      
      
        RDF4J 5.0.3 is now available. This is a patch release fixing 11 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Nov 10, 2024
      
        RDF4J 5.1.0 Milestone 1
      
      
        Milestone number 1 of the upcoming 5.1.0 release of RDF4J is now available for download.
RDF4J 5.1.0 is a minor release focusing on … .
This milestone build is not yet feature-complete, but we are putting it out to receive early feedback on all the improvements we have put in.
        
      
      
        [read more]
      
    
    

    
      
    
      Thu, Nov 7, 2024
      
        RDF4J 4.3.15 released
      
      
        RDF4J 4.3.15 is now available. This is a patch release fixing 4 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Wed, Oct 2, 2024
      
        RDF4J 4.3.14 released
      
      
        RDF4J 4.3.14 is now available. This is a patch release fixing 1 bug.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Fri, Aug 2, 2024
      
        RDF4J 5.0.2 released
      
      
        RDF4J 5.0.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
    
    
      
        ««
      
      
        «
      
      
        1
      
      
        2
      
      
        3
      
      
        4
      
      
        5
      
      
        »
      
      
        »»
      
    
  

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nNews
    

  
   
  
  
    
      
    
      Wed, Apr 16, 2025
      
        RDF4J 4.3.16 released
      
      
        RDF4J 4.3.16 is now available. This is a patch release fixing 2 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Apr 15, 2025
      
        RDF4J 5.1.3 released
      
      
        RDF4J 5.1.3 is now available. This is a patch release fixing 6 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Feb 11, 2025
      
        RDF4J 5.1.2 released
      
      
        RDF4J 5.1.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Tue, Jan 28, 2025
      
        RDF4J 5.1.1 released
      
      
        RDF4J 5.1.1 is now available. This is a patch release fixing 9 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Thu, Nov 21, 2024
      
        RDF4J 5.1.0 released
      
      
        RDF4J 5.1.0 is now available.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Nov 10, 2024
      
        RDF4J 5.0.3 released
      
      
        RDF4J 5.0.3 is now available. This is a patch release fixing 11 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Sun, Nov 10, 2024
      
        RDF4J 5.1.0 Milestone 1
      
      
        Milestone number 1 of the upcoming 5.1.0 release of RDF4J is now available for download.
RDF4J 5.1.0 is a minor release focusing on … .
This milestone build is not yet feature-complete, but we are putting it out to receive early feedback on all the improvements we have put in.
        
      
      
        [read more]
      
    
    

    
      
    
      Thu, Nov 7, 2024
      
        RDF4J 4.3.15 released
      
      
        RDF4J 4.3.15 is now available. This is a patch release fixing 4 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Wed, Oct 2, 2024
      
        RDF4J 4.3.14 released
      
      
        RDF4J 4.3.14 is now available. This is a patch release fixing 1 bug.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
      
    
      Fri, Aug 2, 2024
      
        RDF4J 5.0.2 released
      
      
        RDF4J 5.0.2 is now available. This is a patch release fixing 1 bugs.
For more details, have a look at the release notes.
        
      
      
        [read more]
      
    
    

    
    
    
      
        ««
      
      
        «
      
      
        1
      
      
        2
      
      
        3
      
      
        4
      
      
        5
      
      
        »
      
      
        »»
      
    
  

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nDocumentation
    

  
  
    
      
      
      Tutorials
      
        
            
              
              Getting started with RDF4J
              
              Starting a new Maven project in Eclipse
              
              Creating custom SPARQL functions
              
              Creating SPARQL Queries with the SparqlBuilder
              
          
      
      
      
      
      Programming with RDF4J
      
        
            
              
              Setting up your development environment
              
              The RDF Model API
              
              The Repository API
              
              Parsing and Writing RDF with Rio
              
              The LMDB Store
              
              Full-text indexing with the Lucene SAIL
              
              Reasoning and Validation with SPIN
              
              Validation with SHACL
              
              Federation with FedX
              
              Integration with Spring
              
              GeoSPARQL
              
              RDF-star and SPARQL-star
              
          
      
      
      
      
      Tools
      
        
            
              
              RDF4J Console
              
              RDF4J Server and Workbench
              
              Application directory configuration
              
              Repository configuration templates
              
          
      
      
      
      
      Reference
      
        
RDF4J REST API
RDF4J API Javadoc


            
              
              The SAIL API
              
              RDF4J Binary RDF Format
              
              Repository and SAIL configuration
              
              Sesame to Eclipse RDF4J migration
              
          
      
      
      
      
      Info for RDF4J Developers
      
        
            
              
              Developer workflow and project management
              
              RDF4J merge strategy
              
              Release management
              
              Squashing Commits
              
          
      
      
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nDownload RDF4J
    

  
  You can either retrieve RDF4J via Apache Maven, or download the SDK or onejar directly.
RDF4J 5.1.3 (latest)
RDF4J 5.1.3 is our latest stable release. It requires Java 11 minimally.
For details on what’s new and how to upgrade, see the release and upgrade notes.


RDF4J 5.1.3 SDK (zip)
Full Eclipse RDF4J SDK, containing all libraries, RDF4J Server, Workbench, and Console applications, and Javadoc API.


RDF4J 5.1.3 onejar
Single jar file for easy inclusion of the full RDF4J toolkit in your Java project.


RDF4J artifacts on the Maven Central Repository


Apache Maven
You can include RDF4J as a Maven dependency in your Java project by including the following BOM (Bill-of-Materials):
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.eclipse.rdf4j</groupId>
            <artifactId>rdf4j-bom</artifactId>
            <version>5.1.3</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
RDF4J is a multi-module project, you can pick and choose which libraries you need. To include the full project, simply import the following dependency:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
</dependency>
See the Setup instructions in the
Programmer’s documentation for more details on Maven and
which artifacts RDF4J provides.
Older releases
RDF4J 5.0

RDF4J 5.0.3 SDK (zip)
RDF4J 5.0.3 onejar

RDF4J 4.3

RDF4J 4.3.16 SDK (zip)
RDF4J 4.3.16 onejar

RDF4J 4.2

RDF4J 4.2.4 SDK (zip)
RDF4J 4.2.4 onejar

RDF4J 4.1

RDF4J 4.1.3 SDK (zip)
RDF4J 4.1.3 onejar

RDF4J 4.0

RDF4J 4.0.4 SDK (zip)
RDF4J 4.0.4 onejar

RDF4J 3.7

RDF4J 3.7.7 SDK (zip)
RDF4J 3.7.7 onejar

RDF4J 3.6

RDF4J 3.6.3 SDK (zip)
RDF4J 3.6.3 onejar

RDF4J 3.5

RDF4J 3.5.1 SDK (zip)
RDF4J 3.5.1 onejar

Source code and nightly builds
You can access the RDF4J source code directly from our GitHub repositories. Maven nightly snapshot builds for the main and develop branch are available from the Sonatype snapshot repository.
To include nightly snapshot builds in your project, add this repository to your project’s POM:
<repositories>
    <repository>
        <id>oss.sonatype.org-snapshot</id>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
Then use RDF4J dependencies as normal, using <version>-SNAPSHOT as the version number.
Archives
Old releases of OpenRDF Sesame (the predecessor of Eclipse RDF4J) can be found on Sourceforge.
License
Eclipse RDF4J is licensed to you under the terms of the Eclipse Distribution License (EDL), v1.0.


  

     
      
        
          

  Table of Contents

  
  
    RDF4J 5.1.3 (latest)
      
        Apache Maven
      
    
    Older releases
      
        RDF4J 5.0
        RDF4J 4.3
        RDF4J 4.2
        RDF4J 4.1
        RDF4J 4.0
        RDF4J 3.7
        RDF4J 3.6
        RDF4J 3.5
      
    
    Source code and nightly builds
    Archives
    License\n\n\n\nSupport
    

  
  Ask about or discuss RDF4J
RDF4J Github Discussions is the best place to ask questions, propose new ideas or discuss other issues related to RDF4J.
You can also find us on Gitter, if you prefer an instant messaging style of communication. However, we make no promises that any of the RDF4J developers will be online at any given time.
We previously used a Google group called rdf4j-users. This group has now been archived. You can still browse the archive, but no new posts will be accepted.
The RDF4J development team uses the rdf4j-dev@eclipse.org mailinglist to discuss development progress.
Reporting bugs and requests for improvement
If you think you’ve found a bug in RDF4J, or wish to log a request for a new feature or improvement, please use the RDF4J issue tracker. Before you add your new issue, though, please have a look around to see if it’s not already there.


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nSupport
    

  
  Ask about or discuss RDF4J
RDF4J Github Discussions is the best place to ask questions, propose new ideas or discuss other issues related to RDF4J.
You can also find us on Gitter, if you prefer an instant messaging style of communication. However, we make no promises that any of the RDF4J developers will be online at any given time.
We previously used a Google group called rdf4j-users. This group has now been archived. You can still browse the archive, but no new posts will be accepted.
The RDF4J development team uses the rdf4j-dev@eclipse.org mailinglist to discuss development progress.
Reporting bugs and requests for improvement
If you think you’ve found a bug in RDF4J, or wish to log a request for a new feature or improvement, please use the RDF4J issue tracker. Before you add your new issue, though, please have a look around to see if it’s not already there.


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nSupport
    

  
  Ask about or discuss RDF4J
RDF4J Github Discussions is the best place to ask questions, propose new ideas or discuss other issues related to RDF4J.
You can also find us on Gitter, if you prefer an instant messaging style of communication. However, we make no promises that any of the RDF4J developers will be online at any given time.
We previously used a Google group called rdf4j-users. This group has now been archived. You can still browse the archive, but no new posts will be accepted.
The RDF4J development team uses the rdf4j-dev@eclipse.org mailinglist to discuss development progress.
Reporting bugs and requests for improvement
If you think you’ve found a bug in RDF4J, or wish to log a request for a new feature or improvement, please use the RDF4J issue tracker. Before you add your new issue, though, please have a look around to see if it’s not already there.


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nTutorials
    

  
  
    
        
          
          Getting started with RDF4JIn this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
          
          Starting a new Maven project in EclipseIf you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.
          
          Creating custom SPARQL functionsIn this short tutoral, we’ll create a simple custom function and add it RDF4J’s SPARQL engine.
          
          Creating SPARQL Queries with the SparqlBuilderRDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
          
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nTutorials
    

  
  
    
        
          
          Getting started with RDF4JIn this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
          
          Starting a new Maven project in EclipseIf you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.
          
          Creating custom SPARQL functionsIn this short tutoral, we’ll create a simple custom function and add it RDF4J’s SPARQL engine.
          
          Creating SPARQL Queries with the SparqlBuilderRDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
          
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nTutorials
    

  
  
    
        
          
          Getting started with RDF4JIn this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
          
          Starting a new Maven project in EclipseIf you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.
          
          Creating custom SPARQL functionsIn this short tutoral, we’ll create a simple custom function and add it RDF4J’s SPARQL engine.
          
          Creating SPARQL Queries with the SparqlBuilderRDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
          
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nClass Values

java.lang.Object
org.eclipse.rdf4j.model.util.Values




public class Values
extends Object
Factory methods to quickly create Value objects ( IRI, Literal, BNode, and
 Triple) without having to create a ValueFactory first.
 
 Example usage:

  import static org.eclipse.rdf4j.model.util.Values.iri;

 ...
 IRI foo = iri("http://example.org/foo");
 
 

Since:
3.5.0
Author:
Jeen Broekstra
See Also:


Statements










Method Summary

All MethodsStatic MethodsConcrete Methods


Modifier and Type
Method
Description
static BNode
bnode()

Creates a new BNode

static BNode
bnode(String nodeId)

Creates a new BNode with the supplied node identifier.

static BNode
bnode(ValueFactory vf)

Creates a new BNode

static BNode
bnode(ValueFactory vf,
 String nodeId)

Creates a new BNode with the supplied node identifier.

static ValueFactory
getValueFactory()

Get a ValueFactory.

static IRI
iri(Iterable<Namespace> namespaces,
 String prefixedName)

Create a new IRI from a supplied prefixed name, using the supplied namespaces

static IRI
iri(String iri)

Create a new IRI using the supplied iri string

static IRI
iri(String namespace,
 String localName)

Create a new IRI using the supplied namespace name and local name

static IRI
iri(Namespace namespace,
 String localName)

Create a new IRI using the supplied Namespace and local name

static IRI
iri(ValueFactory vf,
 String iri)

Create a new IRI using the supplied iri string

static IRI
iri(ValueFactory vf,
 String namespace,
 String localName)

Create a new IRI using the supplied namespace and local name

static Literal
literal(boolean booleanValue)

Creates a new Literal with the supplied boolean value

static Literal
literal(byte byteValue)

Creates a new Literal with the supplied byte value

static Literal
literal(double doubleValue)

Creates a new Literal with the supplied double value

static Literal
literal(float floatValue)

Creates a new Literal with the supplied float value

static Literal
literal(int intValue)

Creates a new Literal with the supplied int value

static Literal
literal(long longValue)

Creates a new Literal with the supplied long value

static Literal
literal(short shortValue)

Creates a new Literal with the supplied short value

static Literal
literal(Object object)

Creates a new typed Literal out of the supplied object, mapping the runtime type of the object to the
 appropriate XSD datatype.

static Literal
literal(Object object,
 boolean failOnUnknownType)

Creates a new typed Literal out of the supplied object, mapping the runtime type of the object to the
 appropriate XSD datatype.

static Literal
literal(String lexicalValue)

Creates a new Literal with the supplied lexical value.

static Literal
literal(String lexicalValue,
 String languageTag)

Creates a new Literal with the supplied lexical value.

static Literal
literal(String lexicalValue,
 CoreDatatype datatype)

Creates a new Literal with the supplied lexical value and datatype.

static Literal
literal(String lexicalValue,
 IRI datatype)

Creates a new Literal with the supplied lexical value and datatype.

static Literal
literal(BigDecimal bigDecimal)

Creates a new Literal with the supplied BigDecimal value

static Literal
literal(BigInteger bigInteger)

Creates a new Literal with the supplied BigInteger value

static Literal
literal(TemporalAccessor value)

Creates a new Literal with the supplied TemporalAccessor value

static Literal
literal(ValueFactory vf,
 boolean booleanValue)

Creates a new Literal with the supplied boolean value

static Literal
literal(ValueFactory vf,
 byte byteValue)

Creates a new Literal with the supplied byte value

static Literal
literal(ValueFactory vf,
 double doubleValue)

Creates a new Literal with the supplied double value

static Literal
literal(ValueFactory vf,
 float floatValue)

Creates a new Literal with the supplied float value

static Literal
literal(ValueFactory vf,
 int intValue)

Creates a new Literal with the supplied int value

static Literal
literal(ValueFactory vf,
 long longValue)

Creates a new Literal with the supplied long value

static Literal
literal(ValueFactory vf,
 short shortValue)

Creates a new Literal with the supplied short value

static Literal
literal(ValueFactory vf,
 Object object,
 boolean failOnUnknownType)

Creates a new typed Literal out of the supplied object, mapping the runtime type of the object to the
 appropriate XSD datatype.

static Literal
literal(ValueFactory vf,
 String lexicalValue)

Creates a new Literal with the supplied lexical value.

static Literal
literal(ValueFactory vf,
 String lexicalValue,
 String languageTag)

Creates a new Literal with the supplied lexical value.

static Literal
literal(ValueFactory vf,
 String lexicalValue,
 CoreDatatype datatype)

Creates a new Literal with the supplied lexical value and datatype.

static Literal
literal(ValueFactory vf,
 String lexicalValue,
 IRI datatype)

Creates a new Literal with the supplied lexical value and datatype.

static Literal
literal(ValueFactory vf,
 BigDecimal bigDecimal)

Creates a new Literal with the supplied BigDecimal value

static Literal
literal(ValueFactory vf,
 BigInteger bigInteger)

Creates a new Literal with the supplied BigInteger value

static Literal
literal(ValueFactory vf,
 TemporalAccessor value)

Creates a new Literal with the supplied TemporalAccessor value

static Namespace
namespace(String prefix,
 String name)

Create a new Namespace object.

static Triple
triple(Resource subject,
 IRI predicate,
 Value object)

Creates a new RDF-star embedded triple with the supplied subject, predicate, and object.

static Triple
triple(Statement statement)

Creates a new RDF-star embedded triple using the subject, predicate and object from the supplied
 Statement.

static Triple
triple(ValueFactory vf,
 Resource subject,
 IRI predicate,
 Value object)

Creates a new RDF-star embedded triple with the supplied subject, predicate, and object.

static Triple
triple(ValueFactory vf,
 Statement statement)

Creates a new RDF-star embedded triple using the subject, predicate and object from the supplied
 Statement.





Methods inherited from class java.lang.Object
clone, equals, finalize, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait









Method Details



iri

public static IRI iri(String iri)
               throws IllegalArgumentException
Create a new IRI using the supplied iri string

Parameters:
iri - a string representing a valid (absolute) iri
Returns:
an IRI object for the supplied iri string.
Throws:
NullPointerException - if the suppplied iri is null
IllegalArgumentException - if the supplied iri string can not be parsed as a legal IRI.






iri

public static IRI iri(ValueFactory vf,
 String iri)
               throws IllegalArgumentException
Create a new IRI using the supplied iri string

Parameters:
vf - the ValueFactory to use for creation of the IRI.
iri - a string representing a valid (absolute) iri
Returns:
an IRI object for the supplied iri string.
Throws:
NullPointerException - if any of the input parameters is null
IllegalArgumentException - if the supplied iri string can not be parsed as a legal IRI by the supplied
                                  ValueFactory .






iri

public static IRI iri(String namespace,
 String localName)
               throws IllegalArgumentException
Create a new IRI using the supplied namespace name and local name

Parameters:
namespace - the IRI's namespace name
localName - the IRI's local name
Returns:
an IRI object for the supplied IRI namespace name and localName.
Throws:
NullPointerException - if any of the input parameters is null
IllegalArgumentException - if the supplied iri string can not be parsed as a legal IRI.






iri

public static IRI iri(Namespace namespace,
 String localName)
               throws IllegalArgumentException
Create a new IRI using the supplied Namespace and local name

Parameters:
namespace - the IRI's Namespace
localName - the IRI's local name
Returns:
an IRI object for the supplied IRI namespace and localName.
Throws:
NullPointerException - if any of the input parameters is null
IllegalArgumentException - if the supplied iri string can not be parsed as a legal IRI.
Since:
3.6.0






iri

public static IRI iri(Iterable<Namespace> namespaces,
 String prefixedName)
               throws IllegalArgumentException
Create a new IRI from a supplied prefixed name, using the supplied namespaces

Parameters:
namespaces - the Namespaces from which to find the correct namespace to map the prefixed name to
prefixedName - a prefixed name that is a shorthand for a full iri, using syntax form
                     prefix:localName. For example, rdf:type is a prefixed name where
                     rdf is the prefix. If the correct Namespace definition is also supplied
                     this expands to the full namespace name
                     http://www.w3.org/1999/02/22-rdf-syntax-ns#, leading to a full IRI
                     http://www.w3.org/1999/02/22-rdf-syntax-ns#type.
Returns:
an IRI object for the supplied IRI namespace and localName.
Throws:
NullPointerException - if any of the input parameters is null
IllegalArgumentException - if the supplied prefixed name can not be transformed to a legal IRI.
Since:
3.6.0






iri

public static IRI iri(ValueFactory vf,
 String namespace,
 String localName)
               throws IllegalArgumentException
Create a new IRI using the supplied namespace and local name

Parameters:
vf - the ValueFactory to use for creation of the IRI.
namespace - the IRI's namespace
localName - the IRI's local name
Returns:
an IRI object for the supplied IRI namespace and localName.
Throws:
NullPointerException - if any of the input parameters is null
IllegalArgumentException - if the supplied iri string can not be parsed as a legal IRI by the supplied
                                  ValueFactory






bnode

public static BNode bnode()
Creates a new BNode

Returns:
a new BNode






bnode

public static BNode bnode(ValueFactory vf)
Creates a new BNode

Parameters:
vf - the ValueFactory to use for creation of the BNode
Returns:
a new BNode
Throws:
NullPointerException - if any of the input parameters is null






bnode

public static BNode bnode(String nodeId)
                   throws IllegalArgumentException
Creates a new BNode with the supplied node identifier.

Parameters:
nodeId - the node identifier
Returns:
a new BNode
Throws:
NullPointerException - if the supplied node identifier is null.
IllegalArgumentException - if the supplied node identifier is not valid






bnode

public static BNode bnode(ValueFactory vf,
 String nodeId)
                   throws IllegalArgumentException
Creates a new BNode with the supplied node identifier.

Parameters:
vf - the ValueFactory to use for creation of the BNode
nodeId - the node identifier
Returns:
a new BNode
Throws:
NullPointerException - if any of the input parameters is null
IllegalArgumentException - if the supplied node identifier is not valid






literal

public static Literal literal(String lexicalValue)
Creates a new Literal with the supplied lexical value.

Parameters:
lexicalValue - the lexical value for the literal
Returns:
a new Literal of type XSD.STRING
Throws:
NullPointerException - if the supplied lexical value is null.






literal

public static Literal literal(ValueFactory vf,
 String lexicalValue)
Creates a new Literal with the supplied lexical value.

Parameters:
vf - the ValueFactory to use for creation of the Literal
lexicalValue - the lexical value for the literal
Returns:
a new Literal of type XSD.STRING
Throws:
NullPointerException - if any of the input parameters is null






literal

public static Literal literal(String lexicalValue,
 String languageTag)
Creates a new Literal with the supplied lexical value.

Parameters:
lexicalValue - the lexical value for the literal
languageTag - the language tag for the literal.
Returns:
a new Literal of type RDF.LANGSTRING
Throws:
NullPointerException - if the supplied lexical value or language tag is null.






literal

public static Literal literal(ValueFactory vf,
 String lexicalValue,
 String languageTag)
Creates a new Literal with the supplied lexical value.

Parameters:
vf - the ValueFactory to use for creation of the Literal
lexicalValue - the lexical value for the literal
languageTag - the language tag for the literal.
Returns:
a new Literal of type RDF.LANGSTRING
Throws:
NullPointerException - if any of the input parameters is null






literal

public static Literal literal(String lexicalValue,
 IRI datatype)
                       throws IllegalArgumentException
Creates a new Literal with the supplied lexical value and datatype.

Parameters:
lexicalValue - the lexical value for the literal
datatype - the datatype IRI
Returns:
a new Literal with the supplied lexical value and datatype
Throws:
NullPointerException - if the supplied lexical value or datatype is null.
IllegalArgumentException - if the supplied lexical value is not valid for the given datatype






literal

public static Literal literal(String lexicalValue,
 CoreDatatype datatype)
                       throws IllegalArgumentException
Creates a new Literal with the supplied lexical value and datatype.

Parameters:
lexicalValue - the lexical value for the literal
datatype - the CoreDatatype
Returns:
a new Literal with the supplied lexical value and datatype
Throws:
NullPointerException - if the supplied lexical value or datatype is null.
IllegalArgumentException - if the supplied lexical value is not valid for the given datatype






literal

public static Literal literal(ValueFactory vf,
 String lexicalValue,
 IRI datatype)
                       throws IllegalArgumentException
Creates a new Literal with the supplied lexical value and datatype.

Parameters:
vf - the ValueFactory to use for creation of the Literal
lexicalValue - the lexical value for the literal
datatype - the datatype IRI
Returns:
a new Literal with the supplied lexical value and datatype
Throws:
NullPointerException - if any of the input parameters is null.
IllegalArgumentException - if the supplied lexical value is not valid for the given datatype






literal

public static Literal literal(ValueFactory vf,
 String lexicalValue,
 CoreDatatype datatype)
                       throws IllegalArgumentException
Creates a new Literal with the supplied lexical value and datatype.

Parameters:
vf - the ValueFactory to use for creation of the Literal
lexicalValue - the lexical value for the literal
datatype - the CoreDatatype
Returns:
a new Literal with the supplied lexical value and datatype
Throws:
NullPointerException - if any of the input parameters is null.
IllegalArgumentException - if the supplied lexical value is not valid for the given datatype






literal

public static Literal literal(boolean booleanValue)
Creates a new Literal with the supplied boolean value

Parameters:
booleanValue - a boolean value
Returns:
a Literal of type XSD.BOOLEAN with the supplied value






literal

public static Literal literal(ValueFactory vf,
 boolean booleanValue)
Creates a new Literal with the supplied boolean value

Parameters:
vf - the ValueFactory to use for creation of the Literal
booleanValue - a boolean value
Returns:
a Literal of type XSD.BOOLEAN with the supplied value
Throws:
NullPointerException - if any of the input parameters is null.






literal

public static Literal literal(byte byteValue)
Creates a new Literal with the supplied byte value

Parameters:
byteValue - a byte value
Returns:
a Literal of type XSD.BYTE with the supplied value






literal

public static Literal literal(ValueFactory vf,
 byte byteValue)
Creates a new Literal with the supplied byte value

Parameters:
vf - the ValueFactory to use for creation of the Literal
byteValue - a byte value
Returns:
a Literal of type XSD.BYTE with the supplied value
Throws:
NullPointerException - if any of the input parameters is null.






literal

public static Literal literal(short shortValue)
Creates a new Literal with the supplied short value

Parameters:
shortValue - a short value
Returns:
a Literal of type XSD.SHORT with the supplied value






literal

public static Literal literal(ValueFactory vf,
 short shortValue)
Creates a new Literal with the supplied short value

Parameters:
vf - the ValueFactory to use for creation of the Literal
shortValue - a short value
Returns:
a Literal of type XSD.SHORT with the supplied value
Throws:
NullPointerException - if any of the input parameters is null.






literal

public static Literal literal(int intValue)
Creates a new Literal with the supplied int value

Parameters:
intValue - an int value
Returns:
a Literal of type XSD.INT with the supplied value






literal

public static Literal literal(ValueFactory vf,
 int intValue)
Creates a new Literal with the supplied int value

Parameters:
vf - the ValueFactory to use for creation of the Literal
intValue - an int value
Returns:
a Literal of type XSD.INT with the supplied value
Throws:
NullPointerException - if any of the input parameters is null.






literal

public static Literal literal(long longValue)
Creates a new Literal with the supplied long value

Parameters:
longValue - a long value
Returns:
a Literal of type XSD.LONG with the supplied value






literal

public static Literal literal(ValueFactory vf,
 long longValue)
Creates a new Literal with the supplied long value

Parameters:
vf - the ValueFactory to use for creation of the Literal
longValue - a long value
Returns:
a Literal of type XSD.LONG with the supplied value
Throws:
NullPointerException - if any of the input parameters is null.






literal

public static Literal literal(float floatValue)
Creates a new Literal with the supplied float value

Parameters:
floatValue - a float value
Returns:
a Literal of type XSD.FLOAT with the supplied value






literal

public static Literal literal(ValueFactory vf,
 float floatValue)
Creates a new Literal with the supplied float value

Parameters:
vf - the ValueFactory to use for creation of the Literal
floatValue - a float value
Returns:
a Literal of type XSD.FLOAT with the supplied value
Throws:
NullPointerException - if any of the input parameters is null.






literal

public static Literal literal(double doubleValue)
Creates a new Literal with the supplied double value

Parameters:
doubleValue - a double value
Returns:
a Literal of type XSD.DOUBLE with the supplied value






literal

public static Literal literal(ValueFactory vf,
 double doubleValue)
Creates a new Literal with the supplied double value

Parameters:
vf - the ValueFactory to use for creation of the Literal
doubleValue - a double value
Returns:
a Literal of type XSD.DOUBLE with the supplied value
Throws:
NullPointerException - if any of the input parameters is null.






literal

public static Literal literal(BigDecimal bigDecimal)
Creates a new Literal with the supplied BigDecimal value

Parameters:
bigDecimal - a BigDecimal value
Returns:
a Literal of type XSD.DECIMAL with the supplied value
Throws:
NullPointerException - if the supplied bigDecimal is null.






literal

public static Literal literal(ValueFactory vf,
 BigDecimal bigDecimal)
Creates a new Literal with the supplied BigDecimal value

Parameters:
vf - the ValueFactory to use for creation of the Literal
bigDecimal - a BigDecimal value
Returns:
a Literal of type XSD.DECIMAL with the supplied value
Throws:
NullPointerException - if any of the input parameters is null.






literal

public static Literal literal(BigInteger bigInteger)
Creates a new Literal with the supplied BigInteger value

Parameters:
bigInteger - a BigInteger value
Returns:
a Literal of type XSD.INTEGER with the supplied value
Throws:
NullPointerException - if the supplied bigInteger is null.






literal

public static Literal literal(ValueFactory vf,
 BigInteger bigInteger)
Creates a new Literal with the supplied BigInteger value

Parameters:
vf - the ValueFactory to use for creation of the Literal
bigInteger - a BigInteger value
Returns:
a Literal of type XSD.INTEGER with the supplied value
Throws:
NullPointerException - if any of the input parameters is null.






literal

public static Literal literal(TemporalAccessor value)
                       throws IllegalArgumentException
Creates a new Literal with the supplied TemporalAccessor value

Parameters:
value - a TemporalAccessor value.
Returns:
a Literal with the supplied calendar value and the appropriate XSD date/time datatype for
         the specific value.
Throws:
NullPointerException - if the supplied TemporalAccessor value is null.
IllegalArgumentException - if value cannot be represented by an XML Schema date/time datatype






literal

public static Literal literal(ValueFactory vf,
 TemporalAccessor value)
                       throws IllegalArgumentException
Creates a new Literal with the supplied TemporalAccessor value

Parameters:
vf - the ValueFactory to use for creation of the Literal
value - a TemporalAccessor value.
Returns:
a Literal with the supplied calendar value and the appropriate XSD date/time datatype for
         the specific value.
Throws:
NullPointerException - if any of the input parameters is null..
IllegalArgumentException - if value cannot be represented by an XML Schema date/time datatype






literal

public static Literal literal(Object object)
Creates a new typed Literal out of the supplied object, mapping the runtime type of the object to the
 appropriate XSD datatype. If no mapping is available, the method returns a literal with the string
 representation of the supplied object as the value, and XSD.STRING as the datatype.
 
 Recognized types are Boolean, Byte, Double, Float, Integer, Long,
 Short, XMLGregorianCalendar , TemporalAccessor and Date.

Parameters:
object - an object to be converted to a typed literal.
Returns:
a typed literal representation of the supplied object.
Throws:
NullPointerException - if the input parameter is null..






literal

public static Literal literal(Object object,
 boolean failOnUnknownType)
Creates a new typed Literal out of the supplied object, mapping the runtime type of the object to the
 appropriate XSD datatype.
 
 Recognized types are Boolean, Byte, Double, Float, BigDecimal,
 Integer, BigInteger, Long, Short, XMLGregorianCalendar,
 TemporalAccessor, 

invalid reference
TemporalAmpount

 and Date.

Parameters:
object - an object to be converted to a typed literal.
failOnUnknownType - If no mapping is available and failOnUnknownType is false the
                          method returns a literal with the string representation of the supplied object as the
                          value, and XSD.STRING as the datatype. If set to true the method
                          throws an IllegalArgumentException if no mapping available.
Returns:
a typed literal representation of the supplied object.
Throws:
NullPointerException - if the input parameter is null..






literal

public static Literal literal(ValueFactory vf,
 Object object,
 boolean failOnUnknownType)
Creates a new typed Literal out of the supplied object, mapping the runtime type of the object to the
 appropriate XSD datatype.
 
 Recognized types are Boolean, Byte, Double, Float, Integer, Long,
 Short, XMLGregorianCalendar, TemporalAccessor and Date.

Parameters:
object - an object to be converted to a typed literal.
failOnUnknownType - If no mapping is available and failOnUnknownType is false the
                          method returns a literal with the string representation of the supplied object as the
                          value, and XSD.STRING as the datatype. If set to true the method
                          throws an IllegalArgumentException if no mapping available.
valueFactory - the ValueFactoryto use for creation of the Literal
Returns:
a typed literal representation of the supplied object.
Throws:
NullPointerException - if any of the input parameters is null.
IllegalArgumentException - if failOnUnknownType is set to true and the runtime
                                  type of the supplied object could not be mapped.






triple

public static Triple triple(Resource subject,
 IRI predicate,
 Value object)
Creates a new RDF-star embedded triple with the supplied subject, predicate, and object.

Parameters:
subject - the Triple subject
predicate - the Triple predicate
object - the Triple object
Returns:
a Triple with the supplied subject, predicate, and object.
Throws:
NullPointerException - if any of the supplied input parameters is null.






triple

public static Triple triple(ValueFactory vf,
 Resource subject,
 IRI predicate,
 Value object)
Creates a new RDF-star embedded triple with the supplied subject, predicate, and object.

Parameters:
vf - the ValueFactory to use for creation of the Triple
subject - the Triple subject
predicate - the Triple predicate
object - the Triple object
Returns:
a Triple with the supplied subject, predicate, and object.
Throws:
NullPointerException - if any of the supplied input parameters is null.






triple

public static Triple triple(Statement statement)
Creates a new RDF-star embedded triple using the subject, predicate and object from the supplied
 Statement.

Parameters:
statement - the Statement from which to construct a Triple
Returns:
a Triple with the same subject, predicate, and object as the supplied Statement.
Throws:
NullPointerException - if statement is null.






triple

public static Triple triple(ValueFactory vf,
 Statement statement)
Creates a new RDF-star embedded triple using the subject, predicate and object from the supplied
 Statement.

Parameters:
vf - the ValueFactory to use for creation of the Triple
statement - the Statement from which to construct a Triple
Returns:
a Triple with the same subject, predicate, and object as the supplied Statement.
Throws:
NullPointerException - if any of the supplied input parameters is null.






namespace

public static Namespace namespace(String prefix,
 String name)
Create a new Namespace object.

Parameters:
prefix - the prefix associated with the namespace
name - the namespace name (typically an IRI) for the namespace.
Returns:
a Namespace object.
Since:
3.6.0






getValueFactory

public static ValueFactory getValueFactory()
Get a ValueFactory.

Returns:
a ValueFactory.












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface Model



All Superinterfaces:
Collection<Statement>, Iterable<Statement>, NamespaceAware, Serializable, Set<Statement>


All Known Implementing Classes:
AbstractMemoryOverflowModel, AbstractModel, DynamicModel, EmptyModel, FilteredModel, LinkedHashModel, SailModel, TreeModel



public interface Model
extends Set<Statement>, Serializable, NamespaceAware
An RDF Model, represented as a Set of Statements with predictable iteration order.
 
 Additional utility functionality for working with Model objects is available in the
 org.eclipse.rdf4j.model.util.Models utility class.

Author:
James Leigh, Jeen Broekstra








Method Summary

All MethodsInstance MethodsAbstract MethodsDefault Methods


Modifier and Type
Method
Description
boolean
add(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Adds one or more statements to the model.

boolean
clear(Resource... context)

Removes statements with the specified context exist in this model.

boolean
contains(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Determines if statements with the specified subject, predicate, object and (optionally) context exist in this
 model.

default Set<Resource>
contexts()

Returns a Set view of the contexts contained in this model.

Model
filter(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Returns a filtered view of the statements with the specified subject, predicate, object and (optionally) context.

default Iterable<Statement>
getStatements(Resource subject,
 IRI predicate,
 Value object,
 Resource... contexts)

Returns an Iterable over all Statements in this Model that match the supplied criteria.

Set<Value>
objects()

Returns a Set view of the objects contained in this model.

Set<IRI>
predicates()

Returns a Set view of the predicates contained in this model.

boolean
remove(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Removes statements with the specified subject, predicate, object and (optionally) context exist in this model.

Optional<Namespace>
removeNamespace(String prefix)

Removes a namespace declaration by removing the association between a prefix and a namespace name.

default Namespace
setNamespace(String prefix,
 String name)

Sets the prefix for a namespace.

void
setNamespace(Namespace namespace)

Sets the prefix for a namespace.

Set<Resource>
subjects()

Returns a Set view of the subjects contained in this model.

Model
unmodifiable()

Returns an unmodifiable view of this model.





Methods inherited from interface java.util.Collection
parallelStream, removeIf, stream, toArray

Methods inherited from interface java.lang.Iterable
forEach

Methods inherited from interface org.eclipse.rdf4j.model.NamespaceAware
getNamespace, getNamespaces

Methods inherited from interface java.util.Set
add, addAll, clear, contains, containsAll, equals, hashCode, isEmpty, iterator, remove, removeAll, retainAll, size, spliterator, toArray, toArray









Method Details



unmodifiable

Model unmodifiable()
Returns an unmodifiable view of this model. This method provides "read-only" access to this model. Query
 operations on the returned model "read through" to this model, and attempts to modify the returned model, whether
 direct or via its iterator, result in an UnsupportedOperationException.
 

Returns:
an unmodifiable view of the specified set.






setNamespace

default Namespace setNamespace(String prefix,
 String name)
Sets the prefix for a namespace. This will replace any existing namespace associated to the prefix.

Parameters:
prefix - The new prefix.
name - The namespace name that the prefix maps to.
Returns:
The Namespace object for the given namespace.






setNamespace

void setNamespace(Namespace namespace)
Sets the prefix for a namespace. This will replace any existing namespace associated to the prefix.

Parameters:
namespace - A Namespace object to use in this Model.






removeNamespace

Optional<Namespace> removeNamespace(String prefix)
Removes a namespace declaration by removing the association between a prefix and a namespace name.

Parameters:
prefix - The namespace prefix of which the assocation with a namespace name is to be removed.
Returns:
the previous namespace bound to the prefix or Optional.empty()






contains

boolean contains(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Determines if statements with the specified subject, predicate, object and (optionally) context exist in this
 model. The subject, predicate and object parameters can be null to indicate
 wildcards. The contexts parameter is a wildcard and accepts zero or more values. If no contexts are
 specified, statements will match disregarding their context. If one or more contexts are specified, statements
 with a context matching one of these will match. Note: to match statements without an associated context, specify
 the value null and explicitly cast it to type Resource.
 
 Examples: model.contains(s1, null, null) is true if any statements in this model have subject
 s1,
 model.contains(null, null, null, c1) is true if any statements in this model have context c1,
 model.contains(null, null, null, (Resource)null) is true if any statements in this model have no
 associated context,
 model.contains(null, null, null, c1, c2, c3) is true if any statements in this model have context
 c1, c2 or c3 .

Parameters:
subj - The subject of the statements to match, null to match statements with any subject.
pred - The predicate of the statements to match, null to match statements with any predicate.
obj - The object of the statements to match, null to match statements with any object.
contexts - The contexts of the statements to match. If no contexts are specified, statements will match
                 disregarding their context. If one or more contexts are specified, statements with a context
                 matching one of these will match.
Returns:
true if statements match the specified pattern.






add

boolean add(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Adds one or more statements to the model. This method creates a statement for each specified context and adds
 those to the model. If no contexts are specified, a single statement with no associated context is added. If this
 Model is a filtered Model then null (if context empty) values are permitted and will use the corresponding
 filtered values.

Parameters:
subj - The statement's subject.
pred - The statement's predicate.
obj - The statement's object.
contexts - The contexts to add statements to.
Throws:
IllegalArgumentException - If This Model cannot store the given statement, because it is filtered out
                                       of this view.
UnsupportedOperationException - If this Model cannot accept any statements, because it is filtered to the
                                       empty set.






clear

boolean clear(Resource... context)
Removes statements with the specified context exist in this model.

Parameters:
context - The context of the statements to remove.
Returns:
true if one or more statements have been removed.






remove

boolean remove(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Removes statements with the specified subject, predicate, object and (optionally) context exist in this model.
 The subject, predicate and object parameters can be null to indicate wildcards.
 The contexts parameter is a wildcard and accepts zero or more values. If no contexts are specified,
 statements will be removed disregarding their context. If one or more contexts are specified, statements with a
 context matching one of these will be removed. Note: to remove statements without an associated context, specify
 the value null and explicitly cast it to type Resource.
 
 Examples: model.remove(s1, null, null) removes any statements in this model have subject s1,
 model.remove(null, null, null, c1) removes any statements in this model have context c1 ,
 model.remove(null, null, null, (Resource)null) removes any statements in this model have no associated
 context,
 model.remove(null, null, null, c1, c2, c3) removes any statements in this model have context c1,
 c2 or c3.

Parameters:
subj - The subject of the statements to remove, null to remove statements with any subject.
pred - The predicate of the statements to remove, null to remove statements with any predicate.
obj - The object of the statements to remove, null to remove statements with any object.
contexts - The contexts of the statements to remove. If no contexts are specified, statements will be
                 removed disregarding their context. If one or more contexts are specified, statements with a
                 context matching one of these will be removed.
Returns:
true if one or more statements have been removed.






getStatements

default Iterable<Statement> getStatements(Resource subject,
 IRI predicate,
 Value object,
 Resource... contexts)
Returns an Iterable over all Statements in this Model that match the supplied criteria.
 
 Examples:
 
 model.getStatements(s1, null, null) matches all statements that have subject s1
 model.getStatements(s1, p1, null) matches all statements that have subject s1 and predicate
 p1
 model.getStatements(null, null, null, c1) matches all statements that have context c1
 model.getStatements(null, null, null, (Resource)null) matches all statements that have no associated
 context
 model.getStatements(null, null, null, c1, c2, c3) matches all statements that have context
 c1, c2 or c3
 

Parameters:
subject - The subject of the statements to match, null to match statements with any subject.
predicate - The predicate of the statements to match, null to match statements with any predicate.
object - The object of the statements to match, null to match statements with any object.
contexts - The contexts of the statements to match. If no contexts are specified, statements will match
                  disregarding their context. If one or more contexts are specified, statements with a context
                  matching any one of these will match. To match statements without an associated context, specify
                  the value null and explicitly cast it to type Resource.
Returns:
an Iterable over the statements in this Model that match the specified pattern.
Since:
3.2.0
See Also:


filter(Resource, IRI, Value, Resource...)








filter

Model filter(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Returns a filtered view of the statements with the specified subject, predicate, object and (optionally) context.
 The subject, predicate and object parameters can be null to indicate wildcards.
 The contexts parameter is a wildcard and accepts zero or more values. If no contexts are specified,
 statements will match disregarding their context. If one or more contexts are specified, statements with a
 context matching one of these will match. Note: to match statements without an associated context, specify the
 value null and explicitly cast it to type Resource.
 
 The returned model is backed by this Model, so changes to this Model are reflected in the returned model, and
 vice-versa. If this Model is modified while an iteration over the returned model is in progress (except through
 the iterator's own remove operation), the results of the iteration are undefined. The model supports
 element removal, which removes the corresponding statement from this Model, via the Iterator.remove,
 Set.remove, removeAll, retainAll, and clear operations. The statements passed to
 the add and addAll operations must match the parameter pattern.
 
 Examples: model.filter(s1, null, null) matches all statements that have subject s1,
 model.filter(null, null, null, c1) matches all statements that have context c1,
 model.filter(null, null, null, (Resource)null) matches all statements that have no associated
 context,
 model.filter(null, null, null, c1, c2, c3) matches all statements that have context c1,
 c2 or c3.

Parameters:
subj - The subject of the statements to match, null to match statements with any subject.
pred - The predicate of the statements to match, null to match statements with any predicate.
obj - The object of the statements to match, null to match statements with any object.
contexts - The contexts of the statements to match. If no contexts are specified, statements will match
                 disregarding their context. If one or more contexts are specified, statements with a context
                 matching one of these will match.
Returns:
The statements that match the specified pattern.
See Also:


getStatements(Resource, IRI, Value, Resource...)








subjects

Set<Resource> subjects()
Returns a Set view of the subjects contained in this model. The set is backed by the model, so changes to
 the model are reflected in the set, and vice-versa. If the model is modified while an iteration over the set is
 in progress (except through the iterator's own remove operation), the results of the iteration are
 undefined. The set supports element removal, which removes all statements from the model for which that element
 is a subject value, via the Iterator.remove, Set.remove, removeAll, retainAll,
 and clear operations. It does not support the add or addAll operations if the parameters
 pred or obj are null.

Returns:
a set view of the subjects contained in this model






predicates

Set<IRI> predicates()
Returns a Set view of the predicates contained in this model. The set is backed by the model, so changes
 to the model are reflected in the set, and vice-versa. If the model is modified while an iteration over the set
 is in progress (except through the iterator's own remove operation), the results of the iteration are
 undefined. The set supports element removal, which removes all statements from the model for which that element
 is a predicate value, via the Iterator.remove, Set.remove, removeAll, retainAll,
 and clear operations. It does not support the add or addAll operations if the parameters
 subj or obj are null.

Returns:
a set view of the predicates contained in this model






objects

Set<Value> objects()
Returns a Set view of the objects contained in this model. The set is backed by the model, so changes to
 the model are reflected in the set, and vice-versa. If the model is modified while an iteration over the set is
 in progress (except through the iterator's own remove operation), the results of the iteration are
 undefined. The set supports element removal, which removes all statements from the model for which that element
 is an object value, via the Iterator.remove, Set.remove, removeAll, retainAll,
 and clear operations. It does not support the add or addAll operations if the parameters
 subj or pred are null.

Returns:
a set view of the objects contained in this model






contexts

default Set<Resource> contexts()
Returns a Set view of the contexts contained in this model. The set is backed by the model, so changes to
 the model are reflected in the set, and vice-versa. If the model is modified while an iteration over the set is
 in progress (except through the iterator's own remove operation), the results of the iteration are
 undefined. The set supports element removal, which removes all statements from the model for which that element
 is a context value, via the Iterator.remove, Set.remove, removeAll, retainAll,
 and clear operations. It does not support the add or addAll operations if the parameters
 subj , pred or obj are null.

Returns:
a set view of the contexts contained in this model












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nClass DynamicModel

java.lang.Object
java.util.AbstractCollection<Statement>
java.util.AbstractSet<Statement>
org.eclipse.rdf4j.model.impl.DynamicModel





All Implemented Interfaces:
Serializable, Iterable<Statement>, Collection<Statement>, Set<Statement>, Model, NamespaceAware



public class DynamicModel
extends AbstractSet<Statement>
implements Model
A LinkedHashModel or a TreeModel achieves fast data access at the cost of higher indexing time. The DynamicModel
 postpones this cost until such access is actually needed. It stores all data in a LinkedHashMap and supports adding,
 retrieving and removing data. The model will upgrade to a full model (provided by the modelFactory) if more complex
 operations are called, for instance removing data according to a pattern (eg. all statements with rdf:type as
 predicate).
 
 DynamicModel is thread safe to the extent that the underlying LinkedHashMap or Model is. The upgrade path is
 protected by the actual upgrade method being synchronized. The LinkedHashMap storage is not removed once upgraded, so
 concurrent reads that have started reading from the LinkedHashMap can continue to read even during an upgrade. We do
 make the LinkedHashMap unmodifiable to reduce the chance of there being a bug.

Author:
Håvard Mikkelsen Ottestad
See Also:


Serialized Form










Constructor Summary
Constructors

Constructor
Description
DynamicModel(ModelFactory modelFactory)
 






Method Summary

All MethodsInstance MethodsConcrete Methods


Modifier and Type
Method
Description
boolean
add(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Adds one or more statements to the model.

boolean
add(Statement statement)
 
boolean
addAll(Collection<? extends Statement> c)
 
void
clear()
 
boolean
clear(Resource... context)

Removes statements with the specified context exist in this model.

boolean
contains(Object o)
 
boolean
contains(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Determines if statements with the specified subject, predicate, object and (optionally) context exist in this
 model.

boolean
containsAll(Collection<?> c)
 
Set<Resource>
contexts()

Returns a Set view of the contexts contained in this model.

boolean
equals(Object o)
 
Model
filter(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Returns a filtered view of the statements with the specified subject, predicate, object and (optionally) context.

Optional<Namespace>
getNamespace(String prefix)

Gets the namespace that is associated with the specified prefix, if any.

Set<Namespace>
getNamespaces()

Gets the set that contains the assigned namespaces.

Iterable<Statement>
getStatements(Resource subject,
 IRI predicate,
 Value object,
 Resource... contexts)

Returns an Iterable over all Statements in this Model that match the supplied criteria.

int
hashCode()
 
boolean
isEmpty()
 
Iterator<Statement>
iterator()
 
Set<Value>
objects()

Returns a Set view of the objects contained in this model.

Set<IRI>
predicates()

Returns a Set view of the predicates contained in this model.

boolean
remove(Object o)
 
boolean
remove(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Removes statements with the specified subject, predicate, object and (optionally) context exist in this model.

boolean
removeAll(Collection<?> c)
 
Optional<Namespace>
removeNamespace(String prefix)

Removes a namespace declaration by removing the association between a prefix and a namespace name.

boolean
retainAll(Collection<?> c)
 
Namespace
setNamespace(String prefix,
 String name)

Sets the prefix for a namespace.

void
setNamespace(Namespace namespace)

Sets the prefix for a namespace.

int
size()
 
Set<Resource>
subjects()

Returns a Set view of the subjects contained in this model.

Object[]
toArray()
 
<T> T[]
toArray(T[] a)
 
Model
unmodifiable()

Returns an unmodifiable view of this model.





Methods inherited from class java.util.AbstractCollection
toString

Methods inherited from class java.lang.Object
clone, finalize, getClass, notify, notifyAll, wait, wait, wait

Methods inherited from interface java.util.Collection
parallelStream, removeIf, stream, toArray

Methods inherited from interface java.lang.Iterable
forEach

Methods inherited from interface java.util.Set
spliterator









Constructor Details



DynamicModel

public DynamicModel(ModelFactory modelFactory)









Method Details



unmodifiable

public Model unmodifiable()
Description copied from interface: Model
Returns an unmodifiable view of this model. This method provides "read-only" access to this model. Query
 operations on the returned model "read through" to this model, and attempts to modify the returned model, whether
 direct or via its iterator, result in an UnsupportedOperationException.
 

Specified by:
unmodifiable in interface Model
Returns:
an unmodifiable view of the specified set.






getNamespace

public Optional<Namespace> getNamespace(String prefix)
Description copied from interface: NamespaceAware
Gets the namespace that is associated with the specified prefix, if any. If multiple namespaces match the given
 prefix, the result may not be consistent over successive calls to this method.

Specified by:
getNamespace in interface NamespaceAware
Parameters:
prefix - A namespace prefix.
Returns:
The namespace name that is associated with the specified prefix, or Optional.empty() if there is
         no such namespace.






getNamespaces

public Set<Namespace> getNamespaces()
Description copied from interface: NamespaceAware
Gets the set that contains the assigned namespaces.

Specified by:
getNamespaces in interface NamespaceAware
Returns:
A Set containing the Namespace objects that are available.






setNamespace

public Namespace setNamespace(String prefix,
 String name)
Description copied from interface: Model
Sets the prefix for a namespace. This will replace any existing namespace associated to the prefix.

Specified by:
setNamespace in interface Model
Parameters:
prefix - The new prefix.
name - The namespace name that the prefix maps to.
Returns:
The Namespace object for the given namespace.






setNamespace

public void setNamespace(Namespace namespace)
Description copied from interface: Model
Sets the prefix for a namespace. This will replace any existing namespace associated to the prefix.

Specified by:
setNamespace in interface Model
Parameters:
namespace - A Namespace object to use in this Model.






removeNamespace

public Optional<Namespace> removeNamespace(String prefix)
Description copied from interface: Model
Removes a namespace declaration by removing the association between a prefix and a namespace name.

Specified by:
removeNamespace in interface Model
Parameters:
prefix - The namespace prefix of which the assocation with a namespace name is to be removed.
Returns:
the previous namespace bound to the prefix or Optional.empty()






contains

public boolean contains(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Description copied from interface: Model
Determines if statements with the specified subject, predicate, object and (optionally) context exist in this
 model. The subject, predicate and object parameters can be null to indicate
 wildcards. The contexts parameter is a wildcard and accepts zero or more values. If no contexts are
 specified, statements will match disregarding their context. If one or more contexts are specified, statements
 with a context matching one of these will match. Note: to match statements without an associated context, specify
 the value null and explicitly cast it to type Resource.
 
 Examples: model.contains(s1, null, null) is true if any statements in this model have subject
 s1,
 model.contains(null, null, null, c1) is true if any statements in this model have context c1,
 model.contains(null, null, null, (Resource)null) is true if any statements in this model have no
 associated context,
 model.contains(null, null, null, c1, c2, c3) is true if any statements in this model have context
 c1, c2 or c3 .

Specified by:
contains in interface Model
Parameters:
subj - The subject of the statements to match, null to match statements with any subject.
pred - The predicate of the statements to match, null to match statements with any predicate.
obj - The object of the statements to match, null to match statements with any object.
contexts - The contexts of the statements to match. If no contexts are specified, statements will match
                 disregarding their context. If one or more contexts are specified, statements with a context
                 matching one of these will match.
Returns:
true if statements match the specified pattern.






add

public boolean add(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Description copied from interface: Model
Adds one or more statements to the model. This method creates a statement for each specified context and adds
 those to the model. If no contexts are specified, a single statement with no associated context is added. If this
 Model is a filtered Model then null (if context empty) values are permitted and will use the corresponding
 filtered values.

Specified by:
add in interface Model
Parameters:
subj - The statement's subject.
pred - The statement's predicate.
obj - The statement's object.
contexts - The contexts to add statements to.






clear

public boolean clear(Resource... context)
Description copied from interface: Model
Removes statements with the specified context exist in this model.

Specified by:
clear in interface Model
Parameters:
context - The context of the statements to remove.
Returns:
true if one or more statements have been removed.






remove

public boolean remove(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Description copied from interface: Model
Removes statements with the specified subject, predicate, object and (optionally) context exist in this model.
 The subject, predicate and object parameters can be null to indicate wildcards.
 The contexts parameter is a wildcard and accepts zero or more values. If no contexts are specified,
 statements will be removed disregarding their context. If one or more contexts are specified, statements with a
 context matching one of these will be removed. Note: to remove statements without an associated context, specify
 the value null and explicitly cast it to type Resource.
 
 Examples: model.remove(s1, null, null) removes any statements in this model have subject s1,
 model.remove(null, null, null, c1) removes any statements in this model have context c1 ,
 model.remove(null, null, null, (Resource)null) removes any statements in this model have no associated
 context,
 model.remove(null, null, null, c1, c2, c3) removes any statements in this model have context c1,
 c2 or c3.

Specified by:
remove in interface Model
Parameters:
subj - The subject of the statements to remove, null to remove statements with any subject.
pred - The predicate of the statements to remove, null to remove statements with any predicate.
obj - The object of the statements to remove, null to remove statements with any object.
contexts - The contexts of the statements to remove. If no contexts are specified, statements will be
                 removed disregarding their context. If one or more contexts are specified, statements with a
                 context matching one of these will be removed.
Returns:
true if one or more statements have been removed.






filter

public Model filter(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Description copied from interface: Model
Returns a filtered view of the statements with the specified subject, predicate, object and (optionally) context.
 The subject, predicate and object parameters can be null to indicate wildcards.
 The contexts parameter is a wildcard and accepts zero or more values. If no contexts are specified,
 statements will match disregarding their context. If one or more contexts are specified, statements with a
 context matching one of these will match. Note: to match statements without an associated context, specify the
 value null and explicitly cast it to type Resource.
 
 The returned model is backed by this Model, so changes to this Model are reflected in the returned model, and
 vice-versa. If this Model is modified while an iteration over the returned model is in progress (except through
 the iterator's own remove operation), the results of the iteration are undefined. The model supports
 element removal, which removes the corresponding statement from this Model, via the Iterator.remove,
 Set.remove, removeAll, retainAll, and clear operations. The statements passed to
 the add and addAll operations must match the parameter pattern.
 
 Examples: model.filter(s1, null, null) matches all statements that have subject s1,
 model.filter(null, null, null, c1) matches all statements that have context c1,
 model.filter(null, null, null, (Resource)null) matches all statements that have no associated
 context,
 model.filter(null, null, null, c1, c2, c3) matches all statements that have context c1,
 c2 or c3.

Specified by:
filter in interface Model
Parameters:
subj - The subject of the statements to match, null to match statements with any subject.
pred - The predicate of the statements to match, null to match statements with any predicate.
obj - The object of the statements to match, null to match statements with any object.
contexts - The contexts of the statements to match. If no contexts are specified, statements will match
                 disregarding their context. If one or more contexts are specified, statements with a context
                 matching one of these will match.
Returns:
The statements that match the specified pattern.
See Also:


Model.getStatements(Resource, IRI, Value, Resource...)








subjects

public Set<Resource> subjects()
Description copied from interface: Model
Returns a Set view of the subjects contained in this model. The set is backed by the model, so changes to
 the model are reflected in the set, and vice-versa. If the model is modified while an iteration over the set is
 in progress (except through the iterator's own remove operation), the results of the iteration are
 undefined. The set supports element removal, which removes all statements from the model for which that element
 is a subject value, via the Iterator.remove, Set.remove, removeAll, retainAll,
 and clear operations. It does not support the add or addAll operations if the parameters
 pred or obj are null.

Specified by:
subjects in interface Model
Returns:
a set view of the subjects contained in this model






predicates

public Set<IRI> predicates()
Description copied from interface: Model
Returns a Set view of the predicates contained in this model. The set is backed by the model, so changes
 to the model are reflected in the set, and vice-versa. If the model is modified while an iteration over the set
 is in progress (except through the iterator's own remove operation), the results of the iteration are
 undefined. The set supports element removal, which removes all statements from the model for which that element
 is a predicate value, via the Iterator.remove, Set.remove, removeAll, retainAll,
 and clear operations. It does not support the add or addAll operations if the parameters
 subj or obj are null.

Specified by:
predicates in interface Model
Returns:
a set view of the predicates contained in this model






objects

public Set<Value> objects()
Description copied from interface: Model
Returns a Set view of the objects contained in this model. The set is backed by the model, so changes to
 the model are reflected in the set, and vice-versa. If the model is modified while an iteration over the set is
 in progress (except through the iterator's own remove operation), the results of the iteration are
 undefined. The set supports element removal, which removes all statements from the model for which that element
 is an object value, via the Iterator.remove, Set.remove, removeAll, retainAll,
 and clear operations. It does not support the add or addAll operations if the parameters
 subj or pred are null.

Specified by:
objects in interface Model
Returns:
a set view of the objects contained in this model






contexts

public Set<Resource> contexts()
Description copied from interface: Model
Returns a Set view of the contexts contained in this model. The set is backed by the model, so changes to
 the model are reflected in the set, and vice-versa. If the model is modified while an iteration over the set is
 in progress (except through the iterator's own remove operation), the results of the iteration are
 undefined. The set supports element removal, which removes all statements from the model for which that element
 is a context value, via the Iterator.remove, Set.remove, removeAll, retainAll,
 and clear operations. It does not support the add or addAll operations if the parameters
 subj , pred or obj are null.

Specified by:
contexts in interface Model
Returns:
a set view of the contexts contained in this model






size

public int size()

Specified by:
size in interface Collection<Statement>
Specified by:
size in interface Set<Statement>
Specified by:
size in class AbstractCollection<Statement>






isEmpty

public boolean isEmpty()

Specified by:
isEmpty in interface Collection<Statement>
Specified by:
isEmpty in interface Set<Statement>
Overrides:
isEmpty in class AbstractCollection<Statement>






contains

public boolean contains(Object o)

Specified by:
contains in interface Collection<Statement>
Specified by:
contains in interface Set<Statement>
Overrides:
contains in class AbstractCollection<Statement>






iterator

public Iterator<Statement> iterator()

Specified by:
iterator in interface Collection<Statement>
Specified by:
iterator in interface Iterable<Statement>
Specified by:
iterator in interface Set<Statement>
Specified by:
iterator in class AbstractCollection<Statement>






toArray

public Object[] toArray()

Specified by:
toArray in interface Collection<Statement>
Specified by:
toArray in interface Set<Statement>
Overrides:
toArray in class AbstractCollection<Statement>






toArray

public <T> T[] toArray(T[] a)

Specified by:
toArray in interface Collection<Statement>
Specified by:
toArray in interface Set<Statement>
Overrides:
toArray in class AbstractCollection<Statement>






add

public boolean add(Statement statement)

Specified by:
add in interface Collection<Statement>
Specified by:
add in interface Set<Statement>
Overrides:
add in class AbstractCollection<Statement>






remove

public boolean remove(Object o)

Specified by:
remove in interface Collection<Statement>
Specified by:
remove in interface Set<Statement>
Overrides:
remove in class AbstractCollection<Statement>






containsAll

public boolean containsAll(Collection<?> c)

Specified by:
containsAll in interface Collection<Statement>
Specified by:
containsAll in interface Set<Statement>
Overrides:
containsAll in class AbstractCollection<Statement>






addAll

public boolean addAll(Collection<? extends Statement> c)

Specified by:
addAll in interface Collection<Statement>
Specified by:
addAll in interface Set<Statement>
Overrides:
addAll in class AbstractCollection<Statement>






retainAll

public boolean retainAll(Collection<?> c)

Specified by:
retainAll in interface Collection<Statement>
Specified by:
retainAll in interface Set<Statement>
Overrides:
retainAll in class AbstractCollection<Statement>






removeAll

public boolean removeAll(Collection<?> c)

Specified by:
removeAll in interface Collection<Statement>
Specified by:
removeAll in interface Set<Statement>
Overrides:
removeAll in class AbstractSet<Statement>






clear

public void clear()

Specified by:
clear in interface Collection<Statement>
Specified by:
clear in interface Set<Statement>
Overrides:
clear in class AbstractCollection<Statement>






getStatements

public Iterable<Statement> getStatements(Resource subject,
 IRI predicate,
 Value object,
 Resource... contexts)
Description copied from interface: Model
Returns an Iterable over all Statements in this Model that match the supplied criteria.
 
 Examples:
 
 model.getStatements(s1, null, null) matches all statements that have subject s1
 model.getStatements(s1, p1, null) matches all statements that have subject s1 and predicate
 p1
 model.getStatements(null, null, null, c1) matches all statements that have context c1
 model.getStatements(null, null, null, (Resource)null) matches all statements that have no associated
 context
 model.getStatements(null, null, null, c1, c2, c3) matches all statements that have context
 c1, c2 or c3
 

Specified by:
getStatements in interface Model
Parameters:
subject - The subject of the statements to match, null to match statements with any subject.
predicate - The predicate of the statements to match, null to match statements with any predicate.
object - The object of the statements to match, null to match statements with any object.
contexts - The contexts of the statements to match. If no contexts are specified, statements will match
                  disregarding their context. If one or more contexts are specified, statements with a context
                  matching any one of these will match. To match statements without an associated context, specify
                  the value null and explicitly cast it to type Resource.
Returns:
an Iterable over the statements in this Model that match the specified pattern.
See Also:


Model.filter(Resource, IRI, Value, Resource...)








equals

public boolean equals(Object o)

Specified by:
equals in interface Collection<Statement>
Specified by:
equals in interface Set<Statement>
Overrides:
equals in class AbstractSet<Statement>






hashCode

public int hashCode()

Specified by:
hashCode in interface Collection<Statement>
Specified by:
hashCode in interface Set<Statement>
Overrides:
hashCode in class AbstractSet<Statement>












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nClass TreeModel

java.lang.Object
java.util.AbstractCollection<Statement>
java.util.AbstractSet<Statement>
org.eclipse.rdf4j.model.impl.AbstractModel
org.eclipse.rdf4j.model.impl.TreeModel






All Implemented Interfaces:
Serializable, Iterable<Statement>, Collection<Statement>, Set<Statement>, SortedSet<Statement>, Model, NamespaceAware



public class TreeModel
extends AbstractModel
implements SortedSet<Statement>
A Red-Black tree based Model implementation. The model is sorted according to the lexical ordering of terms.
 
 This implementation provides guaranteed log(n) time cost for filtered access by any number of terms. If an index is
 not yet available for a set of positions, it is created at runtime using a TreeSet.
 
 Note that this implementation is not synchronized. If multiple threads access a model concurrently, even if
 all of them are read operations, it must be synchronized externally. This is typically accomplished by synchronizing
 on some object that naturally encapsulates the model. If no such object exists, the set should be "wrapped" using the
 Models.synchronizedModel method.
 

Author:
James Leigh
See Also:


Serialized Form










Field Summary

Fields inherited from class org.eclipse.rdf4j.model.impl.AbstractModel
NULL_CONTEXT





Constructor Summary
Constructors

Constructor
Description
TreeModel()
 
TreeModel(Collection<? extends Statement> c)
 
TreeModel(Set<Namespace> namespaces)
 
TreeModel(Set<Namespace> namespaces,
 Collection<? extends Statement> c)
 
TreeModel(Model model)
 






Method Summary

All MethodsInstance MethodsConcrete Methods


Modifier and Type
Method
Description
boolean
add(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Adds one or more statements to the model.

Statement
ceiling(Statement e)
 
void
clear()
 
Comparator<? super Statement>
comparator()
 
boolean
contains(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Determines if statements with the specified subject, predicate, object and (optionally) context exist in this
 model.

Model
filter(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Returns a filtered view of the statements with the specified subject, predicate, object and (optionally) context.

Statement
first()
 
Statement
floor(Statement e)
 
Optional<Namespace>
getNamespace(String prefix)

Gets the namespace that is associated with the specified prefix, if any.

Set<Namespace>
getNamespaces()

Gets the set that contains the assigned namespaces.

SortedSet<Statement>
headSet(Statement toElement)
 
Statement
higher(Statement e)
 
boolean
isEmpty()
 
Iterator<Statement>
iterator()
 
Statement
last()
 
Statement
lower(Statement e)
 
Statement
pollFirst()
 
Statement
pollLast()
 
boolean
remove(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Removes statements with the specified subject, predicate, object and (optionally) context exist in this model.

Optional<Namespace>
removeNamespace(String prefix)

Removes a namespace declaration by removing the association between a prefix and a namespace name.

void
removeTermIteration(Iterator<Statement> iterator,
 Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Called by aggregate sets when a term has been removed from a term iterator.

Namespace
setNamespace(String prefix,
 String name)

Sets the prefix for a namespace.

void
setNamespace(Namespace namespace)

Sets the prefix for a namespace.

int
size()
 
SortedSet<Statement>
subSet(Statement fromElement,
 Statement toElement)
 
SortedSet<Statement>
tailSet(Statement fromElement)
 




Methods inherited from class org.eclipse.rdf4j.model.impl.AbstractModel
add, addAll, clear, closeIterator, contains, containsAll, contexts, objects, predicates, remove, removeAll, retainAll, subjects, toArray, toArray, unmodifiable

Methods inherited from class java.util.AbstractSet
equals, hashCode

Methods inherited from class java.util.AbstractCollection
toString

Methods inherited from class java.lang.Object
clone, finalize, getClass, notify, notifyAll, wait, wait, wait

Methods inherited from interface java.util.Collection
parallelStream, removeIf, stream, toArray

Methods inherited from interface java.lang.Iterable
forEach

Methods inherited from interface org.eclipse.rdf4j.model.Model
getStatements

Methods inherited from interface java.util.Set
add, addAll, contains, containsAll, equals, hashCode, remove, removeAll, retainAll, toArray, toArray

Methods inherited from interface java.util.SortedSet
spliterator









Constructor Details



TreeModel

public TreeModel()





TreeModel

public TreeModel(Model model)





TreeModel

public TreeModel(Collection<? extends Statement> c)





TreeModel

public TreeModel(Set<Namespace> namespaces,
 Collection<? extends Statement> c)





TreeModel

public TreeModel(Set<Namespace> namespaces)









Method Details



getNamespace

public Optional<Namespace> getNamespace(String prefix)
Description copied from interface: NamespaceAware
Gets the namespace that is associated with the specified prefix, if any. If multiple namespaces match the given
 prefix, the result may not be consistent over successive calls to this method.

Specified by:
getNamespace in interface NamespaceAware
Parameters:
prefix - A namespace prefix.
Returns:
The namespace name that is associated with the specified prefix, or Optional.empty() if there is
         no such namespace.






getNamespaces

public Set<Namespace> getNamespaces()
Description copied from interface: NamespaceAware
Gets the set that contains the assigned namespaces.

Specified by:
getNamespaces in interface NamespaceAware
Returns:
A Set containing the Namespace objects that are available.






setNamespace

public Namespace setNamespace(String prefix,
 String name)
Description copied from interface: Model
Sets the prefix for a namespace. This will replace any existing namespace associated to the prefix.

Specified by:
setNamespace in interface Model
Parameters:
prefix - The new prefix.
name - The namespace name that the prefix maps to.
Returns:
The Namespace object for the given namespace.






setNamespace

public void setNamespace(Namespace namespace)
Description copied from interface: Model
Sets the prefix for a namespace. This will replace any existing namespace associated to the prefix.

Specified by:
setNamespace in interface Model
Parameters:
namespace - A Namespace object to use in this Model.






removeNamespace

public Optional<Namespace> removeNamespace(String prefix)
Description copied from interface: Model
Removes a namespace declaration by removing the association between a prefix and a namespace name.

Specified by:
removeNamespace in interface Model
Parameters:
prefix - The namespace prefix of which the assocation with a namespace name is to be removed.
Returns:
the previous namespace bound to the prefix or Optional.empty()






size

public int size()

Specified by:
size in interface Collection<Statement>
Specified by:
size in interface Set<Statement>
Specified by:
size in class AbstractCollection<Statement>






clear

public void clear()

Specified by:
clear in interface Collection<Statement>
Specified by:
clear in interface Set<Statement>
Overrides:
clear in class AbstractModel






comparator

public Comparator<? super Statement> comparator()

Specified by:
comparator in interface SortedSet<Statement>






first

public Statement first()

Specified by:
first in interface SortedSet<Statement>






last

public Statement last()

Specified by:
last in interface SortedSet<Statement>






lower

public Statement lower(Statement e)





floor

public Statement floor(Statement e)





ceiling

public Statement ceiling(Statement e)





higher

public Statement higher(Statement e)





pollFirst

public Statement pollFirst()





pollLast

public Statement pollLast()





subSet

public SortedSet<Statement> subSet(Statement fromElement,
 Statement toElement)

Specified by:
subSet in interface SortedSet<Statement>






headSet

public SortedSet<Statement> headSet(Statement toElement)

Specified by:
headSet in interface SortedSet<Statement>






tailSet

public SortedSet<Statement> tailSet(Statement fromElement)

Specified by:
tailSet in interface SortedSet<Statement>






add

public boolean add(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Description copied from interface: Model
Adds one or more statements to the model. This method creates a statement for each specified context and adds
 those to the model. If no contexts are specified, a single statement with no associated context is added. If this
 Model is a filtered Model then null (if context empty) values are permitted and will use the corresponding
 filtered values.

Specified by:
add in interface Model
Parameters:
subj - The statement's subject.
pred - The statement's predicate.
obj - The statement's object.
contexts - The contexts to add statements to.






contains

public boolean contains(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Description copied from interface: Model
Determines if statements with the specified subject, predicate, object and (optionally) context exist in this
 model. The subject, predicate and object parameters can be null to indicate
 wildcards. The contexts parameter is a wildcard and accepts zero or more values. If no contexts are
 specified, statements will match disregarding their context. If one or more contexts are specified, statements
 with a context matching one of these will match. Note: to match statements without an associated context, specify
 the value null and explicitly cast it to type Resource.
 
 Examples: model.contains(s1, null, null) is true if any statements in this model have subject
 s1,
 model.contains(null, null, null, c1) is true if any statements in this model have context c1,
 model.contains(null, null, null, (Resource)null) is true if any statements in this model have no
 associated context,
 model.contains(null, null, null, c1, c2, c3) is true if any statements in this model have context
 c1, c2 or c3 .

Specified by:
contains in interface Model
Parameters:
subj - The subject of the statements to match, null to match statements with any subject.
pred - The predicate of the statements to match, null to match statements with any predicate.
obj - The object of the statements to match, null to match statements with any object.
contexts - The contexts of the statements to match. If no contexts are specified, statements will match
                 disregarding their context. If one or more contexts are specified, statements with a context
                 matching one of these will match.
Returns:
true if statements match the specified pattern.






remove

public boolean remove(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Description copied from interface: Model
Removes statements with the specified subject, predicate, object and (optionally) context exist in this model.
 The subject, predicate and object parameters can be null to indicate wildcards.
 The contexts parameter is a wildcard and accepts zero or more values. If no contexts are specified,
 statements will be removed disregarding their context. If one or more contexts are specified, statements with a
 context matching one of these will be removed. Note: to remove statements without an associated context, specify
 the value null and explicitly cast it to type Resource.
 
 Examples: model.remove(s1, null, null) removes any statements in this model have subject s1,
 model.remove(null, null, null, c1) removes any statements in this model have context c1 ,
 model.remove(null, null, null, (Resource)null) removes any statements in this model have no associated
 context,
 model.remove(null, null, null, c1, c2, c3) removes any statements in this model have context c1,
 c2 or c3.

Specified by:
remove in interface Model
Parameters:
subj - The subject of the statements to remove, null to remove statements with any subject.
pred - The predicate of the statements to remove, null to remove statements with any predicate.
obj - The object of the statements to remove, null to remove statements with any object.
contexts - The contexts of the statements to remove. If no contexts are specified, statements will be
                 removed disregarding their context. If one or more contexts are specified, statements with a
                 context matching one of these will be removed.
Returns:
true if one or more statements have been removed.






iterator

public Iterator<Statement> iterator()

Specified by:
iterator in interface Collection<Statement>
Specified by:
iterator in interface Iterable<Statement>
Specified by:
iterator in interface Set<Statement>
Specified by:
iterator in class AbstractCollection<Statement>






filter

public Model filter(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Description copied from interface: Model
Returns a filtered view of the statements with the specified subject, predicate, object and (optionally) context.
 The subject, predicate and object parameters can be null to indicate wildcards.
 The contexts parameter is a wildcard and accepts zero or more values. If no contexts are specified,
 statements will match disregarding their context. If one or more contexts are specified, statements with a
 context matching one of these will match. Note: to match statements without an associated context, specify the
 value null and explicitly cast it to type Resource.
 
 The returned model is backed by this Model, so changes to this Model are reflected in the returned model, and
 vice-versa. If this Model is modified while an iteration over the returned model is in progress (except through
 the iterator's own remove operation), the results of the iteration are undefined. The model supports
 element removal, which removes the corresponding statement from this Model, via the Iterator.remove,
 Set.remove, removeAll, retainAll, and clear operations. The statements passed to
 the add and addAll operations must match the parameter pattern.
 
 Examples: model.filter(s1, null, null) matches all statements that have subject s1,
 model.filter(null, null, null, c1) matches all statements that have context c1,
 model.filter(null, null, null, (Resource)null) matches all statements that have no associated
 context,
 model.filter(null, null, null, c1, c2, c3) matches all statements that have context c1,
 c2 or c3.

Specified by:
filter in interface Model
Parameters:
subj - The subject of the statements to match, null to match statements with any subject.
pred - The predicate of the statements to match, null to match statements with any predicate.
obj - The object of the statements to match, null to match statements with any object.
contexts - The contexts of the statements to match. If no contexts are specified, statements will match
                 disregarding their context. If one or more contexts are specified, statements with a context
                 matching one of these will match.
Returns:
The statements that match the specified pattern.
See Also:


Model.getStatements(Resource, IRI, Value, Resource...)








removeTermIteration

public void removeTermIteration(Iterator<Statement> iterator,
 Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Description copied from class: AbstractModel
Called by aggregate sets when a term has been removed from a term iterator. Exactly one of the last four terms
 will be non-empty.

Specified by:
removeTermIteration in class AbstractModel
Parameters:
iterator - The iterator used to navigate the live set (never null)
subj - the subject term to be removed or null
pred - the predicate term to be removed or null
obj - the object term to be removed or null
contexts - an array of one context term to be removed or an empty array






isEmpty

public boolean isEmpty()

Specified by:
isEmpty in interface Collection<Statement>
Specified by:
isEmpty in interface Set<Statement>
Overrides:
isEmpty in class AbstractModel












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nClass LinkedHashModel

java.lang.Object
java.util.AbstractCollection<Statement>
java.util.AbstractSet<Statement>
org.eclipse.rdf4j.model.impl.AbstractModel
org.eclipse.rdf4j.model.impl.LinkedHashModel






All Implemented Interfaces:
Serializable, Iterable<Statement>, Collection<Statement>, Set<Statement>, Model, NamespaceAware



public class LinkedHashModel
extends AbstractModel
Hash table based implementation of the Model interface.
 
 This implementation provides constant-time performance for filters using a single term, assuming the hash function
 disperses the elements properly among the buckets. Each term is indexed using a HashMap. When multiple terms
 are provided in a filter the index, of the term that reduces the possible Statements the most, is used and a
 sequential scan is used to filter additional terms.
 
 Note that this implementation is not synchronized. If multiple threads access a model concurrently, and at
 least one of the threads modifies the model, it must be synchronized externally. This is typically accomplished by
 synchronizing on some object that naturally encapsulates the model. If no such object exists, the set should be
 "wrapped" using the * Models.synchronizedModel method.
 

Author:
James Leigh
See Also:


Serialized Form










Nested Class Summary
Nested Classes

Modifier and Type
Class
Description
static class 
LinkedHashModel.ModelStatement
 






Field Summary

Fields inherited from class org.eclipse.rdf4j.model.impl.AbstractModel
NULL_CONTEXT





Constructor Summary
Constructors

Constructor
Description
LinkedHashModel()
 
LinkedHashModel(int size)
 
LinkedHashModel(Collection<? extends Statement> c)
 
LinkedHashModel(Set<Namespace> namespaces)
 
LinkedHashModel(Set<Namespace> namespaces,
 int size)
 
LinkedHashModel(Set<Namespace> namespaces,
 Collection<? extends Statement> c)
 
LinkedHashModel(Model model)
 






Method Summary

All MethodsInstance MethodsConcrete Methods


Modifier and Type
Method
Description
boolean
add(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Adds one or more statements to the model.

boolean
add(Statement statement)
 
void
clear()
 
boolean
contains(Object o)
 
boolean
contains(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Determines if statements with the specified subject, predicate, object and (optionally) context exist in this
 model.

Model
filter(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Returns a filtered view of the statements with the specified subject, predicate, object and (optionally) context.

Optional<Namespace>
getNamespace(String prefix)

Gets the namespace that is associated with the specified prefix, if any.

Set<Namespace>
getNamespaces()

Gets the set that contains the assigned namespaces.

boolean
isEmpty()
 
Iterator
iterator()
 
boolean
remove(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Removes statements with the specified subject, predicate, object and (optionally) context exist in this model.

Optional<Namespace>
removeNamespace(String prefix)

Removes a namespace declaration by removing the association between a prefix and a namespace name.

void
removeTermIteration(Iterator iterator,
 Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Called by aggregate sets when a term has been removed from a term iterator.

Namespace
setNamespace(String prefix,
 String name)

Sets the prefix for a namespace.

void
setNamespace(Namespace namespace)

Sets the prefix for a namespace.

int
size()
 




Methods inherited from class org.eclipse.rdf4j.model.impl.AbstractModel
addAll, clear, closeIterator, containsAll, contexts, objects, predicates, remove, removeAll, retainAll, subjects, toArray, toArray, unmodifiable

Methods inherited from class java.util.AbstractSet
equals, hashCode

Methods inherited from class java.util.AbstractCollection
toString

Methods inherited from class java.lang.Object
clone, finalize, getClass, notify, notifyAll, wait, wait, wait

Methods inherited from interface java.util.Collection
parallelStream, removeIf, stream, toArray

Methods inherited from interface java.lang.Iterable
forEach

Methods inherited from interface org.eclipse.rdf4j.model.Model
getStatements

Methods inherited from interface java.util.Set
equals, hashCode, spliterator









Constructor Details



LinkedHashModel

public LinkedHashModel()





LinkedHashModel

public LinkedHashModel(Model model)





LinkedHashModel

public LinkedHashModel(Collection<? extends Statement> c)





LinkedHashModel

public LinkedHashModel(int size)





LinkedHashModel

public LinkedHashModel(Set<Namespace> namespaces,
 Collection<? extends Statement> c)





LinkedHashModel

public LinkedHashModel(Set<Namespace> namespaces)





LinkedHashModel

public LinkedHashModel(Set<Namespace> namespaces,
 int size)









Method Details



getNamespace

public Optional<Namespace> getNamespace(String prefix)
Description copied from interface: NamespaceAware
Gets the namespace that is associated with the specified prefix, if any. If multiple namespaces match the given
 prefix, the result may not be consistent over successive calls to this method.

Parameters:
prefix - A namespace prefix.
Returns:
The namespace name that is associated with the specified prefix, or Optional.empty() if there is
         no such namespace.






getNamespaces

public Set<Namespace> getNamespaces()
Description copied from interface: NamespaceAware
Gets the set that contains the assigned namespaces.

Returns:
A Set containing the Namespace objects that are available.






setNamespace

public Namespace setNamespace(String prefix,
 String name)
Description copied from interface: Model
Sets the prefix for a namespace. This will replace any existing namespace associated to the prefix.

Parameters:
prefix - The new prefix.
name - The namespace name that the prefix maps to.
Returns:
The Namespace object for the given namespace.






setNamespace

public void setNamespace(Namespace namespace)
Description copied from interface: Model
Sets the prefix for a namespace. This will replace any existing namespace associated to the prefix.

Parameters:
namespace - A Namespace object to use in this Model.






removeNamespace

public Optional<Namespace> removeNamespace(String prefix)
Description copied from interface: Model
Removes a namespace declaration by removing the association between a prefix and a namespace name.

Parameters:
prefix - The namespace prefix of which the assocation with a namespace name is to be removed.
Returns:
the previous namespace bound to the prefix or Optional.empty()






size

public int size()

Specified by:
size in interface Collection<Statement>
Specified by:
size in interface Set<Statement>
Specified by:
size in class AbstractCollection<Statement>






add

public boolean add(Statement statement)

Specified by:
add in interface Collection<Statement>
Specified by:
add in interface Set<Statement>
Overrides:
add in class AbstractModel






add

public boolean add(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Description copied from interface: Model
Adds one or more statements to the model. This method creates a statement for each specified context and adds
 those to the model. If no contexts are specified, a single statement with no associated context is added. If this
 Model is a filtered Model then null (if context empty) values are permitted and will use the corresponding
 filtered values.

Parameters:
subj - The statement's subject.
pred - The statement's predicate.
obj - The statement's object.
contexts - The contexts to add statements to.






clear

public void clear()

Specified by:
clear in interface Collection<Statement>
Specified by:
clear in interface Set<Statement>
Overrides:
clear in class AbstractModel






contains

public boolean contains(Object o)

Specified by:
contains in interface Collection<Statement>
Specified by:
contains in interface Set<Statement>
Overrides:
contains in class AbstractModel






iterator

public Iterator iterator()

Specified by:
iterator in interface Collection<Statement>
Specified by:
iterator in interface Iterable<Statement>
Specified by:
iterator in interface Set<Statement>
Specified by:
iterator in class AbstractCollection<Statement>






contains

public boolean contains(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Description copied from interface: Model
Determines if statements with the specified subject, predicate, object and (optionally) context exist in this
 model. The subject, predicate and object parameters can be null to indicate
 wildcards. The contexts parameter is a wildcard and accepts zero or more values. If no contexts are
 specified, statements will match disregarding their context. If one or more contexts are specified, statements
 with a context matching one of these will match. Note: to match statements without an associated context, specify
 the value null and explicitly cast it to type Resource.
 
 Examples: model.contains(s1, null, null) is true if any statements in this model have subject
 s1,
 model.contains(null, null, null, c1) is true if any statements in this model have context c1,
 model.contains(null, null, null, (Resource)null) is true if any statements in this model have no
 associated context,
 model.contains(null, null, null, c1, c2, c3) is true if any statements in this model have context
 c1, c2 or c3 .

Parameters:
subj - The subject of the statements to match, null to match statements with any subject.
pred - The predicate of the statements to match, null to match statements with any predicate.
obj - The object of the statements to match, null to match statements with any object.
contexts - The contexts of the statements to match. If no contexts are specified, statements will match
                 disregarding their context. If one or more contexts are specified, statements with a context
                 matching one of these will match.
Returns:
true if statements match the specified pattern.






remove

public boolean remove(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Description copied from interface: Model
Removes statements with the specified subject, predicate, object and (optionally) context exist in this model.
 The subject, predicate and object parameters can be null to indicate wildcards.
 The contexts parameter is a wildcard and accepts zero or more values. If no contexts are specified,
 statements will be removed disregarding their context. If one or more contexts are specified, statements with a
 context matching one of these will be removed. Note: to remove statements without an associated context, specify
 the value null and explicitly cast it to type Resource.
 
 Examples: model.remove(s1, null, null) removes any statements in this model have subject s1,
 model.remove(null, null, null, c1) removes any statements in this model have context c1 ,
 model.remove(null, null, null, (Resource)null) removes any statements in this model have no associated
 context,
 model.remove(null, null, null, c1, c2, c3) removes any statements in this model have context c1,
 c2 or c3.

Parameters:
subj - The subject of the statements to remove, null to remove statements with any subject.
pred - The predicate of the statements to remove, null to remove statements with any predicate.
obj - The object of the statements to remove, null to remove statements with any object.
contexts - The contexts of the statements to remove. If no contexts are specified, statements will be
                 removed disregarding their context. If one or more contexts are specified, statements with a
                 context matching one of these will be removed.
Returns:
true if one or more statements have been removed.






filter

public Model filter(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Description copied from interface: Model
Returns a filtered view of the statements with the specified subject, predicate, object and (optionally) context.
 The subject, predicate and object parameters can be null to indicate wildcards.
 The contexts parameter is a wildcard and accepts zero or more values. If no contexts are specified,
 statements will match disregarding their context. If one or more contexts are specified, statements with a
 context matching one of these will match. Note: to match statements without an associated context, specify the
 value null and explicitly cast it to type Resource.
 
 The returned model is backed by this Model, so changes to this Model are reflected in the returned model, and
 vice-versa. If this Model is modified while an iteration over the returned model is in progress (except through
 the iterator's own remove operation), the results of the iteration are undefined. The model supports
 element removal, which removes the corresponding statement from this Model, via the Iterator.remove,
 Set.remove, removeAll, retainAll, and clear operations. The statements passed to
 the add and addAll operations must match the parameter pattern.
 
 Examples: model.filter(s1, null, null) matches all statements that have subject s1,
 model.filter(null, null, null, c1) matches all statements that have context c1,
 model.filter(null, null, null, (Resource)null) matches all statements that have no associated
 context,
 model.filter(null, null, null, c1, c2, c3) matches all statements that have context c1,
 c2 or c3.

Parameters:
subj - The subject of the statements to match, null to match statements with any subject.
pred - The predicate of the statements to match, null to match statements with any predicate.
obj - The object of the statements to match, null to match statements with any object.
contexts - The contexts of the statements to match. If no contexts are specified, statements will match
                 disregarding their context. If one or more contexts are specified, statements with a context
                 matching one of these will match.
Returns:
The statements that match the specified pattern.
See Also:


Model.getStatements(Resource, IRI, Value, Resource...)








removeTermIteration

public void removeTermIteration(Iterator iterator,
 Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Description copied from class: AbstractModel
Called by aggregate sets when a term has been removed from a term iterator. Exactly one of the last four terms
 will be non-empty.

Specified by:
removeTermIteration in class AbstractModel
Parameters:
iterator - The iterator used to navigate the live set (never null)
subj - the subject term to be removed or null
pred - the predicate term to be removed or null
obj - the object term to be removed or null
contexts - an array of one context term to be removed or an empty array






isEmpty

public boolean isEmpty()

Specified by:
isEmpty in interface Collection<Statement>
Specified by:
isEmpty in interface Set<Statement>
Overrides:
isEmpty in class AbstractModel












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nClass ModelBuilder

java.lang.Object
org.eclipse.rdf4j.model.util.ModelBuilder




public class ModelBuilder
extends Object
Builder to facilitate easier creation of new RDF Model objects via a fluent interface. All methods returning
 a ModelBuilder return an immutable reference to the current object, allowing method chaining.
 
 Usage example:

  
    ModelBuilder builder = new ModelBuilder();

    // set some namespaces
    builder.setNamespace("ex", "http://example.org/").setNamespace(FOAF.NS);

    // add a new named graph to the model
    builder.namedGraph("ex:graph1")
               // add statements about resource ex:john
              .subject("ex:john")
                  .add(FOAF.NAME, "John") // add the triple (ex:john, foaf:name "John") to the named graph
                  .add(FOAF.AGE, 42)
                  .add(FOAF.MBOX, "john@example.org");

     // add a triple to the default graph
    builder.defaultGraph().subject("ex:graph1").add(RDF.TYPE, "ex:Graph");

    // return the Model object
    Model m = builder.build();
 
 

Author:
Jeen Broekstra








Constructor Summary
Constructors

Constructor
Description
ModelBuilder()

Create a new ModelBuilder.

ModelBuilder(Model model)

Create a new ModelBuilder which will append to the supplied Model.







Method Summary

All MethodsInstance MethodsConcrete Methods


Modifier and Type
Method
Description
ModelBuilder
add(String predicate,
 Object object)

Add an RDF statement with the predicate and object to the model, using the current subject and graph (either
 named or default).

ModelBuilder
add(String subject,
 String predicate,
 Object object)

Add an RDF statement with the given subject, predicate and object to the model, using the current graph (either
 named or default).

ModelBuilder
add(String subject,
 IRI predicate,
 Object object)

Add an RDF statement with the given subject, predicate and object to the model, using the current graph (either
 named or default).

ModelBuilder
add(IRI predicate,
 Object object)

Add an RDF statement with the predicate and object to the model, using the current subject and graph (either
 named or default).

ModelBuilder
add(Resource subject,
 IRI predicate,
 Object object)

Add an RDF statement with the given subject, predicate and object to the model, using the current graph (either
 named or default).

Model
build()

Return the created Model

ModelBuilder
defaultGraph()

Set the current graph in which to add new statements to the default graph.

ModelBuilder
namedGraph(String prefixedNameOrIRI)

Set the current graph in which to add new statements to the supplied named graph.

ModelBuilder
namedGraph(Resource namedGraph)

Set the current graph in which to add new statements to the supplied named graph.

ModelBuilder
setNamespace(String prefix,
 String namespace)

Set the namespace mapping defined by the supplied prefix and name

ModelBuilder
setNamespace(Namespace ns)

Set the supplied Namespace mapping.

ModelBuilder
subject(String prefixedNameOrIri)

Set the subject about which statements are to be added to the model, defined by a prefixed name or an IRI
 reference.

ModelBuilder
subject(Resource subject)

Set the subject resource about which statements are to be added to the model.





Methods inherited from class java.lang.Object
clone, equals, finalize, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait









Constructor Details



ModelBuilder

public ModelBuilder()
Create a new ModelBuilder.





ModelBuilder

public ModelBuilder(Model model)
Create a new ModelBuilder which will append to the supplied Model.

Parameters:
model - 










Method Details



setNamespace

public ModelBuilder setNamespace(Namespace ns)
Set the supplied Namespace mapping.

Parameters:
ns - a Namespace to add to the model
Returns:
the ModelBuilder






setNamespace

public ModelBuilder setNamespace(String prefix,
 String namespace)
Set the namespace mapping defined by the supplied prefix and name

Parameters:
prefix - prefix of the namespace to add to the model.
namespace - namespace name to add to the model.
Returns:
the ModelBuilder






subject

public ModelBuilder subject(Resource subject)
Set the subject resource about which statements are to be added to the model.

Parameters:
subject - the subject resource about which statements are to be added.
Returns:
the ModelBuilder






subject

public ModelBuilder subject(String prefixedNameOrIri)
Set the subject about which statements are to be added to the model, defined by a prefixed name or an IRI
 reference.

Parameters:
prefixedNameOrIri - the subject resource about which statements are to be added. This can be defined either
                          as a prefixed name string (e.g. "ex:john"), or as a full IRI (e.g.
                          "http://example.org/john"). If supplied as a prefixed name, the ModelBuilder
                          will need to have a namespace mapping for the prefix.
Returns:
the ModelBuilder






namedGraph

public ModelBuilder namedGraph(Resource namedGraph)
Set the current graph in which to add new statements to the supplied named graph. This method resets the current
 subject.

Parameters:
namedGraph - a named graph identifier
Returns:
this ModelBuilder






namedGraph

public ModelBuilder namedGraph(String prefixedNameOrIRI)
Set the current graph in which to add new statements to the supplied named graph. This method clears the current
 subject.

Parameters:
prefixedNameOrIRI - a named graph identifier. This can be defined either as a prefixed name string (e.g.
                          "ex:john"), or as a full IRI (e.g. "http://example.org/john"). If supplied as a prefixed
                          name, the ModelBuilder will need to have a namespace mapping for the prefix.
Returns:
this ModelBuilder






defaultGraph

public ModelBuilder defaultGraph()
Set the current graph in which to add new statements to the default graph. This method clears the current
 subject.

Returns:
this ModelBuilder






add

public ModelBuilder add(Resource subject,
 IRI predicate,
 Object object)
Add an RDF statement with the given subject, predicate and object to the model, using the current graph (either
 named or default).

Parameters:
subject - the statement's subject
predicate - the statement's predicate
object - the statement's object. If the supplied object is a BNode, IRI, or
                  Literal, the object is used directly. If it is a prefixed name String with a known
                  prefix, it is mapped to an IRI. Otherwise a typed Literal is created out of the supplied
                  object, mapping the runtime type of the object to the appropriate XML Schema type. If no mapping
                  is available, the method creates a literal with the string representation of the supplied object
                  as the value, and XSD.STRING as the datatype. Recognized types are Boolean ,
                  Byte, Double, Float, Integer, Long, Short,
                  XMLGregorianCalendar , and Date.
Returns:
this ModelBuilder
See Also:


namedGraph(Resource)
defaultGraph()
Literals.createLiteral(ValueFactory, Object)








add

public ModelBuilder add(String subject,
 IRI predicate,
 Object object)
Add an RDF statement with the given subject, predicate and object to the model, using the current graph (either
 named or default).

Parameters:
subject - the statement's subject. This can be defined either as a prefixed name string (e.g. "ex:john"),
                  or as a full IRI (e.g. "http://example.org/john"). If supplied as a prefixed name, the
                  ModelBuilder will need to have a namespace mapping for the prefix.
predicate - the statement's predicate
object - the statement's object. If the supplied object is a BNode, IRI, or
                  Literal, the object is used directly. If it is a prefixed name String with a known
                  prefix, it is mapped to an IRI. Otherwise a typed Literal is created out of the supplied
                  object, mapping the runtime type of the object to the appropriate XML Schema type. If no mapping
                  is available, the method creates a literal with the string representation of the supplied object
                  as the value, and XSD.STRING as the datatype. Recognized types are Boolean ,
                  Byte, Double, Float, Integer, Long, Short,
                  XMLGregorianCalendar , and Date.
Returns:
this ModelBuilder
See Also:


namedGraph(Resource)
defaultGraph()
Literals.createLiteral(ValueFactory, Object)








add

public ModelBuilder add(String subject,
 String predicate,
 Object object)
Add an RDF statement with the given subject, predicate and object to the model, using the current graph (either
 named or default).

Parameters:
subject - the statement's subject. This can be defined either as a prefixed name string (e.g. "ex:john"),
                  or as a full IRI (e.g. "http://example.org/john"). If supplied as a prefixed name, the
                  ModelBuilder will need to have a namespace mapping for the prefix.
predicate - the statement's predicate. This can be defined either as a prefixed name string (e.g.
                  "ex:john"), or as a full IRI (e.g. "http://example.org/john"). If supplied as a prefixed name,
                  the ModelBuilder will need to have a namespace mapping for the prefix.
object - the statement's object. If the supplied object is a BNode, IRI, or
                  Literal, the object is used directly. If it is a prefixed name String with a known
                  prefix, it is mapped to an IRI. Otherwise a typed Literal is created out of the supplied
                  object, mapping the runtime type of the object to the appropriate XML Schema type. If no mapping
                  is available, the method creates a literal with the string representation of the supplied object
                  as the value, and XSD.STRING as the datatype. Recognized types are Boolean ,
                  Byte, Double, Float, Integer, Long, Short,
                  XMLGregorianCalendar , and Date.
Returns:
this ModelBuilder
See Also:


namedGraph(Resource)
defaultGraph()
Literals.createLiteral(ValueFactory, Object)








add

public ModelBuilder add(IRI predicate,
 Object object)
Add an RDF statement with the predicate and object to the model, using the current subject and graph (either
 named or default).

Parameters:
predicate - the statement's predicate.
object - the statement's object. If the supplied object is a BNode, IRI, or
                  Literal, the object is used directly. If it is a prefixed name String with a known
                  prefix, it is mapped to an IRI. Otherwise a typed Literal is created out of the supplied
                  object, mapping the runtime type of the object to the appropriate XML Schema type. If no mapping
                  is available, the method creates a literal with the string representation of the supplied object
                  as the value, and XSD.STRING as the datatype. Recognized types are Boolean ,
                  Byte, Double, Float, Integer, Long, Short,
                  XMLGregorianCalendar , and Date.
Returns:
this ModelBuilder
Throws:
ModelException - if the current subject is not set using subject(Resource) or
                        subject(String).






add

public ModelBuilder add(String predicate,
 Object object)
Add an RDF statement with the predicate and object to the model, using the current subject and graph (either
 named or default).

Parameters:
predicate - the statement's predicate. This can be defined either as a prefixed name string (e.g.
                  "ex:john"), or as a full IRI (e.g. "http://example.org/john"). If supplied as a prefixed name,
                  the ModelBuilder will need to have a namespace mapping for the prefix.
object - the statement's object. If the supplied object is a BNode, IRI, or
                  Literal, the object is used directly. If it is a prefixed name String with a known
                  prefix, it is mapped to an IRI. Otherwise a typed Literal is created out of the supplied
                  object, mapping the runtime type of the object to the appropriate XML Schema type. If no mapping
                  is available, the method creates a literal with the string representation of the supplied object
                  as the value, and XSD.STRING as the datatype. Recognized types are Boolean ,
                  Byte, Double, Float, Integer, Long, Short,
                  XMLGregorianCalendar , and Date.
Returns:
this ModelBuilder
Throws:
ModelException - if the current subject is not set using subject(Resource) or
                        subject(String).






build

public Model build()
Return the created Model

Returns:
the Model












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nClass Rio

java.lang.Object
org.eclipse.rdf4j.rio.Rio




public class Rio
extends Object
Static methods for parsing and writing RDF for all available syntaxes.
 
 It includes methods for searching for RDFFormats based on MIME types and file extensions, creating
 RDFParsers and RDFWriters, and directly parsing and writing.

Author:
Arjohn Kampman, Peter Ansell








Constructor Summary
Constructors

Constructor
Description
Rio()
 






Method Summary

All MethodsStatic MethodsConcrete Methods


Modifier and Type
Method
Description
static RDFParser
createParser(RDFFormat format)

Convenience methods for creating RDFParser objects.This method uses the registry returned by
 RDFParserRegistry.getInstance() to get a factory for the specified format and uses this factory to create
 the appropriate parser.

static RDFParser
createParser(RDFFormat format,
 ValueFactory valueFactory)

Convenience methods for creating RDFParser objects that use the specified ValueFactory to create RDF model
 objects.

static RDFWriter
createWriter(RDFFormat format,
 OutputStream out)

Convenience methods for creating RDFWriter objects.This method uses the registry returned by
 RDFWriterRegistry.getInstance() to get a factory for the specified format and uses this factory to create
 the appropriate writer.

static RDFWriter
createWriter(RDFFormat format,
 OutputStream out,
 String baseURI)

Convenience methods for creating RDFWriter objects.This method uses the registry returned by
 RDFWriterRegistry.getInstance() to get a factory for the specified format and uses this factory to create
 the appropriate writer.

static RDFWriter
createWriter(RDFFormat format,
 Writer writer)

Convenience methods for creating RDFWriter objects.This method uses the registry returned by
 RDFWriterRegistry.getInstance() to get a factory for the specified format and uses this factory to create
 the appropriate writer.

static RDFWriter
createWriter(RDFFormat format,
 Writer writer,
 String baseURI)

Convenience methods for creating RDFWriter objects.This method uses the registry returned by
 RDFWriterRegistry.getInstance() to get a factory for the specified format and uses this factory to create
 the appropriate writer.

static Optional<RDFFormat>
getParserFormatForFileName(String fileName)

Tries to match the extension of a file name against the list of RDF formats that can be parsed.

static Optional<RDFFormat>
getParserFormatForMIMEType(String mimeType)

Tries to match a MIME type against the list of RDF formats that can be parsed.

static Optional<RDFFormat>
getWriterFormatForFileName(String fileName)

Tries to match the extension of a file name against the list of RDF formats that can be written.

static Optional<RDFFormat>
getWriterFormatForMIMEType(String mimeType)

Tries to match a MIME type against the list of RDF formats that can be written.

static void
main(String[] args)
 
static Model
parse(InputStream in,
 String baseURI,
 RDFFormat dataFormat,
 Resource... contexts)

Adds RDF data from an InputStream to a Model, optionally to one or more named contexts.

static Model
parse(InputStream in,
 String baseURI,
 RDFFormat dataFormat,
 ParserConfig settings,
 Resource... contexts)

Adds RDF data from an InputStream to a Model, optionally to one or more named contexts.

static Model
parse(InputStream in,
 String baseURI,
 RDFFormat dataFormat,
 ParserConfig settings,
 ValueFactory valueFactory,
 ParseErrorListener errors,
 ModelFactory modelFactory,
 Resource... contexts)

Adds RDF data from an InputStream to a Model, optionally to one or more named contexts.

static Model
parse(InputStream in,
 String baseURI,
 RDFFormat dataFormat,
 ParserConfig settings,
 ValueFactory valueFactory,
 ParseErrorListener errors,
 Resource... contexts)

Adds RDF data from an InputStream to a Model, optionally to one or more named contexts.

static Model
parse(InputStream in,
 RDFFormat dataFormat,
 Resource... contexts)

Adds RDF data from an InputStream to a Model, optionally to one or more named contexts.

static Model
parse(InputStream in,
 RDFFormat dataFormat,
 ParserConfig settings,
 Resource... contexts)

Adds RDF data from an InputStream to a Model, optionally to one or more named contexts.

static Model
parse(Reader reader,
 String baseURI,
 RDFFormat dataFormat,
 Resource... contexts)

Adds RDF data from a Reader to a Model, optionally to one or more named contexts.

static Model
parse(Reader reader,
 String baseURI,
 RDFFormat dataFormat,
 ParserConfig settings,
 ValueFactory valueFactory,
 ParseErrorListener errors,
 ModelFactory modelFactory,
 Resource... contexts)

Adds RDF data from a Reader to a Model, optionally to one or more named contexts.

static Model
parse(Reader reader,
 String baseURI,
 RDFFormat dataFormat,
 ParserConfig settings,
 ValueFactory valueFactory,
 ParseErrorListener errors,
 Resource... contexts)

Adds RDF data from a Reader to a Model, optionally to one or more named contexts.

static Model
parse(Reader reader,
 RDFFormat dataFormat,
 Resource... contexts)

Adds RDF data from a Reader to a Model, optionally to one or more named contexts.

static Model
parse(Reader reader,
 RDFFormat dataFormat,
 ParserConfig settings,
 Resource... contexts)

Adds RDF data from a Reader to a Model, optionally to one or more named contexts.

static Supplier<UnsupportedRDFormatException>
unsupportedFormat(String unsupportedFormat)

Helper method to use to create a lambda for Optional.orElseThrow(Supplier) to indicate a format is
 unsupported.

static Supplier<UnsupportedRDFormatException>
unsupportedFormat(RDFFormat unsupportedFormat)

Helper method to use to create a lambda for Optional.orElseThrow(Supplier) to indicate a format is
 unsupported.

static void
write(Iterable<Statement> model,
 OutputStream output,
 String baseURI,
 RDFFormat dataFormat)

Writes the given statements to the given OutputStream in the given format.

static void
write(Iterable<Statement> model,
 OutputStream output,
 String baseURI,
 RDFFormat dataFormat,
 WriterConfig settings)

Writes the given statements to the given OutputStream in the given format.

static void
write(Iterable<Statement> model,
 OutputStream output,
 RDFFormat dataFormat)

Writes the given statements to the given OutputStream in the given format.

static void
write(Iterable<Statement> model,
 OutputStream output,
 RDFFormat dataFormat,
 WriterConfig settings)

Writes the given statements to the given OutputStream in the given format.

static void
write(Iterable<Statement> model,
 Writer output,
 String baseURI,
 RDFFormat dataFormat)

Writes the given statements to the given Writer in the given format.

static void
write(Iterable<Statement> model,
 Writer output,
 String baseURI,
 RDFFormat dataFormat,
 WriterConfig settings)

Writes the given statements to the given Writer in the given format.

static void
write(Iterable<Statement> model,
 Writer output,
 RDFFormat dataFormat)

Writes the given statements to the given Writer in the given format.

static void
write(Iterable<Statement> model,
 Writer output,
 RDFFormat dataFormat,
 WriterConfig settings)

Writes the given statements to the given Writer in the given format.

static void
write(Iterable<Statement> model,
 RDFHandler writer)

Writes the given statements to the given RDFHandler.

static void
write(Statement st,
 OutputStream output,
 RDFFormat dataFormat)

Writes the given statement to the given OutputStream in the given format.

static void
write(Statement st,
 OutputStream output,
 RDFFormat dataFormat,
 WriterConfig settings)

Writes the given single statement to the given OutputStream in the given format.

static void
write(Statement statement,
 Writer output,
 RDFFormat dataFormat)

Writes the given single statement to the given Writer in the given format.

static void
write(Statement statement,
 Writer output,
 RDFFormat dataFormat,
 WriterConfig settings)

Writes the given single statement to the given Writer in the given format.

static void
write(Statement statement,
 RDFHandler writer)

Writes the given single statement to the given RDFHandler.





Methods inherited from class java.lang.Object
clone, equals, finalize, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait









Constructor Details



Rio

public Rio()









Method Details



getParserFormatForMIMEType

public static Optional<RDFFormat> getParserFormatForMIMEType(String mimeType)
Tries to match a MIME type against the list of RDF formats that can be parsed.

Parameters:
mimeType - A MIME type, e.g. "application/rdf+xml".
Returns:
An RDFFormat object if a match was found, or Optional.empty() otherwise.






getParserFormatForFileName

public static Optional<RDFFormat> getParserFormatForFileName(String fileName)
Tries to match the extension of a file name against the list of RDF formats that can be parsed.

Parameters:
fileName - A file name.
Returns:
An RDFFormat object if a match was found, or Optional.empty() otherwise.






getWriterFormatForMIMEType

public static Optional<RDFFormat> getWriterFormatForMIMEType(String mimeType)
Tries to match a MIME type against the list of RDF formats that can be written.

Parameters:
mimeType - A MIME type, e.g. "application/rdf+xml".
Returns:
An RDFFormat object if a match was found, or Optional.empty() otherwise.






getWriterFormatForFileName

public static Optional<RDFFormat> getWriterFormatForFileName(String fileName)
Tries to match the extension of a file name against the list of RDF formats that can be written.

Parameters:
fileName - A file name.
Returns:
An RDFFormat object if a match was found, or Optional.empty() otherwise.






createParser

public static RDFParser createParser(RDFFormat format)
                              throws UnsupportedRDFormatException
Convenience methods for creating RDFParser objects.This method uses the registry returned by
 RDFParserRegistry.getInstance() to get a factory for the specified format and uses this factory to create
 the appropriate parser.

Parameters:
format - 
Returns:
RDF Parser
Throws:
UnsupportedRDFormatException - If no parser is available for the specified RDF format.






createParser

public static RDFParser createParser(RDFFormat format,
 ValueFactory valueFactory)
                              throws UnsupportedRDFormatException
Convenience methods for creating RDFParser objects that use the specified ValueFactory to create RDF model
 objects.

Parameters:
format - 
valueFactory - 
Returns:
RDF Parser
Throws:
UnsupportedRDFormatException - If no parser is available for the specified RDF format.
See Also:


createParser(RDFFormat)
RDFParser.setValueFactory(ValueFactory)








createWriter

public static RDFWriter createWriter(RDFFormat format,
 OutputStream out)
                              throws UnsupportedRDFormatException
Convenience methods for creating RDFWriter objects.This method uses the registry returned by
 RDFWriterRegistry.getInstance() to get a factory for the specified format and uses this factory to create
 the appropriate writer.

Parameters:
format - 
out - 
Returns:
RDF Writer
Throws:
UnsupportedRDFormatException - If no writer is available for the specified RDF format.






createWriter

public static RDFWriter createWriter(RDFFormat format,
 OutputStream out,
 String baseURI)
                              throws UnsupportedRDFormatException,
URISyntaxException
Convenience methods for creating RDFWriter objects.This method uses the registry returned by
 RDFWriterRegistry.getInstance() to get a factory for the specified format and uses this factory to create
 the appropriate writer.

Parameters:
format - 
out - 
baseURI - 
Returns:
RDF Writer
Throws:
UnsupportedRDFormatException - If no writer is available for the specified RDF format.
URISyntaxException - If the baseURI is invalid






createWriter

public static RDFWriter createWriter(RDFFormat format,
 Writer writer)
                              throws UnsupportedRDFormatException
Convenience methods for creating RDFWriter objects.This method uses the registry returned by
 RDFWriterRegistry.getInstance() to get a factory for the specified format and uses this factory to create
 the appropriate writer.

Parameters:
format - 
writer - 
Returns:
RDF Writer
Throws:
UnsupportedRDFormatException - If no writer is available for the specified RDF format.






createWriter

public static RDFWriter createWriter(RDFFormat format,
 Writer writer,
 String baseURI)
                              throws UnsupportedRDFormatException,
URISyntaxException
Convenience methods for creating RDFWriter objects.This method uses the registry returned by
 RDFWriterRegistry.getInstance() to get a factory for the specified format and uses this factory to create
 the appropriate writer.

Parameters:
format - 
writer - 
baseURI - 
Returns:
RDF Writer
Throws:
UnsupportedRDFormatException - If no writer is available for the specified RDF format.
URISyntaxException - If the baseURI is invalid






parse

public static Model parse(InputStream in,
 RDFFormat dataFormat,
 Resource... contexts)
                   throws IOException,
RDFParseException,
UnsupportedRDFormatException
Adds RDF data from an InputStream to a Model, optionally to one or more named contexts.

Parameters:
in - An InputStream from which RDF data can be read.
dataFormat - The serialization format of the data.
contexts - The contexts to add the data to. If one or more contexts are supplied the method ignores
                   contextual information in the actual data. If no contexts are supplied the contextual
                   information in the input stream is used, if no context information is available the data is
                   added without any context.
Returns:
A Model containing the parsed statements.
Throws:
IOException - If an I/O error occurred while reading from the input stream.
UnsupportedRDFormatException - If no RDFParser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.
Since:
3.5.0






parse

public static Model parse(InputStream in,
 RDFFormat dataFormat,
 ParserConfig settings,
 Resource... contexts)
                   throws IOException,
RDFParseException,
UnsupportedRDFormatException
Adds RDF data from an InputStream to a Model, optionally to one or more named contexts.

Parameters:
in - An InputStream from which RDF data can be read.
dataFormat - The serialization format of the data.
settings - The ParserConfig containing settings for configuring the parser.
contexts - The contexts to add the data to. If one or more contexts are supplied the method ignores
                   contextual information in the actual data. If no contexts are supplied the contextual
                   information in the input stream is used, if no context information is available the data is
                   added without any context.
Returns:
A Model containing the parsed statements.
Throws:
IOException - If an I/O error occurred while reading from the input stream.
UnsupportedRDFormatException - If no RDFParser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.
Since:
4.0.0






parse

public static Model parse(InputStream in,
 String baseURI,
 RDFFormat dataFormat,
 Resource... contexts)
                   throws IOException,
RDFParseException,
UnsupportedRDFormatException
Adds RDF data from an InputStream to a Model, optionally to one or more named contexts.

Parameters:
in - An InputStream from which RDF data can be read.
baseURI - The base URI to resolve any relative URIs that are in the data against. May be
                   null.
dataFormat - The serialization format of the data.
contexts - The contexts to add the data to. If one or more contexts are supplied the method ignores
                   contextual information in the actual data. If no contexts are supplied the contextual
                   information in the input stream is used, if no context information is available the data is
                   added without any context.
Returns:
A Model containing the parsed statements.
Throws:
IOException - If an I/O error occurred while reading from the input stream.
UnsupportedRDFormatException - If no RDFParser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.






parse

public static Model parse(InputStream in,
 String baseURI,
 RDFFormat dataFormat,
 ParserConfig settings,
 Resource... contexts)
                   throws IOException,
RDFParseException,
UnsupportedRDFormatException
Adds RDF data from an InputStream to a Model, optionally to one or more named contexts.

Parameters:
in - An InputStream from which RDF data can be read.
baseURI - The base URI to resolve any relative URIs that are in the data against. May be
                   null.
dataFormat - The serialization format of the data.
settings - The ParserConfig containing settings for configuring the parser.
contexts - The contexts to add the data to. If one or more contexts are supplied the method ignores
                   contextual information in the actual data. If no contexts are supplied the contextual
                   information in the input stream is used, if no context information is available the data is
                   added without any context.
Returns:
A Model containing the parsed statements.
Throws:
IOException - If an I/O error occurred while reading from the input stream.
UnsupportedRDFormatException - If no RDFParser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.
Since:
4.0.0






parse

public static Model parse(InputStream in,
 String baseURI,
 RDFFormat dataFormat,
 ParserConfig settings,
 ValueFactory valueFactory,
 ParseErrorListener errors,
 Resource... contexts)
                   throws IOException,
RDFParseException,
UnsupportedRDFormatException
Adds RDF data from an InputStream to a Model, optionally to one or more named contexts.

Parameters:
in - An InputStream from which RDF data can be read.
baseURI - The base URI to resolve any relative URIs that are in the data against. May be
                     null.
dataFormat - The serialization format of the data.
settings - The ParserConfig containing settings for configuring the parser.
valueFactory - The ValueFactory used by the parser to create statements.
errors - The ParseErrorListener used by the parser to signal errors, including errors that do
                     not generate an RDFParseException.
contexts - The contexts to add the data to. If one or more contexts are supplied the method ignores
                     contextual information in the actual data. If no contexts are supplied the contextual
                     information in the input stream is used, if no context information is available the data is
                     added without any context.
Returns:
A Model containing the parsed statements.
Throws:
IOException - If an I/O error occurred while reading from the input stream.
UnsupportedRDFormatException - If no RDFParser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.






parse

public static Model parse(InputStream in,
 String baseURI,
 RDFFormat dataFormat,
 ParserConfig settings,
 ValueFactory valueFactory,
 ParseErrorListener errors,
 ModelFactory modelFactory,
 Resource... contexts)
                   throws IOException,
RDFParseException,
UnsupportedRDFormatException
Adds RDF data from an InputStream to a Model, optionally to one or more named contexts.

Parameters:
in - An InputStream from which RDF data can be read.
baseURI - The base URI to resolve any relative URIs that are in the data against. May be
                     null.
dataFormat - The serialization format of the data.
settings - The ParserConfig containing settings for configuring the parser.
valueFactory - The ValueFactory used by the parser to create statements.
errors - The ParseErrorListener used by the parser to signal errors, including errors that do
                     not generate an RDFParseException.
modelFactory - the ModelFactory used to instantiate the model that gets returned.
contexts - The contexts to add the data to. If one or more contexts are supplied the method ignores
                     contextual information in the actual data. If no contexts are supplied the contextual
                     information in the input stream is used, if no context information is available the data is
                     added without any context.
Returns:
A Model containing the parsed statements.
Throws:
IOException - If an I/O error occurred while reading from the input stream.
UnsupportedRDFormatException - If no RDFParser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.






parse

public static Model parse(Reader reader,
 RDFFormat dataFormat,
 Resource... contexts)
                   throws IOException,
RDFParseException,
UnsupportedRDFormatException
Adds RDF data from a Reader to a Model, optionally to one or more named contexts. Note: using
 a Reader to upload byte-based data means that you have to be careful not to destroy the data's character encoding
 by enforcing a default character encoding upon the bytes. If possible, adding such data using an InputStream is
 to be preferred.

Parameters:
reader - A Reader from which RDF data can be read.
dataFormat - The serialization format of the data.
contexts - The contexts to add the data to. If one or more contexts are specified the data is added to
                   these contexts, ignoring any context information in the data itself.
Returns:
A Model containing the parsed statements.
Throws:
IOException - If an I/O error occurred while reading from the reader.
UnsupportedRDFormatException - If no RDFParser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.
Since:
3.5.0






parse

public static Model parse(Reader reader,
 RDFFormat dataFormat,
 ParserConfig settings,
 Resource... contexts)
                   throws IOException,
RDFParseException,
UnsupportedRDFormatException
Adds RDF data from a Reader to a Model, optionally to one or more named contexts. Note: using
 a Reader to upload byte-based data means that you have to be careful not to destroy the data's character encoding
 by enforcing a default character encoding upon the bytes. If possible, adding such data using an InputStream is
 to be preferred.

Parameters:
reader - A Reader from which RDF data can be read.
dataFormat - The serialization format of the data.
settings - The ParserConfig containing settings for configuring the parser.
contexts - The contexts to add the data to. If one or more contexts are specified the data is added to
                   these contexts, ignoring any context information in the data itself.
Returns:
A Model containing the parsed statements.
Throws:
IOException - If an I/O error occurred while reading from the reader.
UnsupportedRDFormatException - If no RDFParser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.
Since:
4.0.0






parse

public static Model parse(Reader reader,
 String baseURI,
 RDFFormat dataFormat,
 Resource... contexts)
                   throws IOException,
RDFParseException,
UnsupportedRDFormatException
Adds RDF data from a Reader to a Model, optionally to one or more named contexts. Note: using
 a Reader to upload byte-based data means that you have to be careful not to destroy the data's character encoding
 by enforcing a default character encoding upon the bytes. If possible, adding such data using an InputStream is
 to be preferred.

Parameters:
reader - A Reader from which RDF data can be read.
baseURI - The base URI to resolve any relative URIs that are in the data against. May be
                   null.
dataFormat - The serialization format of the data.
contexts - The contexts to add the data to. If one or more contexts are specified the data is added to
                   these contexts, ignoring any context information in the data itself.
Returns:
A Model containing the parsed statements.
Throws:
IOException - If an I/O error occurred while reading from the reader.
UnsupportedRDFormatException - If no RDFParser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.






parse

public static Model parse(Reader reader,
 String baseURI,
 RDFFormat dataFormat,
 ParserConfig settings,
 ValueFactory valueFactory,
 ParseErrorListener errors,
 Resource... contexts)
                   throws IOException,
RDFParseException,
UnsupportedRDFormatException
Adds RDF data from a Reader to a Model, optionally to one or more named contexts. Note: using
 a Reader to upload byte-based data means that you have to be careful not to destroy the data's character encoding
 by enforcing a default character encoding upon the bytes. If possible, adding such data using an InputStream is
 to be preferred.

Parameters:
reader - A Reader from which RDF data can be read.
baseURI - The base URI to resolve any relative URIs that are in the data against. May be
                     null.
dataFormat - The serialization format of the data.
settings - The ParserConfig containing settings for configuring the parser.
valueFactory - The ValueFactory used by the parser to create statements.
errors - The ParseErrorListener used by the parser to signal errors, including errors that do
                     not generate an RDFParseException.
contexts - The contexts to add the data to. If one or more contexts are specified the data is added to
                     these contexts, ignoring any context information in the data itself.
Returns:
A Model containing the parsed statements.
Throws:
IOException - If an I/O error occurred while reading from the reader.
UnsupportedRDFormatException - If no RDFParser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.






parse

public static Model parse(Reader reader,
 String baseURI,
 RDFFormat dataFormat,
 ParserConfig settings,
 ValueFactory valueFactory,
 ParseErrorListener errors,
 ModelFactory modelFactory,
 Resource... contexts)
                   throws IOException,
RDFParseException,
UnsupportedRDFormatException
Adds RDF data from a Reader to a Model, optionally to one or more named contexts. Note: using
 a Reader to upload byte-based data means that you have to be careful not to destroy the data's character encoding
 by enforcing a default character encoding upon the bytes. If possible, adding such data using an InputStream is
 to be preferred.

Parameters:
reader - A Reader from which RDF data can be read.
baseURI - The base URI to resolve any relative URIs that are in the data against. May be
                     null.
dataFormat - The serialization format of the data.
settings - The ParserConfig containing settings for configuring the parser.
valueFactory - The ValueFactory used by the parser to create statements.
errors - The ParseErrorListener used by the parser to signal errors, including errors that do
                     not generate an RDFParseException.
modelFactory - the ModelFactory used to instantiate the model that gets returned.
contexts - The contexts to add the data to. If one or more contexts are specified the data is added to
                     these contexts, ignoring any context information in the data itself.
Returns:
A Model containing the parsed statements.
Throws:
IOException - If an I/O error occurred while reading from the reader.
UnsupportedRDFormatException - If no RDFParser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.






write

public static void write(Iterable<Statement> model,
 OutputStream output,
 RDFFormat dataFormat)
                  throws RDFHandlerException
Writes the given statements to the given OutputStream in the given format.
 
 If the collection is a Model, its namespaces will also be written.

Parameters:
model - A collection of statements, such as a Model, to be written.
output - The OutputStream to write the statements to.
dataFormat - The RDFFormat to use when writing the statements.
Throws:
RDFHandlerException - Thrown if there is an error writing the statements.
UnsupportedRDFormatException - If no RDFWriter is available for the specified RDF format.






write

public static void write(Iterable<Statement> model,
 OutputStream output,
 String baseURI,
 RDFFormat dataFormat)
                  throws RDFHandlerException,
UnsupportedRDFormatException,
URISyntaxException
Writes the given statements to the given OutputStream in the given format.
 
 If the collection is a Model, its namespaces will also be written.

Parameters:
model - A collection of statements, such as a Model, to be written.
output - The OutputStream to write the statements to.
baseURI - The base URI to relativize IRIs against.
dataFormat - The RDFFormat to use when writing the statements.
Throws:
RDFHandlerException - Thrown if there is an error writing the statements.
URISyntaxException - If the baseURI is invalid
UnsupportedRDFormatException - If no RDFWriter is available for the specified RDF format.






write

public static void write(Iterable<Statement> model,
 Writer output,
 RDFFormat dataFormat)
                  throws RDFHandlerException
Writes the given statements to the given Writer in the given format.
 
 If the collection is a Model, its namespaces will also be written.

Parameters:
model - A collection of statements, such as a Model, to be written.
output - The Writer to write the statements to.
dataFormat - The RDFFormat to use when writing the statements.
Throws:
RDFHandlerException - Thrown if there is an error writing the statements.
UnsupportedRDFormatException - If no RDFWriter is available for the specified RDF format.






write

public static void write(Iterable<Statement> model,
 Writer output,
 String baseURI,
 RDFFormat dataFormat)
                  throws RDFHandlerException,
UnsupportedRDFormatException,
URISyntaxException
Writes the given statements to the given Writer in the given format.
 
 If the collection is a Model, its namespaces will also be written.

Parameters:
model - A collection of statements, such as a Model, to be written.
output - The Writer to write the statements to.
baseURI - The base URI to relativize IRIs against.
dataFormat - The RDFFormat to use when writing the statements.
Throws:
RDFHandlerException - Thrown if there is an error writing the statements.
URISyntaxException - If the baseURI is invalid
UnsupportedRDFormatException - If no RDFWriter is available for the specified RDF format.






write

public static void write(Iterable<Statement> model,
 OutputStream output,
 RDFFormat dataFormat,
 WriterConfig settings)
                  throws RDFHandlerException
Writes the given statements to the given OutputStream in the given format.
 
 If the collection is a Model, its namespaces will also be written.

Parameters:
model - A collection of statements, such as a Model, to be written.
output - The OutputStream to write the statements to.
dataFormat - The RDFFormat to use when writing the statements.
settings - The WriterConfig containing settings for configuring the writer.
Throws:
RDFHandlerException - Thrown if there is an error writing the statements.
UnsupportedRDFormatException - If no RDFWriter is available for the specified RDF format.






write

public static void write(Iterable<Statement> model,
 OutputStream output,
 String baseURI,
 RDFFormat dataFormat,
 WriterConfig settings)
                  throws RDFHandlerException,
UnsupportedRDFormatException,
URISyntaxException
Writes the given statements to the given OutputStream in the given format.
 
 If the collection is a Model, its namespaces will also be written.

Parameters:
model - A collection of statements, such as a Model, to be written.
output - The OutputStream to write the statements to.
baseURI - The base URI to relativize IRIs against.
dataFormat - The RDFFormat to use when writing the statements.
settings - The WriterConfig containing settings for configuring the writer.
Throws:
RDFHandlerException - Thrown if there is an error writing the statements.
URISyntaxException - If the baseURI is invalid
UnsupportedRDFormatException - If no RDFWriter is available for the specified RDF format.






write

public static void write(Iterable<Statement> model,
 Writer output,
 RDFFormat dataFormat,
 WriterConfig settings)
                  throws RDFHandlerException
Writes the given statements to the given Writer in the given format.
 
 If the collection is a Model, its namespaces will also be written.

Parameters:
model - A collection of statements, such as a Model, to be written.
output - The Writer to write the statements to.
dataFormat - The RDFFormat to use when writing the statements.
settings - The WriterConfig containing settings for configuring the writer.
Throws:
RDFHandlerException - Thrown if there is an error writing the statements.
UnsupportedRDFormatException - If no RDFWriter is available for the specified RDF format.






write

public static void write(Iterable<Statement> model,
 Writer output,
 String baseURI,
 RDFFormat dataFormat,
 WriterConfig settings)
                  throws RDFHandlerException,
UnsupportedRDFormatException,
URISyntaxException
Writes the given statements to the given Writer in the given format.
 
 If the collection is a Model, its namespaces will also be written.

Parameters:
model - A collection of statements, such as a Model, to be written.
output - The Writer to write the statements to.
baseURI - The base URI to relativize IRIs against.
dataFormat - The RDFFormat to use when writing the statements.
settings - The WriterConfig containing settings for configuring the writer.
Throws:
RDFHandlerException - Thrown if there is an error writing the statements.
URISyntaxException - If the baseURI is invalid
UnsupportedRDFormatException - If no RDFWriter is available for the specified RDF format.






write

public static void write(Iterable<Statement> model,
 RDFHandler writer)
                  throws RDFHandlerException
Writes the given statements to the given RDFHandler.
 
 If the collection is a Model, its namespaces will also be written.

Parameters:
model - A collection of statements, such as a Model, to be written.
writer - 
Throws:
RDFHandlerException - Thrown if there is an error writing the statements.






write

public static void write(Statement st,
 OutputStream output,
 RDFFormat dataFormat)
                  throws RDFHandlerException
Writes the given statement to the given OutputStream in the given format.
 

Parameters:
st - The statement to be written.
output - The OutputStream to write the statement to.
dataFormat - The RDFFormat to use when writing the statement.
Throws:
RDFHandlerException - Thrown if there is an error writing the statement.
UnsupportedRDFormatException - If no RDFWriter is available for the specified RDF format.






write

public static void write(Statement st,
 OutputStream output,
 RDFFormat dataFormat,
 WriterConfig settings)
                  throws RDFHandlerException
Writes the given single statement to the given OutputStream in the given format.

Parameters:
st - The statement to be written.
output - The OutputStream to write the statement to.
dataFormat - The RDFFormat to use when writing the statement.
settings - The WriterConfig containing setting for configuring the writer.
Throws:
RDFHandlerException - Thrown if there is an error writing the statement.
UnsupportedRDFormatException - If no RDFWriter is available for the specified RDF format.






write

public static void write(Statement statement,
 Writer output,
 RDFFormat dataFormat)
                  throws RDFHandlerException
Writes the given single statement to the given Writer in the given format.
 

Parameters:
statement - A statement to be written.
output - The Writer to write the statement to.
dataFormat - The RDFFormat to use when writing the statement.
Throws:
RDFHandlerException - Thrown if there is an error writing the statement.
UnsupportedRDFormatException - If no RDFWriter is available for the specified RDF format.






write

public static void write(Statement statement,
 Writer output,
 RDFFormat dataFormat,
 WriterConfig settings)
                  throws RDFHandlerException
Writes the given single statement to the given Writer in the given format.
 

Parameters:
statement - A statement to be written.
output - The Writer to write the statement to.
dataFormat - The RDFFormat to use when writing the statement.
settings - The WriterConfig containing settings for configuring the writer.
Throws:
RDFHandlerException - Thrown if there is an error writing the statement.
UnsupportedRDFormatException - If no RDFWriter is available for the specified RDF format.






write

public static void write(Statement statement,
 RDFHandler writer)
                  throws RDFHandlerException
Writes the given single statement to the given RDFHandler.
 

Parameters:
statement - A statement, to be written.
writer - 
Throws:
RDFHandlerException - Thrown if there is an error writing the statement.






main

public static void main(String[] args)
                 throws IOException,
RDFParseException,
RDFHandlerException,
UnsupportedRDFormatException

Throws:
IOException
RDFParseException
RDFHandlerException
UnsupportedRDFormatException






unsupportedFormat

public static Supplier<UnsupportedRDFormatException> unsupportedFormat(RDFFormat unsupportedFormat)
Helper method to use to create a lambda for Optional.orElseThrow(Supplier) to indicate a format is
 unsupported.

Parameters:
unsupportedFormat - The format that was not found.
Returns:
A lambda that can be used to generate an exception if the format is not found.






unsupportedFormat

public static Supplier<UnsupportedRDFormatException> unsupportedFormat(String unsupportedFormat)
Helper method to use to create a lambda for Optional.orElseThrow(Supplier) to indicate a format is
 unsupported.

Parameters:
unsupportedFormat - The format that was not found.
Returns:
A lambda that can be used to generate an exception if the format is not found.












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nClass RDFFormat

java.lang.Object
org.eclipse.rdf4j.common.lang.FileFormat
org.eclipse.rdf4j.rio.RDFFormat





public class RDFFormat
extends FileFormat
Represents the concept of an RDF data serialization format. RDF formats are identified by a name
 and can have one or more associated MIME types, zero or more associated file extensions and can specify a (default)
 character encoding. Some formats are able to encode context information while other are not; this is indicated by the
 value of supportsContexts.

Author:
Arjohn Kampman








Field Summary
Fields

Modifier and Type
Field
Description
static final RDFFormat
BINARY

A binary RDF format.

static final RDFFormat
HDT

The HDT file format, an RDF serialization format.

static final RDFFormat
JSONLD

The JSON-LD file format, an RDF serialization format that supports
 recording of named graphs.

static final RDFFormat
N3

The N3/Notation3 file format.

static final RDFFormat
NDJSONLD

The NDJSON-LD is a Newline Delimited JSON-LD format.

static final boolean
NO_CONTEXTS

Indicates that the Statement.getContext() URI will NOT be serialized for this format.

static final boolean
NO_NAMESPACES

Indicates that all calls to RDFHandler.handleNamespace(String, String) will be ignored when serializing
 to this format.

static final boolean
NO_RDF_STAR

Indicates that RDF-star triples will NOT be serialized natively for this format.

static final RDFFormat
NQUADS

The N-Quads file format, an RDF serialization format that supports
 recording of named graphs.

static final RDFFormat
NTRIPLES

The N-Triples file format.

static final RDFFormat
RDFA

The RDFa file format, an RDF serialization format.

static final RDFFormat
RDFJSON

The RDF/JSON file format, an RDF serialization format that supports
 recording of named graphs.

static final RDFFormat
RDFXML

The RDF/XML file format.

static final boolean
SUPPORTS_CONTEXTS

Indicates that the Statement.getContext() URI may be serialized for this format.

static final boolean
SUPPORTS_NAMESPACES

Indicates that calls to RDFHandler.handleNamespace(String, String) may be serialised when serializing to
 this format.

static final boolean
SUPPORTS_RDF_STAR

Indicates that RDF-star triples can be serialized natively for this format.

static final RDFFormat
TRIG

The TriG file format, a Turtle-based RDF serialization format that
 supports recording of named graphs.

static final RDFFormat
TRIGSTAR

The TriG-star file format, a TriG-based RDF serialization format that supports RDF-star triples.

static final RDFFormat
TRIX

The TriX file format, an XML-based RDF serialization format that
 supports recording of named graphs.

static final RDFFormat
TURTLE

The Turtle file format.

static final RDFFormat
TURTLESTAR

The Turtle-star file format, a Turtle-based RDF serialization format that supports RDF-star triples.







Constructor Summary
Constructors

Constructor
Description
RDFFormat(String name,
 String mimeType,
 Charset charset,
 String fileExtension,
 boolean supportsNamespaces,
 boolean supportsContexts)

Deprecated.
since 3.2.0


RDFFormat(String name,
 String mimeType,
 Charset charset,
 String fileExtension,
 boolean supportsNamespaces,
 boolean supportsContexts,
 boolean supportsRDFStar)

Creates a new RDFFormat object.

RDFFormat(String name,
 String mimeType,
 Charset charset,
 Collection<String> fileExtensions,
 boolean supportsNamespaces,
 boolean supportsContexts)

Deprecated.
since 3.2.0


RDFFormat(String name,
 String mimeType,
 Charset charset,
 Collection<String> fileExtensions,
 boolean supportsNamespaces,
 boolean supportsContexts,
 boolean supportsRDFStar)

Creates a new RDFFormat object.

RDFFormat(String name,
 Collection<String> mimeTypes,
 Charset charset,
 Collection<String> fileExtensions,
 boolean supportsNamespaces,
 boolean supportsContexts)

Deprecated.
since 3.2.0


RDFFormat(String name,
 Collection<String> mimeTypes,
 Charset charset,
 Collection<String> fileExtensions,
 boolean supportsNamespaces,
 boolean supportsContexts,
 boolean supportsRDFStar)

Creates a new RDFFormat object.

RDFFormat(String name,
 Collection<String> mimeTypes,
 Charset charset,
 Collection<String> fileExtensions,
 IRI standardURI,
 boolean supportsNamespaces,
 boolean supportsContexts)

Deprecated.
since 3.2.0


RDFFormat(String name,
 Collection<String> mimeTypes,
 Charset charset,
 Collection<String> fileExtensions,
 IRI standardURI,
 boolean supportsNamespaces,
 boolean supportsContexts,
 boolean supportsRDFStar)

Creates a new RDFFormat object.







Method Summary

All MethodsStatic MethodsInstance MethodsConcrete Methods


Modifier and Type
Method
Description
static List<String>
getAcceptParams(Iterable<RDFFormat> rdfFormats,
 boolean requireContext,
 RDFFormat preferredFormat)

Processes the supplied collection of RDFFormats and assigns quality values to each based on whether
 context must be supported and whether the format is preferred.

IRI
getStandardURI()
 
boolean
hasStandardURI()
 
boolean
supportsContexts()

Return true if the RDFFormat supports the encoding of contexts/named graphs.

boolean
supportsNamespaces()

Return true if the RDFFormat supports the encoding of namespace/prefix information.

boolean
supportsRDFStar()

Return true if the RDFFormat supports the encoding of RDF-star triples natively.





Methods inherited from class org.eclipse.rdf4j.common.lang.FileFormat
equals, getCharset, getDefaultFileExtension, getDefaultMIMEType, getFileExtensions, getMIMETypes, getName, hasCharset, hasDefaultFileExtension, hasDefaultMIMEType, hasFileExtension, hashCode, hasMIMEType, matchFileName, matchMIMEType, toString

Methods inherited from class java.lang.Object
clone, finalize, getClass, notify, notifyAll, wait, wait, wait









Field Details



SUPPORTS_NAMESPACES

public static final boolean SUPPORTS_NAMESPACES
Indicates that calls to RDFHandler.handleNamespace(String, String) may be serialised when serializing to
 this format.

See Also:


Constant Field Values








NO_NAMESPACES

public static final boolean NO_NAMESPACES
Indicates that all calls to RDFHandler.handleNamespace(String, String) will be ignored when serializing
 to this format.

See Also:


Constant Field Values








SUPPORTS_CONTEXTS

public static final boolean SUPPORTS_CONTEXTS
Indicates that the Statement.getContext() URI may be serialized for this format.

See Also:


Constant Field Values








NO_CONTEXTS

public static final boolean NO_CONTEXTS
Indicates that the Statement.getContext() URI will NOT be serialized for this format.

See Also:


Constant Field Values








SUPPORTS_RDF_STAR

public static final boolean SUPPORTS_RDF_STAR
Indicates that RDF-star triples can be serialized natively for this format.

See Also:


Constant Field Values








NO_RDF_STAR

public static final boolean NO_RDF_STAR
Indicates that RDF-star triples will NOT be serialized natively for this format.

See Also:


Constant Field Values








RDFXML

public static final RDFFormat RDFXML
The RDF/XML file format.
 
 Several file extensions are accepted for RDF/XML documents, including .rdf, .rdfs (for
 RDF Schema files), .owl (for OWL ontologies), and .xml. The media type is
 application/rdf+xml, but application/xml and text/xml are also accepted.
 The character encoding is UTF-8.
 

See Also:


RDF/XML Syntax Specification (Revised)








NTRIPLES

public static final RDFFormat NTRIPLES
The N-Triples file format.
 
 The file extension .nt is recommend for N-Triples documents. The media type is
 application/n-triples and encoding is in UTF-8.
 

See Also:


N-Triples








TURTLE

public static final RDFFormat TURTLE
The Turtle file format.
 
 The file extension .ttl is recommend for Turtle documents. The media type is
 text/turtle, but application/x-turtle is also accepted. Character encoding is UTF-8.
 

See Also:


Turtle - Terse RDF Triple Language








TURTLESTAR

public static final RDFFormat TURTLESTAR
The Turtle-star file format, a Turtle-based RDF serialization format that supports RDF-star triples.
 
 The file extension .ttls is recommended for Turtle-star documents. The media type is
 application/x-turtlestar and the encoding is UTF-8.
 

See Also:


Foundations of an Alternative Approach to Reification in
      RDF
RDF-star and SPARQL-star Draft Community Group Report








N3

public static final RDFFormat N3
The N3/Notation3 file format.
 
 The file extension .n3 is recommended for N3 documents. The media type is text/n3, but
 text/rdf+n3 is also accepted. Character encoding is UTF-8.
 

See Also:


Notation3 (N3): A readable RDF syntax








TRIX

public static final RDFFormat TRIX
The TriX file format, an XML-based RDF serialization format that
 supports recording of named graphs.
 
 The file extension .xml is recommended for TriX documents, .trix is also accepted. The
 media type is application/trix and the encoding is UTF-8.
 

See Also:


TriX: RDF Triples in XML








TRIG

public static final RDFFormat TRIG
The TriG file format, a Turtle-based RDF serialization format that
 supports recording of named graphs.
 
 The file extension .trig is recommend for TriG documents. The media type is
 application/trig and the encoding is UTF-8.
 

See Also:


The TriG Syntax








TRIGSTAR

public static final RDFFormat TRIGSTAR
The TriG-star file format, a TriG-based RDF serialization format that supports RDF-star triples. This builds upon
 the idea for the Turtle-star format but adds support for named graphs.
 
 The file extension .trigs is recommended for TriG-star documents. The media type is
 application/x-trigstar and the encoding is UTF-8.
 

See Also:


Foundations of an Alternative Approach to Reification in
      RDF
RDF-star and SPARQL-star Draft Community Group Report








BINARY

public static final RDFFormat BINARY
A binary RDF format.
 
 The file extension .brf is recommend for binary RDF documents. The media type is
 application/x-binary-rdf.
 

See Also:


Binary RDF in Sesame








NQUADS

public static final RDFFormat NQUADS
The N-Quads file format, an RDF serialization format that supports
 recording of named graphs.
 
 The file extension .nq is recommended for N-Quads documents. The media type is
 application/n-quads and the encoding is UTF-8.
 

See Also:


N-Quads: Extending N-Triples with Context








JSONLD

public static final RDFFormat JSONLD
The JSON-LD file format, an RDF serialization format that supports
 recording of named graphs.
 
 The file extension .jsonld is recommended for JSON-LD documents. The media type is
 application/ld+json and the encoding is UTF-8.
 

See Also:


JSON-LD 1.0








NDJSONLD

public static final RDFFormat NDJSONLD
The NDJSON-LD is a Newline Delimited JSON-LD format.
 
 The file extension .ndjsonld is recommended for NDJSON-LD documents. The media type is
 application/x-ld+ndjson and the encoding is UTF-8.
 





RDFJSON

public static final RDFFormat RDFJSON
The RDF/JSON file format, an RDF serialization format that supports
 recording of named graphs.
 
 The file extension .rj is recommended for RDF/JSON documents. The media type is
 application/rdf+json and the encoding is UTF-8.
 

See Also:


RDF 1.1 JSON Alternate Serialization (RDF/JSON)








RDFA

public static final RDFFormat RDFA
The RDFa file format, an RDF serialization format.
 
 The file extension .xhtml is recommended for RDFa documents. The preferred media type is
 application/xhtml+xml and the encoding is UTF-8.
 

See Also:


XHTML+RDFa 1.1








HDT

public static final RDFFormat HDT
The HDT file format, an RDF serialization format.
 
 The file extension .hdt is recommended for HDT documents.
 

See Also:


HDT v1.0












Constructor Details



RDFFormat

@Deprecated(since="3.2.0")
public RDFFormat(String name,
 String mimeType,
 Charset charset,
 String fileExtension,
 boolean supportsNamespaces,
 boolean supportsContexts)
Deprecated.
since 3.2.0

Creates a new RDFFormat object.

Parameters:
name - The name of the RDF file format, e.g. "RDF/XML".
mimeType - The MIME type of the RDF file format, e.g. application/rdf+xml for the
                           RDF/XML file format.
charset - The default character encoding of the RDF file format. Specify null if not
                           applicable.
fileExtension - The (default) file extension for the RDF file format, e.g. rdf for RDF/XML
                           files.
supportsNamespaces - True if the RDFFormat supports the encoding of namespace/prefix information
                           and false otherwise.
supportsContexts - True if the RDFFormat supports the encoding of contexts/named graphs and
                           false otherwise.






RDFFormat

public RDFFormat(String name,
 String mimeType,
 Charset charset,
 String fileExtension,
 boolean supportsNamespaces,
 boolean supportsContexts,
 boolean supportsRDFStar)
Creates a new RDFFormat object.

Parameters:
name - The name of the RDF file format, e.g. "RDF/XML".
mimeType - The MIME type of the RDF file format, e.g. application/rdf+xml for the
                           RDF/XML file format.
charset - The default character encoding of the RDF file format. Specify null if not
                           applicable.
fileExtension - The (default) file extension for the RDF file format, e.g. rdf for RDF/XML
                           files.
supportsNamespaces - True if the RDFFormat supports the encoding of namespace/prefix information
                           and false otherwise.
supportsContexts - True if the RDFFormat supports the encoding of contexts/named graphs and
                           false otherwise.
supportsRDFStar - True if the RDFFormat supports the encoding of RDF-star triples natively and
                           false otherwise.






RDFFormat

@Deprecated(since="3.2.0")
public RDFFormat(String name,
 String mimeType,
 Charset charset,
 Collection<String> fileExtensions,
 boolean supportsNamespaces,
 boolean supportsContexts)
Deprecated.
since 3.2.0

Creates a new RDFFormat object.

Parameters:
name - The name of the RDF file format, e.g. "RDF/XML".
mimeType - The MIME type of the RDF file format, e.g. application/rdf+xml for the
                           RDF/XML file format.
charset - The default character encoding of the RDF file format. Specify null if not
                           applicable.
fileExtensions - The RDF format's file extensions, e.g. rdf for RDF/XML files. The first item
                           in the list is interpreted as the default file extension for the format.
supportsNamespaces - True if the RDFFormat supports the encoding of namespace/prefix information
                           and false otherwise.
supportsContexts - True if the RDFFormat supports the encoding of contexts/named graphs and
                           false otherwise.






RDFFormat

public RDFFormat(String name,
 String mimeType,
 Charset charset,
 Collection<String> fileExtensions,
 boolean supportsNamespaces,
 boolean supportsContexts,
 boolean supportsRDFStar)
Creates a new RDFFormat object.

Parameters:
name - The name of the RDF file format, e.g. "RDF/XML".
mimeType - The MIME type of the RDF file format, e.g. application/rdf+xml for the
                           RDF/XML file format.
charset - The default character encoding of the RDF file format. Specify null if not
                           applicable.
fileExtensions - The RDF format's file extensions, e.g. rdf for RDF/XML files. The first item
                           in the list is interpreted as the default file extension for the format.
supportsNamespaces - True if the RDFFormat supports the encoding of namespace/prefix information
                           and false otherwise.
supportsContexts - True if the RDFFormat supports the encoding of contexts/named graphs and
                           false otherwise.
supportsRDFStar - True if the RDFFormat supports the encoding of RDF-star triples natively and
                           false otherwise.






RDFFormat

@Deprecated(since="3.2.0")
public RDFFormat(String name,
 Collection<String> mimeTypes,
 Charset charset,
 Collection<String> fileExtensions,
 boolean supportsNamespaces,
 boolean supportsContexts)
Deprecated.
since 3.2.0

Creates a new RDFFormat object.

Parameters:
name - The name of the RDF file format, e.g. "RDF/XML".
mimeTypes - The MIME types of the RDF file format, e.g. application/rdf+xml for the
                           RDF/XML file format. The first item in the list is interpreted as the default MIME type
                           for the format.
charset - The default character encoding of the RDF file format. Specify null if not
                           applicable.
fileExtensions - The RDF format's file extensions, e.g. rdf for RDF/XML files. The first item
                           in the list is interpreted as the default file extension for the format.
supportsNamespaces - True if the RDFFormat supports the encoding of namespace/prefix information
                           and false otherwise.
supportsContexts - True if the RDFFormat supports the encoding of contexts/named graphs and
                           false otherwise.






RDFFormat

public RDFFormat(String name,
 Collection<String> mimeTypes,
 Charset charset,
 Collection<String> fileExtensions,
 boolean supportsNamespaces,
 boolean supportsContexts,
 boolean supportsRDFStar)
Creates a new RDFFormat object.

Parameters:
name - The name of the RDF file format, e.g. "RDF/XML".
mimeTypes - The MIME types of the RDF file format, e.g. application/rdf+xml for the
                           RDF/XML file format. The first item in the list is interpreted as the default MIME type
                           for the format.
charset - The default character encoding of the RDF file format. Specify null if not
                           applicable.
fileExtensions - The RDF format's file extensions, e.g. rdf for RDF/XML files. The first item
                           in the list is interpreted as the default file extension for the format.
supportsNamespaces - True if the RDFFormat supports the encoding of namespace/prefix information
                           and false otherwise.
supportsContexts - True if the RDFFormat supports the encoding of contexts/named graphs and
                           false otherwise.
supportsRDFStar - True if the RDFFormat supports the encoding of RDF-star triples natively and
                           false otherwise.






RDFFormat

@Deprecated(since="3.2.0")
public RDFFormat(String name,
 Collection<String> mimeTypes,
 Charset charset,
 Collection<String> fileExtensions,
 IRI standardURI,
 boolean supportsNamespaces,
 boolean supportsContexts)
Deprecated.
since 3.2.0

Creates a new RDFFormat object.

Parameters:
name - The name of the RDF file format, e.g. "RDF/XML".
mimeTypes - The MIME types of the RDF file format, e.g. application/rdf+xml for the
                           RDF/XML file format. The first item in the list is interpreted as the default MIME type
                           for the format.
charset - The default character encoding of the RDF file format. Specify null if not
                           applicable.
fileExtensions - The RDF format's file extensions, e.g. rdf for RDF/XML files. The first item
                           in the list is interpreted as the default file extension for the format.
standardURI - The standard URI that has been assigned to this format by a standards organisation or
                           null if it does not currently have a standard URI.
supportsNamespaces - True if the RDFFormat supports the encoding of namespace/prefix information
                           and false otherwise.
supportsContexts - True if the RDFFormat supports the encoding of contexts/named graphs and
                           false otherwise.






RDFFormat

public RDFFormat(String name,
 Collection<String> mimeTypes,
 Charset charset,
 Collection<String> fileExtensions,
 IRI standardURI,
 boolean supportsNamespaces,
 boolean supportsContexts,
 boolean supportsRDFStar)
Creates a new RDFFormat object.

Parameters:
name - The name of the RDF file format, e.g. "RDF/XML".
mimeTypes - The MIME types of the RDF file format, e.g. application/rdf+xml for the
                           RDF/XML file format. The first item in the list is interpreted as the default MIME type
                           for the format.
charset - The default character encoding of the RDF file format. Specify null if not
                           applicable.
fileExtensions - The RDF format's file extensions, e.g. rdf for RDF/XML files. The first item
                           in the list is interpreted as the default file extension for the format.
standardURI - The standard URI that has been assigned to this format by a standards organisation or
                           null if it does not currently have a standard URI.
supportsNamespaces - True if the RDFFormat supports the encoding of namespace/prefix information
                           and false otherwise.
supportsContexts - True if the RDFFormat supports the encoding of contexts/named graphs and
                           false otherwise.
supportsRDFStar - True if the RDFFormat supports the encoding of RDF-star triples natively and
                           false otherwise.










Method Details



getAcceptParams

public static List<String> getAcceptParams(Iterable<RDFFormat> rdfFormats,
 boolean requireContext,
 RDFFormat preferredFormat)
Processes the supplied collection of RDFFormats and assigns quality values to each based on whether
 context must be supported and whether the format is preferred.

Parameters:
rdfFormats - The RDFFormats to process.
requireContext - True to decrease the quality value for formats where supportsContexts()
                        returns false.
preferredFormat - The preferred RDFFormat. If it is not in the list then the quality of all formats will be
                        processed as if they are not preferred.
Returns:
A list of strings containing the content types and an attached q-value specifying the quality for the
         format for each type.






supportsNamespaces

public boolean supportsNamespaces()
Return true if the RDFFormat supports the encoding of namespace/prefix information.





supportsContexts

public boolean supportsContexts()
Return true if the RDFFormat supports the encoding of contexts/named graphs.





supportsRDFStar

public boolean supportsRDFStar()
Return true if the RDFFormat supports the encoding of RDF-star triples natively.





hasStandardURI

public boolean hasStandardURI()

Returns:
True if a standard URI has been assigned to this format by a standards organisation.






getStandardURI

public IRI getStandardURI()

Returns:
The standard URI that has been assigned to this format by a standards organisation or null if it does not
         currently have a standard URI.












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface Model



All Superinterfaces:
Collection<Statement>, Iterable<Statement>, NamespaceAware, Serializable, Set<Statement>


All Known Implementing Classes:
AbstractMemoryOverflowModel, AbstractModel, DynamicModel, EmptyModel, FilteredModel, LinkedHashModel, SailModel, TreeModel



public interface Model
extends Set<Statement>, Serializable, NamespaceAware
An RDF Model, represented as a Set of Statements with predictable iteration order.
 
 Additional utility functionality for working with Model objects is available in the
 org.eclipse.rdf4j.model.util.Models utility class.

Author:
James Leigh, Jeen Broekstra








Method Summary

All MethodsInstance MethodsAbstract MethodsDefault Methods


Modifier and Type
Method
Description
boolean
add(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Adds one or more statements to the model.

boolean
clear(Resource... context)

Removes statements with the specified context exist in this model.

boolean
contains(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Determines if statements with the specified subject, predicate, object and (optionally) context exist in this
 model.

default Set<Resource>
contexts()

Returns a Set view of the contexts contained in this model.

Model
filter(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Returns a filtered view of the statements with the specified subject, predicate, object and (optionally) context.

default Iterable<Statement>
getStatements(Resource subject,
 IRI predicate,
 Value object,
 Resource... contexts)

Returns an Iterable over all Statements in this Model that match the supplied criteria.

Set<Value>
objects()

Returns a Set view of the objects contained in this model.

Set<IRI>
predicates()

Returns a Set view of the predicates contained in this model.

boolean
remove(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Removes statements with the specified subject, predicate, object and (optionally) context exist in this model.

Optional<Namespace>
removeNamespace(String prefix)

Removes a namespace declaration by removing the association between a prefix and a namespace name.

default Namespace
setNamespace(String prefix,
 String name)

Sets the prefix for a namespace.

void
setNamespace(Namespace namespace)

Sets the prefix for a namespace.

Set<Resource>
subjects()

Returns a Set view of the subjects contained in this model.

Model
unmodifiable()

Returns an unmodifiable view of this model.





Methods inherited from interface java.util.Collection
parallelStream, removeIf, stream, toArray

Methods inherited from interface java.lang.Iterable
forEach

Methods inherited from interface org.eclipse.rdf4j.model.NamespaceAware
getNamespace, getNamespaces

Methods inherited from interface java.util.Set
add, addAll, clear, contains, containsAll, equals, hashCode, isEmpty, iterator, remove, removeAll, retainAll, size, spliterator, toArray, toArray









Method Details



unmodifiable

Model unmodifiable()
Returns an unmodifiable view of this model. This method provides "read-only" access to this model. Query
 operations on the returned model "read through" to this model, and attempts to modify the returned model, whether
 direct or via its iterator, result in an UnsupportedOperationException.
 

Returns:
an unmodifiable view of the specified set.






setNamespace

default Namespace setNamespace(String prefix,
 String name)
Sets the prefix for a namespace. This will replace any existing namespace associated to the prefix.

Parameters:
prefix - The new prefix.
name - The namespace name that the prefix maps to.
Returns:
The Namespace object for the given namespace.






setNamespace

void setNamespace(Namespace namespace)
Sets the prefix for a namespace. This will replace any existing namespace associated to the prefix.

Parameters:
namespace - A Namespace object to use in this Model.






removeNamespace

Optional<Namespace> removeNamespace(String prefix)
Removes a namespace declaration by removing the association between a prefix and a namespace name.

Parameters:
prefix - The namespace prefix of which the assocation with a namespace name is to be removed.
Returns:
the previous namespace bound to the prefix or Optional.empty()






contains

boolean contains(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Determines if statements with the specified subject, predicate, object and (optionally) context exist in this
 model. The subject, predicate and object parameters can be null to indicate
 wildcards. The contexts parameter is a wildcard and accepts zero or more values. If no contexts are
 specified, statements will match disregarding their context. If one or more contexts are specified, statements
 with a context matching one of these will match. Note: to match statements without an associated context, specify
 the value null and explicitly cast it to type Resource.
 
 Examples: model.contains(s1, null, null) is true if any statements in this model have subject
 s1,
 model.contains(null, null, null, c1) is true if any statements in this model have context c1,
 model.contains(null, null, null, (Resource)null) is true if any statements in this model have no
 associated context,
 model.contains(null, null, null, c1, c2, c3) is true if any statements in this model have context
 c1, c2 or c3 .

Parameters:
subj - The subject of the statements to match, null to match statements with any subject.
pred - The predicate of the statements to match, null to match statements with any predicate.
obj - The object of the statements to match, null to match statements with any object.
contexts - The contexts of the statements to match. If no contexts are specified, statements will match
                 disregarding their context. If one or more contexts are specified, statements with a context
                 matching one of these will match.
Returns:
true if statements match the specified pattern.






add

boolean add(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Adds one or more statements to the model. This method creates a statement for each specified context and adds
 those to the model. If no contexts are specified, a single statement with no associated context is added. If this
 Model is a filtered Model then null (if context empty) values are permitted and will use the corresponding
 filtered values.

Parameters:
subj - The statement's subject.
pred - The statement's predicate.
obj - The statement's object.
contexts - The contexts to add statements to.
Throws:
IllegalArgumentException - If This Model cannot store the given statement, because it is filtered out
                                       of this view.
UnsupportedOperationException - If this Model cannot accept any statements, because it is filtered to the
                                       empty set.






clear

boolean clear(Resource... context)
Removes statements with the specified context exist in this model.

Parameters:
context - The context of the statements to remove.
Returns:
true if one or more statements have been removed.






remove

boolean remove(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Removes statements with the specified subject, predicate, object and (optionally) context exist in this model.
 The subject, predicate and object parameters can be null to indicate wildcards.
 The contexts parameter is a wildcard and accepts zero or more values. If no contexts are specified,
 statements will be removed disregarding their context. If one or more contexts are specified, statements with a
 context matching one of these will be removed. Note: to remove statements without an associated context, specify
 the value null and explicitly cast it to type Resource.
 
 Examples: model.remove(s1, null, null) removes any statements in this model have subject s1,
 model.remove(null, null, null, c1) removes any statements in this model have context c1 ,
 model.remove(null, null, null, (Resource)null) removes any statements in this model have no associated
 context,
 model.remove(null, null, null, c1, c2, c3) removes any statements in this model have context c1,
 c2 or c3.

Parameters:
subj - The subject of the statements to remove, null to remove statements with any subject.
pred - The predicate of the statements to remove, null to remove statements with any predicate.
obj - The object of the statements to remove, null to remove statements with any object.
contexts - The contexts of the statements to remove. If no contexts are specified, statements will be
                 removed disregarding their context. If one or more contexts are specified, statements with a
                 context matching one of these will be removed.
Returns:
true if one or more statements have been removed.






getStatements

default Iterable<Statement> getStatements(Resource subject,
 IRI predicate,
 Value object,
 Resource... contexts)
Returns an Iterable over all Statements in this Model that match the supplied criteria.
 
 Examples:
 
 model.getStatements(s1, null, null) matches all statements that have subject s1
 model.getStatements(s1, p1, null) matches all statements that have subject s1 and predicate
 p1
 model.getStatements(null, null, null, c1) matches all statements that have context c1
 model.getStatements(null, null, null, (Resource)null) matches all statements that have no associated
 context
 model.getStatements(null, null, null, c1, c2, c3) matches all statements that have context
 c1, c2 or c3
 

Parameters:
subject - The subject of the statements to match, null to match statements with any subject.
predicate - The predicate of the statements to match, null to match statements with any predicate.
object - The object of the statements to match, null to match statements with any object.
contexts - The contexts of the statements to match. If no contexts are specified, statements will match
                  disregarding their context. If one or more contexts are specified, statements with a context
                  matching any one of these will match. To match statements without an associated context, specify
                  the value null and explicitly cast it to type Resource.
Returns:
an Iterable over the statements in this Model that match the specified pattern.
Since:
3.2.0
See Also:


filter(Resource, IRI, Value, Resource...)








filter

Model filter(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
Returns a filtered view of the statements with the specified subject, predicate, object and (optionally) context.
 The subject, predicate and object parameters can be null to indicate wildcards.
 The contexts parameter is a wildcard and accepts zero or more values. If no contexts are specified,
 statements will match disregarding their context. If one or more contexts are specified, statements with a
 context matching one of these will match. Note: to match statements without an associated context, specify the
 value null and explicitly cast it to type Resource.
 
 The returned model is backed by this Model, so changes to this Model are reflected in the returned model, and
 vice-versa. If this Model is modified while an iteration over the returned model is in progress (except through
 the iterator's own remove operation), the results of the iteration are undefined. The model supports
 element removal, which removes the corresponding statement from this Model, via the Iterator.remove,
 Set.remove, removeAll, retainAll, and clear operations. The statements passed to
 the add and addAll operations must match the parameter pattern.
 
 Examples: model.filter(s1, null, null) matches all statements that have subject s1,
 model.filter(null, null, null, c1) matches all statements that have context c1,
 model.filter(null, null, null, (Resource)null) matches all statements that have no associated
 context,
 model.filter(null, null, null, c1, c2, c3) matches all statements that have context c1,
 c2 or c3.

Parameters:
subj - The subject of the statements to match, null to match statements with any subject.
pred - The predicate of the statements to match, null to match statements with any predicate.
obj - The object of the statements to match, null to match statements with any object.
contexts - The contexts of the statements to match. If no contexts are specified, statements will match
                 disregarding their context. If one or more contexts are specified, statements with a context
                 matching one of these will match.
Returns:
The statements that match the specified pattern.
See Also:


getStatements(Resource, IRI, Value, Resource...)








subjects

Set<Resource> subjects()
Returns a Set view of the subjects contained in this model. The set is backed by the model, so changes to
 the model are reflected in the set, and vice-versa. If the model is modified while an iteration over the set is
 in progress (except through the iterator's own remove operation), the results of the iteration are
 undefined. The set supports element removal, which removes all statements from the model for which that element
 is a subject value, via the Iterator.remove, Set.remove, removeAll, retainAll,
 and clear operations. It does not support the add or addAll operations if the parameters
 pred or obj are null.

Returns:
a set view of the subjects contained in this model






predicates

Set<IRI> predicates()
Returns a Set view of the predicates contained in this model. The set is backed by the model, so changes
 to the model are reflected in the set, and vice-versa. If the model is modified while an iteration over the set
 is in progress (except through the iterator's own remove operation), the results of the iteration are
 undefined. The set supports element removal, which removes all statements from the model for which that element
 is a predicate value, via the Iterator.remove, Set.remove, removeAll, retainAll,
 and clear operations. It does not support the add or addAll operations if the parameters
 subj or obj are null.

Returns:
a set view of the predicates contained in this model






objects

Set<Value> objects()
Returns a Set view of the objects contained in this model. The set is backed by the model, so changes to
 the model are reflected in the set, and vice-versa. If the model is modified while an iteration over the set is
 in progress (except through the iterator's own remove operation), the results of the iteration are
 undefined. The set supports element removal, which removes all statements from the model for which that element
 is an object value, via the Iterator.remove, Set.remove, removeAll, retainAll,
 and clear operations. It does not support the add or addAll operations if the parameters
 subj or pred are null.

Returns:
a set view of the objects contained in this model






contexts

default Set<Resource> contexts()
Returns a Set view of the contexts contained in this model. The set is backed by the model, so changes to
 the model are reflected in the set, and vice-versa. If the model is modified while an iteration over the set is
 in progress (except through the iterator's own remove operation), the results of the iteration are
 undefined. The set supports element removal, which removes all statements from the model for which that element
 is a context value, via the Iterator.remove, Set.remove, removeAll, retainAll,
 and clear operations. It does not support the add or addAll operations if the parameters
 subj , pred or obj are null.

Returns:
a set view of the contexts contained in this model












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nClass Models

java.lang.Object
org.eclipse.rdf4j.model.util.Models




public class Models
extends Object
Utility functions for working with Models and other Statement collections.

Author:
Jeen Broekstra, Arjohn Kampman
See Also:


Model
ModelBuilder










Constructor Summary
Constructors

Modifier
Constructor
Description
protected 
Models()
 






Method Summary

All MethodsStatic MethodsConcrete MethodsDeprecated Methods


Modifier and Type
Method
Description
static Model
convertRDFStarToReification(Model model)

Converts the statements in the supplied RDF-star model to a new RDF model using reification.

static void
convertRDFStarToReification(Model model,
 Consumer<Statement> consumer)

Converts the supplied RDF-star model to RDF reification statements.

static Model
convertRDFStarToReification(ValueFactory vf,
 Model model)

Converts the statements in supplied RDF-star model to a new RDF model using reificiation.

static void
convertRDFStarToReification(ValueFactory vf,
 Model model,
 Consumer<Statement> consumer)

Converts the supplied RDF-star model to RDF reification statements.

static Model
convertRDFStarToReification(ValueFactory vf,
 Model model,
 ModelFactory modelFactory)

Converts the statements in supplied RDF-star model to a new RDF model using reificiation.

static Model
convertReificationToRDFStar(Model model)

Converts the supplied RDF reification model to a new RDF-star model.

static void
convertReificationToRDFStar(Model model,
 Consumer<Statement> consumer)

Converts the supplied RDF reification model to RDF-star statements.

static Model
convertReificationToRDFStar(ValueFactory vf,
 Model model)

Converts the statements in supplied RDF reification model to a new RDF-star model.

static void
convertReificationToRDFStar(ValueFactory vf,
 Model model,
 Consumer<Statement> consumer)

Converts the supplied RDF reification model to RDF-star statements.

static Model
convertReificationToRDFStar(ValueFactory vf,
 Model model,
 ModelFactory modelFactory)

Converts the statements in supplied RDF reification model to a new RDF-star model.

static Set<Value>
getProperties(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)

Retrieve all property values for the supplied subject and property from the given model.

static Optional<Value>
getProperty(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)

Retrieve a property value for the supplied subject from the given model.

static Optional<IRI>
getPropertyIRI(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)

Retrieve a property value as an IRI for the supplied subject from the given model.

static Set<IRI>
getPropertyIRIs(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)

Retrieve all property IRI values for the supplied subject and property from the given model.

static Optional<Literal>
getPropertyLiteral(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)

Retrieve a property value as a Literal for the supplied subject from the given model.

static Set<Literal>
getPropertyLiterals(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)

Retrieve all property Literal values for the supplied subject and property from the given model.

static Optional<Resource>
getPropertyResource(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)

Retrieve a property value as an IRI for the supplied subject from the given model.

static Set<Resource>
getPropertyResources(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)

Retrieve all property Resource values for the supplied subject and property from the given model.

static Optional<String>
getPropertyString(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)

Retrieve a property value as a String for the supplied subject from the given model.

static Set<String>
getPropertyStrings(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)

Retrieve all property values as Strings for the supplied subject and property from the given model.

static boolean
isomorphic(Iterable<? extends Statement> model1,
 Iterable<? extends Statement> model2)

Compares two RDF models, and returns true if they consist of isomorphic graphs and the isomorphic
 graph identifiers map 1:1 to each other.

static boolean
isSubset(Iterable<? extends Statement> model1,
 Iterable<? extends Statement> model2)

Compares two RDF models, and returns true if the first model is a subset of the second model, using
 graph isomorphism to map statements between models.

static boolean
isSubset(Set<? extends Statement> model1,
 Set<? extends Statement> model2)

Compares two RDF models, and returns true if the first model is a subset of the second model, using
 graph isomorphism to map statements between models.

static boolean
legacyIsomorphic(Iterable<? extends Statement> model1,
 Iterable<? extends Statement> model2)

Deprecated.
Use isomorphic(Iterable, Iterable) instead.


static Supplier<ModelException>
modelException(String message)

Creates a Supplier of ModelException objects that be passed to
 Optional.orElseThrow(Supplier) to generate exceptions as necessary.

static Optional<Value>
object(Iterable<Statement> statements)

Retrieves an object Value from the supplied statements.

static Optional<Value>
object(Model m)

Retrieves an object Value from the statements in the given model.

static Optional<IRI>
objectIRI(Iterable<Statement> statements)

Retrieves an object IRI value from the supplied statements.

static Optional<IRI>
objectIRI(Model m)

Retrieves an object IRI value from the supplied statements in the given model.

static Set<IRI>
objectIRIs(Iterable<Statement> statements)

Retrieves all object IRI values from the supplied statements.

static Set<IRI>
objectIRIs(Model m)

Retrieves all object IRI values from the statements in the given model.

static Optional<Literal>
objectLiteral(Iterable<Statement> statements)

Retrieves an object Literal value from the supplied statements.

static Optional<Literal>
objectLiteral(Model m)

Retrieves an object Literal value from the statements in the given model.

static Set<Literal>
objectLiterals(Iterable<Statement> statements)

Retrieves all object Literal values from the supplied statements.

static Set<Literal>
objectLiterals(Model m)

Retrieves all object Literal values from the statements in the given model.

static Optional<Resource>
objectResource(Iterable<Statement> statements)

Retrieves an object Resource value from the supplied statements.

static Optional<Resource>
objectResource(Model m)

Retrieves an object Resource value from the statements in the given model.

static Set<Resource>
objectResources(Iterable<Statement> statements)

Retrieves all object Resource values from the supplied statements.

static Set<Resource>
objectResources(Model m)

Retrieves all object Resource values from the supplied model.

static Optional<String>
objectString(Iterable<Statement> statements)

Retrieves an object value as a String from the supplied statements.

static Optional<String>
objectString(Model m)

Retrieves an object value as a String from the statements in the given model.

static Set<String>
objectStrings(Iterable<Statement> statements)

Retrieves all object String values from the supplied statements.

static Set<String>
objectStrings(Model m)

Retrieves all object String values from the statements in the given model.

static Optional<IRI>
predicate(Iterable<Statement> statements)

Retrieves a predicate from the supplied statements.

static Optional<IRI>
predicate(Model m)

Retrieves a predicate from the statements in the given model.

static Model
setProperty(Model m,
 Resource subject,
 IRI property,
 Value value,
 Resource... contexts)

Sets the property value for the given subject to the given object value, replacing any existing value(s) for the
 subject's property.

static Model
stripContexts(Model model,
 Resource... contexts)

Strips contexts from the input model.

static Optional<Resource>
subject(Iterable<Statement> statements)

Retrieves a subject Resource from the supplied statements.

static Optional<Resource>
subject(Model m)

Retrieves a subject Resource from the statements in the given model.

static Optional<BNode>
subjectBNode(Iterable<Statement> statements)

Retrieves a subject BNode from the supplied statements.

static Optional<BNode>
subjectBNode(Model m)

Retrieves a subject BNode from the statements in the given model.

static Set<BNode>
subjectBNodes(Iterable<Statement> statements)

Retrieves all subject BNodes from the supplied statements.

static Set<BNode>
subjectBNodes(Model m)

Retrieves all subject BNodes from the statements in the given model.

static Optional<IRI>
subjectIRI(Iterable<Statement> statements)

Retrieves a subject IRI from the supplied statements.

static Optional<IRI>
subjectIRI(Model m)

Retrieves a subject IRI from the statements in the given model.

static Set<IRI>
subjectIRIs(Iterable<Statement> statements)

Retrieves all subject IRIs from the supplied statements.

static Set<IRI>
subjectIRIs(Model m)

Retrieves all subject IRIs from the statements in the given model.

static Model
synchronizedModel(Model toSynchronize)

Make a model thread-safe by synchronizing all its methods.





Methods inherited from class java.lang.Object
clone, equals, finalize, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait









Constructor Details



Models

protected Models()









Method Details



object

public static Optional<Value> object(Iterable<Statement> statements)
Retrieves an object Value from the supplied statements. If more than one possible object value exists,
 any one value is picked and returned.

Parameters:
statements - the Statement Iterable from which to retrieve an object value.
Returns:
an object value from the given statement collection, or Optional.empty() if no such value exists.






object

public static Optional<Value> object(Model m)
Retrieves an object Value from the statements in the given model. If more than one possible object value
 exists, any one value is picked and returned.

Parameters:
m - the model from which to retrieve an object value.
Returns:
an object value from the given model, or Optional.empty() if no such value exists.






objectLiteral

public static Optional<Literal> objectLiteral(Iterable<Statement> statements)
Retrieves an object Literal value from the supplied statements. If more than one possible Literal value
 exists, any one Literal value is picked and returned.

Parameters:
statements - the Statement Iterable from which to retrieve an object Literal value.
Returns:
an object Literal value from the given model, or Optional.empty() if no such value exists.






objectLiteral

public static Optional<Literal> objectLiteral(Model m)
Retrieves an object Literal value from the statements in the given model. If more than one possible
 Literal value exists, any one Literal value is picked and returned.

Parameters:
m - the Model from which to retrieve an object Literal value.
Returns:
an object Literal value from the given model, or Optional.empty() if no such value exists.






objectLiterals

public static Set<Literal> objectLiterals(Iterable<Statement> statements)
Retrieves all object Literal values from the supplied statements.

Parameters:
statements - the Statement Iterable from which to retrieve all object Literal
                   values.
Returns:
a Set containing object Literal values from the given model, which will be empty if no
         such value exists.
See Also:


Model.objects()








objectLiterals

public static Set<Literal> objectLiterals(Model m)
Retrieves all object Literal values from the statements in the given model.

Parameters:
m - the model from which to retrieve all object Literal values.
Returns:
a Set containing object Literal values from the given model, which will be empty if no
         such value exists.
See Also:


Model.objects()








objectResource

public static Optional<Resource> objectResource(Iterable<Statement> statements)
Retrieves an object Resource value from the supplied statements. If more than one possible Resource value
 exists, any one Resource value is picked and returned.

Parameters:
statements - the Statement Iterable from which to retrieve an object Resource value.
Returns:
an Optional object Resource value from the given model, which will be empty if no such value exists.






objectResource

public static Optional<Resource> objectResource(Model m)
Retrieves an object Resource value from the statements in the given model. If more than one possible
 Resource value exists, any one Resource value is picked and returned.

Parameters:
m - the model from which to retrieve an object Resource value.
Returns:
an Optional object Resource value from the given model, which will be empty if no such value exists.






objectResources

public static Set<Resource> objectResources(Iterable<Statement> statements)
Retrieves all object Resource values from the supplied statements.

Parameters:
statements - the Statement Iterable from which to retrieve all object Resource
                   values.
Returns:
a Set containing object Resource values from the given model, which will be empty if no
         such value exists.
See Also:


Model.objects()








objectResources

public static Set<Resource> objectResources(Model m)
Retrieves all object Resource values from the supplied model.

Parameters:
m - the Model from which to retrieve all object Resource values.
Returns:
a Set containing object Resource values from the given model, which will be empty if no
         such value exists.
See Also:


Model.objects()








objectIRI

public static Optional<IRI> objectIRI(Iterable<Statement> statements)
Retrieves an object IRI value from the supplied statements. If more than one possible IRI value exists,
 any one value is picked and returned.

Parameters:
statements - the Statement Iterable from which to retrieve an object IRI value.
Returns:
an Optional object IRI value from the given model, which will be empty
         if no such value exists.






objectIRI

public static Optional<IRI> objectIRI(Model m)
Retrieves an object IRI value from the supplied statements in the given model. If more than one possible
 IRI value exists, any one value is picked and returned.

Parameters:
m - the model from which to retrieve an object IRI value.
Returns:
an Optional object IRI value from the given model, which will be empty
         if no such value exists.






objectIRIs

public static Set<IRI> objectIRIs(Iterable<Statement> statements)
Retrieves all object IRI values from the supplied statements.

Parameters:
statements - the Statement Iterable from which to retrieve all object IRI values.
Returns:
a Set containing object IRI values from the given model, which will be empty if no such value
         exists.
See Also:


Model.objects()








objectIRIs

public static Set<IRI> objectIRIs(Model m)
Retrieves all object IRI values from the statements in the given model.

Parameters:
m - the Model from which to retrieve all object IRI values.
Returns:
a Set containing object IRI values from the given model, which will be empty if no such value
         exists.
See Also:


Model.objects()








objectString

public static Optional<String> objectString(Iterable<Statement> statements)
Retrieves an object value as a String from the supplied statements. If more than one possible object value
 exists, any one value is picked and returned.

Parameters:
statements - the Statement Iterable from which to retrieve an object String value.
Returns:
an Optional object String value from the given model, which will be empty if no such value exists.






objectString

public static Optional<String> objectString(Model m)
Retrieves an object value as a String from the statements in the given model. If more than one possible object
 value exists, any one value is picked and returned.

Parameters:
m - the model from which to retrieve an object String value.
Returns:
an Optional object String value from the given model, which will be empty if no such value exists.






objectStrings

public static Set<String> objectStrings(Iterable<Statement> statements)
Retrieves all object String values from the supplied statements.

Parameters:
statements - the Statement Iterable from which to retrieve all object String values.
Returns:
a Set containing object String values from the given model, which will be empty if no such value
         exists.
See Also:


Model.objects()








objectStrings

public static Set<String> objectStrings(Model m)
Retrieves all object String values from the statements in the given model.

Parameters:
m - the model from which to retrieve all object String values.
Returns:
a Set containing object String values from the given model, which will be empty if no such value
         exists.
See Also:


Model.objects()








subject

public static Optional<Resource> subject(Iterable<Statement> statements)
Retrieves a subject Resource from the supplied statements. If more than one possible resource value
 exists, any one resource value is picked and returned.

Parameters:
statements - the Statement Iterable from which to retrieve a subject Resource.
Returns:
an Optional subject resource from the given model, which will be empty
         if no such value exists.






subject

public static Optional<Resource> subject(Model m)
Retrieves a subject Resource from the statements in the given model. If more than one possible resource
 value exists, any one resource value is picked and returned.

Parameters:
m - the model from which to retrieve a subject Resource.
Returns:
an Optional subject resource from the given model, which will be empty
         if no such value exists.






subjectIRI

public static Optional<IRI> subjectIRI(Iterable<Statement> statements)
Retrieves a subject IRI from the supplied statements. If more than one possible IRI value exists, any one
 IRI value is picked and returned.

Parameters:
statements - the Statement Iterable from which to retrieve a subject IRI value.
Returns:
an Optional subject IRI value from the given model, which will be empty
         if no such value exists.






subjectIRI

public static Optional<IRI> subjectIRI(Model m)
Retrieves a subject IRI from the statements in the given model. If more than one possible IRI value
 exists, any one IRI value is picked and returned.

Parameters:
m - the model from which to retrieve a subject IRI value.
Returns:
an Optional subject IRI value from the given model, which will be empty
         if no such value exists.






subjectIRIs

public static Set<IRI> subjectIRIs(Iterable<Statement> statements)
Retrieves all subject IRIs from the supplied statements.

Parameters:
statements - the Statement Iterable from which to retrieve a subject IRI value.
Returns:
a Set of subject IRI values from the given model. The returned Set may be empty.






subjectIRIs

public static Set<IRI> subjectIRIs(Model m)
Retrieves all subject IRIs from the statements in the given model.

Parameters:
m - the model from which to retrieve a subject IRI value.
Returns:
a Set of subject IRI values from the given model. The returned Set may be empty.






subjectBNode

public static Optional<BNode> subjectBNode(Iterable<Statement> statements)
Retrieves a subject BNode from the supplied statements. If more than one possible blank node value
 exists, any one blank node value is picked and returned.

Parameters:
statements - the Statement Iterable from which to retrieve a subject BNode value.
Returns:
an Optional subject BNode value from the given model, which will be empty if no such value exists.






subjectBNode

public static Optional<BNode> subjectBNode(Model m)
Retrieves a subject BNode from the statements in the given model. If more than one possible blank node
 value exists, any one blank node value is picked and returned.

Parameters:
m - the model from which to retrieve a subject BNode value.
Returns:
an Optional subject BNode value from the given model, which will be empty if no such value exists.






subjectBNodes

public static Set<BNode> subjectBNodes(Iterable<Statement> statements)
Retrieves all subject BNodes from the supplied statements.

Parameters:
statements - the Statement Iterable from which to retrieve a subject IRI value.
Returns:
a Set of subject BNode values from the given model. The returned Set may be empty.






subjectBNodes

public static Set<BNode> subjectBNodes(Model m)
Retrieves all subject BNodes from the statements in the given model.

Parameters:
m - the model from which to retrieve a subject IRI value.
Returns:
a Set of subject BNode values from the given model. The returned Set may be empty.






predicate

public static Optional<IRI> predicate(Iterable<Statement> statements)
Retrieves a predicate from the supplied statements. If more than one possible predicate value exists, any one
 value is picked and returned.

Parameters:
statements - the Statement Iterable from which to retrieve a predicate value.
Returns:
an Optional predicate value from the given model, which will be empty if
         no such value exists.






predicate

public static Optional<IRI> predicate(Model m)
Retrieves a predicate from the statements in the given model. If more than one possible predicate value exists,
 any one value is picked and returned.

Parameters:
m - the model from which to retrieve a predicate value.
Returns:
an Optional predicate value from the given model, which will be empty if
         no such value exists.






setProperty

public static Model setProperty(Model m,
 Resource subject,
 IRI property,
 Value value,
 Resource... contexts)
Sets the property value for the given subject to the given object value, replacing any existing value(s) for the
 subject's property. This method updates the original input Model and then returns that same Model object.

Parameters:
m - the model in which to set the property value. May not be null.
subject - the subject for which to set/replace the property value. May not be null.
property - the property for which to set/replace the value. May not be null.
value - the value to set for the given subject and property. May not be null.
contexts - the context(s) in which to set/replace the property value. Optional vararg argument. If not
                 specified the operations works on the entire Model.
Returns:
the Model object, containing the updated property value.






getProperty

public static Optional<Value> getProperty(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)
Retrieve a property value for the supplied subject from the given model. If more than one property value exists,
 any one value is picked and returned.

Parameters:
m - the model from which to retrieve an object value.
subject - the subject resource for which to retrieve a property value.
property - the property for which to retrieve a value.
contexts - the contexts from which to retrieve the property value. Optional vararg argument. If not
                 specified the operations works on the entire Model.
Returns:
a property value from the given model, or Optional.empty() if no such value exists.






getProperties

public static Set<Value> getProperties(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)
Retrieve all property values for the supplied subject and property from the given model.

Parameters:
m - the model from which to retrieve the property values.
subject - the subject resource for which to retrieve all property values.
property - the property for which to retrieve all values.
contexts - the contexts from which to retrieve the property values. Optional vararg argument. If not
                 specified the operations works on the entire Model.
Returns:
a Set of all property values for the supplied input. The resulting set may be empty.






getPropertyResource

public static Optional<Resource> getPropertyResource(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)
Retrieve a property value as an IRI for the supplied subject from the given model. If more than one property
 value exists, any one value is picked and returned.

Parameters:
m - the model from which to retrieve an object value.
subject - the subject resource for which to retrieve a property value.
property - the property for which to retrieve a value.
contexts - the contexts from which to retrieve the property value. Optional vararg argument. If not
                 specified the operations works on the entire Model.
Returns:
a property value Resource from the given model, or Optional.empty() if no such value exists.






getPropertyResources

public static Set<Resource> getPropertyResources(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)
Retrieve all property Resource values for the supplied subject and property from the given model.

Parameters:
m - the model from which to retrieve the property Resource values.
subject - the subject resource for which to retrieve all property Resource values.
property - the property for which to retrieve all Resource values.
contexts - the contexts from which to retrieve the property values. Optional vararg argument. If not
                 specified the operations works on the entire Model.
Returns:
a Set of all property Resource values for the supplied input. The resulting set may be empty.






getPropertyIRI

public static Optional<IRI> getPropertyIRI(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)
Retrieve a property value as an IRI for the supplied subject from the given model. If more than one property
 value exists, any one value is picked and returned.

Parameters:
m - the model from which to retrieve an object value.
subject - the subject resource for which to retrieve a property value.
property - the property for which to retrieve a value.
contexts - the contexts from which to retrieve the property value. Optional vararg argument. If not
                 specified the operations works on the entire Model.
Returns:
a property value IRI from the given model, or Optional.empty() if no such value exists.






getPropertyIRIs

public static Set<IRI> getPropertyIRIs(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)
Retrieve all property IRI values for the supplied subject and property from the given model.

Parameters:
m - the model from which to retrieve the property IRI values.
subject - the subject resource for which to retrieve all property IRI values.
property - the property for which to retrieve all IRI values.
contexts - the contexts from which to retrieve the property values. Optional vararg argument. If not
                 specified the operations works on the entire Model.
Returns:
a Set of all property IRI values for the supplied input. The resulting set may be empty.






getPropertyLiteral

public static Optional<Literal> getPropertyLiteral(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)
Retrieve a property value as a Literal for the supplied subject from the given model. If more than one
 property value exists, any one value is picked and returned.

Parameters:
m - the model from which to retrieve an object value.
subject - the subject resource for which to retrieve a property literal value.
property - the property for which to retrieve a value.
contexts - the contexts from which to retrieve the property value. Optional vararg argument. If not
                 specified the operations works on the entire Model.
Returns:
a property value Literal from the given model, or Optional.empty() if no such value exists.






getPropertyLiterals

public static Set<Literal> getPropertyLiterals(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)
Retrieve all property Literal values for the supplied subject and property from the given model.

Parameters:
m - the model from which to retrieve the property Literal values.
subject - the subject resource for which to retrieve all property Literal values.
property - the property for which to retrieve all Literal values.
contexts - the contexts from which to retrieve the property values. Optional vararg argument. If not
                 specified the operations works on the entire Model.
Returns:
a Set of all property IRI values for the supplied input. The resulting set may be empty.






getPropertyString

public static Optional<String> getPropertyString(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)
Retrieve a property value as a String for the supplied subject from the given model. If more than one property
 value exists, any one value is picked and returned.

Parameters:
m - the model from which to retrieve an object value.
subject - the subject resource for which to retrieve a property literal value.
property - the property for which to retrieve a value.
contexts - the contexts from which to retrieve the property value. Optional vararg argument. If not
                 specified the operations works on the entire Model.
Returns:
a property value String from the given model, or Optional.empty() if no such value exists.






getPropertyStrings

public static Set<String> getPropertyStrings(Model m,
 Resource subject,
 IRI property,
 Resource... contexts)
Retrieve all property values as Strings for the supplied subject and property from the given model.

Parameters:
m - the model from which to retrieve the property values as Strings.
subject - the subject resource for which to retrieve all property values as Strings.
property - the property for which to retrieve all values as Strings.
contexts - the contexts from which to retrieve the property values. Optional vararg argument. If not
                 specified the operations works on the entire Model.
Returns:
a Set of all property values as Strings for the supplied input. The resulting set may be empty.






isomorphic

public static boolean isomorphic(Iterable<? extends Statement> model1,
 Iterable<? extends Statement> model2)
Compares two RDF models, and returns true if they consist of isomorphic graphs and the isomorphic
 graph identifiers map 1:1 to each other. RDF graphs are isomorphic graphs if statements from one graphs can be
 mapped 1:1 on to statements in the other graphs. In this mapping, blank nodes are not considered mapped when
 having an identical internal id, but are mapped from one graph to the other by looking at the statements in which
 the blank nodes occur. A Model can consist of more than one graph (denoted by context identifiers). Two models
 are considered isomorphic if for each of the graphs in one model, an isomorphic graph exists in the other model,
 and the context identifiers of these graphs are either identical or (in the case of blank nodes) map 1:1 on each
 other.

See Also:


RDF Concepts & Abstract Syntax, section
      3.6 (Graph Comparison)








legacyIsomorphic

@Experimental
@Deprecated(since="3.6.0")
public static boolean legacyIsomorphic(Iterable<? extends Statement> model1,
 Iterable<? extends Statement> model2)
Deprecated.
Use isomorphic(Iterable, Iterable) instead.

Legacy implementation of isomorphic comparison. This method is offered as
 a temporary fallback for corner cases where the newly introduced isomorphism algorithm (in release 3.6.0) has
 worse performance or an unexpected result.

Since:
3.6.0
See Also:


isomorphic(Iterable, Iterable)








isSubset

public static boolean isSubset(Iterable<? extends Statement> model1,
 Iterable<? extends Statement> model2)
Compares two RDF models, and returns true if the first model is a subset of the second model, using
 graph isomorphism to map statements between models.





isSubset

public static boolean isSubset(Set<? extends Statement> model1,
 Set<? extends Statement> model2)
Compares two RDF models, and returns true if the first model is a subset of the second model, using
 graph isomorphism to map statements between models.





stripContexts

public static Model stripContexts(Model model,
 Resource... contexts)
Strips contexts from the input model. This method provides a new Model containing all statements from the
 input model, with the supplied contexts removed from those statements.

Parameters:
model - the input model
contexts - the contexts to remove. This is a vararg and as such is optional. If not supplied, the method
                 strips all contexts.
Returns:
a new Model object containg the same statements as the input model, with the supplied contexts
         stripped.






modelException

public static Supplier<ModelException> modelException(String message)
Creates a Supplier of ModelException objects that be passed to
 Optional.orElseThrow(Supplier) to generate exceptions as necessary.

Parameters:
message - The message to be used for the exception
Returns:
A Supplier that will create ModelException objects with the given message.






synchronizedModel

public static Model synchronizedModel(Model toSynchronize)
Make a model thread-safe by synchronizing all its methods. Iterators will still not be thread-safe!

Parameters:
toSynchronize - the model that should be synchronized
Returns:
Synchronized Model






convertRDFStarToReification

@Experimental
public static void convertRDFStarToReification(ValueFactory vf,
 Model model,
 Consumer<Statement> consumer)
Converts the supplied RDF-star model to RDF reification statements. The converted statements are sent to the
 supplied consumer function.
 
 The supplied value factory is used to create all new statements.

Parameters:
vf - the ValueFactory to use for creating statements.
model - the Model to convert.
consumer - the Consumer function for the produced statements.






convertRDFStarToReification

@Experimental
public static void convertRDFStarToReification(Model model,
 Consumer<Statement> consumer)
Converts the supplied RDF-star model to RDF reification statements. The converted statements are sent to the
 supplied consumer function.

Parameters:
model - the Model to convert.
consumer - the Consumer function for the produced statements.






convertRDFStarToReification

@Experimental
public static Model convertRDFStarToReification(ValueFactory vf,
 Model model)
Converts the statements in supplied RDF-star model to a new RDF model using reificiation.
 
 The supplied value factory is used to create all new statements.

Parameters:
vf - the ValueFactory to use for creating statements.
model - the Model to convert.
Returns:
a new Model with RDF-star statements converted to reified triples.






convertRDFStarToReification

@Experimental
public static Model convertRDFStarToReification(ValueFactory vf,
 Model model,
 ModelFactory modelFactory)
Converts the statements in supplied RDF-star model to a new RDF model using reificiation.
 
 The supplied value factory is used to create all new statements.

Parameters:
vf - the ValueFactory to use for creating statements.
model - the Model to convert.
modelFactory - the ModelFactory used to create the new output Model.
Returns:
a new Model with RDF-star statements converted to reified triples.






convertRDFStarToReification

@Experimental
public static Model convertRDFStarToReification(Model model)
Converts the statements in the supplied RDF-star model to a new RDF model using reification.

Parameters:
model - the Model to convert.
Returns:
a new Model with RDF-star statements converted to reified triples.






convertReificationToRDFStar

@Experimental
public static void convertReificationToRDFStar(ValueFactory vf,
 Model model,
 Consumer<Statement> consumer)
Converts the supplied RDF reification model to RDF-star statements. The converted statements are sent to the
 supplied consumer function.
 
 The supplied value factory is used to create all new statements.

Parameters:
vf - the ValueFactory to use for creating statements.
model - the Model to convert.
consumer - the Consumer function for the produced statements.






convertReificationToRDFStar

@Experimental
public static void convertReificationToRDFStar(Model model,
 Consumer<Statement> consumer)
Converts the supplied RDF reification model to RDF-star statements. The converted statements are sent to the
 supplied consumer function.

Parameters:
model - the Model to convert.
consumer - the Consumer function for the produced statements.






convertReificationToRDFStar

@Experimental
public static Model convertReificationToRDFStar(ValueFactory vf,
 Model model,
 ModelFactory modelFactory)
Converts the statements in supplied RDF reification model to a new RDF-star model.
 
 The supplied value factory is used to create all new statements.

Parameters:
vf - the ValueFactory to use for creating statements.
model - the Model to convert.
modelFactory - the ModelFactory to use for creating a new Model object for the output.
Returns:
a new Model with reification statements converted to RDF-star Triples.






convertReificationToRDFStar

@Experimental
public static Model convertReificationToRDFStar(ValueFactory vf,
 Model model)
Converts the statements in supplied RDF reification model to a new RDF-star model.
 
 The supplied value factory is used to create all new statements.

Parameters:
vf - the ValueFactory to use for creating statements.
model - the Model to convert.
Returns:
a new Model with reification statements converted to RDF-star Triples.






convertReificationToRDFStar

@Experimental
public static Model convertReificationToRDFStar(Model model)
Converts the supplied RDF reification model to a new RDF-star model.

Parameters:
model - the Model to convert.
Returns:
a new Model with reification statements converted to RDF-star Triples.












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nProgramming With RDF4J
    

  
  
    
        
          
          Setting up your development environmentThis chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
          
          The RDF Model APIThe RDF Model API is the core of the RDF4J framework. It provides the basic building blocks for manipulating RDF data in Java.
          
          The Repository APIThe Repository API is the central access point for RDF4J-compatible RDF databases (a.k.a. triplestores), as well as for SPARQL endpoints. This is what you use to execute SPARQL queries and update your data.
          
          Parsing and Writing RDF with RioThe RDF4J framework includes a set of parsers and writers for RDF called Rio. Rio (“RDF I/O”) is a toolkit that can be used independently from the rest of RDF4J.
          
          The LMDB StoreNew in RDF4J 4.0

Experimental

The RDF4J LMDB Store is a new SAIL database, using the Symas Lightning
Memory-Mapped Database: a fast embeddable
key-value database using memory-mapped IO for great performance and stability.
          
          Full-text indexing with the Lucene SAILThe LuceneSail enables you to add full text search of RDF literals to find subject resources to any Sail stack.
          
          Reasoning and Validation with SPINThe SPARQL Inferencing Notation (SPIN) is a way to represent a wide range of business rules on top of an RDF dataset. These rules can be anything from constraint validation to inferred property value calculation.
          
          Validation with SHACLThe SHapes Constraint Language (SHACL) is a language for validating RDF graphs.
          
          Federation with FedXFedX provides transparent federation of multiple SPARQL endpoints under a single virtual endpoint.
          
          Integration with SpringThe rdf4j-spring
 module allows for using an RDF4J repository as the data backend of a spring application.
          
          GeoSPARQLRDF4J offers an extended algebra for partial GeoSPARQL support. When enabled, this offers additional geospatial functionality as part of the SPARQL engine, on top of any RDF4J repository, using the well-known Spatial4J and JTS libraries for geospatial reasoning.
          
          RDF-star and SPARQL-starRDF4J has (experimental) support for RDF-star and SPARQL-star.
          
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nInterface Repository



All Known Subinterfaces:
DelegatingRepository, InterceptingRepository, NotifyingRepository


All Known Implementing Classes:
AbstractRepository, ContextAwareRepository, DatasetRepository, FedXRepository, FedXRepositoryWrapper, HTTPRepository, InterceptingRepositoryWrapper, NotifyingRepositoryWrapper, ProxyRepository, RepositoryConfigRepository, RepositoryWrapper, SailRepository, SPARQLRepository



public interface Repository
An RDF4J repository that contains RDF data that can be queried and updated. Access to the repository can be acquired
 by opening a connection to it. This connection can then be used to query and/or update the contents of the
 repository. Depending on the implementation of the repository, it may or may not support multiple concurrent
 connections.
 
 Please note that a repository should be shut down before it is discarded/garbage collected. Forgetting the latter can
 result in loss of data (depending on the Repository implementation)!
 
 Repository implementations are thread-safe unless specifically documented otherwise.

Author:
Arjohn Kampman








Method Summary

All MethodsInstance MethodsAbstract Methods


Modifier and Type
Method
Description
RepositoryConnection
getConnection()

Opens a connection to this repository that can be used for querying and updating the contents of the repository.

File
getDataDir()

Get the directory where data and logging for this repository is stored.

ValueFactory
getValueFactory()

Gets a ValueFactory for this Repository.

void
init()

Initializes this repository.

boolean
isInitialized()

Indicates if the Repository has been initialized.

boolean
isWritable()

Checks whether this repository is writable, i.e.

void
setDataDir(File dataDir)

Set the directory where data and logging for this repository is stored.

void
shutDown()

Shuts the repository down, releasing any resources that it keeps hold of.













Method Details



setDataDir

void setDataDir(File dataDir)
Set the directory where data and logging for this repository is stored.

Parameters:
dataDir - the directory where data for this repository is stored






getDataDir

File getDataDir()
Get the directory where data and logging for this repository is stored.

Returns:
the directory where data for this repository is stored.






init

void init()
   throws RepositoryException
Initializes this repository. A repository needs to be initialized before it can be used, however explicitly
 calling this method is not necessary: the repository will automatically initialize itself if an operation is
 executed on it that requires it to be initialized.

Throws:
RepositoryException - If the initialization failed.
Since:
2.5






isInitialized

boolean isInitialized()
Indicates if the Repository has been initialized. Note that the initialization status may change if the
 Repository is shut down.

Returns:
true iff the repository has been initialized.






shutDown

void shutDown()
       throws RepositoryException
Shuts the repository down, releasing any resources that it keeps hold of. Once shut down, the repository can no
 longer be used until it is re-initialized.

Throws:
RepositoryException






isWritable

boolean isWritable()
            throws RepositoryException
Checks whether this repository is writable, i.e. if the data contained in this repository can be changed. The
 writability of the repository is determined by the writability of the Sail that this repository operates on.

Throws:
RepositoryException






getConnection

RepositoryConnection getConnection()
                            throws RepositoryException
Opens a connection to this repository that can be used for querying and updating the contents of the repository.
 Created connections need to be closed to make sure that any resources they keep hold of are released. The best
 way to do this is to use a try-with-resources block, as follows:

  try (RepositoryConnection conn = repository.getConnection()) {
        // perform operations on the connection
 }
 
 
 Note that RepositoryConnection is not guaranteed to be thread-safe! The recommended pattern for
 repository access in a multi-threaded application is to share the Repository object between threads, but have
 each thread create and use its own RepositoryConnections.

Returns:
A connection that allows operations on this repository.
Throws:
RepositoryException - If something went wrong during the creation of the Connection.






getValueFactory

ValueFactory getValueFactory()
Gets a ValueFactory for this Repository.

Returns:
A repository-specific ValueFactory.












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nClass SailRepository

java.lang.Object
org.eclipse.rdf4j.repository.base.AbstractRepository
org.eclipse.rdf4j.repository.sail.SailRepository




All Implemented Interfaces:
HttpClientDependent, SessionManagerDependent, FederatedServiceResolverClient, Repository, RepositoryResolverClient


Direct Known Subclasses:
FedXRepository



public class SailRepository
extends AbstractRepository
implements FederatedServiceResolverClient, RepositoryResolverClient, HttpClientDependent, SessionManagerDependent
An implementation of the Repository interface that operates on a (stack of) Sail object(s). The
 behaviour of the repository is determined by the Sail stack that it operates on; for example, the repository will
 only support RDF Schema or OWL semantics if the Sail stack includes an inferencer for this.
 
 Creating a repository object of this type is very easy. For example, the following code creates and initializes a
 main-memory store with RDF Schema semantics:

  Repository repository = new SailRepository(new ForwardChainingRDFSInferencer(new MemoryStore()));
 repository.initialize();
 
 
 Or, alternatively:

  Sail sailStack = new MemoryStore();
 sailStack = new ForwardChainingRDFSInferencer(sailStack);

 Repository repository = new SailRepository(sailStack);
 repository.initialize();
 

Author:
Arjohn Kampman








Field Summary

Fields inherited from class org.eclipse.rdf4j.repository.base.AbstractRepository
logger





Constructor Summary
Constructors

Constructor
Description
SailRepository(Sail sail)

Creates a new repository object that operates on the supplied Sail.







Method Summary

All MethodsInstance MethodsConcrete Methods


Modifier and Type
Method
Description
SailRepositoryConnection
getConnection()

Opens a connection to this repository that can be used for querying and updating the contents of the repository.

File
getDataDir()

Get the directory where data and logging for this repository is stored.

FederatedServiceResolver
getFederatedServiceResolver()

Gets the FederatedServiceResolver used by this client.

org.apache.http.client.HttpClient
getHttpClient()

HttpClient that has been assigned or has been used by this object.

HttpClientSessionManager
getHttpClientSessionManager()

HttpClientSessionManager that has been assigned or has been used by this object.

Sail
getSail()

Gets the Sail object that is on top of the Sail stack that this repository operates on.

ValueFactory
getValueFactory()

Gets a ValueFactory for this Repository.

protected void
initializeInternal()
 
boolean
isWritable()

Checks whether this repository is writable, i.e.

void
setDataDir(File dataDir)

Set the directory where data and logging for this repository is stored.

void
setFederatedServiceResolver(FederatedServiceResolver resolver)

Sets the FederatedServiceResolver to use for this client.

void
setHttpClient(org.apache.http.client.HttpClient client)

Assign an HttpClient that this object should use.

void
setHttpClientSessionManager(HttpClientSessionManager client)

Assign an HttpClientSessionManager that this object should use.

void
setRepositoryResolver(RepositoryResolver resolver)
 
protected void
shutDownInternal()
 
String
toString()
 




Methods inherited from class org.eclipse.rdf4j.repository.base.AbstractRepository
init, isInitialized, shutDown

Methods inherited from class java.lang.Object
clone, equals, finalize, getClass, hashCode, notify, notifyAll, wait, wait, wait









Constructor Details



SailRepository

public SailRepository(Sail sail)
Creates a new repository object that operates on the supplied Sail.

Parameters:
sail - A Sail object.










Method Details



getDataDir

public File getDataDir()
Description copied from interface: Repository
Get the directory where data and logging for this repository is stored.

Specified by:
getDataDir in interface Repository
Returns:
the directory where data for this repository is stored.






setDataDir

public void setDataDir(File dataDir)
Description copied from interface: Repository
Set the directory where data and logging for this repository is stored.

Specified by:
setDataDir in interface Repository
Parameters:
dataDir - the directory where data for this repository is stored






setFederatedServiceResolver

public void setFederatedServiceResolver(FederatedServiceResolver resolver)
Description copied from interface: FederatedServiceResolverClient
Sets the FederatedServiceResolver to use for this client.

Specified by:
setFederatedServiceResolver in interface FederatedServiceResolverClient
Parameters:
resolver - The resolver to use.






getFederatedServiceResolver

public FederatedServiceResolver getFederatedServiceResolver()
Description copied from interface: FederatedServiceResolverClient
Gets the FederatedServiceResolver used by this client.

Specified by:
getFederatedServiceResolver in interface FederatedServiceResolverClient






setRepositoryResolver

public void setRepositoryResolver(RepositoryResolver resolver)

Specified by:
setRepositoryResolver in interface RepositoryResolverClient






getHttpClientSessionManager

public HttpClientSessionManager getHttpClientSessionManager()
Description copied from interface: SessionManagerDependent
HttpClientSessionManager that has been assigned or has been used by this object. The life cycle might not
 be or might be tied to this object, depending on whether HttpClientSessionManager was passed to or
 created by this object respectively.

Specified by:
getHttpClientSessionManager in interface SessionManagerDependent
Returns:
a HttpClientSessionManager instance or null






setHttpClientSessionManager

public void setHttpClientSessionManager(HttpClientSessionManager client)
Description copied from interface: SessionManagerDependent
Assign an HttpClientSessionManager that this object should use. The life cycle of the given
 HttpClientSessionManager is independent of this object. Closing or shutting down this object does not
 have any impact on the given client. Callers must ensure that the given client is properly closed elsewhere.

Specified by:
setHttpClientSessionManager in interface SessionManagerDependent
Parameters:
client - 






getHttpClient

public org.apache.http.client.HttpClient getHttpClient()
Description copied from interface: HttpClientDependent
HttpClient that has been assigned or has been used by this object. The life cycle might not be or might
 be tied to this object, depending on whether HttpClient was passed to or created by this object
 respectively.

Specified by:
getHttpClient in interface HttpClientDependent
Returns:
an HttpClient instance or null






setHttpClient

public void setHttpClient(org.apache.http.client.HttpClient client)
Description copied from interface: HttpClientDependent
Assign an HttpClient that this object should use. The life cycle of the given HttpClient is
 independent of this object. Closing or shutting down this object does not have any impact on the given client.
 Callers must ensure that the given client is properly closed elsewhere.

Specified by:
setHttpClient in interface HttpClientDependent
Parameters:
client - 






initializeInternal

protected void initializeInternal()
                           throws RepositoryException

Specified by:
initializeInternal in class AbstractRepository
Throws:
RepositoryException






shutDownInternal

protected void shutDownInternal()
                         throws RepositoryException

Specified by:
shutDownInternal in class AbstractRepository
Throws:
RepositoryException






getSail

public Sail getSail()
Gets the Sail object that is on top of the Sail stack that this repository operates on.

Returns:
A Sail object.






isWritable

public boolean isWritable()
                   throws RepositoryException
Description copied from interface: Repository
Checks whether this repository is writable, i.e. if the data contained in this repository can be changed. The
 writability of the repository is determined by the writability of the Sail that this repository operates on.

Specified by:
isWritable in interface Repository
Throws:
RepositoryException






getValueFactory

public ValueFactory getValueFactory()
Description copied from interface: Repository
Gets a ValueFactory for this Repository.

Specified by:
getValueFactory in interface Repository
Returns:
A repository-specific ValueFactory.






getConnection

public SailRepositoryConnection getConnection()
                                       throws RepositoryException
Description copied from interface: Repository
Opens a connection to this repository that can be used for querying and updating the contents of the repository.
 Created connections need to be closed to make sure that any resources they keep hold of are released. The best
 way to do this is to use a try-with-resources block, as follows:

  try (RepositoryConnection conn = repository.getConnection()) {
        // perform operations on the connection
 }
 
 
 Note that RepositoryConnection is not guaranteed to be thread-safe! The recommended pattern for
 repository access in a multi-threaded application is to share the Repository object between threads, but have
 each thread create and use its own RepositoryConnections.

Specified by:
getConnection in interface Repository
Returns:
A connection that allows operations on this repository.
Throws:
RepositoryException - If something went wrong during the creation of the Connection.






toString

public String toString()

Overrides:
toString in class Object












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nThe Eclipse RDF4J Framework
    

  
  Eclipse RDF4J™ is an open source modular Java framework for working with RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of Linked Data and Semantic Web.
RDF4J offers two out-of-the-box RDF databases (the in-memory store and the native store), and in addition many third party storage solutions are available. The framework offers a large scala of tools to developers to leverage the power of RDF and related standards. RDF4J fully supports the SPARQL 1.1 query and update language for expressive querying and offers transparent access to remote RDF repositories using the exact same API as for local access. Finally, RDF4J supports all mainstream RDF file formats, including RDF/XML, Turtle, N-Triples,  N-Quads, JSON-LD, TriG and TriX.


Eclipse RDF4J Modular Architecture

RDF4J databases
The RDF4J core framework provides a set of vendor-neutral APIs for highly scalable storage, reasoning, and retrieval of RDF and OWL. Here, we list some available database solutions that implement the RDF4J APIs.
Core databases
RDF4J offers a set of database implementations out of the box.
The RDF4J Memory Store is a transactional RDF database using main memory with optional persistent sync to disk. It is fast with excellent performance for small  datasets. It scales with amount of RAM available.
The RDF4J Native Store is a transactional RDF database using direct disk IO for persistence. It is a more scalable solution than the memory store, with a smaller memory footprint, and also offers better consistency and durability. It is currently aimed at medium-sized datasets in the order of 100 million triples.
The RDF4J ElasticsearchStore is an experimental RDF database that uses Elasticsearch for storage.
This is useful if you are already using Elasticsearch for other things in your project and you want to add some small scale graph data.
A good usecase is if you need reference data or an ontology for your application. The built-in read cache makes it a good choice for data that updates infrequently,
though for most usecases the NativeStore will be considerably faster.
On top of these core databases, RDF4J offers a number of functional extensions. These extensions add functionality such as improved full-text search, RDFS inferencing, rule-based reasoning and validation using SHACL/SPIN, and geospatial querying support. For more information see the RDF4J documentation.
Third party database solutions
The core RDF4J databases are mainly intended for small to medium-sized datasets. However, RDF4J-compatible databases are developed by several third parties, both open-source/free and commercial, and they often offer better scalability or other extended features. Because these triplestores are compatible with the RDF4J APIs, you will be able to switch your project to a different database with a minimal amount of code changes. Here, we list a few options, in no particular order of preference.
Ontotext GraphDB
Ontotext GraphDB is a leading RDF triplestore built on OWL (Ontology Web Language) standards.  GraphDB handles massive loads, queries and OWL inferencing in real time. Ontotext offers GraphDB in several editions, including  GraphDB™ Free, GraphDB™ Standard and GraphDB™ Enterprise.
Ontotext are a long-term contributor to the RDF4J project.
Halyard
Halyard is an RDF4J-based horizontally scalable triplestore with full support for named graphs and SPARQL, implemented on top of Apache HBase.
Stardog
Stardog is a fast, lightweight, pure Java RDF store for mission-critical apps. It supports highly scalable storage and retrieval as well as OWL reasoning.
Amazon Neptune
Amazone Neptune is a fast, reliable, fully managed graph database service on Amazon Web Services (AWS) that makes it easy to build and run applications that work with highly connected datasets.
Systap Blazegraph™
Blazegraph (formerly known as Bigdata) is an enterprise graph database by Systap, LLC that provides a horizontally scaling storage and retrieval solution for very large volumes of RDF.
Oracle RDF Graph Adapter for RDF4J
Oracle RDF Graph Adapter for Eclipse RDF4J utilizes the popular Eclipse RDF4J framework to provide Java developers support to use the RDF Semantic Graph feature of Oracle Database.
MarkLogic RDF4J API
The MarkLogic RDF4J API is a full-featured, easy-to-use interface, that provides access to the MarkLogic triplestore via the RDF4J APIs. It offers several additional features such as permissions, and combination queries. More details can be found in the MarkLogic Developer documentation.
Strabon
Strabon is a spatiotemporal RDF store based on RDF4J. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions.
Openlink Virtuoso RDF4J Provider
The Openlink Virtuoso RDF4J Provider is a fully operational Native Graph Model Storage Provider for the Eclipse RDF4J Framework, allowing users of Virtuoso to leverage the Eclipse RDF4J framework to modify, query, and reason with the Virtuoso quad store using the Java language.
Related projects
Several projects extend or make use of RDF4J in some way, and provide additional functionality on top of the core RDF4J framework. Here, we offer a non-exhaustive list of such projects, both commercial and free/open-source.
metaphactory
metaphactory supports knowledge graph management, rapid application development, and end-user oriented interaction. metaphactory runs on top of your on-premise, cloud, or managed graph database and offers capabilities and features to support the entire lifecycle of dealing with knowledge graphs. It is a commercial platform with RDF4J at its core.
The metaphactory platform is developed by metaphacts GmbH, who are a significant contributor to the RDF4J project.
Neosemantics
Neosemantics is a plugin that enables the use of RDF in Neo4j. You can use it to import existing RDF datasets, build integrations with RDF generating endpoints or easily construct RDF endpoints on Neo4j, and more.
Other

Apache Marmotta
a Linked Data publication platform.
Carml
a library that transforms structured sources to RDF based and declared in an RML mapping.
KOMMA
a framework for the management and editing of RDF, RDFS and OWL. It provides Object-Triple-Mapping (comparable to JPA), an Editing framework, Eclipse RCP and RAP integration, on top of Eclipse RDF4J.
LinkedPipes
a standards compliant ETL and visualization suite for linked data.
RDF4J Schema Generator
a command line tool and maven plugin to generate vocabulary java classes from RDFS or OWL.
RML-Mapper
another RML mapping library.
Semantic Turkey
an RDF service backend for Knowledge Management, used by thesaurus management platform VocBench.
Sesame Tools
a collection of utility classes for use with Sesame/RDF4J.
Jelly
a high-performance binary RDF serialization format with an implementation for RDF4J. Can be used as a JAR plugin.



  

     
      
        
          

  Table of Contents

  
  
    RDF4J databases
      
        Core databases
        Third party database solutions
          
            Ontotext GraphDB
            Halyard
            Stardog
            Amazon Neptune
            Systap Blazegraph™
            Oracle RDF Graph Adapter for RDF4J
            MarkLogic RDF4J API
            Strabon
            Openlink Virtuoso RDF4J Provider
          
        
      
    
    Related projects
      
        metaphactory
        Neosemantics
        Other\n\n\n\nInterface RepositoryConnection



All Superinterfaces:
AutoCloseable


All Known Subinterfaces:
DelegatingRepositoryConnection, InterceptingRepositoryConnection, NotifyingRepositoryConnection


All Known Implementing Classes:
AbstractRepositoryConnection, CachingRepositoryConnection, ContextAwareConnection, DatasetRepositoryConnection, EndpointBase.ManagedRepositoryConnection, FedXRepositoryConnection, InterceptingRepositoryConnectionWrapper, LoggingRepositoryConnection, NotifyingRepositoryConnectionWrapper, PooledRepositoryConnection, RepositoryConnectionWrapper, SailRepositoryConnection, SPARQLConnection, TransactionalRepositoryConnection



public interface RepositoryConnection
extends AutoCloseable
Main interface for updating data in and performing queries on an RDF4J Repository.
 
 By default, a RepositoryConnection is in auto-commit mode, meaning that each operation corresponds to a single
 transaction on the underlying store. Multiple operations can be bundled in a single transaction by using
 begin() and commit/ rollback, which may improve performance
 considerably when dealing with many thousands of statements. Care should be taking to always properly close a
 RepositoryConnection after one is finished with it, to free up resources and avoid unnecessary locks.
 
 RepositoryConnection is not guaranteed to be thread-safe. The recommended access pattern in a multithreaded
 application is to ensure that each thread creates/uses its own RepositoryConnections (which can be obtained from a
 shared Repository).
 
 Several methods take a vararg argument that optionally specifies one or more contexts (named graphs) on which the
 method should operate. A vararg parameter is optional, it can be completely left out of the method call, in which
 case a method either operates on a provided statements context (if one of the method parameters is a statement or
 collection of statements), or operates on the repository as a whole, completely ignoring context. A vararg argument
 may also be 'null' (cast to Resource) meaning that the method operates on those statements which have no associated
 context only.
 
 Examples:

  
 // Ex 1: this method retrieves all statements that appear in either context1 or
 // context2, or both.
 RepositoryConnection.getStatements(null, null, null, true, context1, context2);

 // Ex 2: this method retrieves all statements that appear in the repository
 // (regardless of context).
 RepositoryConnection.getStatements(null, null, null, true);

 // Ex 3: this method retrieves all statements that have no associated context in
 // the repository.
 // Observe that this is not equivalent to the previous method call.
 RepositoryConnection.getStatements(null, null, null, true, (Resource) null);

 // Ex 4: this method adds a statement to the store. If the statement object
 // itself has a context (i.e. statement.getContext() != null) the statement is added
 // to that context. Otherwise, it is added without any associated context.
 RepositoryConnection.add(statement);

 // Ex 5: this method adds a statement to context1 in the store. It completely
 // ignores any context the statement itself has.
 RepositoryConnection.add(statement, context1);
 
 

Author:
Arjohn Kampman, Jeen Broekstra
See Also:


Repositories










Method Summary

All MethodsInstance MethodsAbstract MethodsDefault MethodsDeprecated Methods


Modifier and Type
Method
Description
void
add(File file,
 String baseURI,
 RDFFormat dataFormat,
 Resource... contexts)

Adds RDF data from the specified file to a specific contexts in the repository.

default void
add(File file,
 Resource... contexts)

Adds RDF data from the specified file to a specific contexts in the repository.

default void
add(File file,
 RDFFormat dataFormat,
 Resource... contexts)

Adds RDF data from the specified file to a specific contexts in the repository.

void
add(InputStream in,
 String baseURI,
 RDFFormat dataFormat,
 Resource... contexts)

Adds RDF data from an InputStream to the repository, optionally to one or more named contexts.

default void
add(InputStream in,
 RDFFormat dataFormat,
 Resource... contexts)

Adds RDF data from an InputStream to the repository, optionally to one or more named contexts.

void
add(Reader reader,
 String baseURI,
 RDFFormat dataFormat,
 Resource... contexts)

Adds RDF data from a Reader to the repository, optionally to one or more named contexts.

default void
add(Reader reader,
 RDFFormat dataFormat,
 Resource... contexts)

Adds RDF data from a Reader to the repository, optionally to one or more named contexts.

void
add(Iterable<? extends Statement> statements,
 Resource... contexts)

Adds the supplied statements to this repository, optionally to one or more named contexts.

void
add(URL url,
 String baseURI,
 RDFFormat dataFormat,
 Resource... contexts)

Adds the RDF data that can be found at the specified URL to the repository, optionally to one or more named
 contexts.

default void
add(URL url,
 Resource... contexts)

Adds the RDF data that can be found at the specified URL to the repository, optionally to one or more named
 contexts.

default void
add(URL url,
 RDFFormat dataFormat,
 Resource... contexts)

Adds the RDF data that can be found at the specified URL to the repository, optionally to one or more named
 contexts.

void
add(CloseableIteration<? extends Statement> statements,
 Resource... contexts)

Adds the supplied statements to this repository, optionally to one or more named contexts.

void
add(Resource subject,
 IRI predicate,
 Value object,
 Resource... contexts)

Adds a statement with the specified subject, predicate and object to this repository, optionally to one or more
 named contexts.

void
add(Statement st,
 Resource... contexts)

Adds the supplied statement to this repository, optionally to one or more named contexts.

default void
add(RepositoryResult<Statement> statements,
 Resource... contexts)

Adds the supplied statements to this repository, optionally to one or more named contexts.

void
begin()

Begins a new transaction, requiring commit() or rollback() to be called to end the transaction.

void
begin(IsolationLevel level)

Begins a new transaction with the supplied IsolationLevel, requiring commit() or
 rollback() to be called to end the transaction.

default void
begin(TransactionSetting... settings)

Begins a new transaction with the supplied TransactionSetting, requiring commit() or
 rollback() to be called to end the transaction.

void
clear(Resource... contexts)

Removes all statements from a specific contexts in the repository.

void
clearNamespaces()

Removes all namespace declarations from the repository.

default void
close()

Closes the connection, freeing resources.

void
commit()

Commits the active transaction.

void
export(RDFHandler handler,
 Resource... contexts)

Exports all explicit statements in the specified contexts to the supplied RDFHandler.

void
exportStatements(Resource subj,
 IRI pred,
 Value obj,
 boolean includeInferred,
 RDFHandler handler,
 Resource... contexts)

Exports all statements with a specific subject, predicate and/or object from the repository, optionally from the
 specified contexts.

RepositoryResult<Resource>
getContextIDs()

Gets all resources that are used as context identifiers.

IsolationLevel
getIsolationLevel()

Retrieves the current transaction isolation level of the connection.

String
getNamespace(String prefix)

Gets the namespace that is associated with the specified prefix, if any.

RepositoryResult<Namespace>
getNamespaces()

Gets all declared namespaces as a RepositoryResult of Namespace objects.

ParserConfig
getParserConfig()

Returns the parser configuration this connection uses for Rio-based operations.

Repository
getRepository()

Returns the Repository object to which this connection belongs.

RepositoryResult<Statement>
getStatements(Resource subj,
 IRI pred,
 Value obj,
 boolean includeInferred,
 Resource... contexts)

Gets all statements with a specific subject, predicate and/or object from the repository.

default RepositoryResult<Statement>
getStatements(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)

Gets all statements with a specific subject, predicate and/or object from the repository.

ValueFactory
getValueFactory()

Gets a ValueFactory for this RepositoryConnection.

boolean
hasStatement(Resource subj,
 IRI pred,
 Value obj,
 boolean includeInferred,
 Resource... contexts)

Checks whether the repository contains statements with a specific subject, predicate and/or object, optionally in
 the specified contexts.

boolean
hasStatement(Statement st,
 boolean includeInferred,
 Resource... contexts)

Checks whether the repository contains the specified statement, optionally in the specified contexts.

boolean
isActive()

Indicates if a transaction is currently active on the connection.

boolean
isAutoCommit()

Deprecated.
Use isActive() instead.


boolean
isEmpty()

Returns true if this repository does not contain any (explicit) statements.

boolean
isOpen()

Checks whether this connection is open.

default void
prepare()

Checks for an error state in the active transaction that would force the transaction to be rolled back.

default BooleanQuery
prepareBooleanQuery(String query)

Prepares SPARQL queries that return true or false, that is, SPARQL ASK queries.

BooleanQuery
prepareBooleanQuery(QueryLanguage ql,
 String query)

Prepares queries that return true or false.

BooleanQuery
prepareBooleanQuery(QueryLanguage ql,
 String query,
 String baseURI)

Prepares queries that return true or false.

default GraphQuery
prepareGraphQuery(String query)

Prepares SPARQL queries that produce RDF graphs, that is, SPARQL CONSTRUCT or DESCRIBE queries.

GraphQuery
prepareGraphQuery(QueryLanguage ql,
 String query)

Prepares queries that produce RDF graphs.

GraphQuery
prepareGraphQuery(QueryLanguage ql,
 String query,
 String baseURI)

Prepares queries that produce RDF graphs.

default Query
prepareQuery(String query)

Prepares a SPARQL query for evaluation on this repository (optional operation).

Query
prepareQuery(QueryLanguage ql,
 String query)

Prepares a query for evaluation on this repository (optional operation).

Query
prepareQuery(QueryLanguage ql,
 String query,
 String baseURI)

Prepares a query for evaluation on this repository (optional operation).

default TupleQuery
prepareTupleQuery(String query)

Prepares a SPARQL query that produces sets of value tuples, that is a SPARQL SELECT query.

TupleQuery
prepareTupleQuery(QueryLanguage ql,
 String query)

Prepares a query that produces sets of value tuples.

TupleQuery
prepareTupleQuery(QueryLanguage ql,
 String query,
 String baseURI)

Prepares a query that produces sets of value tuples.

default Update
prepareUpdate(String update)

Prepares a SPARQL Update operation.

Update
prepareUpdate(QueryLanguage ql,
 String update)

Prepares an Update operation.

Update
prepareUpdate(QueryLanguage ql,
 String update,
 String baseURI)

Prepares an Update operation.

void
remove(Iterable<? extends Statement> statements,
 Resource... contexts)

Removes the supplied statements from the specified contexts in this repository.

void
remove(CloseableIteration<? extends Statement> statements,
 Resource... contexts)

Removes the supplied statements from a specific context in this repository, ignoring any context information
 carried by the statements themselves.

void
remove(Resource subject,
 IRI predicate,
 Value object,
 Resource... contexts)

Removes the statement(s) with the specified subject, predicate and object from the repository, optionally
 restricted to the specified contexts.

void
remove(Statement st,
 Resource... contexts)

Removes the supplied statement from the specified contexts in the repository.

default void
remove(RepositoryResult<Statement> statements,
 Resource... contexts)

Removes the supplied statements from a specific context in this repository, ignoring any context information
 carried by the statements themselves.

void
removeNamespace(String prefix)

Removes a namespace declaration by removing the association between a prefix and a namespace name.

void
rollback()

Rolls back all updates in the active transaction.

void
setAutoCommit(boolean autoCommit)

Deprecated.
Use begin() instead.


void
setIsolationLevel(IsolationLevel level)

Sets the transaction isolation level for the next transaction(s) on this connection.

void
setNamespace(String prefix,
 String name)

Sets the prefix for a namespace.

void
setParserConfig(ParserConfig config)

Set the parser configuration this connection should use for RDFParser-based operations.

long
size(Resource... contexts)

Returns the number of (explicit) statements that are in the specified contexts in this repository.













Method Details



getRepository

Repository getRepository()
Returns the Repository object to which this connection belongs.





setParserConfig

void setParserConfig(ParserConfig config)
Set the parser configuration this connection should use for RDFParser-based operations.

Parameters:
config - a Rio RDF Parser configuration.






getParserConfig

ParserConfig getParserConfig()
Returns the parser configuration this connection uses for Rio-based operations.

Returns:
a Rio RDF parser configuration.






getValueFactory

ValueFactory getValueFactory()
Gets a ValueFactory for this RepositoryConnection.

Returns:
A repository-specific ValueFactory.






isOpen

boolean isOpen()
        throws RepositoryException
Checks whether this connection is open. A connection is open from the moment it is created until it is closed.

Throws:
RepositoryException
See Also:


close()








close

default void close()
            throws RepositoryException
Closes the connection, freeing resources. If a transaction is active on the
 connection, all non-committed operations will be lost by actively calling rollback() on any active
 transactions.
 
 Implementation note: All implementations must override this method if they have any resources that they need to
 free.

Specified by:
close in interface AutoCloseable
Throws:
RepositoryException - If the connection could not be closed.






prepareQuery

default Query prepareQuery(String query)
                    throws RepositoryException,
MalformedQueryException
Prepares a SPARQL query for evaluation on this repository (optional operation). In case the query contains
 relative URIs that need to be resolved against an external base URI, one should use
 prepareQuery(QueryLanguage, String, String) instead.
 
 If you already know the type of query, using the more specific prepareTupleQuery(java.lang.String),
 prepareGraphQuery(java.lang.String) or prepareBooleanQuery(java.lang.String) is likely to be more efficient.

Parameters:
query - The query string, in SPARQL syntax.
Returns:
A query ready to be evaluated on this repository.
Throws:
MalformedQueryException - If the supplied query is malformed.
UnsupportedOperationException - If the prepareQuery method is not supported by this repository.
RepositoryException
See Also:


prepareQuery(QueryLanguage, String)








prepareQuery

Query prepareQuery(QueryLanguage ql,
 String query)
            throws RepositoryException,
MalformedQueryException
Prepares a query for evaluation on this repository (optional operation). In case the query contains relative URIs
 that need to be resolved against an external base URI, one should use
 prepareQuery(QueryLanguage, String, String) instead.
 
 If you already know the type of query, using the more specific prepareTupleQuery(java.lang.String),
 prepareGraphQuery(java.lang.String) or prepareBooleanQuery(java.lang.String) is likely to be more efficient.

Parameters:
ql - The query language in which the query is formulated.
query - The query string.
Returns:
A query ready to be evaluated on this repository.
Throws:
MalformedQueryException - If the supplied query is malformed.
UnsupportedQueryLanguageException - If the supplied query language is not supported.
UnsupportedOperationException - If the prepareQuery method is not supported by this
                                           repository.
RepositoryException






prepareQuery

Query prepareQuery(QueryLanguage ql,
 String query,
 String baseURI)
            throws RepositoryException,
MalformedQueryException
Prepares a query for evaluation on this repository (optional operation).
 
 If you already know the type of query, using the more specific prepareTupleQuery(java.lang.String),
 prepareGraphQuery(java.lang.String) or prepareBooleanQuery(java.lang.String) is likely to be more efficient.

Parameters:
ql - The query language in which the query is formulated.
query - The query string.
baseURI - The base URI to resolve any relative URIs that are in the query against, can be null if
                the query does not contain any relative URIs.
Returns:
A query ready to be evaluated on this repository.
Throws:
MalformedQueryException - If the supplied query is malformed.
UnsupportedQueryLanguageException - If the supplied query language is not supported.
UnsupportedOperationException - If the prepareQuery method is not supported by this
                                           repository.
RepositoryException






prepareTupleQuery

default TupleQuery prepareTupleQuery(String query)
                              throws RepositoryException,
MalformedQueryException
Prepares a SPARQL query that produces sets of value tuples, that is a SPARQL SELECT query. In case the query
 contains relative URIs that need to be resolved against an external base URI, one should use
 prepareTupleQuery(QueryLanguage, String, String) instead.

Parameters:
query - The query string, in SPARQL syntax.
Returns:
a TupleQuery ready to be evaluated on this RepositoryConnection.
Throws:
IllegalArgumentException - If the supplied query is not a tuple query.
MalformedQueryException - If the supplied query is malformed.
RepositoryException
See Also:


prepareTupleQuery(QueryLanguage, String)








prepareTupleQuery

TupleQuery prepareTupleQuery(QueryLanguage ql,
 String query)
                      throws RepositoryException,
MalformedQueryException
Prepares a query that produces sets of value tuples. In case the query contains relative URIs that need to be
 resolved against an external base URI, one should use prepareTupleQuery(QueryLanguage, String, String)
 instead.

Parameters:
ql - The query language in which the query is formulated.
query - The query string.
Returns:
a TupleQuery ready to be evaluated on this RepositoryConnection.
Throws:
IllegalArgumentException - If the supplied query is not a tuple query.
MalformedQueryException - If the supplied query is malformed.
UnsupportedQueryLanguageException - If the supplied query language is not supported.
RepositoryException






prepareTupleQuery

TupleQuery prepareTupleQuery(QueryLanguage ql,
 String query,
 String baseURI)
                      throws RepositoryException,
MalformedQueryException
Prepares a query that produces sets of value tuples.

Parameters:
ql - The query language in which the query is formulated.
query - The query string.
baseURI - The base URI to resolve any relative URIs that are in the query against, can be null if
                the query does not contain any relative URIs.
Returns:
a TupleQuery ready to be evaluated on this RepositoryConnection.
Throws:
IllegalArgumentException - If the supplied query is not a tuple query.
MalformedQueryException - If the supplied query is malformed.
UnsupportedQueryLanguageException - If the supplied query language is not supported.
RepositoryException






prepareGraphQuery

default GraphQuery prepareGraphQuery(String query)
                              throws RepositoryException,
MalformedQueryException
Prepares SPARQL queries that produce RDF graphs, that is, SPARQL CONSTRUCT or DESCRIBE queries. In case the query
 contains relative URIs that need to be resolved against an external base URI, one should use
 prepareGraphQuery(QueryLanguage, String, String) instead.

Parameters:
query - The query string, in SPARQL syntax.
Returns:
a GraphQuery ready to be evaluated on this RepositoryConnection.
Throws:
IllegalArgumentException - If the supplied query is not a graph query.
MalformedQueryException - If the supplied query is malformed.
RepositoryException
See Also:


prepareGraphQuery(QueryLanguage, String)








prepareGraphQuery

GraphQuery prepareGraphQuery(QueryLanguage ql,
 String query)
                      throws RepositoryException,
MalformedQueryException
Prepares queries that produce RDF graphs. In case the query contains relative URIs that need to be resolved
 against an external base URI, one should use prepareGraphQuery(QueryLanguage, String, String) instead.

Parameters:
ql - The query language in which the query is formulated.
query - The query string.
Returns:
a GraphQuery ready to be evaluated on this RepositoryConnection.
Throws:
IllegalArgumentException - If the supplied query is not a graph query.
MalformedQueryException - If the supplied query is malformed.
UnsupportedQueryLanguageException - If the supplied query language is not supported.
RepositoryException






prepareGraphQuery

GraphQuery prepareGraphQuery(QueryLanguage ql,
 String query,
 String baseURI)
                      throws RepositoryException,
MalformedQueryException
Prepares queries that produce RDF graphs.

Parameters:
ql - The query language in which the query is formulated.
query - The query string.
baseURI - The base URI to resolve any relative URIs that are in the query against, can be null if
                the query does not contain any relative URIs.
Returns:
a GraphQuery ready to be evaluated on this RepositoryConnection.
Throws:
IllegalArgumentException - If the supplied query is not a graph query.
MalformedQueryException - If the supplied query is malformed.
UnsupportedQueryLanguageException - If the supplied query language is not supported.
RepositoryException






prepareBooleanQuery

default BooleanQuery prepareBooleanQuery(String query)
                                  throws RepositoryException,
MalformedQueryException
Prepares SPARQL queries that return true or false, that is, SPARQL ASK queries. In case the
 query contains relative URIs that need to be resolved against an external base URI, one should use
 prepareBooleanQuery(QueryLanguage, String, String) instead.

Parameters:
query - The query string, in SPARQL syntax.
Returns:
a BooleanQuery ready to be evaluated on this RepositoryConnection.
Throws:
IllegalArgumentException - If the supplied query is not a boolean query.
MalformedQueryException - If the supplied SPARQL query is malformed.
RepositoryException
See Also:


prepareBooleanQuery(QueryLanguage, String)








prepareBooleanQuery

BooleanQuery prepareBooleanQuery(QueryLanguage ql,
 String query)
                          throws RepositoryException,
MalformedQueryException
Prepares queries that return true or false. In case the query contains relative URIs that
 need to be resolved against an external base URI, one should use
 prepareBooleanQuery(QueryLanguage, String, String) instead.

Parameters:
ql - The query language in which the query is formulated.
query - The query string.
Returns:
a BooleanQuery ready to be evaluated on this RepositoryConnection.
Throws:
IllegalArgumentException - If the supplied query is not a boolean query.
MalformedQueryException - If the supplied query is malformed.
UnsupportedQueryLanguageException - If the supplied query language is not supported.
RepositoryException






prepareBooleanQuery

BooleanQuery prepareBooleanQuery(QueryLanguage ql,
 String query,
 String baseURI)
                          throws RepositoryException,
MalformedQueryException
Prepares queries that return true or false.

Parameters:
ql - The query language in which the query is formulated.
query - The query string.
baseURI - The base URI to resolve any relative URIs that are in the query against, can be null if
                the query does not contain any relative URIs.
Returns:
a BooleanQuery ready to be evaluated on this RepositoryConnection.
Throws:
IllegalArgumentException - If the supplied query is not a boolean query.
MalformedQueryException - If the supplied query is malformed.
UnsupportedQueryLanguageException - If the supplied query language is not supported.
RepositoryException






prepareUpdate

default Update prepareUpdate(String update)
                      throws RepositoryException,
MalformedQueryException
Prepares a SPARQL Update operation. In case the update string contains relative URIs that need to be resolved
 against an external base URI, one should use prepareUpdate(QueryLanguage, String, String) instead.

Parameters:
update - The update operation string, in SPARQL syntax.
Returns:
a Update ready to be executed on this RepositoryConnection.
Throws:
MalformedQueryException - If the supplied update operation string is malformed.
RepositoryException
See Also:


prepareUpdate(QueryLanguage, String)








prepareUpdate

Update prepareUpdate(QueryLanguage ql,
 String update)
              throws RepositoryException,
MalformedQueryException
Prepares an Update operation. In case the update string contains relative URIs that need to be resolved against
 an external base URI, one should use prepareUpdate(QueryLanguage, String, String) instead.

Parameters:
ql - The query language in which the update operation is formulated.
update - The update operation string.
Returns:
a Update ready to be executed on this RepositoryConnection.
Throws:
MalformedQueryException - If the supplied update operation string is malformed.
RepositoryException






prepareUpdate

Update prepareUpdate(QueryLanguage ql,
 String update,
 String baseURI)
              throws RepositoryException,
MalformedQueryException
Prepares an Update operation.

Parameters:
ql - The query language in which the update operation is formulated.
update - The update operation string.
baseURI - The base URI to resolve any relative URIs that are in the update against, can be null
                if the update does not contain any relative URIs.
Returns:
a Update ready to be executed on this RepositoryConnection.
Throws:
MalformedQueryException - If the supplied update operation string is malformed.
RepositoryException






getContextIDs

RepositoryResult<Resource> getContextIDs()
                                  throws RepositoryException
Gets all resources that are used as context identifiers. Care should be taken that the returned
 RepositoryResult is closed to free any resources that it keeps hold of.

Returns:
a RepositoryResult object containing Resources that are used as context identifiers.
Throws:
RepositoryException






getStatements

default RepositoryResult<Statement> getStatements(Resource subj,
 IRI pred,
 Value obj,
 Resource... contexts)
                                           throws RepositoryException
Gets all statements with a specific subject, predicate and/or object from the repository. The result is
 optionally restricted to the specified set of named contexts. If the repository supports inferencing, inferred
 statements will be included in the result.

Parameters:
subj - A Resource specifying the subject, or null for a wildcard.
pred - A URI specifying the predicate, or null for a wildcard.
obj - A Value specifying the object, or null for a wildcard.
contexts - The context(s) to get the data from. Note that this parameter is a vararg and as such is
                 optional. If no contexts are supplied the method operates on the entire repository.
Returns:
The statements matching the specified pattern. The result object is a RepositoryResult object, a
         lazy Iterator-like object containing Statements and optionally throwing a
         RepositoryException when an error when a problem occurs during retrieval.
Throws:
RepositoryException






getStatements

RepositoryResult<Statement> getStatements(Resource subj,
 IRI pred,
 Value obj,
 boolean includeInferred,
 Resource... contexts)
                                   throws RepositoryException
Gets all statements with a specific subject, predicate and/or object from the repository. The result is
 optionally restricted to the specified set of named contexts.

Parameters:
subj - A Resource specifying the subject, or null for a wildcard.
pred - An IRI specifying the predicate, or null for a wildcard.
obj - A Value specifying the object, or null for a wildcard.
includeInferred - if false, no inferred statements are returned; if true, inferred statements are returned
                        if available. The default is true.
contexts - The context(s) to get the data from. Note that this parameter is a vararg and as such is
                        optional. If no contexts are supplied the method operates on the entire repository.
Returns:
The statements matching the specified pattern. The result object is a RepositoryResult object, a
         lazy Iterator-like object containing Statements and optionally throwing a
         RepositoryException when an error when a problem occurs during retrieval.
Throws:
RepositoryException






hasStatement

boolean hasStatement(Resource subj,
 IRI pred,
 Value obj,
 boolean includeInferred,
 Resource... contexts)
              throws RepositoryException
Checks whether the repository contains statements with a specific subject, predicate and/or object, optionally in
 the specified contexts.

Parameters:
subj - A Resource specifying the subject, or null for a wildcard.
pred - An IRI specifying the predicate, or null for a wildcard.
obj - A Value specifying the object, or null for a wildcard.
includeInferred - if false, no inferred statements are considered; if true, inferred statements are
                        considered if available
contexts - The context(s) the need to be searched. Note that this parameter is a vararg and as such
                        is optional. If no contexts are supplied the method operates on the entire repository.
Returns:
true If a matching statement is in the repository in the specified context, false otherwise.
Throws:
RepositoryException






hasStatement

boolean hasStatement(Statement st,
 boolean includeInferred,
 Resource... contexts)
              throws RepositoryException
Checks whether the repository contains the specified statement, optionally in the specified contexts.

Parameters:
st - The statement to look for. Context information in the statement is ignored.
includeInferred - if false, no inferred statements are considered; if true, inferred statements are
                        considered if available
contexts - The context(s) to get the data from. Note that this parameter is a vararg and as such is
                        optional. If no contexts are supplied the method operates on the entire repository.
Returns:
true If the repository contains the specified statement, false otherwise.
Throws:
RepositoryException






exportStatements

void exportStatements(Resource subj,
 IRI pred,
 Value obj,
 boolean includeInferred,
 RDFHandler handler,
 Resource... contexts)
               throws RepositoryException,
RDFHandlerException
Exports all statements with a specific subject, predicate and/or object from the repository, optionally from the
 specified contexts. This method supplies the RDFHandler with all namespace declarations available in the
 repository.

Parameters:
subj - The subject, or null if the subject doesn't matter.
pred - The predicate, or null if the predicate doesn't matter.
obj - The object, or null if the object doesn't matter.
includeInferred - if false, no inferred statements are returned; if true, inferred statements are returned
                        if available
handler - The handler that will handle the RDF data.
contexts - The context(s) to get the data from. Note that this parameter is a vararg and as such is
                        optional. If no contexts are supplied the method operates on the entire repository.
Throws:
RDFHandlerException - If the handler encounters an unrecoverable error.
RepositoryException






export

void export(RDFHandler handler,
 Resource... contexts)
     throws RepositoryException,
RDFHandlerException
Exports all explicit statements in the specified contexts to the supplied RDFHandler. This method supplies the
 RDFHandler with all namespace declarations available in the repository.

Parameters:
handler - The handler that will handle the RDF data.
contexts - The context(s) to get the data from. Note that this parameter is a vararg and as such is
                 optional. If no contexts are supplied the method operates on the entire repository.
Throws:
RDFHandlerException - If the handler encounters an unrecoverable error.
RepositoryException






size

long size(Resource... contexts)
   throws RepositoryException
Returns the number of (explicit) statements that are in the specified contexts in this repository.

Parameters:
contexts - The context(s) to get the data from. Note that this parameter is a vararg and as such is
                 optional. If no contexts are supplied the method operates on the entire repository.
Returns:
The number of explicit statements from the specified contexts in this repository.
Throws:
RepositoryException






isEmpty

boolean isEmpty()
         throws RepositoryException
Returns true if this repository does not contain any (explicit) statements.

Returns:
true if this repository is empty, false otherwise.
Throws:
RepositoryException - If the repository could not be checked to be empty.






setAutoCommit

@Deprecated(since="2.7.0")
void setAutoCommit(boolean autoCommit)
            throws RepositoryException
Deprecated.
Use begin() instead.

Enables or disables auto-commit mode for the connection. If a connection is in auto-commit mode, then all updates
 will be executed and committed as individual transactions. Otherwise, the updates are grouped into transactions
 that are terminated by a call to either commit() or rollback(). By default, new connections are in
 auto-commit mode.
 
 NOTE: If this connection is switched to auto-commit mode during a transaction, the transaction is
 committed.

Throws:
RepositoryException - In case the mode switch failed, for example because a currently active transaction
                             failed to commit.
See Also:


commit()








isAutoCommit

@Deprecated(since="2.0")
boolean isAutoCommit()
              throws RepositoryException
Deprecated.
Use isActive() instead.

Indicates if the connection is in auto-commit mode. The connection is in auto-commit mode when no transaction is
 currently active, that is, when:
 
 begin() has not been called or;
 commit() or rollback() have been called to finish the transaction.
 

Throws:
RepositoryException - If a repository access error occurs.






isActive

boolean isActive()
          throws UnknownTransactionStateException,
RepositoryException
Indicates if a transaction is currently active on the connection. A transaction is active if begin() has
 been called, and becomes inactive after commit() or rollback() has been called.

Returns:
true iff a transaction is active, false iff no transaction is active.
Throws:
UnknownTransactionStateException - if the transaction state can not be determined. This can happen for
                                          instance when communication with a repository fails or times out.
RepositoryException






setIsolationLevel

void setIsolationLevel(IsolationLevel level)
                throws IllegalStateException
Sets the transaction isolation level for the next transaction(s) on this connection. If the level is set to a
 value that is not supported by the underlying repository, this method will still succeed but a subsequent call to
 begin() will result in an exception.

Parameters:
level - the transaction isolation level to set.
Throws:
IllegalStateException - if the method is called while a transaction is already active.






getIsolationLevel

IsolationLevel getIsolationLevel()
Retrieves the current transaction isolation level of the connection.

Returns:
the current transaction isolation level.






begin

void begin()
    throws RepositoryException
Begins a new transaction, requiring commit() or rollback() to be called to end the transaction.
 The transaction will use the currently set isolation level for this connection.

Throws:
RepositoryException - If the connection could not start the transaction. One possible reason this may
                             happen is if a transaction is already active on the current
                             connection.
See Also:


begin(IsolationLevel)
isActive()
commit()
rollback()
setIsolationLevel(IsolationLevel)








begin

void begin(IsolationLevel level)
    throws RepositoryException
Begins a new transaction with the supplied IsolationLevel, requiring commit() or
 rollback() to be called to end the transaction.

Parameters:
level - The IsolationLevel at which this transaction will operate. If set to null the
              default isolation level of the underlying store will be used. If the specified isolation level is
              not supported by the underlying store, it will attempt to use a supported
              compatible level instead.
Throws:
RepositoryException - If the connection could not start the transaction. Possible reasons this may happen
                             are:
                             
                             a transaction is already active on the current connection.
                             the specified IsolationLevel is not supported by the store, and no
                             compatible level could be found.
                             
See Also:


begin()
isActive()
commit()
rollback()
setIsolationLevel(IsolationLevel)








begin

default void begin(TransactionSetting... settings)
Begins a new transaction with the supplied TransactionSetting, requiring commit() or
 rollback() to be called to end the transaction.

Parameters:
settings - The TransactionSetting (zero or more) for this transaction. If an isolation level is
                 provided in the settings this will be used for the transaction. If none is provided then the
                 default will be used. Behaviour of this method is undefined if more than one isolation level is
                 provided. Behaviour of this method is undefined if one or more settings is null.
Throws:
RepositoryException - If the connection could not start the transaction. Possible reasons this may happen
                             are:
                             
                             a transaction is already active on the current connection.
                             the specified IsolationLevel is not supported by the store, and no
                             compatible level could be found.
                             
Since:
3.3.0
See Also:


begin()
isActive()
commit()
rollback()
setIsolationLevel(IsolationLevel)








prepare

default void prepare()
              throws RepositoryException
Checks for an error state in the active transaction that would force the transaction to be rolled back. This is
 an optional call; calling or not calling this method should have no effect on the outcome of commit() or
 rollback(). A call to this method must be followed by (in the same thread) with a call to
 prepare() , commit(), rollback(), or close() . This method may be called
 multiple times within the same transaction by the same thread. If this method returns normally, the caller can
 reasonably expect that a subsequent call to commit() will also return normally. If this method returns
 with an exception the caller should treat the exception as if it came from a call to commit().

Throws:
UnknownTransactionStateException - If the transaction state can not be determined (this can happen for
                                          instance when communication between client and server fails or
                                          times-out). It does not indicate a problem with the integrity of the
                                          store.
RepositoryException - If there is an active transaction and it cannot be committed.
IllegalStateException - If the connection has been closed or prepare was already called by
                                          another thread.
Since:
3.5.0
See Also:


commit()
begin()
rollback()








commit

void commit()
     throws RepositoryException
Commits the active transaction. This operation ends the active transaction.

Throws:
UnknownTransactionStateException - if the transaction state can not be determined. This can happen for
                                          instance when communication with a repository fails or times out.
RepositoryException - If the connection could not be committed, or if the connection does not
                                          have an active transaction.
See Also:


isActive()
begin()
rollback()
prepare()








rollback

void rollback()
       throws RepositoryException
Rolls back all updates in the active transaction. This operation ends the active transaction.

Throws:
UnknownTransactionStateException - if the transaction state can not be determined. This can happen for
                                          instance when communication with a repository fails or times out.
RepositoryException - If the transaction could not be rolled back, or if the connection does
                                          not have an active transaction.
See Also:


isActive()
begin()
commit()








add

default void add(InputStream in,
 RDFFormat dataFormat,
 Resource... contexts)
          throws IOException,
RDFParseException,
RepositoryException
Adds RDF data from an InputStream to the repository, optionally to one or more named contexts.

Parameters:
in - An InputStream from which RDF data can be read.
dataFormat - The serialization format of the data.
contexts - The contexts to add the data to. If one or more contexts are supplied the method ignores
                   contextual information in the actual data. If no contexts are supplied the contextual
                   information in the input stream is used, if no context information is available the data is
                   added without any context.
Throws:
IOException - If an I/O error occurred while reading from the input stream.
UnsupportedRDFormatException - If no parser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.
RepositoryException - If the data could not be added to the repository, for example because the
                                      repository is not writable.
Since:
3.5.0






add

void add(InputStream in,
 String baseURI,
 RDFFormat dataFormat,
 Resource... contexts)
  throws IOException,
RDFParseException,
RepositoryException
Adds RDF data from an InputStream to the repository, optionally to one or more named contexts.

Parameters:
in - An InputStream from which RDF data can be read.
baseURI - The base URI to resolve any relative URIs that are in the data against. May be
                   null.
                   
                   Note that if the data contains an embedded base URI, that embedded base URI will overrule the
                   value supplied here (see RFC 3986 section
                   5.1 for details).
dataFormat - The serialization format of the data.
contexts - The contexts to add the data to. If one or more contexts are supplied the method ignores
                   contextual information in the actual data. If no contexts are supplied the contextual
                   information in the input stream is used, if no context information is available the data is
                   added without any context.
Throws:
IOException - If an I/O error occurred while reading from the input stream.
UnsupportedRDFormatException - If no parser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.
RepositoryException - If the data could not be added to the repository, for example because the
                                      repository is not writable.






add

default void add(Reader reader,
 RDFFormat dataFormat,
 Resource... contexts)
          throws IOException,
RDFParseException,
RepositoryException
Adds RDF data from a Reader to the repository, optionally to one or more named contexts. Note: using a Reader
 to upload byte-based data means that you have to be careful not to destroy the data's character encoding by
 enforcing a default character encoding upon the bytes. If possible, adding such data using an InputStream is to
 be preferred.

Parameters:
reader - A Reader from which RDF data can be read.
dataFormat - The serialization format of the data.
contexts - The contexts to add the data to. If one or more contexts are specified the data is added to
                   these contexts, ignoring any context information in the data itself.
Throws:
IOException - If an I/O error occurred while reading from the reader.
UnsupportedRDFormatException - If no parser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.
RepositoryException - If the data could not be added to the repository, for example because the
                                      repository is not writable.
Since:
3.5.0






add

void add(Reader reader,
 String baseURI,
 RDFFormat dataFormat,
 Resource... contexts)
  throws IOException,
RDFParseException,
RepositoryException
Adds RDF data from a Reader to the repository, optionally to one or more named contexts. Note: using a Reader
 to upload byte-based data means that you have to be careful not to destroy the data's character encoding by
 enforcing a default character encoding upon the bytes. If possible, adding such data using an InputStream is to
 be preferred.

Parameters:
reader - A Reader from which RDF data can be read.
baseURI - The base URI to resolve any relative URIs that are in the data against. May be
                   null.
                   
                   Note that if the data contains an embedded base URI, that embedded base URI will overrule the
                   value supplied here (see RFC 3986 section
                   5.1 for details).
dataFormat - The serialization format of the data.
contexts - The contexts to add the data to. If one or more contexts are specified the data is added to
                   these contexts, ignoring any context information in the data itself.
Throws:
IOException - If an I/O error occurred while reading from the reader.
UnsupportedRDFormatException - If no parser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.
RepositoryException - If the data could not be added to the repository, for example because the
                                      repository is not writable.






add

default void add(URL url,
 Resource... contexts)
          throws IOException,
RDFParseException,
RepositoryException
Adds the RDF data that can be found at the specified URL to the repository, optionally to one or more named
 contexts.

Parameters:
url - The URL of the RDF data.
contexts - The contexts to add the data to. If one or more contexts are specified the data is added to these
                 contexts, ignoring any context information in the data itself.
Throws:
IOException - If an I/O error occurred while reading from the URL.
UnsupportedRDFormatException - If the RDF format could not be recognized.
RDFParseException - If an error was found while parsing the RDF data.
RepositoryException - If the data could not be added to the repository, for example because the
                                      repository is not writable.
Since:
3.5.0






add

default void add(URL url,
 RDFFormat dataFormat,
 Resource... contexts)
          throws IOException,
RDFParseException,
RepositoryException
Adds the RDF data that can be found at the specified URL to the repository, optionally to one or more named
 contexts.

Parameters:
url - The URL of the RDF data.
dataFormat - The serialization format of the data. If set to null, the format will be
                   automatically determined by examining the content type in the HTTP response header, and failing
                   that, the file name extension of the supplied URL.
contexts - The contexts to add the data to. If one or more contexts are specified the data is added to
                   these contexts, ignoring any context information in the data itself.
Throws:
IOException - If an I/O error occurred while reading from the URL.
UnsupportedRDFormatException - If no parser is available for the specified RDF format, or the RDF format
                                      could not be automatically determined.
RDFParseException - If an error was found while parsing the RDF data.
RepositoryException - If the data could not be added to the repository, for example because the
                                      repository is not writable.
Since:
3.5.0






add

void add(URL url,
 String baseURI,
 RDFFormat dataFormat,
 Resource... contexts)
  throws IOException,
RDFParseException,
RepositoryException
Adds the RDF data that can be found at the specified URL to the repository, optionally to one or more named
 contexts.

Parameters:
url - The URL of the RDF data.
baseURI - The base URI to resolve any relative URIs that are in the data against. This defaults to the
                   value of url.toExternalForm() if the value is set to
                   null.
                   
                   Note that if the data contains an embedded base URI, that embedded base URI will overrule the
                   value supplied here (see RFC 3986 section
                   5.1 for details).
dataFormat - The serialization format of the data. If set to null, the format will be
                   automatically determined by examining the content type in the HTTP response header, and failing
                   that, the file name extension of the supplied URL.
contexts - The contexts to add the data to. If one or more contexts are specified the data is added to
                   these contexts, ignoring any context information in the data itself.
Throws:
IOException - If an I/O error occurred while reading from the URL.
UnsupportedRDFormatException - If no parser is available for the specified RDF format, or the RDF format
                                      could not be automatically determined.
RDFParseException - If an error was found while parsing the RDF data.
RepositoryException - If the data could not be added to the repository, for example because the
                                      repository is not writable.






add

default void add(File file,
 Resource... contexts)
          throws IOException,
RDFParseException,
RepositoryException
Adds RDF data from the specified file to a specific contexts in the repository.

Parameters:
file - A file containing RDF data.
contexts - The contexts to add the data to. Note that this parameter is a vararg and as such is optional. If
                 no contexts are specified, the data is added to any context specified in the actual data file, or
                 if the data contains no context, it is added without context. If one or more contexts are
                 specified the data is added to these contexts, ignoring any context information in the data
                 itself.
Throws:
IOException - If an I/O error occurred while reading from the file.
UnsupportedRDFormatException - If the RDF format of the supplied file could not be recognized.
RDFParseException - If an error was found while parsing the RDF data.
RepositoryException - If the data could not be added to the repository, for example because the
                                      repository is not writable.
Since:
3.5.0






add

default void add(File file,
 RDFFormat dataFormat,
 Resource... contexts)
          throws IOException,
RDFParseException,
RepositoryException
Adds RDF data from the specified file to a specific contexts in the repository.

Parameters:
file - A file containing RDF data.
dataFormat - The serialization format of the data. If set to null, the format will be
                   automatically determined by examining the file name extension of the supplied File.
contexts - The contexts to add the data to. Note that this parameter is a vararg and as such is optional.
                   If no contexts are specified, the data is added to any context specified in the actual data
                   file, or if the data contains no context, it is added without context. If one or more contexts
                   are specified the data is added to these contexts, ignoring any context information in the data
                   itself.
Throws:
IOException - If an I/O error occurred while reading from the file.
UnsupportedRDFormatException - If no parser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.
RepositoryException - If the data could not be added to the repository, for example because the
                                      repository is not writable.
Since:
3.5.0






add

void add(File file,
 String baseURI,
 RDFFormat dataFormat,
 Resource... contexts)
  throws IOException,
RDFParseException,
RepositoryException
Adds RDF data from the specified file to a specific contexts in the repository.

Parameters:
file - A file containing RDF data.
baseURI - The base URI to resolve any relative URIs that are in the data against. This defaults to the
                   value of file.toURI() if the value is set to null.
                   
                   Note that if the data contains an embedded base URI, that embedded base URI will overrule the
                   value supplied here (see RFC 3986 section
                   5.1 for details).
dataFormat - The serialization format of the data. If set to null, the format will be
                   automatically determined by examining the file name extension of the supplied File.
contexts - The contexts to add the data to. Note that this parameter is a vararg and as such is optional.
                   If no contexts are specified, the data is added to any context specified in the actual data
                   file, or if the data contains no context, it is added without context. If one or more contexts
                   are specified the data is added to these contexts, ignoring any context information in the data
                   itself.
Throws:
IOException - If an I/O error occurred while reading from the file.
UnsupportedRDFormatException - If no parser is available for the specified RDF format.
RDFParseException - If an error was found while parsing the RDF data.
RepositoryException - If the data could not be added to the repository, for example because the
                                      repository is not writable.






add

void add(Resource subject,
 IRI predicate,
 Value object,
 Resource... contexts)
  throws RepositoryException
Adds a statement with the specified subject, predicate and object to this repository, optionally to one or more
 named contexts.

Parameters:
subject - The statement's subject.
predicate - The statement's predicate.
object - The statement's object.
contexts - The contexts to add the data to. Note that this parameter is a vararg and as such is optional.
                  If no contexts are specified, the data is added to any context specified in the actual data
                  file, or if the data contains no context, it is added without context. If one or more contexts
                  are specified the data is added to these contexts, ignoring any context information in the data
                  itself.
Throws:
RepositoryException - If the data could not be added to the repository, for example because the repository
                             is not writable.






add

void add(Statement st,
 Resource... contexts)
  throws RepositoryException
Adds the supplied statement to this repository, optionally to one or more named contexts.

Parameters:
st - The statement to add.
contexts - The contexts to add the statements to. Note that this parameter is a vararg and as such is
                 optional. If no contexts are specified, the statement is added to any context specified in each
                 statement, or if the statement contains no context, it is added without context. If one or more
                 contexts are specified the statement is added to these contexts, ignoring any context information
                 in the statement itself.
Throws:
RepositoryException - If the statement could not be added to the repository, for example because the
                             repository is not writable.






add

void add(Iterable<? extends Statement> statements,
 Resource... contexts)
  throws RepositoryException
Adds the supplied statements to this repository, optionally to one or more named contexts.

Parameters:
statements - The statements that should be added. In case the iterable is
                   NamespaceAware and the target repository supports it, the
                   iterable's namespaces are also added to the repository, without overwriting existing ones.
contexts - The contexts to add the statements to. Note that this parameter is a vararg and as such is
                   optional. If no contexts are specified, each statement is added to any context specified in the
                   statement, or if the statement contains no context, it is added without context. If one or more
                   contexts are specified each statement is added to these contexts, ignoring any context
                   information in the statement itself. ignored.
Throws:
RepositoryException - If the statements could not be added to the repository, for example because the
                             repository is not writable.






add

void add(CloseableIteration<? extends Statement> statements,
 Resource... contexts)
  throws RepositoryException
Adds the supplied statements to this repository, optionally to one or more named contexts.

Parameters:
statements - The statements to add. The iteration will be closed.
contexts - The contexts to add the statements to. Note that this parameter is a vararg and as such is
                   optional. If no contexts are specified, each statement is added to any context specified in the
                   statement, or if the statement contains no context, it is added without context. If one or more
                   contexts are specified each statement is added to these contexts, ignoring any context
                   information in the statement itself. ignored.
Throws:
RepositoryException - If the statements could not be added to the repository, for example because the
                             repository is not writable.






add

default void add(RepositoryResult<Statement> statements,
 Resource... contexts)
          throws RepositoryException
Adds the supplied statements to this repository, optionally to one or more named contexts.

Parameters:
statements - The statements to add. The @{link RepositoryResult} will be closed before this method returns.
contexts - The contexts to add the statements to. Note that this parameter is a vararg and as such is
                   optional. If no contexts are specified, each statement is added to any context specified in the
                   statement, or if the statement contains no context, it is added without context. If one or more
                   contexts are specified each statement is added to these contexts, ignoring any context
                   information in the statement itself. ignored.
Throws:
RepositoryException - If the statements could not be added to the repository, for example because the
                             repository is not writable.






remove

void remove(Resource subject,
 IRI predicate,
 Value object,
 Resource... contexts)
     throws RepositoryException
Removes the statement(s) with the specified subject, predicate and object from the repository, optionally
 restricted to the specified contexts.

Parameters:
subject - The statement's subject, or null for a wildcard.
predicate - The statement's predicate, or null for a wildcard.
object - The statement's object, or null for a wildcard.
contexts - The context(s) to remove the data from. Note that this parameter is a vararg and as such is
                  optional. If no contexts are supplied the method operates on the entire repository.
Throws:
RepositoryException - If the statement(s) could not be removed from the repository, for example because the
                             repository is not writable.






remove

void remove(Statement st,
 Resource... contexts)
     throws RepositoryException
Removes the supplied statement from the specified contexts in the repository.

Parameters:
st - The statement to remove.
contexts - The context(s) to remove the data from. Note that this parameter is a vararg and as such is
                 optional. If no contexts are supplied the method operates on the contexts associated with the
                 statement itself, and if no context is associated with the statement, on the entire repository.
Throws:
RepositoryException - If the statement could not be removed from the repository, for example because the
                             repository is not writable.






remove

void remove(Iterable<? extends Statement> statements,
 Resource... contexts)
     throws RepositoryException
Removes the supplied statements from the specified contexts in this repository.

Parameters:
statements - The statements that should be added.
contexts - The context(s) to remove the data from. Note that this parameter is a vararg and as such is
                   optional. If no contexts are supplied the method operates on the contexts associated with the
                   statement itself, and if no context is associated with the statement, on the entire repository.
Throws:
RepositoryException - If the statements could not be added to the repository, for example because the
                             repository is not writable.






remove

void remove(CloseableIteration<? extends Statement> statements,
 Resource... contexts)
     throws RepositoryException
Removes the supplied statements from a specific context in this repository, ignoring any context information
 carried by the statements themselves.

Parameters:
statements - The statements to remove. The iteration will be closed.
contexts - The context(s) to remove the data from. Note that this parameter is a vararg and as such is
                   optional. If no contexts are supplied the method operates on the contexts associated with the
                   statement itself, and if no context is associated with the statement, on the entire repository.
Throws:
RepositoryException - If the statements could not be removed from the repository, for example because the
                             repository is not writable.






remove

default void remove(RepositoryResult<Statement> statements,
 Resource... contexts)
             throws RepositoryException
Removes the supplied statements from a specific context in this repository, ignoring any context information
 carried by the statements themselves.

Parameters:
statements - The statements to remove. The RepositoryResult will be closed before this method
                   returns.
contexts - The context(s) to remove the data from. Note that this parameter is a vararg and as such is
                   optional. If no contexts are supplied the method operates on the contexts associated with the
                   statement itself, and if no context is associated with the statement, on the entire repository.
Throws:
RepositoryException - If the statements could not be removed from the repository, for example because the
                             repository is not writable.






clear

void clear(Resource... contexts)
    throws RepositoryException
Removes all statements from a specific contexts in the repository.

Parameters:
contexts - The context(s) to remove the data from. Note that this parameter is a vararg and as such is
                 optional. If no contexts are supplied the method operates on the entire repository.
Throws:
RepositoryException - If the statements could not be removed from the repository, for example because the
                             repository is not writable.






getNamespaces

RepositoryResult<Namespace> getNamespaces()
                                   throws RepositoryException
Gets all declared namespaces as a RepositoryResult of Namespace objects. Each Namespace object consists
 of a prefix and a namespace name.

Returns:
A RepositoryResult containing Namespace objects. Care should be taken to close the RepositoryResult after
         use.
Throws:
RepositoryException - If the namespaces could not be read from the repository.






getNamespace

String getNamespace(String prefix)
             throws RepositoryException
Gets the namespace that is associated with the specified prefix, if any.

Parameters:
prefix - A namespace prefix, or an empty string in case of the default namespace.
Returns:
The namespace name that is associated with the specified prefix, or null if there is no such
         namespace.
Throws:
RepositoryException - If the namespace could not be read from the repository.
NullPointerException - In case prefix is null.






setNamespace

void setNamespace(String prefix,
 String name)
           throws RepositoryException
Sets the prefix for a namespace.

Parameters:
prefix - The new prefix, or an empty string in case of the default namespace.
name - The namespace name that the prefix maps to.
Throws:
RepositoryException - If the namespace could not be set in the repository, for example because the
                              repository is not writable.
NullPointerException - In case prefix or name is null.






removeNamespace

void removeNamespace(String prefix)
              throws RepositoryException
Removes a namespace declaration by removing the association between a prefix and a namespace name.

Parameters:
prefix - The namespace prefix, or an empty string in case of the default namespace.
Throws:
RepositoryException - If the namespace prefix could not be removed.
NullPointerException - In case prefix is null.






clearNamespaces

void clearNamespaces()
              throws RepositoryException
Removes all namespace declarations from the repository.

Throws:
RepositoryException - If the namespace declarations could not be removed.












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface Query



All Superinterfaces:
Operation


All Known Subinterfaces:
BooleanQuery, GraphQuery, TupleQuery


All Known Implementing Classes:
AbstractHTTPQuery, AbstractParserQuery, AbstractParserQuery, AbstractQuery, DelegatingGraphQuery, DelegatingTupleQuery, FedXBooleanQuery, FedXGraphQuery, FedXTupleQuery, HTTPBooleanQuery, HTTPGraphQuery, HTTPTupleQuery, LoggingGraphQuery, LoggingTupleQuery, ResultCachingGraphQuery, ResultCachingTupleQuery, SailBooleanQuery, SailGraphQuery, SailQuery, SailTupleQuery, SPARQLBooleanQuery, SPARQLGraphQuery, SPARQLTupleQuery



public interface Query
extends Operation
A query on a repository that can be formulated in one of the supported query languages (for example SPARQL). It
 allows one to predefine bindings in the query to be able to reuse the same query with different bindings.

Author:
Arjohn Kampman, jeen








Nested Class Summary
Nested Classes

Modifier and Type
Interface
Description
static enum 
Query.QueryType

The different types of queries that RDF4J recognizes: boolean queries, graph queries, and tuple queries.







Method Summary

All MethodsInstance MethodsAbstract MethodsDefault MethodsDeprecated Methods


Modifier and Type
Method
Description
default Explanation
explain(Explanation.Level level)


 Explain how the query will be (or has been) executed/evaluated by returning an explanation of the query plan.

int
getMaxQueryTime()

Deprecated.
Use Operation.getMaxExecutionTime() instead.


void
setMaxQueryTime(int maxQueryTime)

Deprecated.
Use Operation.setMaxExecutionTime(int) instead.






Methods inherited from interface org.eclipse.rdf4j.query.Operation
clearBindings, getBindings, getDataset, getIncludeInferred, getMaxExecutionTime, removeBinding, setBinding, setDataset, setIncludeInferred, setMaxExecutionTime









Method Details



setMaxQueryTime

@Deprecated(since="2.0")
void setMaxQueryTime(int maxQueryTime)
Deprecated.
Use Operation.setMaxExecutionTime(int) instead.

Specifies the maximum time that a query is allowed to run. The query will be interrupted when it exceeds the time
 limit. Any consecutive requests to fetch query results will result in QueryInterruptedExceptions.

Parameters:
maxQueryTime - The maximum query time, measured in seconds. A negative or zero value indicates an unlimited
                     query time (which is the default).






getMaxQueryTime

@Deprecated(since="2.0")
int getMaxQueryTime()
Deprecated.
Use Operation.getMaxExecutionTime() instead.

Returns the maximum query evaluation time.

Returns:
The maximum query evaluation time, measured in seconds.
See Also:


setMaxQueryTime(int)








explain

@Experimental
default Explanation explain(Explanation.Level level)

 Explain how the query will be (or has been) executed/evaluated by returning an explanation of the query plan.
 

 
 This method is useful for understanding why a particular query is slow. The most useful level is Executed, but
 this takes as long as it takes to execute/evaluate the query.
 

 
 When timing a query you should keep in mind that the query performance will vary based on how much the JIT
 compiler has compiled the code (C1 vs C2) and based on what is or isn't cached in memory. If Timed explanations
 are considerably slower than Executed explanations the overhead with timing the query may be large on your system
 and should not be trusted.
 

 
 WARNING: This method is experimental and is subject to change or removal without warning. Same goes for the
 returned explanation. There is currently only partial support for this method in RDF4J and and
 UnsupportedOperationException where support is lacking.
 

Parameters:
level - The explanation level that should be used to create the explanation. Choose between: Unoptimized (as
              parsed without optimizations) , Optimized (as is actually going to be used), Executed (as was
              executed/evaluated, including some real performance metrics), Timed (as was executed/evaluated
              including all real performance metrics). Executed and Timed level can potentially be slow.
Returns:
The explanation that we generated, which can be viewed in a human readable format with toString(), as
         JSON or as a simplified query plan object structure.












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface TupleQuery



All Superinterfaces:
Operation, Query


All Known Implementing Classes:
DelegatingTupleQuery, FedXTupleQuery, HTTPTupleQuery, LoggingTupleQuery, ResultCachingTupleQuery, SailTupleQuery, SPARQLTupleQuery



public interface TupleQuery
extends Query







Nested Class Summary

Nested classes/interfaces inherited from interface org.eclipse.rdf4j.query.Query
Query.QueryType





Method Summary

All MethodsInstance MethodsAbstract Methods


Modifier and Type
Method
Description
TupleQueryResult
evaluate()
 
void
evaluate(TupleQueryResultHandler handler)
 




Methods inherited from interface org.eclipse.rdf4j.query.Operation
clearBindings, getBindings, getDataset, getIncludeInferred, getMaxExecutionTime, removeBinding, setBinding, setDataset, setIncludeInferred, setMaxExecutionTime

Methods inherited from interface org.eclipse.rdf4j.query.Query
explain, getMaxQueryTime, setMaxQueryTime









Method Details



evaluate

TupleQueryResult evaluate()
                   throws QueryEvaluationException

Throws:
QueryEvaluationException






evaluate

void evaluate(TupleQueryResultHandler handler)
       throws QueryEvaluationException,
TupleQueryResultHandlerException

Throws:
QueryEvaluationException
TupleQueryResultHandlerException












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface TupleQueryResult



All Superinterfaces:
AutoCloseable, CloseableIteration<BindingSet>, Iterable<BindingSet>, Iterator<BindingSet>, QueryResult<BindingSet>


All Known Implementing Classes:
BackgroundTupleResult, CachedTupleQueryResult, CleanerTupleQueryResult, IteratingTupleQueryResult, MutableTupleQueryResult, ReusableTupleQueryResult



public interface TupleQueryResult
extends QueryResult<BindingSet>
A representation of a variable-binding query result as a sequence of BindingSet objects. Each query result
 consists of zero or more solutions, each of which represents a single query solution as a set of bindings. Note: take
 care to always close a TupleQueryResult after use to free any resources it keeps hold of.

Author:
jeen








Method Summary

All MethodsInstance MethodsAbstract Methods


Modifier and Type
Method
Description
List<String>
getBindingNames()

Gets the names of the bindings, in order of projection.





Methods inherited from interface org.eclipse.rdf4j.common.iteration.CloseableIteration
close

Methods inherited from interface java.lang.Iterable
forEach, spliterator

Methods inherited from interface java.util.Iterator
forEachRemaining, remove

Methods inherited from interface org.eclipse.rdf4j.query.QueryResult
hasNext, iterator, next, stream









Method Details



getBindingNames

List<String> getBindingNames()
                      throws QueryEvaluationException
Gets the names of the bindings, in order of projection.

Returns:
The binding names, in order of projection.
Throws:
QueryEvaluationException












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface BindingSet



All Superinterfaces:
Iterable<Binding>, Serializable


All Known Subinterfaces:
MutableBindingSet


All Known Implementing Classes:
AbstractBindingSet, ArrayBindingSet, EmptyBindingSet, FedXPathIteration.ValuePair, ListBindingSet, MapBindingSet, PathIteration.ValuePair, QueryBindingSet, SimpleBindingSet, SingletonBindingSet, SPARQLQueryBindingSet



public interface BindingSet
extends Iterable<Binding>, Serializable
A BindingSet is a set of named value bindings, which is used a.o. to represent a single query solution. Values are
 indexed by name of the binding which typically corresponds to the names of the variables used in the projection of
 the orginal query.







Method Summary

All MethodsInstance MethodsAbstract MethodsDefault Methods


Modifier and Type
Method
Description
boolean
equals(Object o)

Compares a BindingSet object to another object.

Binding
getBinding(String bindingName)

Gets the binding with the specified name from this BindingSet.

Set<String>
getBindingNames()

Gets the names of the bindings in this BindingSet.

Value
getValue(String bindingName)

Gets the value of the binding with the specified name from this BindingSet.

boolean
hasBinding(String bindingName)

Checks whether this BindingSet has a binding with the specified name.

int
hashCode()

The hash code of a binding is defined as the bit-wise XOR of the hash codes of its bindings:

default boolean
isEmpty()
 
Iterator<Binding>
iterator()

Creates an iterator over the bindings in this BindingSet.

int
size()

Returns the number of bindings in this BindingSet.





Methods inherited from interface java.lang.Iterable
forEach, spliterator









Method Details



iterator

Iterator<Binding> iterator()
Creates an iterator over the bindings in this BindingSet. This only returns bindings with non-null values. An
 implementation is free to return the bindings in arbitrary order.

Specified by:
iterator in interface Iterable<Binding>






getBindingNames

Set<String> getBindingNames()
Gets the names of the bindings in this BindingSet.

Returns:
A set of binding names.






getBinding

Binding getBinding(String bindingName)
Gets the binding with the specified name from this BindingSet.

Parameters:
bindingName - The name of the binding.
Returns:
The binding with the specified name, or null if there is no such binding in this BindingSet.






hasBinding

boolean hasBinding(String bindingName)
Checks whether this BindingSet has a binding with the specified name.

Parameters:
bindingName - The name of the binding.
Returns:
true if this BindingSet has a binding with the specified name, false otherwise.






getValue

Value getValue(String bindingName)
Gets the value of the binding with the specified name from this BindingSet.

Parameters:
bindingName - The name of the binding.
Returns:
The value of the binding with the specified name, or null if there is no such binding in this
         BindingSet.






size

int size()
Returns the number of bindings in this BindingSet.

Returns:
The number of bindings in this BindingSet.






equals

boolean equals(Object o)
Compares a BindingSet object to another object.

Overrides:
equals in class Object
Parameters:
o - The object to compare this binding to.
Returns:
true if the other object is an instance of BindingSet and it contains the same set of
         bindings (disregarding order), false otherwise.






hashCode

int hashCode()
The hash code of a binding is defined as the bit-wise XOR of the hash codes of its bindings:

  int hashCode = 0;

 for (Binding binding : this) {
        hashCode ˆ= binding.getName().hashCode() ˆ binding.getValue().hashCode();
 }
 
 
 Note: the calculated hash code intentionally does not depend on the order in which the bindings are iterated
 over.

Overrides:
hashCode in class Object
Returns:
A hash code for the BindingSet.






isEmpty

default boolean isEmpty()











Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nClass QueryResults

java.lang.Object
org.eclipse.rdf4j.common.iteration.Iterations
org.eclipse.rdf4j.query.QueryResults





public class QueryResults
extends Iterations
Utility methods related to query results.

Author:
Jeen Broekstra








Constructor Summary
Constructors

Constructor
Description
QueryResults()
 






Method Summary

All MethodsStatic MethodsConcrete Methods


Modifier and Type
Method
Description
static <T> List<T>
asList(QueryResult<T> queryResult)

Get a List containing all elements obtained from the specified QueryResult.

static Model
asModel(CloseableIteration<? extends Statement> iteration)

Get a Model containing all elements obtained from the specified query result.

static Model
asModel(CloseableIteration<? extends Statement> iteration,
 ModelFactory modelFactory)

Get a Model containing all elements obtained from the specified query result.

static <T> Set<T>
asSet(QueryResult<T> queryResult)

Get a Set containing all elements obtained from the specified QueryResult.

static boolean
bindingSetsCompatible(BindingSet bs1,
 BindingSet bs2)

Check whether two BindingSets are compatible.

static GraphQueryResult
distinctResults(GraphQueryResult queryResult)

Returns a GraphQueryResult that filters out any duplicate solutions from the supplied queryResult.

static TupleQueryResult
distinctResults(TupleQueryResult queryResult)

Returns a TupleQueryResult that filters out any duplicate solutions from the supplied queryResult.

static boolean
equals(GraphQueryResult result1,
 GraphQueryResult result2)

Compares two graph query results and returns true if they are equal.

static boolean
equals(TupleQueryResult tqr1,
 TupleQueryResult tqr2)

Compares two tuple query results and returns true if they are equal.Tuple query results are equal if they
 contain the same set of BindingSets and have the same headers.

static List<Value>
getAllValues(TupleQueryResult result,
 String var)

Returns a list of values of a particular variable out of the QueryResult.

static boolean
isSubset(TupleQueryResult tqr1,
 TupleQueryResult tqr2)
 
static GraphQueryResult
limitResults(GraphQueryResult queryResult,
 long limit,
 long offset)

Returns a GraphQueryResult that returns at most the specified maximum number of solutions, starting at
 the supplied offset.

static TupleQueryResult
limitResults(TupleQueryResult queryResult,
 long limit,
 long offset)

Returns a TupleQueryResult that returns at most the specified maximum number of solutions, starting at
 the supplied offset.

static GraphQueryResult
parseGraphBackground(InputStream in,
 String baseURI,
 RDFFormat format)

Parses an RDF document and returns it as a GraphQueryResult object, with parsing done on a separate thread in the
 background.
 IMPORTANT: As this method will spawn a new thread in the background, it is vitally important that the resulting
 GraphQueryResult be closed consistently when it is no longer required, to prevent resource leaks.

static GraphQueryResult
parseGraphBackground(InputStream in,
 String baseURI,
 RDFParser parser)

Parses an RDF document and returns it as a GraphQueryResult object, with parsing done on a separate thread in the
 background.
 IMPORTANT: As this method will spawn a new thread in the background, it is vitally important that the resulting
 GraphQueryResult be closed consistently when it is no longer required, to prevent resource leaks.

static void
report(GraphQueryResult graphQueryResult,
 RDFHandler rdfHandler)

Reports a graph query result to an RDFHandler.

static void
report(TupleQueryResult tqr,
 QueryResultHandler handler)

Reports a tuple query result to a TupleQueryResultHandler.

static Statement
singleResult(GraphQueryResult result)

Returns a single element from the query result.The QueryResult is automatically closed by this method.

static BindingSet
singleResult(TupleQueryResult result)

Returns a single element from the query result.The QueryResult is automatically closed by this method.





Methods inherited from class org.eclipse.rdf4j.common.iteration.Iterations
addAll, asList, asSet, asSet, stream, toString, toString

Methods inherited from class java.lang.Object
clone, equals, finalize, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait









Constructor Details



QueryResults

public QueryResults()









Method Details



asModel

public static Model asModel(CloseableIteration<? extends Statement> iteration)
                     throws QueryEvaluationException
Get a Model containing all elements obtained from the specified query result.

Parameters:
iteration - the source iteration to get the statements from.
Returns:
a Model containing all statements obtained from the specified source iteration.
Throws:
QueryEvaluationException






asModel

public static Model asModel(CloseableIteration<? extends Statement> iteration,
 ModelFactory modelFactory)
                     throws QueryEvaluationException
Get a Model containing all elements obtained from the specified query result.

Parameters:
iteration - the source iteration to get the statements from.
modelFactory - the ModelFactory used to instantiate the model that gets returned.
Returns:
a Model containing all statements obtained from the specified source iteration.
Throws:
QueryEvaluationException






asList

public static <T> List<T> asList(QueryResult<T> queryResult)
                          throws QueryEvaluationException
Get a List containing all elements obtained from the specified QueryResult.

Parameters:
queryResult - the QueryResult to get the elements from
Returns:
a List containing all elements obtained from the specified query result.
Throws:
QueryEvaluationException






asSet

public static <T> Set<T> asSet(QueryResult<T> queryResult)
                        throws QueryEvaluationException
Get a Set containing all elements obtained from the specified QueryResult.

Parameters:
queryResult - the QueryResult to get the elements from
Returns:
a Set containing all elements obtained from the specified query result.
Throws:
QueryEvaluationException






getAllValues

public static List<Value> getAllValues(TupleQueryResult result,
 String var)
                                throws QueryEvaluationException
Returns a list of values of a particular variable out of the QueryResult.

Parameters:
result - 
var - variable for which list of values needs to be returned
Returns:
a list of Values of var
Throws:
QueryEvaluationException






singleResult

public static Statement singleResult(GraphQueryResult result)
                              throws QueryEvaluationException
Returns a single element from the query result.The QueryResult is automatically closed by this method.

Parameters:
result - 
Returns:
a single query result element or null
Throws:
QueryEvaluationException






singleResult

public static BindingSet singleResult(TupleQueryResult result)
                               throws QueryEvaluationException
Returns a single element from the query result.The QueryResult is automatically closed by this method.

Parameters:
result - 
Returns:
a single query result element or null
Throws:
QueryEvaluationException






distinctResults

public static GraphQueryResult distinctResults(GraphQueryResult queryResult)
Returns a GraphQueryResult that filters out any duplicate solutions from the supplied queryResult.

Parameters:
queryResult - a queryResult containing possible duplicate statements.
Returns:
a GraphQueryResult with any duplicates filtered out.






distinctResults

public static TupleQueryResult distinctResults(TupleQueryResult queryResult)
Returns a TupleQueryResult that filters out any duplicate solutions from the supplied queryResult.

Parameters:
queryResult - a queryResult containing possible duplicate solutions.
Returns:
a TupleQueryResult with any duplicates filtered out.






limitResults

public static TupleQueryResult limitResults(TupleQueryResult queryResult,
 long limit,
 long offset)
Returns a TupleQueryResult that returns at most the specified maximum number of solutions, starting at
 the supplied offset.

Parameters:
queryResult - a query result possibly containing more solutions than the specified maximum.
limit - the maximum number of solutions to return. If set to 0 or lower, no limit will be applied.
offset - the number of solutions to skip at the beginning. If set to 0 or lower, no offset will be
                    applied.
Returns:
A TupleQueryResult that will at return at most the specified maximum number of solutions. If
         neither limit nor offset are applied, this returns the original queryResult.






limitResults

public static GraphQueryResult limitResults(GraphQueryResult queryResult,
 long limit,
 long offset)
Returns a GraphQueryResult that returns at most the specified maximum number of solutions, starting at
 the supplied offset.

Parameters:
queryResult - a query result possibly containing more solutions than the specified maximum.
limit - the maximum number of solutions to return. If set to 0 or lower, no limit will be applied.
offset - the number of solutions to skip at the beginning. If set to 0 or lower, no offset will be
                    applied.
Returns:
A GraphQueryResult that will at return at most the specified maximum number of solutions. If
         neither limit nor offset are applied, this returns the original queryResult.






parseGraphBackground

public static GraphQueryResult parseGraphBackground(InputStream in,
 String baseURI,
 RDFFormat format)
                                             throws UnsupportedRDFormatException
Parses an RDF document and returns it as a GraphQueryResult object, with parsing done on a separate thread in the
 background.
 IMPORTANT: As this method will spawn a new thread in the background, it is vitally important that the resulting
 GraphQueryResult be closed consistently when it is no longer required, to prevent resource leaks.

Parameters:
in - The InputStream containing the RDF document.
baseURI - The base URI for the RDF document.
format - The RDFFormat of the RDF document.
Returns:
A GraphQueryResult that parses in the background, and must be closed to prevent resource leaks.
Throws:
UnsupportedRDFormatException






parseGraphBackground

public static GraphQueryResult parseGraphBackground(InputStream in,
 String baseURI,
 RDFParser parser)
Parses an RDF document and returns it as a GraphQueryResult object, with parsing done on a separate thread in the
 background.
 IMPORTANT: As this method will spawn a new thread in the background, it is vitally important that the resulting
 GraphQueryResult be closed consistently when it is no longer required, to prevent resource leaks.

Parameters:
in - The InputStream containing the RDF document.
baseURI - The base URI for the RDF document.
parser - The RDFParser.
Returns:
A GraphQueryResult that parses in the background, and must be closed to prevent resource leaks.






report

public static void report(TupleQueryResult tqr,
 QueryResultHandler handler)
                   throws TupleQueryResultHandlerException,
QueryEvaluationException
Reports a tuple query result to a TupleQueryResultHandler. 
 The CloseableIteration.close() method will always be called before this method returns. 
 If there is an exception generated by the TupleQueryResult, QueryResultHandler.endQueryResult() will not
 be called.

Parameters:
tqr - The query result to report.
handler - The handler to report the query result to.
Throws:
TupleQueryResultHandlerException - If such an exception is thrown by the used query result writer.
QueryEvaluationException






report

public static void report(GraphQueryResult graphQueryResult,
 RDFHandler rdfHandler)
                   throws RDFHandlerException,
QueryEvaluationException
Reports a graph query result to an RDFHandler. 
 The CloseableIteration.close() method will always be called before this method returns.
 If there is an exception generated by the GraphQueryResult, RDFHandler.endRDF() will not be called.

Parameters:
graphQueryResult - The query result to report.
rdfHandler - The handler to report the query result to.
Throws:
RDFHandlerException - If such an exception is thrown by the used RDF writer.
QueryEvaluationException






equals

public static boolean equals(TupleQueryResult tqr1,
 TupleQueryResult tqr2)
                      throws QueryEvaluationException
Compares two tuple query results and returns true if they are equal.Tuple query results are equal if they
 contain the same set of BindingSets and have the same headers. Blank nodes identifiers are not relevant
 for equality, they are matched by trying to find compatible mappings between BindingSets. Note that the method
 consumes both query results fully.

Parameters:
tqr1 - the first TupleQueryResult to compare.
tqr2 - the second TupleQueryResult to compare.
Returns:
true if equal
Throws:
QueryEvaluationException






isSubset

public static boolean isSubset(TupleQueryResult tqr1,
 TupleQueryResult tqr2)
                        throws QueryEvaluationException

Throws:
QueryEvaluationException






equals

public static boolean equals(GraphQueryResult result1,
 GraphQueryResult result2)
                      throws QueryEvaluationException
Compares two graph query results and returns true if they are equal. Two graph query results are
 considered equal if they are isomorphic graphs. Note that the method consumes both query results fully.

Parameters:
result1 - the first query result to compare
result2 - the second query result to compare.
Returns:
true if the supplied graph query results are isomorphic graphs, false otherwise.
Throws:
QueryEvaluationException
See Also:


Models.isomorphic(Iterable, Iterable)








bindingSetsCompatible

public static boolean bindingSetsCompatible(BindingSet bs1,
 BindingSet bs2)
Check whether two BindingSets are compatible. Two binding sets are compatible if they have equal values
 for each variable that is bound in both binding sets.

Parameters:
bs1 - 
bs2 - 
Returns:
true if compatible












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface TupleQueryResultHandler



All Superinterfaces:
QueryResultHandler


All Known Subinterfaces:
TupleQueryResultWriter


All Known Implementing Classes:
AbstractTupleQueryResultHandler, BackgroundTupleResult, BinaryQueryResultWriter, DAWGTestResultSetWriter, QueryResultCollector, SPARQLResultsCSVWriter, SPARQLResultsJSONWriter, SPARQLResultsTSVWriter, SPARQLResultsXMLWriter, SPARQLStarResultsJSONWriter, SPARQLStarResultsTSVWriter, SPARQLStarResultsXMLWriter, TupleQueryResultBuilder



public interface TupleQueryResultHandler
extends QueryResultHandler
An interface defining methods related to handling sequences of Solutions.
 
 Instances of this interface are capable of handling tuple results using the QueryResultHandler.startQueryResult(List) ,
 QueryResultHandler.handleSolution(BindingSet) and QueryResultHandler.endQueryResult() methods.







Method Summary

Methods inherited from interface org.eclipse.rdf4j.query.QueryResultHandler
endQueryResult, handleBoolean, handleLinks, handleSolution, startQueryResult







Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface GraphQuery



All Superinterfaces:
Operation, Query


All Known Implementing Classes:
DelegatingGraphQuery, FedXGraphQuery, HTTPGraphQuery, LoggingGraphQuery, ResultCachingGraphQuery, SailGraphQuery, SPARQLGraphQuery



public interface GraphQuery
extends Query







Nested Class Summary

Nested classes/interfaces inherited from interface org.eclipse.rdf4j.query.Query
Query.QueryType





Method Summary

All MethodsInstance MethodsAbstract Methods


Modifier and Type
Method
Description
GraphQueryResult
evaluate()
 
void
evaluate(RDFHandler handler)
 




Methods inherited from interface org.eclipse.rdf4j.query.Operation
clearBindings, getBindings, getDataset, getIncludeInferred, getMaxExecutionTime, removeBinding, setBinding, setDataset, setIncludeInferred, setMaxExecutionTime

Methods inherited from interface org.eclipse.rdf4j.query.Query
explain, getMaxQueryTime, setMaxQueryTime









Method Details



evaluate

GraphQueryResult evaluate()
                   throws QueryEvaluationException

Throws:
QueryEvaluationException






evaluate

void evaluate(RDFHandler handler)
       throws QueryEvaluationException,
RDFHandlerException

Throws:
QueryEvaluationException
RDFHandlerException












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface GraphQueryResult



All Superinterfaces:
AutoCloseable, CloseableIteration<Statement>, Iterable<Statement>, Iterator<Statement>, QueryResult<Statement>


All Known Implementing Classes:
BackgroundGraphResult, CachedGraphQueryResult, CleanerGraphQueryResult, IteratingGraphQueryResult, ReusableGraphQueryResult



public interface GraphQueryResult
extends QueryResult<Statement>
A representation of a query result as a sequence of Statement objects. Each query result consists of zero or
 more Statements and additionaly carries information about relevant namespace declarations. Note: take care to always
 close a GraphQueryResult after use to free any resources it keeps hold of.

Author:
Jeen Broekstra








Method Summary

All MethodsInstance MethodsAbstract Methods


Modifier and Type
Method
Description
Map<String,String>
getNamespaces()

Retrieves relevant namespaces from the query result.





Methods inherited from interface org.eclipse.rdf4j.common.iteration.CloseableIteration
close

Methods inherited from interface java.lang.Iterable
forEach, spliterator

Methods inherited from interface java.util.Iterator
forEachRemaining, remove

Methods inherited from interface org.eclipse.rdf4j.query.QueryResult
hasNext, iterator, next, stream









Method Details



getNamespaces

Map<String,String> getNamespaces()
                          throws QueryEvaluationException
Retrieves relevant namespaces from the query result. 
 The contents of the Map may be modified after it is returned, as the initial return may be performed when the
 first RDF Statement is encountered.

Returns:
a Map<String, String> object containing (prefix, namespace) pairs.
Throws:
QueryEvaluationException












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface Statement



All Superinterfaces:
Serializable


All Known Subinterfaces:
ExtensibleStatement


All Known Implementing Classes:
AbstractStatement, ExtensibleStatementImpl, GenericStatement, LinkedHashModel.ModelStatement, MemStatement, UnboundStatement



public interface Statement
extends Serializable
An RDF statement, with optional associated context. A statement can have an associated context in specific cases, for
 example when fetched from a repository.
 
 Additional utility functionality for working with Statement objects is available in the
 org.eclipse.rdf4j.model.util.Statements utility class.







Method Summary

All MethodsInstance MethodsAbstract Methods


Modifier and Type
Method
Description
boolean
equals(Object other)

Compares this statement to another object.

Resource
getContext()

Gets the context of this statement.

Value
getObject()

Gets the object of this statement.

IRI
getPredicate()

Gets the predicate of this statement.

Resource
getSubject()

Gets the subject of this statement.

int
hashCode()

Computes the hash code of this statement.













Method Details



getSubject

Resource getSubject()
Gets the subject of this statement.

Returns:
The statement's subject.






getPredicate

IRI getPredicate()
Gets the predicate of this statement.

Returns:
The statement's predicate.






getObject

Value getObject()
Gets the object of this statement.

Returns:
The statement's object.






getContext

Resource getContext()
Gets the context of this statement.

Returns:
The statement's context, or null in case of the null context or if not applicable.






equals

boolean equals(Object other)
Compares this statement to another object.

Overrides:
equals in class Object
Parameters:
other - the object to compare this statement to
Returns:
true if the other object is an instance of Statement and if their
         subjects, predicates, objects and contexts are equal; false otherwise






hashCode

int hashCode()
Computes the hash code of this statement.

Overrides:
hashCode in class Object
Returns:
a hash code for this statement computed as Objects.hash(
         getSubject(), getPredicate(), getObject(), getContext())












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface RDFHandler



All Known Subinterfaces:
RDFWriter


All Known Implementing Classes:
AbstractRDFHandler, AbstractRDFInserter, AbstractRDFWriter, BackgroundGraphResult, BinaryRDFWriter, BlackHoleRDFHandler, BufferedGroupingRDFHandler, ConsoleRDFWriter, ConstraintViolationRDFHandler, ContextStatementCollector, DAWGTestBooleanParser, DAWGTestResultSetParser, EndpointFactory.DefaultRDFHandler, JSONLDWriter, JSONLDWriter, N3Writer, NDJSONLDWriter, NDJSONLDWriter, NQuadsWriter, NTriplesWriter, RDFHandlerWrapper, RDFInferencerInserter, RDFInserter, RDFJSONWriter, RDFRemover, RDFSailInserter, RDFXMLPrettyWriter, RDFXMLWriter, StatementCollector, TimeLimitRDFHandler, TriGStarWriter, TriGWriter, TriXWriter, TurtleStarWriter, TurtleWriter, VerificationListener



public interface RDFHandler
An interface defining methods related to RDF data handling. RDFHandler is both used as a "consumer" and as
 a "producer" interface. As such it can be used both as an interface for receiving RDF data, for example by listening
 to the results of an RDF parser, and as an interface for reporting RDF data, for example to an object that serializes
 RDF data to an RDF/XML document.







Method Summary

All MethodsInstance MethodsAbstract Methods


Modifier and Type
Method
Description
void
endRDF()

Signals the end of the RDF data.

void
handleComment(String comment)

Handles a comment.

void
handleNamespace(String prefix,
 String uri)

Handles a namespace declaration/definition.

void
handleStatement(Statement st)

Handles a statement.

void
startRDF()

Signals the start of the RDF data.













Method Details



startRDF

void startRDF()
       throws RDFHandlerException
Signals the start of the RDF data. This method is called before any data is reported.

Throws:
RDFHandlerException - If the RDF handler has encountered an unrecoverable error.






endRDF

void endRDF()
     throws RDFHandlerException
Signals the end of the RDF data. This method is called when all data has been reported.

Throws:
RDFHandlerException - If the RDF handler has encountered an unrecoverable error.






handleNamespace

void handleNamespace(String prefix,
 String uri)
              throws RDFHandlerException
Handles a namespace declaration/definition. A namespace declaration associates a (short) prefix string with the
 namespace's URI. The prefix for default namespaces, which do not have an associated prefix, are represented as
 empty strings.

Parameters:
prefix - The prefix for the namespace, or an empty string in case of a default namespace.
uri - The URI that the prefix maps to.
Throws:
RDFHandlerException - If the RDF handler has encountered an unrecoverable error.






handleStatement

void handleStatement(Statement st)
              throws RDFHandlerException
Handles a statement.

Parameters:
st - The statement.
Throws:
RDFHandlerException - If the RDF handler has encountered an unrecoverable error.






handleComment

void handleComment(String comment)
            throws RDFHandlerException
Handles a comment.

Parameters:
comment - The comment.
Throws:
RDFHandlerException - If the RDF handler has encountered an unrecoverable error.












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nStarting a New Maven Project in Eclipse
    

  
  If you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.

    
    
You don't have to use Apache Maven or Eclipse IDE if you want to work with RDF4J. These are simply very useful tools for quickly getting a Java project started. Maven is good because it allows you to just define which libraries you want to use and never worry about any further third-party libraries you might need, and Eclipse IDE is good because it has good integration with Maven, code completion features, and is just generally a great Java development environment. But you can work with RDF4J just as well in a different build tool or IDE.



In this tutorial, we assume that you have a basic understanding of programming in Java, and have at least an inkling of what RDF is. However, we do not assume that you know how to use either RDF4J, Maven, or Eclipse, so we’ll go through it all one step at a time. If anything is already sufficiently familiar to you, you are of course free to skip ahead!
Setting up your environment
Before we kick off, you will need to install the Eclipse IDE. In this tutorial, we will use Eclipse for Java Developers version 4.18.0 (2020-12), but it shouldn’t matter too much which exact version you have installed. Select either the “Eclipse for Java developers” or “Eclipse for Java EE developers” installation option.
Eclipse IDE comes with a Maven plugin (called m2e) already installed. We will use this plugin to work with Maven. You are of course free to also install the Maven command line tools, but we won’t be using those directly in this tutorial.
When starting Eclipse for the first time, you will be asked for a workspace directory – this will be where Eclipse stores all project data as well as some configuration information. Feel free to pick/create a directory of your choice, or just accept the default.
Once Eclipse is started (and any welcome messages have been closed), you will be presented with a screen that should look roughly like this:

Tweaking Eclipse preferences
Before we start creating our actual project in Eclipse, there are a few preferences that we should tweak. This step is not required, but it will make things a little easier in the rest of this tutorial.
From the menu, select Eclipse -> Preferences. The Preferences dialog pops up. On the left, select Maven. You will see the Maven configuration settings. The options “Download Artifact Sources” and “Download Artifact JavaDoc” are unchecked by default. Check them both, then click OK.

The reason we want this, by the way, is that having either the sources or the Javadoc available really helps when you use code autocompletion in Eclipse – Eclipse will automatically read the Javadoc and provide the documentation in a little tooltip. As said: not required, but very useful.
Creating a new project
Now that we’re all set up, we can kick things off by creating a new project. Select the File menu, then New -> New Project... . A dialog will appear. In this dialog, select the option Maven Project:

Once you click Next, you will be presented with a screen to create a new Maven Project. Make you sure the option ‘Create a simple project’ is checked:

Click Next again. In the following screen you define further details of your Maven project, such as group and artifact ids, version number, and so on. For this tutorial,  you only have to fill in three fields:

group id – typically you use something like a Java package name for this. We will use org.example.
artifact id –  name of the maven artifact if you publish our project as one. We will use rdf4j-getting-started.
name – the project name. We will use HelloRDF4J .


Once this has been filled in you can click Finish. We now have our first Eclipse project, huzzah!
Defining the POM
So we have a project. That’s great, but it’s still empty: we haven’t added any code or necessary libraries (such as the RDF4J libraries) yet.
We will remedy the library-situation first. In Maven, requirements for libraries (we’re talking about jar files here) are specified in a special project configuration file called pom.xml, often referred to as “the POM”. Open your project and you will see that a file with that name is already present. Doubleclick it to open it in the POM editor:

As you can see, the POM already contains some information – in fact the information that we added when we created our project. However, we need to add more information about our project. Specifically, we want to add which external Java libraries we want to include in our project. In Maven, you do this by specifying dependencies.
The first dependency we will add is for RDF4J itself. RDF4J is not a single library, but consists of a large collection of separate modules, allowing you to pick and choose which functionality you need. To make handling of all the various modules and the dependencies they all need easier, we will first add a “Bill Of Materials” (BOM) dependency. The easiest way to do this is to edit the XML source code directly. Switch to the ‘pom.xml’ tab of the POM editor, and add the following XML fragment:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.6.0</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
It should look like this after you’ve added this and saved the file:

Next we need to add the actual RDF4J modules we want to use. As said, this is where you could pick and choose the specific modules (parsers, databases, etc) that you want. For this tutorial, however, we’ll be lazy and just add the entire core framework. We do this by adding the rdf4j-storage dependency, as follows:
<dependencies>
  <dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
  </dependency>
</dependencies>
Notice that because we already have a BOM added, we don’t need to specify a version number for this dependency. Once you have done this and save the file, Eclipse will automatically begin downloading the RDF4J libraries. This may take a little while, just let it do its thing. Once it’s complete, your POM should look like this:

We are going to add one more dependency, namely for a logging framework. RDF4J uses the SLF4J logging API, which requires a logging implementation, so we’ll provide one, in this case slf4j-simple (which is, as the name implies, a very simple logging library that by default just logs to the console). Add a new dependency, with group id org.slf4j, artifact id slf4j-simple. Again we don’t need to specify a version number: the RDF4J Bill of Materials already specifies which version of slf4j is compatible with RDF4J:
<dependency>
  <groupId>org.slf4j</groupId>
  <artifactId>slf4j-simple</artifactId>
  <scope>runtime</scope>
</dependency>
Once you have saved your changes to the POM, your project should be automatically refreshed by Eclipse again, and you should see a new section called ‘Maven dependencies’ in the Package Explorer (If your project does not automatically refresh and you don’t see the above section, right-click on your project in the Package Explorer, and select Maven -> Update Project...). This section, when opened, shows a list of jar files which can now be used in your project:

As you can see, this list contains not just all the RDF4J libraries, but also a lot of other libraries that RDF4J depends on.

    
    
If you are interested in having more control over which libraries are and aren't included, please see Setting up your development environment.



Configuring the Java compiler
Unfortunately the default Maven archetype that we used to create our new project uses a very old Java compiler (1.5) by default. Since RDF4J requires Java 8 at a minimum (we actually recommend Java 11 for better performance), we will need to change this.
Copy-paste (or type if you prefer) the following section into the xml file (put it just above the dependencyManagement section we added in earlier). This will set the version to Java 11.
<properties>
  <maven.compiler.source>11</maven.compiler.source>
  <maven.compiler.target>11</maven.compiler.target>
</properties>
Once you have done this, and saved your POM, you will need to manually update your Maven project for Eclipse to accept the changes. You do this by right-clicking on your project (in the project explorer), and then selecting ‘Maven’ -> ‘Update Project…':

Programming our first Semantic Web application
We’re done preparing, we can start programming! Right-click on src/main/java and select New -> Class. In the dialog shown, set the package name to org.example, the class name to HelloRDF4J, and make sure the option public static void main (String[] args) is checked:

Click ‘Finish’ and Eclipse will create the new class and automatically open it in an editor.
Since we will want to work with RDF data, we will start by creating something to keep that RDF data in. In RDF4J, RDF data is usually stored in a Repository. There many different types of Repository, both for local access as well as for accessing remote services. In this tutorial, we will create a simple local repository, namely one that uses an in-memory store, without any sort of persistent storage functionality (so the data will be lost when the program exits). Add the following code to your main method:
Repository rep = new SailRepository(new MemoryStore());
Once you have done this and saved the file, you will notice some red lines appearing:

This is Eclipse telling you that there is something wrong with your code. In this case, the problem is that several import statements are missing (note that Eclipse tells you what is wrong in detail as well in the ‘Problems’ tab underneath the editor). We’ll need to add those import statements. You can add each import manually of course, but luckily Eclipse has a shortcut. Hit Ctrl+Shift+O and Eclipse should automatically resolve all missing imports for you. It will pop up a dialog if there is more than one possibility for a particular import, which will like happen for the Repository interface. Just pick the one from org.eclipse.rdf4j:

Hit ‘Finish’, then save your code.
Now that we have created our repository, we are going to put some data in it, and then get that data out again and print it to the console.
Adding data can be done in numerous ways. In this tutorial, we will be creating a few RDF statements directly in Java (rather than, say, loading them from a file). To do this, we need a namespace that we can use to create any new IRIs we need. With this namespace, we can create a new IRI. We will create an identifier for a person called “John”, about whom we will later add some data to our repository:
Namespace ex = Values.namespace("ex", "http://example.org/");
IRI john = Values.iri(ex, "john");
We use a static factory(the Values class) to easily create new Namespaces, IRIs and other types of values.
We have now created a IRI, but have not yet added any data to our repository. To do this, we first need to open a RepositoryConnection. Such a connection allows us to add, remove, and retrieve data from our Repository. We do this as follows:
RepositoryConnection conn = rep.getConnection();
It is important to keep track of open connections and to close them again when we are finished with them, to free up any resources the connection may keep hold of. RDF4J connections are “AutoCloseable”, which means we can let Java handle the closing of the connection when we’re done with it, rather than having to deal with this ourselves. We use a try-with-resources construction for this, like so:
try (RepositoryConnection conn = rep.getConnection()) {
}
Your code should now look as follows:

Using the connection, we can start adding statements to our repository. We will add two triples, one to assert that John is a Person, and one to assert that John’s name is “John”:
conn.add(john, RDF.TYPE, FOAF.PERSON);
conn.add(john, RDFS.LABEL, Values.literal("John"));
Note how for well-known RDF vocabularies, such as RDF, RDFS, and FOAF, RDF4J provides constants that you can easily reuse to create/read data with. Also, you see another use of the Values static factory here, this time to create a Literal object.
Once we have added our data, we can retrieve it back from the repository again. You can do this in several ways, with a SPARQL query or using the RepositoryConnection API methods directly. Here we will use the latter:
RepositoryResult<Statement> statements = conn.getStatements(null, null, null);
We use the getStatements() method to retrieve data from the repository. The three arguments (all null) are for the subject, predicate, and object we wish to match. Since they are all set to null, this will retrieve all statements.
The object returned by getStatements() is a RepositoryResult, which implements both Iteration and Iterable: it allows you to process the result in a streaming fashion, using its hasNext() and next() methods, as well as using Java’s for-each loop to iterate over it. Almost all the methods that retrieve data from a Repository return such a streaming/iterating object: they are very useful for processsing very large result sets, because they do not require that the entire result is kept in memory all at the same time.
It is important to always call close() on this kind of result object when you are done with them, to avoid memory leaks and other problems. Since the RepositoryResult is also a AutoCloseable, you can use a try-with-resources construction (similar to what we used for opening the connection) to handle this.
However, since our Repository only contains 2 statements, we can safely just transfer the entire result into a Java Collection. RDF4J has the Model interface for this, which is an extension of the standard Java collections that has some special tricks up its sleeve for dealing with RDF data, but which you can also use in the same manner as a Set or a List. We convert our result to a Model as follows:
Model model = QueryResults.asModel(statements);
The QueryResults utility retrieves all the statements from the supplied result, and puts them in a Model. It also automatically closes the result object for you.
Finally, we want to print out our data to the console. This too is something you can do in numerous ways, but let’s just say we would like to print out our data in Turtle format. Here’s how:
Rio.write(model, System.out, RDFFormat.TURTLE);
Rio (which stands for “RDF I/O”) is the RDF4J parser/writer toolkit. We can just pass it our model, specify the outputstream, and the format in which we’d like it written, and Rio does the rest. Your code should now look like this:

You now have created your first Semantic Web application! After saving, just right-click on HelloRDF4J.java (in the project explorer) and select Run as -> Java application. You will see the following output in the Console:
<http://example.org/john> a <http://xmlns.com/foaf/0.1/Person> ;
         <http://www.w3.org/2000/01/rdf-schema#label> "John" .
As you can see, it contains exactly the two statements that we added earlier, in Turtle syntax.
However, it is a bit ugly, using all those long IRIs. We can make the output a bit prettier, by adding some namespace definitions to our Model:
model.setNamespace(RDF.NS);
model.setNamespace(RDFS.NS);
model.setNamespace(FOAF.NS);
model.setNamespace(ex);
Add these lines to your code, directly after where you have created the model. Then run your program again. You will now see the following:

That’s it! Obviously there is still loads to learn about how to use RDF4J effectively, but you’ve got the basics under control now: you can set up a new project using RDF4J,  and have seen some basic ways to add, write, and retrieve data. The rest is up to you. Good sources of further documentation are the Getting Started tutorial, Programming with RDF4J, and of course the API Javadoc.

  

     
      
        
          

  Table of Contents

  
  
    Setting up your environment
      
        Tweaking Eclipse preferences
      
    
    Creating a new project
    Defining the POM
    Configuring the Java compiler
    Programming our first Semantic Web application\n\n\n\nSupport
    

  
  Ask about or discuss RDF4J
RDF4J Github Discussions is the best place to ask questions, propose new ideas or discuss other issues related to RDF4J.
You can also find us on Gitter, if you prefer an instant messaging style of communication. However, we make no promises that any of the RDF4J developers will be online at any given time.
We previously used a Google group called rdf4j-users. This group has now been archived. You can still browse the archive, but no new posts will be accepted.
The RDF4J development team uses the rdf4j-dev@eclipse.org mailinglist to discuss development progress.
Reporting bugs and requests for improvement
If you think you’ve found a bug in RDF4J, or wish to log a request for a new feature or improvement, please use the RDF4J issue tracker. Before you add your new issue, though, please have a look around to see if it’s not already there.


  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nGetting Started With RDF4J
    

  
  In this tutorial, we go through the basics of what RDF is, and we show how you can use the Eclipse RDF4J framework to create, process, store, and query RDF data.
We assume that you know a little about programming in Java, but no prior knowledge on RDF is assumed.

    
    
 The code examples in this tutorial are available for download from the examples directory in the RDF4J GitHub repository. We encourage you to download these examples and play around with them. The easiest way to do this is to download the GitHub repository in your favorite Java IDE as an Apache Maven project.
 


Introducing RDF
The Resource Description Framework (RDF) is a standard (or more accurately, a “recommendation”) formulated by the World Wide Web Consortium (W3C). The purpose of RDF is to provide a framework for expressing information about resources in a machine-processable, interoperable fashion.
A resource can be anything that we can stick an identifier on: a web page, an image, but also more abstract/real-world things like you, me, the concept of “world peace”, the number 42, and that library book you never returned.
RDF is intended for modeling information that needs to be processed by applications, rather than just being shown to people.
In this tutorial, we will be modeling information about artists . Let’s start with a simple fact: “Picasso’s first name is Pablo”. In RDF, this could be expressed as follows:

So what exactly are we looking at here? Well, we have a resource “Picasso”, denoted by an IRI (Internationalized Resource Identifier): http://example.org/Picasso. In RDF, resources have properties. Here we are using the foaf:firstName property to denote the relation between the resource “Picasso” and the value “Pablo”. foaf:firstName is also an IRI, though to make things easier to read we use an abbreviated syntax, called prefixed names (more about this later). Finally, the property value, “Pablo”, is a literal value: it is not represented using a resource identifier, but simply as a string of characters.
NOTE: the foaf:firstName property is part of the FOAF (Friend-of-a-Friend) vocabulary. This is an example of reusing an existing vocabuary to describe our own data. After all, if someone else already defined a property for describing people’s first names, why not use it? More about this later.
As you may have noticed, we have depicted our fact about Picasso as a simple graph: two nodes, connected by an edge. It is very helpful to think about RDF models as graphs, and a lot of the tools we will be using to create and query RDF data make a lot more sense if you do.
In RDF, each fact is called a statement. Each statement consists of three parts (for this reason, it is also often called a triple):

the subject is the starting node of the statement, representing the resource that the fact is “about”;
the predicate is the property that denotes the edge between two nodes;
the object is the end node of the statement, representing the resource or literal that is the property value.

Let’s expand our example slightly: we don’t just have a single statement about Picasso, we know another fact as well: “Picasso is an artist”. We can extend our RDF model as follows:

Notice how the second statement was added to our graph depiction by simply adding a second edge to an already existing node , labeled with the rdf:type property, and the value ex:Artist. As you continue to add new facts to your data model, nodes and edges continue to be added to the graph.
IRIs, namespaces, and prefixed names
IRIs are at the core of what makes RDF powerful. They provide a mechanism that allows global identification of any resource: no matter who authors a dataset or where that data is physically stored, if that data shares an identical IRI with another dataset you know that both datasets are talking about the same thing.
In many RDF data sets, you will see IRIs that start with ‘http://…’. This does not necessarily mean that you can open this link in your browser and get anything meaningful, though. Quite often, IRIs are merely used as unique identifiers, and not as actual addresses. Some RDF sources do make sure that their IRIs can be looked up on the Web, and that you actually get back data (in RDF) that describes the resource identified by the IRI. This is known as a Linked Data architecture. The ins and outs of Linked Data are beyond the scope of this tutorial, but it’s worth exploring once you understand the basics of RDF.
You will often see IRIs in abbreviated form whenever you encounter examples of RDF data: <prefix>:<name> This abbreviated form, known as “prefixed names”, has no impact on the meaning of the data, but it makes it easier for people to read the data.
Prefixed names work by defining a prefix that is a replacement for a namespace. A namespace is the first part of an IRI that is shared by several resources. For example, the IRIs http://example.org/Picasso, http://example.org/Rodin, and http://example.org/Rembrandt all share the the namespace http://example.org/. By defining a new prefix ex as the abbreviation for this namespace, we can use the string ex:Picasso instead of its full IRI.
Creating and reusing IRIs
In the running example for this tutorial, we use a namespace prefix http://example.org/ that we indiscriminately use for various resource and property IRIs we want to use. In a real world scenario, that is not very practical: we don’t own the domain ‘example.org’, for one thing, and moreover it is not very descriptive of what our resources actually are about.
So, how do you pick good IRIs for your resources and properties? There’s a lot to be said about this topic, some of it beyond the scope of this tutorial. You should at least keep the following in mind:

use a domain name that you own for your own resources. Don’t reuse other people’s domain, and don’t add new resources or properties to existing vocabularies.
try and reuse existing vocabularies. Instead of creating new resources and relations to describe all your data, see if somebody else has already published a collection of IRIs (known as a vocabulary, or sometimes an ontology) that describes the same kind of things you want to describe. Then use their IRIs as part of your own data.

There are several major benefits to reusing existing vocabulary:

you don’t have to reinvent the wheel;
when the time comes to share your data with a third party, chances are that they also reuse
the existing vocabulary, making data integration easier.

Of course we can’t list every possible reusable RDF vocabulary here, but there are several very generic RDF vocabularies that get reused very often:

RDF Schema (RDFS) - the RDF Schema vocabulary provides some basic properties and resources that you can use to create class hierarchies, define your own properties in more detail, and so on. One commonly used property from RDFS is rdfs:label, which is used to give a resource a human-readable name, as a string value.
Web Ontology Language (OWL) - the Web Ontology Language OWL provides an extensive and powerful (but also quite complex) set of resources and properties that can be used to model complex domain models, a.k.a. ontologies. It can be used to say things like “this class of things here is exactly the same as that class over there” or “resources of type BlueCar must have a property Color with value “Blue”. Learning about OWL goes beyond the scope of this tutorial.
Simple Knowledge Organization System (SKOS) provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary. It has properties such as skos:broader, skos:narrower (to indicate that one term is a broader/narrower term than some other term), skos:prefLabel, skos:altLabel (to give preferred and alternative names for concepts), and more.
Friend-Of-A-Friend (FOAF) - the FOAF vocabulary provides resources and properties to model people and their social networks. You can use it to say that some resource describes a foaf:Person, and you can use properties such as foaf:firstName, foaf:surname, foaf:mbox to describe all sorts of data about that person.
Dublin Core (DC) Elements - the Dublin Core Metadata Initiative (DCMI) has a defined a vocabulary of 15 commonly used properties for describing resources from a library/digital archiving perspective. It includes properties such as dc:creator (to indicate the creator of a work), dc:subject, dc:title, and more.

The flexibility of RDF makes it easy to mix and match models as you need them. You will, in practice, often see RDF data sets that have some “home-grown” IRIs, combined with properties and class names from a variety of different other vocabularies. It’s not uncommon to see 3 or more different vocabularies all reused in the same dataset.
Using RDF4J to create RDF models
Enough background, let’s get our hands dirty.
Eclipse RDF4J is a Java API for RDF: it allows you to create, parse, write, store, query and reason with RDF data in a highly scalable manner. So let’s see two examples of how we can use RDF4J to create the above RDF model in Java.
Example 01: building a simple Model
Example 01
 shows how we can create the RDF model we introduced above using RDF4J:
 1// We want to reuse this namespace when creating several building blocks.
 2String ex = "http://example.org/";
 3
 4// Create IRIs for the resources we want to add.
 5IRI picasso = Values.iri(ex, "Picasso");
 6IRI artist = Values.iri(ex, "Artist");
 7
 8// Create a new, empty Model object.
 9Model model = new TreeModel();
10
11// add our first statement: Picasso is an Artist
12model.add(picasso, RDF.TYPE, artist);
13
14// second statement: Picasso's first name is "Pablo".
15model.add(picasso, FOAF.FIRST_NAME, Values.literal("Pablo"));
Let’s take a closer look at this. Lines 1-6 are necessary preparation: we use Values
 factory methods to create resources, which we will later use to add facts to our model.
On line 9, we create a new, empty model. RDF4J comes with several Model
 implementations, the ones you will most commonly encounter are DynamicModel
, TreeModel
 and LinkedHashModel
. The difference is in how they index data internally - which has a performance impact when working with very large models, and in the ordering with which statements are returned. For our purposes however, it doesn’t really matter which implementation you use.
On lines 12 and 15, we add our two facts that we know about Picasso: that’s he’s an artist, and that his first name is “Pablo”.
In RDF4J, a Model
 is simply an in-memory collection of RDF statements. We can add statements to an existing model, remove statements from it, and of course iterate over the model to do things with its contents. As an example, let’s iterate over all statements in our Model using a for-each loop, and print them to the screen:
for (Statement statement: model) {
    System.out.println(statement);
}
Or, even shorter:
model.forEach(System.out::println);
When you run this, the output will look something like this:
(http://example.org/Picasso, http://xmlns.com/foaf/0.1/firstName, "Pablo"^^<http://www.w3.org/2001/XMLSchema#string>) [null]
(http://example.org/Picasso, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://example.org/art/Artist) [null]

Not very pretty perhaps, but at least you should be able to recognize the RDF statements that we originally added to our model. Each line is a single statement, with the subject, predicate, and object value in comma-separated form. The [null] behind each statement is a context identifier or named graph identifier, which you can safely ignore for now. The bit ^^<http://www.w3.org/2001/XMLSchema#string> is a datatype that RDF4J assigned to the literal value we added (in this case, the datatype is simply string).
Example 02: using the ModelBuilder
The previous code example shows that you need to do a bit of preparation before actually adding anything to your model: defining common namespaces, creating IRIs, etc. As a convenience, RDF4J provides a ModelBuilder
 that simplifies things.
Example 02
 shows how we can create the exact same model using a ModelBuilder:
1ModelBuilder builder = new ModelBuilder();
2Model model = builder.setNamespace("ex", "http://example.org/")
3      .subject("ex:Picasso")
4      .add(RDF.TYPE, "ex:Artist")
5      .add(FOAF.FIRST_NAME, "Pablo")
6      .build();
The above bit of code creates the exact same model that we saw in the previous example, but with far less prep code. ModelBuilder accepts IRIs and prefixed names supplied as simple Java strings. On line 3 we define a namespace prefix we want to use, and then on lines 4-6 we use simple prefixed name strings, which the ModelBuilder internally maps to full IRIs.
Literal values: datatypes and language tags
We have sofar seen literal values that were just simple strings. However, in RDF, every literal has an associated datatype that determines what kind of value the literal is: a string, an integer number, a date, and so on. In addition, a String literal can optionally have a language tag that indicates the language the string is in.
Datatypes are associated with a literal by means of a datatype IRI, usually for a datatype defined in XML Schema. Examples are http://www.w3.org/2001/XMLSchema#string, http://www.w3.org/2001/XMLSchema#integer, http://www.w3.org/2001/XMLSchema#dateTime (commonly abbreviated as xsd:string, xsd:integer, xsd:dateTime, respectively). A longer (though not exhaustive) list of supported data types is available in the RDF 1.1 Concepts specification.
Languages are associated with a string literal by means of a “language tag”, as identified by BCP 47. Examples of language tags are “en” (English), “fr” (French), “en-US” (US English), etc.
We will demonstrate the use of language tags and data types by adding some additional data to our model. Specifically, we will add some information about a painting created by van Gogh, namely “The Potato Eaters”.
Example 03: adding a date and a number
Example 03
 shows how we can add the creation date (as an xsd:date) and the number of people depicted in the painting (as an xsd:integer):
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // this painting was created on April 1, 1885
 6    .add("ex:creationDate", LocalDate.parse("1885-04-01"))
 7    // instead of a java.time value, you can directly create a date-typed literal as well
 8    // .add("ex:creationDate", literal("1885-04-01", XSD.DATE))
 9
10    // the painting shows 5 people
11    .add("ex:peopleDepicted", 5)
12    .build();
13
14// To see what's in our model, let's just print stuff to the screen
15for (Statement st : model) {
16  // we want to see the object values of each property
17  IRI property = st.getPredicate();
18  Value value = st.getObject();
19  if (value.isLiteral()) {
20    Literal literal = (Literal) value;
21    System.out.println("datatype: " + literal.getDatatype());
22
23    // get the value of the literal directly as a Java primitive.
24    if (property.getLocalName().equals("peopleDepicted")) {
25      int peopleDepicted = literal.intValue();
26      System.out.println(peopleDepicted + " people are depicted in this painting");
27    } else if (property.getLocalName().equals("creationDate")) {
28      LocalDate date = LocalDate.from(literal.temporalAccessorValue());
29      System.out.println("The painting was created on " + date);
30    }
31
32    // you can also just get the lexical value (a string) without worrying about the datatype
33    System.out.println("Lexical value: '" + literal.getLabel() + "'");
34  }
35}
Example 04: adding an artwork’s title in Dutch and English
Example 04
 shows how we can add the title of the painting in both Dutch and English, and how we can retrieve this information back from the model:
 1ModelBuilder builder = new ModelBuilder();
 2Model model = builder
 3    .setNamespace("ex", "http://example.org/")
 4    .subject("ex:PotatoEaters")
 5    // In English, this painting is called "The Potato Eaters"
 6    .add(DC.TITLE, Values.literal("The Potato Eaters", "en"))
 7    // In Dutch, it's called "De Aardappeleters"
 8    .add(DC.TITLE, Values.literal("De Aardappeleters", "nl"))
 9    .build();
10
11// To see what's in our model, let's just print it to the screen
12for (Statement st : model) {
13  // we want to see the object values of each statement
14  Value value = st.getObject();
15  if (value.isLiteral()) {
16    Literal title = (Literal) value;
17    System.out.println("language: " + title.getLanguage().orElse("unknown"));
18    System.out.println(" title: " + title.getLabel());
19  }
20}
Blank nodes
Sometimes, we want to model some facts without explicitly giving all resources involved in that fact an identifier. For example, consider the following sentence: “Picasso has created a painting depicting cubes, and using a blue color scheme”. There are several facts in this sentence:

Picasso created some painting;
that painting depicts cubes;
that painting uses the color blue.

All of the above may be true, but it doesn’t involve identifying a specific painting. All we know is that there is some (unknown) painting for which all of this is true. We can express this in RDF using a blank node.
When looking at a graph depiction of the RDF, it becomes obvious why it is called a blank node:

Other possible uses for blank nodes are for modeling a collection of facts that are strongly tied together. For example, “Picasso’s home address is ‘31 Art Gallery, Madrid, Spain’” could be modeled as follows:

The address itself has no identifier, but is a sort of “compound object” consisting of multiple attributes.

    
    
Blank nodes can be useful, but they can also complicate things. They can not be directly addressed (they have no identifier, after all, hence "blank"), so you can only query them via their property values. And since they have no identifier, it's often hard to determine if two blank nodes are really the same resource, or two separate ones. A good rule of thumb is to only use blank nodes if it really conceptually makes no sense to give something its own global identifier.




Example 05: adding blank nodes to a Model
Example 05
 shows how we can add the address of Picasso to our Model:
 1// Create a bnode for the address
 2BNode address = Values.bnode();
 3
 4// First we do the same thing we did in example 02: create a new ModelBuilder, and add
 5// two statements about Picasso.
 6ModelBuilder builder = new ModelBuilder();
 7builder
 8    .setNamespace("ex", "http://example.org/")
 9    .subject("ex:Picasso")
10    .add(RDF.TYPE, "ex:Artist")
11    .add(FOAF.FIRST_NAME, "Pablo")
12    // this is where it becomes new: we add the address by linking the blank node
13    // to picasso via the `ex:homeAddress` property, and then adding facts _about_ the address
14    .add("ex:homeAddress", address) // link the blank node
15    .subject(address) // switch the subject
16    .add("ex:street", "31 Art Gallery")
17    .add("ex:city", "Madrid")
18    .add("ex:country", "Spain");
19
20Model model = builder.build();
21
22// To see what's in our model, let's just print it to the screen
23for (Statement st : model) {
24  System.out.println(st);
25}
Reading and Writing RDF
In the previous sections we saw how to print the contents of an RDF4J Model to the screen, However, this is of limited use: the format is not easy to read, and certainly not by any other tools that you may wish to share the information with.
Fortunately, RDF4J provides tools for reading and writing RDF models in several syntax formats, all of which are standardized. These syntax formats can be used to share data between applications. The most commonly used formats are RDF/XML, Turtle, and N-Triples.
Example 06: Writing to RDF/XML
Example 06
 shows how we can write our Model as RDF/XML, using the RDF4J Rio
 parser/writer tools:
Rio.write(model, System.out, RDFFormat.RDFXML);
The output will be similar to this:
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
  xmlns:ex="http://example.org/"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

<rdf:Description rdf:about="http://example.org/Picasso">
  <rdf:type rdf:resource="http://example.org/Artist"/>
  <firstName xmlns="http://xmlns.com/foaf/0.1/" rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Pablo</firstName>
  <ex:homeAddress rdf:nodeID="node1b4koa8edx1"/>
</rdf:Description>

<rdf:Description rdf:nodeID="node1b4koa8edx1">
  <ex:street rdf:datatype="http://www.w3.org/2001/XMLSchema#string">31 Art Gallery</ex:street>
  <ex:city rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Madrid</ex:city>
  <ex:country rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Spain</ex:country>
</rdf:Description>

</rdf:RDF>
The Rio.write method takes a java.io.OutputStream or a java.io.Writer as an argument, so if we wish to write to file instead of to the screen, we can simply use a FileOutputStream or a FileWriter and point it at the desired file location.
Example 07: Writing to Turtle and other formats
Example 07
 shows how we can write our Model in the Turtle
 syntax format:
Rio.write(model, System.out, RDFFormat.TURTLE);
To produce other syntax formats, simply vary the supplied RDFFormat. Try out a few different formats yourself, to get a feel for what they look like.
The output in Turtle format looks like this:
1@prefix ex: <http://example.org/> .
2
3ex:Picasso a ex:Artist ;
4        <http://xmlns.com/foaf/0.1/firstName> "Pablo" ;
5        ex:homeAddress _:node1b4koq381x1 .
6
7_:node1b4koq381x1 ex:street "31 Art Gallery" ;
8        ex:city "Madrid" ;
9        ex:country "Spain" .
If you compare this with the output of writing to RDF/XML, you will notice that the Turtle syntax format is a lot more compact, and also easier to read for a human. Let’s quickly go through it:
On the first line, a namespaces prefix is defined. It is one we recognize: the ex namespace that we added to our RDF model earlier. Turtle syntax supports using prefixed names to make the format more compact, and easier to read.
Lines 3-5 show three RDF statements, all about ex:Picasso.
The first statement, on line 3, says that Picasso is of type Artist. In Turtle, a is a shortcut for the rdf:type property. Notice that the line ends with a ;. This indicates that the next line in the file will be about the same subject.
Line 4 says that Picasso’s first name is “Pablo”. Notice that here the full IRI is used for the property - this happens because we didn’t set a namespace prefix for it when we created our model.

    
    
 In Turtle syntax, a full IRI always starts with < and ends with >. This makes them easy to distinguish from prefixed names, and from blank node identifiers.



Line 5, finally, states that Picasso has a homeAddress, which is some blank node (a blank node identifier in Turtle syntax always starts with _:). Note that this line ends with a ., which indicates that we are done stating facts about the current subject.
Line 7 and further, finally, state facts about the blank node (the home address of Picasso): its street is “31 Art Gallery”, its city is “Madrid”, and its Country is “Spain”.
Example 08: Reading a Turtle RDF file
Very similar to how we can write RDF models to files in various syntaxes, we can also use RDF4J Rio to read files to produce an RDF model.
Example 08
 shows how we can read a Turtle file and produce a Model object out of it:
1String filename = "example-data-artists.ttl";
2
3// read the file 'example-data-artists.ttl' as an InputStream.
4InputStream input = Example06ReadTurtle.class.getResourceAsStream("/" + filename);
5
6// Rio also accepts a java.io.Reader as input for the parser.
7Model model = Rio.parse(input, "", RDFFormat.TURTLE);
Accessing a Model
Now that we know how to create, read, and save an RDF Models, it is time to look at how we can access the information in a Model.
We have already seen one simple way of accessing a Model
: we can iterate over its contents using a for-each loop. The reason this works is that Model extends the Java Collection API, more particularly it is a java.util.Set<Statement>.
We have more sophisticated options at our disposal, however.
Example 09: filtering on a specific subject
Example 09
 shows how we can use Model.filter
 to “zoom in” on a specific subject in our model. We’re also using the opportunity to show how you can print out RDF statements in a slightly prettier way:
 1// We want to find all information about the artist `ex:VanGogh`.
 2IRI vanGogh = Values.iri("http://example.org/VanGogh");
 3
 4// By filtering on a specific subject we zoom in on the data that is about that subject.
 5// The filter method takes a subject, predicate, object (and optionally a named graph/context)
 6// argument. The more arguments we set to a value, the more specific the filter becomes.
 7Model aboutVanGogh = model.filter(vanGogh, null, null);
 8
 9// Iterate over the statements that are about Van Gogh
10for (Statement st : aboutVanGogh) {
11  // the subject will always be `ex:VanGogh`, an IRI, so we can safely cast it
12  IRI subject = (IRI) st.getSubject();
13  // the property predicate can be anything, but it's always an IRI
14  IRI predicate = st.getPredicate();
15
16  // the property value could be an IRI, a BNode, a Literal, or an RDF-star Triple. In RDF4J, Value is
17  // is the supertype of all possible kinds of RDF values.
18  Value object = st.getObject();
19
20  // let's print out the statement in a nice way. We ignore the namespaces and only print the
21  // local name of each IRI
22  System.out.print(subject.getLocalName() + " " + predicate.getLocalName() + " ");
23  if (object.isLiteral()) {
24    // it's a literal value. Let's print it out nicely, in quotes, and without any ugly
25    // datatype stuff
26    System.out.println("\"" + ((Literal) object).getLabel() + "\"");
27  } else if (object.isIRI()) {
28    // it's an IRI. Just print out the local part (without the namespace)
29    System.out.println(((IRI) object).getLocalName());
30  } else {
31    // it's a blank node or an RDF-star Triple. Just print it out as-is.
32    System.out.println(object);
33  }
34}
Example 10: Getting all property values for a resource
Example 10
 shows how we can directly get all values of a property, for a given resource, from the model. To simply retrieve all paintings by van Gogh, we can do this:
1Set<Value> paintings = model.filter(vanGogh, EX.CREATOR_OF, null).objects();

    
    
Notice that we are suddenly using a new vocabulary constant for our property: EX.CREATOR_OF. It is generally a good idea to create a class containing  constants for your own IRIs when you program with RDF4J: it makes it easier to reuse them and avoids introducing typos (not to mention a lot of hassle if you later decide to rename one of your resources). See the EX vocabulary class
 for an example of how to create your own vocabulary classes.



Once we have selected the values, we can iterate and do something with them. For example, we could try and retrieve further information about each value, like so:
 1for (Value painting: paintings) {
 2  if (painting instanceof Resource) {
 3    // our value is either an IRI or a blank node. Retrieve its properties and print.
 4    Model paintingProperties = model.filter((Resource)painting, null, null);
 5
 6    // write the info about this painting to the console in Turtle format
 7    System.out.println("--- information about painting: " + painting);
 8    Rio.write(paintingProperties, System.out, RDFFormat.TURTLE);
 9    System.out.println();
10  }
11}
The Model.filter method does not actually return a new Model object: it returns a filtered view of the original Model. This means that invoking filter is very cheap, because it doesn’t have to copy the contents into a new Collection. It also means that any modifications to the original Model object will show up in the filter result, and vice versa.
Example 11: Retrieving a single property value
Example 11
 shows how we can directly get a single value of a property, from the model. In this example, we retrieve the first name of each known artist, and print it to the console:
 1// iterate over all resources that are of type 'ex:Artist'
 2for (Resource artist : model.filter(null, RDF.TYPE, EX.ARTIST).subjects()) {
 3  // get all RDF triples that denote values for the `foaf:firstName` property
 4  // of the current artist
 5  Model firstNameTriples = model.filter(artist, FOAF.FIRST_NAME, null);
 6
 7  // Get the actual first name by just selecting any property value. If no value
 8  // can be found, set the first name to '(unknown)'.
 9  String firstName = Models.objectString(firstNameTriples).orElse("(unknown)");
10
11  System.out.println(artist + " has first name '" + firstName + "'");
12}
In this code example, we use two steps to retrieve the first name for each artist. The first step, on line 5, is that we use Model.filter again. This zooms in to select only the foaf:firstName statements about the current artist (notice that I say statements, plural: there could very well be an artist with more than one first name).
For the second step, the actual selection of a single property value, we use the Models
 utility. This class provides several useful shortcuts for working with data in a model. In this example, we are using the objectString method. What this method does is retrieve an arbitrary object-value from the supplied model, and return it converted to a String. Since the model we supply only contains foaf:firstName statements about the current artist, we know that the object we get back will be a first name of the current artist.
NOTE: The Models utility methods for selecting single values, such as Models.objectString, return any one arbitrary suitable value: if there is more than one possible object value in the supplied model, it just picks one. There is no guarantee that it will always pick the same value on consecutive calls.
Named Graphs and Contexts
As we have seen, the RDF data model can be viewed as a graph. Sometimes it is useful to group together sets of RDF data as separate graphs. For example, you may want to use several files together, but still keep track of which statements come from which file. An RDF4J Model
 facilitates this by having an optional context parameter for most of it methods. This parameter allows you to identify a named graph in the Model, that is a subset of the complete model. In this section, we will look at some examples of this mechanism in action.
Example 12: Adding statements to two named graphs
Example 12
 shows how we can add information to separate named graphs in a single Model, and using that named graph information to retrieve those subsets again:
 1// We'll use a ModelBuilder to create two named graphs, one containing data about
 2// Picasso, the other about Van Gogh.
 3ModelBuilder builder = new ModelBuilder();
 4builder.setNamespace("ex", "http://example.org/");
 5
 6// In named graph 1, we add info about Picasso
 7builder.namedGraph("ex:namedGraph1")
 8    .subject("ex:Picasso")
 9      .add(RDF.TYPE, EX.ARTIST)
10      .add(FOAF.FIRST_NAME, "Pablo");
11
12// In named graph 2, we add info about Van Gogh.
13builder.namedGraph("ex:namedGraph2")
14  .subject("ex:VanGogh")
15    .add(RDF.TYPE, EX.ARTIST)
16    .add(FOAF.FIRST_NAME, "Vincent");
17
18
19// We're done building, create our Model
20Model model = builder.build();
21
22// Each named graph is stored as a separate context in our Model
23for (Resource context: model.contexts()) {
24  System.out.println("Named graph " + context + " contains: ");
25
26  // write _only_ the statemements in the current named graph to the console,
27  // in N-Triples format
28  Rio.write(model.filter(null, null, null, context), System.out, RDFFormat.NTRIPLES);
29  System.out.println();
30}
On line 7 (and 13, respectively), you can see how ModelBuilder
 can add statements to a specific named graph using the namedGraph method. Similarly to how the subject method defines what subject each added statement is about (until we set a new subject), namedGraph defines what named graph (or ‘context’) each statement is added to, until either a new named graph is set, or the state is reset using the defaultGraph method.
On lines 23 and further, you can see two examples of how this information can be accessed from the resulting Model. You can explicitly retrieve all available contexts (line 23). You can also use a context identifier as a parameter for the filter method, as shown on line 28.
Databases and SPARQL querying
When RDF models grow larger and more complex, simply keeping all the data in an in-memory collection is no longer an option: large amounts of data will simply not fit, and querying the data will require more sophisticated indexing mechanisms. Moreover, data consistency ensurance mechanisms (transactions, etc) will be necessary. In short: you need a database.
RDF4J has a standardized access API for RDF databases, called the Repository API. This API provides all the things we need from a database: a sophisticated transaction handling mechanism, controls to work efficiently with high data volumes, and, perhaps most importantly: support for querying your data using the SPARQL query language.
In this part of the tutorial, we will show the basics of how to use the Repository API and execute some simple SPARQL queries over your RDF data. Explaining SPARQL or the Repository API in detail is out of scope, however. For more details on how to use the Repository API, have a look at Programming with RDF4J.
Example 13: Adding an RDF Model to a database
Example 13
 shows how we can add our RDF Model to a database:
 1// First load our RDF file as a Model.
 2String filename = "example-data-artists.ttl";
 3InputStream input = Example11AddRDFToDatabase.class.getResourceAsStream("/" + filename);
 4Model model = Rio.parse(input, "", RDFFormat.TURTLE);
 5
 6// Create a new Repository. Here, we choose a database implementation
 7// that simply stores everything in main memory.
 8Repository db = new SailRepository(new MemoryStore());
 9
10// Open a connection to the database
11try (RepositoryConnection conn = db.getConnection()) {
12  // add the model
13  conn.add(model);
14
15  // let's check that our data is actually in the database
16  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
17    for (Statement st: result) {
18      System.out.println("db contains: " + st);
19    }
20  }
21}
22finally {
23  // before our program exits, make sure the database is properly shut down.
24  db.shutDown();
25}
In this code example (line 8), we simply create a new Repository
 on the fly. We use a SailRepository
 as the implementing class of the Repository interface, which takes a database implementation (known in RDF4J as a SAIL - “Storage and Inferencing Layer”) as its constructor. In this case, we use a simple in-memory database implementation.

    
    
RDF4J itself provides several database implementations, and many third parties provide full connectivity for their own RDF database to work with the RDF4J APIs. See this list of third-party databases. For more detailed information on how to create and maintain databases, see Programming with RDF4J.



Once we have created and initialized our database, we open a RepositoryConnection
 to it (line 11). This connection is an AutoCloseable resource that offers all sorts of methods for executing commands on the database: adding and removing data, querying, starting transactions, and so on.
Example 14: load a file directly into a database
In the code example in the previous section, we first loaded an RDF file into a Model object, and then we added that Model object to our database. This works fine for smaller files, but as data gets larger, you really don’t want to have to load it completely in main memory before storing it in your database.
Example 14
 shows how we can add our RDF data to a database directly, without first creating a Model:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example14AddRDFToDatabase.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // let's check that our data is actually in the database
13  try (RepositoryResult<Statement> result = conn.getStatements(null, null, null)) {
14    for (Statement st : result) {
15      System.out.println("db contains: " + st);
16    }
17  }
18} finally {
19  // before our program exits, make sure the database is properly shut down.
20  db.shutDown();
21}
The main difference with the previous example is on lines 7-11: we still open an InputStream to access our RDF file, but we now provide that stream directly to the Repository, which then takes care of reading the file and adding the data without the need to keep the fully processed model in main memory.
Example 15: SPARQL SELECT Queries
Example 15
 shows how, once we have added data to our database, we can execute a simple SPARQL SELECT-query:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6  String filename = "example-data-artists.ttl";
 7  try (InputStream input = Example15SimpleSPARQLQuery.class.getResourceAsStream("/" + filename)) {
 8    // add the RDF data from the inputstream directly to our database
 9    conn.add(input, "", RDFFormat.TURTLE);
10  }
11
12  // We do a simple SPARQL SELECT-query that retrieves all resources of type `ex:Artist`,
13  // and their first names.
14  String queryString = "PREFIX ex: <http://example.org/> \n";
15  queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
16  queryString += "SELECT ?s ?n \n";
17  queryString += "WHERE { \n";
18  queryString += "    ?s a ex:Artist; \n";
19  queryString += "       foaf:firstName ?n .";
20  queryString += "}";
21
22  TupleQuery query = conn.prepareTupleQuery(queryString);
23
24  // A QueryResult is also an AutoCloseable resource, so make sure it gets closed when done.
25  try (TupleQueryResult result = query.evaluate()) {
26    // we just iterate over all solutions in the result...
27    for (BindingSet solution : result) {
28      // ... and print out the value of the variable binding for ?s and ?n
29      System.out.println("?s = " + solution.getValue("s"));
30      System.out.println("?n = " + solution.getValue("n"));
31    }
32  }
33} finally {
34  // Before our program exits, make sure the database is properly shut down.
35  db.shutDown();
36}
On lines 15-21, we define our SPARQL query string, and on line 22 we turn this into a prepared Query
 object. We are using a SPARQL SELECT-query, which will return a result consisting of tuples of variable-bindings (each tuple containing a binding for each variable in the SELECT-clause). Hence, RDF4J calls the constructed query a TupleQuery
, and the result of the query a TupleQueryResult
. Lines 26-34 is where the actual work gets done: on line 25, the query is evaluated, returning a result object. RDF4J QueryResult objects execute lazily: the actual data is not retrieved from the database until we start iterating over the result (as we do on lines 27-33). On line 27 we grab the next solution from the result, which is a BindingSet
. You can think about a BindingSet as being similar to a row in a table (the binding names are the columns, the binding values the value for each column in this particular row). We then grab the value of the binding of variable ?s (line 30) and ?n (line 31) and print them out.
There are a number of variations possible on how you execute a query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 13 in your own editor and executing the modified code, to see what happens.
One variation is that we can materialize the TupleQueryResult iterator into a simple java List, containing the entire query result:
1List<BindingSet> result = QueryResults.asList(query.evaluate());
2for (BindingSet solution: result) {
3     System.out.println("?s = " + solution.getValue("s"));
4     System.out.println("?n = " + solution.getValue("n"));
5}
On line 1, we turn the result of the query into a List using the QueryResults
 utility. This utility reads the result completely and also takes care of closing the result (even in case of errors), so there is no need to use a try-with-resources clause in this variation.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a TupleQueryResultHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in tab-separated-values (TSV) format:
1TupleQueryResultHandler tsvWriter = new SPARQLResultsTSVWriter(System.out);
2query.evaluate(tsvWriter);
Example 16: SPARQL CONSTRUCT Queries
Another type of SPARQL query is the CONSTRUCT-query: instead of returning the result as a sequence of variable bindings, CONSTRUCT-queries return RDF statements. CONSTRUCT queries are very useful for quickly retrieving data subsets from an RDF database, and for transforming that data.
Example 16
 shows how we can execute a SPARQL CONSTRUCT query in RDF4J. As you can see, most of the code is quite similar to previous examples:
 1// Create a new Repository.
 2Repository db = new SailRepository(new MemoryStore());
 3
 4// Open a connection to the database
 5try (RepositoryConnection conn = db.getConnection()) {
 6    String filename = "example-data-artists.ttl";
 7    try (InputStream input =
 8      Example14SPARQLConstructQuery.class.getResourceAsStream("/" + filename)) {
 9      // add the RDF data from the inputstream directly to our database
10      conn.add(input, "", RDFFormat.TURTLE );
11    }
12
13    // We do a simple SPARQL CONSTRUCT-query that retrieves all statements
14    // about artists, and their first names.
15    String queryString = "PREFIX ex: <http://example.org/> \n";
16    queryString += "PREFIX foaf: <" + FOAF.NAMESPACE + "> \n";
17    queryString += "CONSTRUCT \n";
18    queryString += "WHERE { \n";
19    queryString += "    ?s a ex:Artist; \n";
20    queryString += "       foaf:firstName ?n .";
21    queryString += "}";
22
23    GraphQuery query = conn.prepareGraphQuery(queryString);
24
25    // A QueryResult is also an AutoCloseable resource, so make sure it gets
26    // closed when done.
27    try (GraphQueryResult result = query.evaluate()) {
28  // we just iterate over all solutions in the result...
29  for (Statement st: result) {
30      // ... and print them out
31      System.out.println(st);
32  }
33    }
34}
35finally {
36    // Before our program exits, make sure the database is properly shut down.
37    db.shutDown();
38}
On lines 15-21 we create our SPARQL CONSTRUCT-query. The only real difference is line 17, where we use a CONSTRUCT-clause (instead of the SELECT-clause we saw previously). Line 23 turns the query string into a prepared Query object. Since the result of a CONSTRUCT-query is a set of RDF statements (in other words: a graph), RDF4J calls such a query a GraphQuery
, and its result a GraphQueryResult
.
On line 27 and further we execute the query and iterate over the result. The main difference with previous examples is that this time, the individual solutions in the result are Statements
.
As with SELECT-queries, there are a number of variations on how you execute a CONSTRUCT-query and process the result. We’ll show some of these variations here, and we recommend that you try them out by modifying code example 14 in your own editor and executing the modified code, to see what happens.
One variation is that we can turn the GraphQueryResult iterator into a Model, containing the entire query result:
1Model result = QueryResults.asModel(query.evaluate());
2for (Statement st: result) {
3     System.out.println(st);
4}
In this particular example, we then iterate over this model to print out the Statements, but obviously we can access the information in this Model in the same ways we have already seen in previous sections.
Another variation is that instead of retrieving the query result as an iterator object, we let the query send its result directly to a RDFHandler
. This is a useful way to directly stream a query result to a file on disk. Here, we show how to use this mechanism to write the result to the console in Turtle format
1RDFHandler turtleWriter = Rio.createWriter(RDFFormat.TURTLE, System.out);
2query.evaluate(turtleWriter);
Further reading
You should now have a basic understanding of the RDF data model, and have a decent grasp on how you can use RDF4J to read, write, create, store, and query RDF data. For more information on how to use RDF4J, we recommend the following sources:

Programming with RDF4J - an extensive guide to using the RDF4J framework from Java, covering basics and more advanced configurations.
RDF4J API JavaDoc - the complete API reference. Pay particular attention to the various util packages scattered throughout the API, these often contain very useful helper classes and utilities.
Getting Started with RDF4J, Maven and Eclipse - a tutorial on how to set up your first RDF4J-based project with the help of Apache Maven and the Eclipse IDE.

For more detailed information about RDF, and SPARQL, consult the following sources:


The W3C RDF 1.1 Primer introduces the basic concepts of RDF and shows concrete examples of the use of RDF.


The W3C SPARQL 1.1 Query Language Recommendation is the normative specification of the SPARQL Query Language. It contains a complete overview of all SPARQL operators and capabilities, including many useful examples.


The W3C SPARQL 1.1 Update Recommendation is the normative specification for SPARQL Update operations. SPARQL Update allows you to insert, modify, delete, copy and move RDF data.


If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    Introducing RDF
      
        IRIs, namespaces, and prefixed names
        Creating and reusing IRIs
      
    
    Using RDF4J to create RDF models
      
        Example 01: building a simple Model
        Example 02: using the ModelBuilder
      
    
    Literal values: datatypes and language tags
      
        Example 03: adding a date and a number
        Example 04: adding an artwork’s title in Dutch and English
      
    
    Blank nodes
      
        Example 05: adding blank nodes to a Model
      
    
    Reading and Writing RDF
      
        Example 06: Writing to RDF/XML
        Example 07: Writing to Turtle and other formats
        Example 08: Reading a Turtle RDF file
      
    
    Accessing a Model
      
        Example 09: filtering on a specific subject
        Example 10: Getting all property values for a resource
        Example 11: Retrieving a single property value
      
    
    Named Graphs and Contexts
      
        Example 12: Adding statements to two named graphs
      
    
    Databases and SPARQL querying
      
        Example 13: Adding an RDF Model to a database
        Example 14: load a file directly into a database
        Example 15: SPARQL SELECT Queries
        Example 16: SPARQL CONSTRUCT Queries
      
    
    Further reading\n\n\n\nStarting a New Maven Project in Eclipse
    

  
  If you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.

    
    
You don't have to use Apache Maven or Eclipse IDE if you want to work with RDF4J. These are simply very useful tools for quickly getting a Java project started. Maven is good because it allows you to just define which libraries you want to use and never worry about any further third-party libraries you might need, and Eclipse IDE is good because it has good integration with Maven, code completion features, and is just generally a great Java development environment. But you can work with RDF4J just as well in a different build tool or IDE.



In this tutorial, we assume that you have a basic understanding of programming in Java, and have at least an inkling of what RDF is. However, we do not assume that you know how to use either RDF4J, Maven, or Eclipse, so we’ll go through it all one step at a time. If anything is already sufficiently familiar to you, you are of course free to skip ahead!
Setting up your environment
Before we kick off, you will need to install the Eclipse IDE. In this tutorial, we will use Eclipse for Java Developers version 4.18.0 (2020-12), but it shouldn’t matter too much which exact version you have installed. Select either the “Eclipse for Java developers” or “Eclipse for Java EE developers” installation option.
Eclipse IDE comes with a Maven plugin (called m2e) already installed. We will use this plugin to work with Maven. You are of course free to also install the Maven command line tools, but we won’t be using those directly in this tutorial.
When starting Eclipse for the first time, you will be asked for a workspace directory – this will be where Eclipse stores all project data as well as some configuration information. Feel free to pick/create a directory of your choice, or just accept the default.
Once Eclipse is started (and any welcome messages have been closed), you will be presented with a screen that should look roughly like this:

Tweaking Eclipse preferences
Before we start creating our actual project in Eclipse, there are a few preferences that we should tweak. This step is not required, but it will make things a little easier in the rest of this tutorial.
From the menu, select Eclipse -> Preferences. The Preferences dialog pops up. On the left, select Maven. You will see the Maven configuration settings. The options “Download Artifact Sources” and “Download Artifact JavaDoc” are unchecked by default. Check them both, then click OK.

The reason we want this, by the way, is that having either the sources or the Javadoc available really helps when you use code autocompletion in Eclipse – Eclipse will automatically read the Javadoc and provide the documentation in a little tooltip. As said: not required, but very useful.
Creating a new project
Now that we’re all set up, we can kick things off by creating a new project. Select the File menu, then New -> New Project... . A dialog will appear. In this dialog, select the option Maven Project:

Once you click Next, you will be presented with a screen to create a new Maven Project. Make you sure the option ‘Create a simple project’ is checked:

Click Next again. In the following screen you define further details of your Maven project, such as group and artifact ids, version number, and so on. For this tutorial,  you only have to fill in three fields:

group id – typically you use something like a Java package name for this. We will use org.example.
artifact id –  name of the maven artifact if you publish our project as one. We will use rdf4j-getting-started.
name – the project name. We will use HelloRDF4J .


Once this has been filled in you can click Finish. We now have our first Eclipse project, huzzah!
Defining the POM
So we have a project. That’s great, but it’s still empty: we haven’t added any code or necessary libraries (such as the RDF4J libraries) yet.
We will remedy the library-situation first. In Maven, requirements for libraries (we’re talking about jar files here) are specified in a special project configuration file called pom.xml, often referred to as “the POM”. Open your project and you will see that a file with that name is already present. Doubleclick it to open it in the POM editor:

As you can see, the POM already contains some information – in fact the information that we added when we created our project. However, we need to add more information about our project. Specifically, we want to add which external Java libraries we want to include in our project. In Maven, you do this by specifying dependencies.
The first dependency we will add is for RDF4J itself. RDF4J is not a single library, but consists of a large collection of separate modules, allowing you to pick and choose which functionality you need. To make handling of all the various modules and the dependencies they all need easier, we will first add a “Bill Of Materials” (BOM) dependency. The easiest way to do this is to edit the XML source code directly. Switch to the ‘pom.xml’ tab of the POM editor, and add the following XML fragment:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.6.0</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
It should look like this after you’ve added this and saved the file:

Next we need to add the actual RDF4J modules we want to use. As said, this is where you could pick and choose the specific modules (parsers, databases, etc) that you want. For this tutorial, however, we’ll be lazy and just add the entire core framework. We do this by adding the rdf4j-storage dependency, as follows:
<dependencies>
  <dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
  </dependency>
</dependencies>
Notice that because we already have a BOM added, we don’t need to specify a version number for this dependency. Once you have done this and save the file, Eclipse will automatically begin downloading the RDF4J libraries. This may take a little while, just let it do its thing. Once it’s complete, your POM should look like this:

We are going to add one more dependency, namely for a logging framework. RDF4J uses the SLF4J logging API, which requires a logging implementation, so we’ll provide one, in this case slf4j-simple (which is, as the name implies, a very simple logging library that by default just logs to the console). Add a new dependency, with group id org.slf4j, artifact id slf4j-simple. Again we don’t need to specify a version number: the RDF4J Bill of Materials already specifies which version of slf4j is compatible with RDF4J:
<dependency>
  <groupId>org.slf4j</groupId>
  <artifactId>slf4j-simple</artifactId>
  <scope>runtime</scope>
</dependency>
Once you have saved your changes to the POM, your project should be automatically refreshed by Eclipse again, and you should see a new section called ‘Maven dependencies’ in the Package Explorer (If your project does not automatically refresh and you don’t see the above section, right-click on your project in the Package Explorer, and select Maven -> Update Project...). This section, when opened, shows a list of jar files which can now be used in your project:

As you can see, this list contains not just all the RDF4J libraries, but also a lot of other libraries that RDF4J depends on.

    
    
If you are interested in having more control over which libraries are and aren't included, please see Setting up your development environment.



Configuring the Java compiler
Unfortunately the default Maven archetype that we used to create our new project uses a very old Java compiler (1.5) by default. Since RDF4J requires Java 8 at a minimum (we actually recommend Java 11 for better performance), we will need to change this.
Copy-paste (or type if you prefer) the following section into the xml file (put it just above the dependencyManagement section we added in earlier). This will set the version to Java 11.
<properties>
  <maven.compiler.source>11</maven.compiler.source>
  <maven.compiler.target>11</maven.compiler.target>
</properties>
Once you have done this, and saved your POM, you will need to manually update your Maven project for Eclipse to accept the changes. You do this by right-clicking on your project (in the project explorer), and then selecting ‘Maven’ -> ‘Update Project…':

Programming our first Semantic Web application
We’re done preparing, we can start programming! Right-click on src/main/java and select New -> Class. In the dialog shown, set the package name to org.example, the class name to HelloRDF4J, and make sure the option public static void main (String[] args) is checked:

Click ‘Finish’ and Eclipse will create the new class and automatically open it in an editor.
Since we will want to work with RDF data, we will start by creating something to keep that RDF data in. In RDF4J, RDF data is usually stored in a Repository. There many different types of Repository, both for local access as well as for accessing remote services. In this tutorial, we will create a simple local repository, namely one that uses an in-memory store, without any sort of persistent storage functionality (so the data will be lost when the program exits). Add the following code to your main method:
Repository rep = new SailRepository(new MemoryStore());
Once you have done this and saved the file, you will notice some red lines appearing:

This is Eclipse telling you that there is something wrong with your code. In this case, the problem is that several import statements are missing (note that Eclipse tells you what is wrong in detail as well in the ‘Problems’ tab underneath the editor). We’ll need to add those import statements. You can add each import manually of course, but luckily Eclipse has a shortcut. Hit Ctrl+Shift+O and Eclipse should automatically resolve all missing imports for you. It will pop up a dialog if there is more than one possibility for a particular import, which will like happen for the Repository interface. Just pick the one from org.eclipse.rdf4j:

Hit ‘Finish’, then save your code.
Now that we have created our repository, we are going to put some data in it, and then get that data out again and print it to the console.
Adding data can be done in numerous ways. In this tutorial, we will be creating a few RDF statements directly in Java (rather than, say, loading them from a file). To do this, we need a namespace that we can use to create any new IRIs we need. With this namespace, we can create a new IRI. We will create an identifier for a person called “John”, about whom we will later add some data to our repository:
Namespace ex = Values.namespace("ex", "http://example.org/");
IRI john = Values.iri(ex, "john");
We use a static factory(the Values class) to easily create new Namespaces, IRIs and other types of values.
We have now created a IRI, but have not yet added any data to our repository. To do this, we first need to open a RepositoryConnection. Such a connection allows us to add, remove, and retrieve data from our Repository. We do this as follows:
RepositoryConnection conn = rep.getConnection();
It is important to keep track of open connections and to close them again when we are finished with them, to free up any resources the connection may keep hold of. RDF4J connections are “AutoCloseable”, which means we can let Java handle the closing of the connection when we’re done with it, rather than having to deal with this ourselves. We use a try-with-resources construction for this, like so:
try (RepositoryConnection conn = rep.getConnection()) {
}
Your code should now look as follows:

Using the connection, we can start adding statements to our repository. We will add two triples, one to assert that John is a Person, and one to assert that John’s name is “John”:
conn.add(john, RDF.TYPE, FOAF.PERSON);
conn.add(john, RDFS.LABEL, Values.literal("John"));
Note how for well-known RDF vocabularies, such as RDF, RDFS, and FOAF, RDF4J provides constants that you can easily reuse to create/read data with. Also, you see another use of the Values static factory here, this time to create a Literal object.
Once we have added our data, we can retrieve it back from the repository again. You can do this in several ways, with a SPARQL query or using the RepositoryConnection API methods directly. Here we will use the latter:
RepositoryResult<Statement> statements = conn.getStatements(null, null, null);
We use the getStatements() method to retrieve data from the repository. The three arguments (all null) are for the subject, predicate, and object we wish to match. Since they are all set to null, this will retrieve all statements.
The object returned by getStatements() is a RepositoryResult, which implements both Iteration and Iterable: it allows you to process the result in a streaming fashion, using its hasNext() and next() methods, as well as using Java’s for-each loop to iterate over it. Almost all the methods that retrieve data from a Repository return such a streaming/iterating object: they are very useful for processsing very large result sets, because they do not require that the entire result is kept in memory all at the same time.
It is important to always call close() on this kind of result object when you are done with them, to avoid memory leaks and other problems. Since the RepositoryResult is also a AutoCloseable, you can use a try-with-resources construction (similar to what we used for opening the connection) to handle this.
However, since our Repository only contains 2 statements, we can safely just transfer the entire result into a Java Collection. RDF4J has the Model interface for this, which is an extension of the standard Java collections that has some special tricks up its sleeve for dealing with RDF data, but which you can also use in the same manner as a Set or a List. We convert our result to a Model as follows:
Model model = QueryResults.asModel(statements);
The QueryResults utility retrieves all the statements from the supplied result, and puts them in a Model. It also automatically closes the result object for you.
Finally, we want to print out our data to the console. This too is something you can do in numerous ways, but let’s just say we would like to print out our data in Turtle format. Here’s how:
Rio.write(model, System.out, RDFFormat.TURTLE);
Rio (which stands for “RDF I/O”) is the RDF4J parser/writer toolkit. We can just pass it our model, specify the outputstream, and the format in which we’d like it written, and Rio does the rest. Your code should now look like this:

You now have created your first Semantic Web application! After saving, just right-click on HelloRDF4J.java (in the project explorer) and select Run as -> Java application. You will see the following output in the Console:
<http://example.org/john> a <http://xmlns.com/foaf/0.1/Person> ;
         <http://www.w3.org/2000/01/rdf-schema#label> "John" .
As you can see, it contains exactly the two statements that we added earlier, in Turtle syntax.
However, it is a bit ugly, using all those long IRIs. We can make the output a bit prettier, by adding some namespace definitions to our Model:
model.setNamespace(RDF.NS);
model.setNamespace(RDFS.NS);
model.setNamespace(FOAF.NS);
model.setNamespace(ex);
Add these lines to your code, directly after where you have created the model. Then run your program again. You will now see the following:

That’s it! Obviously there is still loads to learn about how to use RDF4J effectively, but you’ve got the basics under control now: you can set up a new project using RDF4J,  and have seen some basic ways to add, write, and retrieve data. The rest is up to you. Good sources of further documentation are the Getting Started tutorial, Programming with RDF4J, and of course the API Javadoc.

  

     
      
        
          

  Table of Contents

  
  
    Setting up your environment
      
        Tweaking Eclipse preferences
      
    
    Creating a new project
    Defining the POM
    Configuring the Java compiler
    Programming our first Semantic Web application\n\n\n\nStarting a New Maven Project in Eclipse
    

  
  If you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.

    
    
You don't have to use Apache Maven or Eclipse IDE if you want to work with RDF4J. These are simply very useful tools for quickly getting a Java project started. Maven is good because it allows you to just define which libraries you want to use and never worry about any further third-party libraries you might need, and Eclipse IDE is good because it has good integration with Maven, code completion features, and is just generally a great Java development environment. But you can work with RDF4J just as well in a different build tool or IDE.



In this tutorial, we assume that you have a basic understanding of programming in Java, and have at least an inkling of what RDF is. However, we do not assume that you know how to use either RDF4J, Maven, or Eclipse, so we’ll go through it all one step at a time. If anything is already sufficiently familiar to you, you are of course free to skip ahead!
Setting up your environment
Before we kick off, you will need to install the Eclipse IDE. In this tutorial, we will use Eclipse for Java Developers version 4.18.0 (2020-12), but it shouldn’t matter too much which exact version you have installed. Select either the “Eclipse for Java developers” or “Eclipse for Java EE developers” installation option.
Eclipse IDE comes with a Maven plugin (called m2e) already installed. We will use this plugin to work with Maven. You are of course free to also install the Maven command line tools, but we won’t be using those directly in this tutorial.
When starting Eclipse for the first time, you will be asked for a workspace directory – this will be where Eclipse stores all project data as well as some configuration information. Feel free to pick/create a directory of your choice, or just accept the default.
Once Eclipse is started (and any welcome messages have been closed), you will be presented with a screen that should look roughly like this:

Tweaking Eclipse preferences
Before we start creating our actual project in Eclipse, there are a few preferences that we should tweak. This step is not required, but it will make things a little easier in the rest of this tutorial.
From the menu, select Eclipse -> Preferences. The Preferences dialog pops up. On the left, select Maven. You will see the Maven configuration settings. The options “Download Artifact Sources” and “Download Artifact JavaDoc” are unchecked by default. Check them both, then click OK.

The reason we want this, by the way, is that having either the sources or the Javadoc available really helps when you use code autocompletion in Eclipse – Eclipse will automatically read the Javadoc and provide the documentation in a little tooltip. As said: not required, but very useful.
Creating a new project
Now that we’re all set up, we can kick things off by creating a new project. Select the File menu, then New -> New Project... . A dialog will appear. In this dialog, select the option Maven Project:

Once you click Next, you will be presented with a screen to create a new Maven Project. Make you sure the option ‘Create a simple project’ is checked:

Click Next again. In the following screen you define further details of your Maven project, such as group and artifact ids, version number, and so on. For this tutorial,  you only have to fill in three fields:

group id – typically you use something like a Java package name for this. We will use org.example.
artifact id –  name of the maven artifact if you publish our project as one. We will use rdf4j-getting-started.
name – the project name. We will use HelloRDF4J .


Once this has been filled in you can click Finish. We now have our first Eclipse project, huzzah!
Defining the POM
So we have a project. That’s great, but it’s still empty: we haven’t added any code or necessary libraries (such as the RDF4J libraries) yet.
We will remedy the library-situation first. In Maven, requirements for libraries (we’re talking about jar files here) are specified in a special project configuration file called pom.xml, often referred to as “the POM”. Open your project and you will see that a file with that name is already present. Doubleclick it to open it in the POM editor:

As you can see, the POM already contains some information – in fact the information that we added when we created our project. However, we need to add more information about our project. Specifically, we want to add which external Java libraries we want to include in our project. In Maven, you do this by specifying dependencies.
The first dependency we will add is for RDF4J itself. RDF4J is not a single library, but consists of a large collection of separate modules, allowing you to pick and choose which functionality you need. To make handling of all the various modules and the dependencies they all need easier, we will first add a “Bill Of Materials” (BOM) dependency. The easiest way to do this is to edit the XML source code directly. Switch to the ‘pom.xml’ tab of the POM editor, and add the following XML fragment:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.6.0</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
It should look like this after you’ve added this and saved the file:

Next we need to add the actual RDF4J modules we want to use. As said, this is where you could pick and choose the specific modules (parsers, databases, etc) that you want. For this tutorial, however, we’ll be lazy and just add the entire core framework. We do this by adding the rdf4j-storage dependency, as follows:
<dependencies>
  <dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
  </dependency>
</dependencies>
Notice that because we already have a BOM added, we don’t need to specify a version number for this dependency. Once you have done this and save the file, Eclipse will automatically begin downloading the RDF4J libraries. This may take a little while, just let it do its thing. Once it’s complete, your POM should look like this:

We are going to add one more dependency, namely for a logging framework. RDF4J uses the SLF4J logging API, which requires a logging implementation, so we’ll provide one, in this case slf4j-simple (which is, as the name implies, a very simple logging library that by default just logs to the console). Add a new dependency, with group id org.slf4j, artifact id slf4j-simple. Again we don’t need to specify a version number: the RDF4J Bill of Materials already specifies which version of slf4j is compatible with RDF4J:
<dependency>
  <groupId>org.slf4j</groupId>
  <artifactId>slf4j-simple</artifactId>
  <scope>runtime</scope>
</dependency>
Once you have saved your changes to the POM, your project should be automatically refreshed by Eclipse again, and you should see a new section called ‘Maven dependencies’ in the Package Explorer (If your project does not automatically refresh and you don’t see the above section, right-click on your project in the Package Explorer, and select Maven -> Update Project...). This section, when opened, shows a list of jar files which can now be used in your project:

As you can see, this list contains not just all the RDF4J libraries, but also a lot of other libraries that RDF4J depends on.

    
    
If you are interested in having more control over which libraries are and aren't included, please see Setting up your development environment.



Configuring the Java compiler
Unfortunately the default Maven archetype that we used to create our new project uses a very old Java compiler (1.5) by default. Since RDF4J requires Java 8 at a minimum (we actually recommend Java 11 for better performance), we will need to change this.
Copy-paste (or type if you prefer) the following section into the xml file (put it just above the dependencyManagement section we added in earlier). This will set the version to Java 11.
<properties>
  <maven.compiler.source>11</maven.compiler.source>
  <maven.compiler.target>11</maven.compiler.target>
</properties>
Once you have done this, and saved your POM, you will need to manually update your Maven project for Eclipse to accept the changes. You do this by right-clicking on your project (in the project explorer), and then selecting ‘Maven’ -> ‘Update Project…':

Programming our first Semantic Web application
We’re done preparing, we can start programming! Right-click on src/main/java and select New -> Class. In the dialog shown, set the package name to org.example, the class name to HelloRDF4J, and make sure the option public static void main (String[] args) is checked:

Click ‘Finish’ and Eclipse will create the new class and automatically open it in an editor.
Since we will want to work with RDF data, we will start by creating something to keep that RDF data in. In RDF4J, RDF data is usually stored in a Repository. There many different types of Repository, both for local access as well as for accessing remote services. In this tutorial, we will create a simple local repository, namely one that uses an in-memory store, without any sort of persistent storage functionality (so the data will be lost when the program exits). Add the following code to your main method:
Repository rep = new SailRepository(new MemoryStore());
Once you have done this and saved the file, you will notice some red lines appearing:

This is Eclipse telling you that there is something wrong with your code. In this case, the problem is that several import statements are missing (note that Eclipse tells you what is wrong in detail as well in the ‘Problems’ tab underneath the editor). We’ll need to add those import statements. You can add each import manually of course, but luckily Eclipse has a shortcut. Hit Ctrl+Shift+O and Eclipse should automatically resolve all missing imports for you. It will pop up a dialog if there is more than one possibility for a particular import, which will like happen for the Repository interface. Just pick the one from org.eclipse.rdf4j:

Hit ‘Finish’, then save your code.
Now that we have created our repository, we are going to put some data in it, and then get that data out again and print it to the console.
Adding data can be done in numerous ways. In this tutorial, we will be creating a few RDF statements directly in Java (rather than, say, loading them from a file). To do this, we need a namespace that we can use to create any new IRIs we need. With this namespace, we can create a new IRI. We will create an identifier for a person called “John”, about whom we will later add some data to our repository:
Namespace ex = Values.namespace("ex", "http://example.org/");
IRI john = Values.iri(ex, "john");
We use a static factory(the Values class) to easily create new Namespaces, IRIs and other types of values.
We have now created a IRI, but have not yet added any data to our repository. To do this, we first need to open a RepositoryConnection. Such a connection allows us to add, remove, and retrieve data from our Repository. We do this as follows:
RepositoryConnection conn = rep.getConnection();
It is important to keep track of open connections and to close them again when we are finished with them, to free up any resources the connection may keep hold of. RDF4J connections are “AutoCloseable”, which means we can let Java handle the closing of the connection when we’re done with it, rather than having to deal with this ourselves. We use a try-with-resources construction for this, like so:
try (RepositoryConnection conn = rep.getConnection()) {
}
Your code should now look as follows:

Using the connection, we can start adding statements to our repository. We will add two triples, one to assert that John is a Person, and one to assert that John’s name is “John”:
conn.add(john, RDF.TYPE, FOAF.PERSON);
conn.add(john, RDFS.LABEL, Values.literal("John"));
Note how for well-known RDF vocabularies, such as RDF, RDFS, and FOAF, RDF4J provides constants that you can easily reuse to create/read data with. Also, you see another use of the Values static factory here, this time to create a Literal object.
Once we have added our data, we can retrieve it back from the repository again. You can do this in several ways, with a SPARQL query or using the RepositoryConnection API methods directly. Here we will use the latter:
RepositoryResult<Statement> statements = conn.getStatements(null, null, null);
We use the getStatements() method to retrieve data from the repository. The three arguments (all null) are for the subject, predicate, and object we wish to match. Since they are all set to null, this will retrieve all statements.
The object returned by getStatements() is a RepositoryResult, which implements both Iteration and Iterable: it allows you to process the result in a streaming fashion, using its hasNext() and next() methods, as well as using Java’s for-each loop to iterate over it. Almost all the methods that retrieve data from a Repository return such a streaming/iterating object: they are very useful for processsing very large result sets, because they do not require that the entire result is kept in memory all at the same time.
It is important to always call close() on this kind of result object when you are done with them, to avoid memory leaks and other problems. Since the RepositoryResult is also a AutoCloseable, you can use a try-with-resources construction (similar to what we used for opening the connection) to handle this.
However, since our Repository only contains 2 statements, we can safely just transfer the entire result into a Java Collection. RDF4J has the Model interface for this, which is an extension of the standard Java collections that has some special tricks up its sleeve for dealing with RDF data, but which you can also use in the same manner as a Set or a List. We convert our result to a Model as follows:
Model model = QueryResults.asModel(statements);
The QueryResults utility retrieves all the statements from the supplied result, and puts them in a Model. It also automatically closes the result object for you.
Finally, we want to print out our data to the console. This too is something you can do in numerous ways, but let’s just say we would like to print out our data in Turtle format. Here’s how:
Rio.write(model, System.out, RDFFormat.TURTLE);
Rio (which stands for “RDF I/O”) is the RDF4J parser/writer toolkit. We can just pass it our model, specify the outputstream, and the format in which we’d like it written, and Rio does the rest. Your code should now look like this:

You now have created your first Semantic Web application! After saving, just right-click on HelloRDF4J.java (in the project explorer) and select Run as -> Java application. You will see the following output in the Console:
<http://example.org/john> a <http://xmlns.com/foaf/0.1/Person> ;
         <http://www.w3.org/2000/01/rdf-schema#label> "John" .
As you can see, it contains exactly the two statements that we added earlier, in Turtle syntax.
However, it is a bit ugly, using all those long IRIs. We can make the output a bit prettier, by adding some namespace definitions to our Model:
model.setNamespace(RDF.NS);
model.setNamespace(RDFS.NS);
model.setNamespace(FOAF.NS);
model.setNamespace(ex);
Add these lines to your code, directly after where you have created the model. Then run your program again. You will now see the following:

That’s it! Obviously there is still loads to learn about how to use RDF4J effectively, but you’ve got the basics under control now: you can set up a new project using RDF4J,  and have seen some basic ways to add, write, and retrieve data. The rest is up to you. Good sources of further documentation are the Getting Started tutorial, Programming with RDF4J, and of course the API Javadoc.

  

     
      
        
          

  Table of Contents

  
  
    Setting up your environment
      
        Tweaking Eclipse preferences
      
    
    Creating a new project
    Defining the POM
    Configuring the Java compiler
    Programming our first Semantic Web application\n\n\n\nStarting a New Maven Project in Eclipse
    

  
  If you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.

    
    
You don't have to use Apache Maven or Eclipse IDE if you want to work with RDF4J. These are simply very useful tools for quickly getting a Java project started. Maven is good because it allows you to just define which libraries you want to use and never worry about any further third-party libraries you might need, and Eclipse IDE is good because it has good integration with Maven, code completion features, and is just generally a great Java development environment. But you can work with RDF4J just as well in a different build tool or IDE.



In this tutorial, we assume that you have a basic understanding of programming in Java, and have at least an inkling of what RDF is. However, we do not assume that you know how to use either RDF4J, Maven, or Eclipse, so we’ll go through it all one step at a time. If anything is already sufficiently familiar to you, you are of course free to skip ahead!
Setting up your environment
Before we kick off, you will need to install the Eclipse IDE. In this tutorial, we will use Eclipse for Java Developers version 4.18.0 (2020-12), but it shouldn’t matter too much which exact version you have installed. Select either the “Eclipse for Java developers” or “Eclipse for Java EE developers” installation option.
Eclipse IDE comes with a Maven plugin (called m2e) already installed. We will use this plugin to work with Maven. You are of course free to also install the Maven command line tools, but we won’t be using those directly in this tutorial.
When starting Eclipse for the first time, you will be asked for a workspace directory – this will be where Eclipse stores all project data as well as some configuration information. Feel free to pick/create a directory of your choice, or just accept the default.
Once Eclipse is started (and any welcome messages have been closed), you will be presented with a screen that should look roughly like this:

Tweaking Eclipse preferences
Before we start creating our actual project in Eclipse, there are a few preferences that we should tweak. This step is not required, but it will make things a little easier in the rest of this tutorial.
From the menu, select Eclipse -> Preferences. The Preferences dialog pops up. On the left, select Maven. You will see the Maven configuration settings. The options “Download Artifact Sources” and “Download Artifact JavaDoc” are unchecked by default. Check them both, then click OK.

The reason we want this, by the way, is that having either the sources or the Javadoc available really helps when you use code autocompletion in Eclipse – Eclipse will automatically read the Javadoc and provide the documentation in a little tooltip. As said: not required, but very useful.
Creating a new project
Now that we’re all set up, we can kick things off by creating a new project. Select the File menu, then New -> New Project... . A dialog will appear. In this dialog, select the option Maven Project:

Once you click Next, you will be presented with a screen to create a new Maven Project. Make you sure the option ‘Create a simple project’ is checked:

Click Next again. In the following screen you define further details of your Maven project, such as group and artifact ids, version number, and so on. For this tutorial,  you only have to fill in three fields:

group id – typically you use something like a Java package name for this. We will use org.example.
artifact id –  name of the maven artifact if you publish our project as one. We will use rdf4j-getting-started.
name – the project name. We will use HelloRDF4J .


Once this has been filled in you can click Finish. We now have our first Eclipse project, huzzah!
Defining the POM
So we have a project. That’s great, but it’s still empty: we haven’t added any code or necessary libraries (such as the RDF4J libraries) yet.
We will remedy the library-situation first. In Maven, requirements for libraries (we’re talking about jar files here) are specified in a special project configuration file called pom.xml, often referred to as “the POM”. Open your project and you will see that a file with that name is already present. Doubleclick it to open it in the POM editor:

As you can see, the POM already contains some information – in fact the information that we added when we created our project. However, we need to add more information about our project. Specifically, we want to add which external Java libraries we want to include in our project. In Maven, you do this by specifying dependencies.
The first dependency we will add is for RDF4J itself. RDF4J is not a single library, but consists of a large collection of separate modules, allowing you to pick and choose which functionality you need. To make handling of all the various modules and the dependencies they all need easier, we will first add a “Bill Of Materials” (BOM) dependency. The easiest way to do this is to edit the XML source code directly. Switch to the ‘pom.xml’ tab of the POM editor, and add the following XML fragment:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.6.0</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
It should look like this after you’ve added this and saved the file:

Next we need to add the actual RDF4J modules we want to use. As said, this is where you could pick and choose the specific modules (parsers, databases, etc) that you want. For this tutorial, however, we’ll be lazy and just add the entire core framework. We do this by adding the rdf4j-storage dependency, as follows:
<dependencies>
  <dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
  </dependency>
</dependencies>
Notice that because we already have a BOM added, we don’t need to specify a version number for this dependency. Once you have done this and save the file, Eclipse will automatically begin downloading the RDF4J libraries. This may take a little while, just let it do its thing. Once it’s complete, your POM should look like this:

We are going to add one more dependency, namely for a logging framework. RDF4J uses the SLF4J logging API, which requires a logging implementation, so we’ll provide one, in this case slf4j-simple (which is, as the name implies, a very simple logging library that by default just logs to the console). Add a new dependency, with group id org.slf4j, artifact id slf4j-simple. Again we don’t need to specify a version number: the RDF4J Bill of Materials already specifies which version of slf4j is compatible with RDF4J:
<dependency>
  <groupId>org.slf4j</groupId>
  <artifactId>slf4j-simple</artifactId>
  <scope>runtime</scope>
</dependency>
Once you have saved your changes to the POM, your project should be automatically refreshed by Eclipse again, and you should see a new section called ‘Maven dependencies’ in the Package Explorer (If your project does not automatically refresh and you don’t see the above section, right-click on your project in the Package Explorer, and select Maven -> Update Project...). This section, when opened, shows a list of jar files which can now be used in your project:

As you can see, this list contains not just all the RDF4J libraries, but also a lot of other libraries that RDF4J depends on.

    
    
If you are interested in having more control over which libraries are and aren't included, please see Setting up your development environment.



Configuring the Java compiler
Unfortunately the default Maven archetype that we used to create our new project uses a very old Java compiler (1.5) by default. Since RDF4J requires Java 8 at a minimum (we actually recommend Java 11 for better performance), we will need to change this.
Copy-paste (or type if you prefer) the following section into the xml file (put it just above the dependencyManagement section we added in earlier). This will set the version to Java 11.
<properties>
  <maven.compiler.source>11</maven.compiler.source>
  <maven.compiler.target>11</maven.compiler.target>
</properties>
Once you have done this, and saved your POM, you will need to manually update your Maven project for Eclipse to accept the changes. You do this by right-clicking on your project (in the project explorer), and then selecting ‘Maven’ -> ‘Update Project…':

Programming our first Semantic Web application
We’re done preparing, we can start programming! Right-click on src/main/java and select New -> Class. In the dialog shown, set the package name to org.example, the class name to HelloRDF4J, and make sure the option public static void main (String[] args) is checked:

Click ‘Finish’ and Eclipse will create the new class and automatically open it in an editor.
Since we will want to work with RDF data, we will start by creating something to keep that RDF data in. In RDF4J, RDF data is usually stored in a Repository. There many different types of Repository, both for local access as well as for accessing remote services. In this tutorial, we will create a simple local repository, namely one that uses an in-memory store, without any sort of persistent storage functionality (so the data will be lost when the program exits). Add the following code to your main method:
Repository rep = new SailRepository(new MemoryStore());
Once you have done this and saved the file, you will notice some red lines appearing:

This is Eclipse telling you that there is something wrong with your code. In this case, the problem is that several import statements are missing (note that Eclipse tells you what is wrong in detail as well in the ‘Problems’ tab underneath the editor). We’ll need to add those import statements. You can add each import manually of course, but luckily Eclipse has a shortcut. Hit Ctrl+Shift+O and Eclipse should automatically resolve all missing imports for you. It will pop up a dialog if there is more than one possibility for a particular import, which will like happen for the Repository interface. Just pick the one from org.eclipse.rdf4j:

Hit ‘Finish’, then save your code.
Now that we have created our repository, we are going to put some data in it, and then get that data out again and print it to the console.
Adding data can be done in numerous ways. In this tutorial, we will be creating a few RDF statements directly in Java (rather than, say, loading them from a file). To do this, we need a namespace that we can use to create any new IRIs we need. With this namespace, we can create a new IRI. We will create an identifier for a person called “John”, about whom we will later add some data to our repository:
Namespace ex = Values.namespace("ex", "http://example.org/");
IRI john = Values.iri(ex, "john");
We use a static factory(the Values class) to easily create new Namespaces, IRIs and other types of values.
We have now created a IRI, but have not yet added any data to our repository. To do this, we first need to open a RepositoryConnection. Such a connection allows us to add, remove, and retrieve data from our Repository. We do this as follows:
RepositoryConnection conn = rep.getConnection();
It is important to keep track of open connections and to close them again when we are finished with them, to free up any resources the connection may keep hold of. RDF4J connections are “AutoCloseable”, which means we can let Java handle the closing of the connection when we’re done with it, rather than having to deal with this ourselves. We use a try-with-resources construction for this, like so:
try (RepositoryConnection conn = rep.getConnection()) {
}
Your code should now look as follows:

Using the connection, we can start adding statements to our repository. We will add two triples, one to assert that John is a Person, and one to assert that John’s name is “John”:
conn.add(john, RDF.TYPE, FOAF.PERSON);
conn.add(john, RDFS.LABEL, Values.literal("John"));
Note how for well-known RDF vocabularies, such as RDF, RDFS, and FOAF, RDF4J provides constants that you can easily reuse to create/read data with. Also, you see another use of the Values static factory here, this time to create a Literal object.
Once we have added our data, we can retrieve it back from the repository again. You can do this in several ways, with a SPARQL query or using the RepositoryConnection API methods directly. Here we will use the latter:
RepositoryResult<Statement> statements = conn.getStatements(null, null, null);
We use the getStatements() method to retrieve data from the repository. The three arguments (all null) are for the subject, predicate, and object we wish to match. Since they are all set to null, this will retrieve all statements.
The object returned by getStatements() is a RepositoryResult, which implements both Iteration and Iterable: it allows you to process the result in a streaming fashion, using its hasNext() and next() methods, as well as using Java’s for-each loop to iterate over it. Almost all the methods that retrieve data from a Repository return such a streaming/iterating object: they are very useful for processsing very large result sets, because they do not require that the entire result is kept in memory all at the same time.
It is important to always call close() on this kind of result object when you are done with them, to avoid memory leaks and other problems. Since the RepositoryResult is also a AutoCloseable, you can use a try-with-resources construction (similar to what we used for opening the connection) to handle this.
However, since our Repository only contains 2 statements, we can safely just transfer the entire result into a Java Collection. RDF4J has the Model interface for this, which is an extension of the standard Java collections that has some special tricks up its sleeve for dealing with RDF data, but which you can also use in the same manner as a Set or a List. We convert our result to a Model as follows:
Model model = QueryResults.asModel(statements);
The QueryResults utility retrieves all the statements from the supplied result, and puts them in a Model. It also automatically closes the result object for you.
Finally, we want to print out our data to the console. This too is something you can do in numerous ways, but let’s just say we would like to print out our data in Turtle format. Here’s how:
Rio.write(model, System.out, RDFFormat.TURTLE);
Rio (which stands for “RDF I/O”) is the RDF4J parser/writer toolkit. We can just pass it our model, specify the outputstream, and the format in which we’d like it written, and Rio does the rest. Your code should now look like this:

You now have created your first Semantic Web application! After saving, just right-click on HelloRDF4J.java (in the project explorer) and select Run as -> Java application. You will see the following output in the Console:
<http://example.org/john> a <http://xmlns.com/foaf/0.1/Person> ;
         <http://www.w3.org/2000/01/rdf-schema#label> "John" .
As you can see, it contains exactly the two statements that we added earlier, in Turtle syntax.
However, it is a bit ugly, using all those long IRIs. We can make the output a bit prettier, by adding some namespace definitions to our Model:
model.setNamespace(RDF.NS);
model.setNamespace(RDFS.NS);
model.setNamespace(FOAF.NS);
model.setNamespace(ex);
Add these lines to your code, directly after where you have created the model. Then run your program again. You will now see the following:

That’s it! Obviously there is still loads to learn about how to use RDF4J effectively, but you’ve got the basics under control now: you can set up a new project using RDF4J,  and have seen some basic ways to add, write, and retrieve data. The rest is up to you. Good sources of further documentation are the Getting Started tutorial, Programming with RDF4J, and of course the API Javadoc.

  

     
      
        
          

  Table of Contents

  
  
    Setting up your environment
      
        Tweaking Eclipse preferences
      
    
    Creating a new project
    Defining the POM
    Configuring the Java compiler
    Programming our first Semantic Web application\n\n\n\nStarting a New Maven Project in Eclipse
    

  
  If you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.

    
    
You don't have to use Apache Maven or Eclipse IDE if you want to work with RDF4J. These are simply very useful tools for quickly getting a Java project started. Maven is good because it allows you to just define which libraries you want to use and never worry about any further third-party libraries you might need, and Eclipse IDE is good because it has good integration with Maven, code completion features, and is just generally a great Java development environment. But you can work with RDF4J just as well in a different build tool or IDE.



In this tutorial, we assume that you have a basic understanding of programming in Java, and have at least an inkling of what RDF is. However, we do not assume that you know how to use either RDF4J, Maven, or Eclipse, so we’ll go through it all one step at a time. If anything is already sufficiently familiar to you, you are of course free to skip ahead!
Setting up your environment
Before we kick off, you will need to install the Eclipse IDE. In this tutorial, we will use Eclipse for Java Developers version 4.18.0 (2020-12), but it shouldn’t matter too much which exact version you have installed. Select either the “Eclipse for Java developers” or “Eclipse for Java EE developers” installation option.
Eclipse IDE comes with a Maven plugin (called m2e) already installed. We will use this plugin to work with Maven. You are of course free to also install the Maven command line tools, but we won’t be using those directly in this tutorial.
When starting Eclipse for the first time, you will be asked for a workspace directory – this will be where Eclipse stores all project data as well as some configuration information. Feel free to pick/create a directory of your choice, or just accept the default.
Once Eclipse is started (and any welcome messages have been closed), you will be presented with a screen that should look roughly like this:

Tweaking Eclipse preferences
Before we start creating our actual project in Eclipse, there are a few preferences that we should tweak. This step is not required, but it will make things a little easier in the rest of this tutorial.
From the menu, select Eclipse -> Preferences. The Preferences dialog pops up. On the left, select Maven. You will see the Maven configuration settings. The options “Download Artifact Sources” and “Download Artifact JavaDoc” are unchecked by default. Check them both, then click OK.

The reason we want this, by the way, is that having either the sources or the Javadoc available really helps when you use code autocompletion in Eclipse – Eclipse will automatically read the Javadoc and provide the documentation in a little tooltip. As said: not required, but very useful.
Creating a new project
Now that we’re all set up, we can kick things off by creating a new project. Select the File menu, then New -> New Project... . A dialog will appear. In this dialog, select the option Maven Project:

Once you click Next, you will be presented with a screen to create a new Maven Project. Make you sure the option ‘Create a simple project’ is checked:

Click Next again. In the following screen you define further details of your Maven project, such as group and artifact ids, version number, and so on. For this tutorial,  you only have to fill in three fields:

group id – typically you use something like a Java package name for this. We will use org.example.
artifact id –  name of the maven artifact if you publish our project as one. We will use rdf4j-getting-started.
name – the project name. We will use HelloRDF4J .


Once this has been filled in you can click Finish. We now have our first Eclipse project, huzzah!
Defining the POM
So we have a project. That’s great, but it’s still empty: we haven’t added any code or necessary libraries (such as the RDF4J libraries) yet.
We will remedy the library-situation first. In Maven, requirements for libraries (we’re talking about jar files here) are specified in a special project configuration file called pom.xml, often referred to as “the POM”. Open your project and you will see that a file with that name is already present. Doubleclick it to open it in the POM editor:

As you can see, the POM already contains some information – in fact the information that we added when we created our project. However, we need to add more information about our project. Specifically, we want to add which external Java libraries we want to include in our project. In Maven, you do this by specifying dependencies.
The first dependency we will add is for RDF4J itself. RDF4J is not a single library, but consists of a large collection of separate modules, allowing you to pick and choose which functionality you need. To make handling of all the various modules and the dependencies they all need easier, we will first add a “Bill Of Materials” (BOM) dependency. The easiest way to do this is to edit the XML source code directly. Switch to the ‘pom.xml’ tab of the POM editor, and add the following XML fragment:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.6.0</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
It should look like this after you’ve added this and saved the file:

Next we need to add the actual RDF4J modules we want to use. As said, this is where you could pick and choose the specific modules (parsers, databases, etc) that you want. For this tutorial, however, we’ll be lazy and just add the entire core framework. We do this by adding the rdf4j-storage dependency, as follows:
<dependencies>
  <dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
  </dependency>
</dependencies>
Notice that because we already have a BOM added, we don’t need to specify a version number for this dependency. Once you have done this and save the file, Eclipse will automatically begin downloading the RDF4J libraries. This may take a little while, just let it do its thing. Once it’s complete, your POM should look like this:

We are going to add one more dependency, namely for a logging framework. RDF4J uses the SLF4J logging API, which requires a logging implementation, so we’ll provide one, in this case slf4j-simple (which is, as the name implies, a very simple logging library that by default just logs to the console). Add a new dependency, with group id org.slf4j, artifact id slf4j-simple. Again we don’t need to specify a version number: the RDF4J Bill of Materials already specifies which version of slf4j is compatible with RDF4J:
<dependency>
  <groupId>org.slf4j</groupId>
  <artifactId>slf4j-simple</artifactId>
  <scope>runtime</scope>
</dependency>
Once you have saved your changes to the POM, your project should be automatically refreshed by Eclipse again, and you should see a new section called ‘Maven dependencies’ in the Package Explorer (If your project does not automatically refresh and you don’t see the above section, right-click on your project in the Package Explorer, and select Maven -> Update Project...). This section, when opened, shows a list of jar files which can now be used in your project:

As you can see, this list contains not just all the RDF4J libraries, but also a lot of other libraries that RDF4J depends on.

    
    
If you are interested in having more control over which libraries are and aren't included, please see Setting up your development environment.



Configuring the Java compiler
Unfortunately the default Maven archetype that we used to create our new project uses a very old Java compiler (1.5) by default. Since RDF4J requires Java 8 at a minimum (we actually recommend Java 11 for better performance), we will need to change this.
Copy-paste (or type if you prefer) the following section into the xml file (put it just above the dependencyManagement section we added in earlier). This will set the version to Java 11.
<properties>
  <maven.compiler.source>11</maven.compiler.source>
  <maven.compiler.target>11</maven.compiler.target>
</properties>
Once you have done this, and saved your POM, you will need to manually update your Maven project for Eclipse to accept the changes. You do this by right-clicking on your project (in the project explorer), and then selecting ‘Maven’ -> ‘Update Project…':

Programming our first Semantic Web application
We’re done preparing, we can start programming! Right-click on src/main/java and select New -> Class. In the dialog shown, set the package name to org.example, the class name to HelloRDF4J, and make sure the option public static void main (String[] args) is checked:

Click ‘Finish’ and Eclipse will create the new class and automatically open it in an editor.
Since we will want to work with RDF data, we will start by creating something to keep that RDF data in. In RDF4J, RDF data is usually stored in a Repository. There many different types of Repository, both for local access as well as for accessing remote services. In this tutorial, we will create a simple local repository, namely one that uses an in-memory store, without any sort of persistent storage functionality (so the data will be lost when the program exits). Add the following code to your main method:
Repository rep = new SailRepository(new MemoryStore());
Once you have done this and saved the file, you will notice some red lines appearing:

This is Eclipse telling you that there is something wrong with your code. In this case, the problem is that several import statements are missing (note that Eclipse tells you what is wrong in detail as well in the ‘Problems’ tab underneath the editor). We’ll need to add those import statements. You can add each import manually of course, but luckily Eclipse has a shortcut. Hit Ctrl+Shift+O and Eclipse should automatically resolve all missing imports for you. It will pop up a dialog if there is more than one possibility for a particular import, which will like happen for the Repository interface. Just pick the one from org.eclipse.rdf4j:

Hit ‘Finish’, then save your code.
Now that we have created our repository, we are going to put some data in it, and then get that data out again and print it to the console.
Adding data can be done in numerous ways. In this tutorial, we will be creating a few RDF statements directly in Java (rather than, say, loading them from a file). To do this, we need a namespace that we can use to create any new IRIs we need. With this namespace, we can create a new IRI. We will create an identifier for a person called “John”, about whom we will later add some data to our repository:
Namespace ex = Values.namespace("ex", "http://example.org/");
IRI john = Values.iri(ex, "john");
We use a static factory(the Values class) to easily create new Namespaces, IRIs and other types of values.
We have now created a IRI, but have not yet added any data to our repository. To do this, we first need to open a RepositoryConnection. Such a connection allows us to add, remove, and retrieve data from our Repository. We do this as follows:
RepositoryConnection conn = rep.getConnection();
It is important to keep track of open connections and to close them again when we are finished with them, to free up any resources the connection may keep hold of. RDF4J connections are “AutoCloseable”, which means we can let Java handle the closing of the connection when we’re done with it, rather than having to deal with this ourselves. We use a try-with-resources construction for this, like so:
try (RepositoryConnection conn = rep.getConnection()) {
}
Your code should now look as follows:

Using the connection, we can start adding statements to our repository. We will add two triples, one to assert that John is a Person, and one to assert that John’s name is “John”:
conn.add(john, RDF.TYPE, FOAF.PERSON);
conn.add(john, RDFS.LABEL, Values.literal("John"));
Note how for well-known RDF vocabularies, such as RDF, RDFS, and FOAF, RDF4J provides constants that you can easily reuse to create/read data with. Also, you see another use of the Values static factory here, this time to create a Literal object.
Once we have added our data, we can retrieve it back from the repository again. You can do this in several ways, with a SPARQL query or using the RepositoryConnection API methods directly. Here we will use the latter:
RepositoryResult<Statement> statements = conn.getStatements(null, null, null);
We use the getStatements() method to retrieve data from the repository. The three arguments (all null) are for the subject, predicate, and object we wish to match. Since they are all set to null, this will retrieve all statements.
The object returned by getStatements() is a RepositoryResult, which implements both Iteration and Iterable: it allows you to process the result in a streaming fashion, using its hasNext() and next() methods, as well as using Java’s for-each loop to iterate over it. Almost all the methods that retrieve data from a Repository return such a streaming/iterating object: they are very useful for processsing very large result sets, because they do not require that the entire result is kept in memory all at the same time.
It is important to always call close() on this kind of result object when you are done with them, to avoid memory leaks and other problems. Since the RepositoryResult is also a AutoCloseable, you can use a try-with-resources construction (similar to what we used for opening the connection) to handle this.
However, since our Repository only contains 2 statements, we can safely just transfer the entire result into a Java Collection. RDF4J has the Model interface for this, which is an extension of the standard Java collections that has some special tricks up its sleeve for dealing with RDF data, but which you can also use in the same manner as a Set or a List. We convert our result to a Model as follows:
Model model = QueryResults.asModel(statements);
The QueryResults utility retrieves all the statements from the supplied result, and puts them in a Model. It also automatically closes the result object for you.
Finally, we want to print out our data to the console. This too is something you can do in numerous ways, but let’s just say we would like to print out our data in Turtle format. Here’s how:
Rio.write(model, System.out, RDFFormat.TURTLE);
Rio (which stands for “RDF I/O”) is the RDF4J parser/writer toolkit. We can just pass it our model, specify the outputstream, and the format in which we’d like it written, and Rio does the rest. Your code should now look like this:

You now have created your first Semantic Web application! After saving, just right-click on HelloRDF4J.java (in the project explorer) and select Run as -> Java application. You will see the following output in the Console:
<http://example.org/john> a <http://xmlns.com/foaf/0.1/Person> ;
         <http://www.w3.org/2000/01/rdf-schema#label> "John" .
As you can see, it contains exactly the two statements that we added earlier, in Turtle syntax.
However, it is a bit ugly, using all those long IRIs. We can make the output a bit prettier, by adding some namespace definitions to our Model:
model.setNamespace(RDF.NS);
model.setNamespace(RDFS.NS);
model.setNamespace(FOAF.NS);
model.setNamespace(ex);
Add these lines to your code, directly after where you have created the model. Then run your program again. You will now see the following:

That’s it! Obviously there is still loads to learn about how to use RDF4J effectively, but you’ve got the basics under control now: you can set up a new project using RDF4J,  and have seen some basic ways to add, write, and retrieve data. The rest is up to you. Good sources of further documentation are the Getting Started tutorial, Programming with RDF4J, and of course the API Javadoc.

  

     
      
        
          

  Table of Contents

  
  
    Setting up your environment
      
        Tweaking Eclipse preferences
      
    
    Creating a new project
    Defining the POM
    Configuring the Java compiler
    Programming our first Semantic Web application\n\n\n\nStarting a New Maven Project in Eclipse
    

  
  If you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.

    
    
You don't have to use Apache Maven or Eclipse IDE if you want to work with RDF4J. These are simply very useful tools for quickly getting a Java project started. Maven is good because it allows you to just define which libraries you want to use and never worry about any further third-party libraries you might need, and Eclipse IDE is good because it has good integration with Maven, code completion features, and is just generally a great Java development environment. But you can work with RDF4J just as well in a different build tool or IDE.



In this tutorial, we assume that you have a basic understanding of programming in Java, and have at least an inkling of what RDF is. However, we do not assume that you know how to use either RDF4J, Maven, or Eclipse, so we’ll go through it all one step at a time. If anything is already sufficiently familiar to you, you are of course free to skip ahead!
Setting up your environment
Before we kick off, you will need to install the Eclipse IDE. In this tutorial, we will use Eclipse for Java Developers version 4.18.0 (2020-12), but it shouldn’t matter too much which exact version you have installed. Select either the “Eclipse for Java developers” or “Eclipse for Java EE developers” installation option.
Eclipse IDE comes with a Maven plugin (called m2e) already installed. We will use this plugin to work with Maven. You are of course free to also install the Maven command line tools, but we won’t be using those directly in this tutorial.
When starting Eclipse for the first time, you will be asked for a workspace directory – this will be where Eclipse stores all project data as well as some configuration information. Feel free to pick/create a directory of your choice, or just accept the default.
Once Eclipse is started (and any welcome messages have been closed), you will be presented with a screen that should look roughly like this:

Tweaking Eclipse preferences
Before we start creating our actual project in Eclipse, there are a few preferences that we should tweak. This step is not required, but it will make things a little easier in the rest of this tutorial.
From the menu, select Eclipse -> Preferences. The Preferences dialog pops up. On the left, select Maven. You will see the Maven configuration settings. The options “Download Artifact Sources” and “Download Artifact JavaDoc” are unchecked by default. Check them both, then click OK.

The reason we want this, by the way, is that having either the sources or the Javadoc available really helps when you use code autocompletion in Eclipse – Eclipse will automatically read the Javadoc and provide the documentation in a little tooltip. As said: not required, but very useful.
Creating a new project
Now that we’re all set up, we can kick things off by creating a new project. Select the File menu, then New -> New Project... . A dialog will appear. In this dialog, select the option Maven Project:

Once you click Next, you will be presented with a screen to create a new Maven Project. Make you sure the option ‘Create a simple project’ is checked:

Click Next again. In the following screen you define further details of your Maven project, such as group and artifact ids, version number, and so on. For this tutorial,  you only have to fill in three fields:

group id – typically you use something like a Java package name for this. We will use org.example.
artifact id –  name of the maven artifact if you publish our project as one. We will use rdf4j-getting-started.
name – the project name. We will use HelloRDF4J .


Once this has been filled in you can click Finish. We now have our first Eclipse project, huzzah!
Defining the POM
So we have a project. That’s great, but it’s still empty: we haven’t added any code or necessary libraries (such as the RDF4J libraries) yet.
We will remedy the library-situation first. In Maven, requirements for libraries (we’re talking about jar files here) are specified in a special project configuration file called pom.xml, often referred to as “the POM”. Open your project and you will see that a file with that name is already present. Doubleclick it to open it in the POM editor:

As you can see, the POM already contains some information – in fact the information that we added when we created our project. However, we need to add more information about our project. Specifically, we want to add which external Java libraries we want to include in our project. In Maven, you do this by specifying dependencies.
The first dependency we will add is for RDF4J itself. RDF4J is not a single library, but consists of a large collection of separate modules, allowing you to pick and choose which functionality you need. To make handling of all the various modules and the dependencies they all need easier, we will first add a “Bill Of Materials” (BOM) dependency. The easiest way to do this is to edit the XML source code directly. Switch to the ‘pom.xml’ tab of the POM editor, and add the following XML fragment:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.6.0</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
It should look like this after you’ve added this and saved the file:

Next we need to add the actual RDF4J modules we want to use. As said, this is where you could pick and choose the specific modules (parsers, databases, etc) that you want. For this tutorial, however, we’ll be lazy and just add the entire core framework. We do this by adding the rdf4j-storage dependency, as follows:
<dependencies>
  <dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
  </dependency>
</dependencies>
Notice that because we already have a BOM added, we don’t need to specify a version number for this dependency. Once you have done this and save the file, Eclipse will automatically begin downloading the RDF4J libraries. This may take a little while, just let it do its thing. Once it’s complete, your POM should look like this:

We are going to add one more dependency, namely for a logging framework. RDF4J uses the SLF4J logging API, which requires a logging implementation, so we’ll provide one, in this case slf4j-simple (which is, as the name implies, a very simple logging library that by default just logs to the console). Add a new dependency, with group id org.slf4j, artifact id slf4j-simple. Again we don’t need to specify a version number: the RDF4J Bill of Materials already specifies which version of slf4j is compatible with RDF4J:
<dependency>
  <groupId>org.slf4j</groupId>
  <artifactId>slf4j-simple</artifactId>
  <scope>runtime</scope>
</dependency>
Once you have saved your changes to the POM, your project should be automatically refreshed by Eclipse again, and you should see a new section called ‘Maven dependencies’ in the Package Explorer (If your project does not automatically refresh and you don’t see the above section, right-click on your project in the Package Explorer, and select Maven -> Update Project...). This section, when opened, shows a list of jar files which can now be used in your project:

As you can see, this list contains not just all the RDF4J libraries, but also a lot of other libraries that RDF4J depends on.

    
    
If you are interested in having more control over which libraries are and aren't included, please see Setting up your development environment.



Configuring the Java compiler
Unfortunately the default Maven archetype that we used to create our new project uses a very old Java compiler (1.5) by default. Since RDF4J requires Java 8 at a minimum (we actually recommend Java 11 for better performance), we will need to change this.
Copy-paste (or type if you prefer) the following section into the xml file (put it just above the dependencyManagement section we added in earlier). This will set the version to Java 11.
<properties>
  <maven.compiler.source>11</maven.compiler.source>
  <maven.compiler.target>11</maven.compiler.target>
</properties>
Once you have done this, and saved your POM, you will need to manually update your Maven project for Eclipse to accept the changes. You do this by right-clicking on your project (in the project explorer), and then selecting ‘Maven’ -> ‘Update Project…':

Programming our first Semantic Web application
We’re done preparing, we can start programming! Right-click on src/main/java and select New -> Class. In the dialog shown, set the package name to org.example, the class name to HelloRDF4J, and make sure the option public static void main (String[] args) is checked:

Click ‘Finish’ and Eclipse will create the new class and automatically open it in an editor.
Since we will want to work with RDF data, we will start by creating something to keep that RDF data in. In RDF4J, RDF data is usually stored in a Repository. There many different types of Repository, both for local access as well as for accessing remote services. In this tutorial, we will create a simple local repository, namely one that uses an in-memory store, without any sort of persistent storage functionality (so the data will be lost when the program exits). Add the following code to your main method:
Repository rep = new SailRepository(new MemoryStore());
Once you have done this and saved the file, you will notice some red lines appearing:

This is Eclipse telling you that there is something wrong with your code. In this case, the problem is that several import statements are missing (note that Eclipse tells you what is wrong in detail as well in the ‘Problems’ tab underneath the editor). We’ll need to add those import statements. You can add each import manually of course, but luckily Eclipse has a shortcut. Hit Ctrl+Shift+O and Eclipse should automatically resolve all missing imports for you. It will pop up a dialog if there is more than one possibility for a particular import, which will like happen for the Repository interface. Just pick the one from org.eclipse.rdf4j:

Hit ‘Finish’, then save your code.
Now that we have created our repository, we are going to put some data in it, and then get that data out again and print it to the console.
Adding data can be done in numerous ways. In this tutorial, we will be creating a few RDF statements directly in Java (rather than, say, loading them from a file). To do this, we need a namespace that we can use to create any new IRIs we need. With this namespace, we can create a new IRI. We will create an identifier for a person called “John”, about whom we will later add some data to our repository:
Namespace ex = Values.namespace("ex", "http://example.org/");
IRI john = Values.iri(ex, "john");
We use a static factory(the Values class) to easily create new Namespaces, IRIs and other types of values.
We have now created a IRI, but have not yet added any data to our repository. To do this, we first need to open a RepositoryConnection. Such a connection allows us to add, remove, and retrieve data from our Repository. We do this as follows:
RepositoryConnection conn = rep.getConnection();
It is important to keep track of open connections and to close them again when we are finished with them, to free up any resources the connection may keep hold of. RDF4J connections are “AutoCloseable”, which means we can let Java handle the closing of the connection when we’re done with it, rather than having to deal with this ourselves. We use a try-with-resources construction for this, like so:
try (RepositoryConnection conn = rep.getConnection()) {
}
Your code should now look as follows:

Using the connection, we can start adding statements to our repository. We will add two triples, one to assert that John is a Person, and one to assert that John’s name is “John”:
conn.add(john, RDF.TYPE, FOAF.PERSON);
conn.add(john, RDFS.LABEL, Values.literal("John"));
Note how for well-known RDF vocabularies, such as RDF, RDFS, and FOAF, RDF4J provides constants that you can easily reuse to create/read data with. Also, you see another use of the Values static factory here, this time to create a Literal object.
Once we have added our data, we can retrieve it back from the repository again. You can do this in several ways, with a SPARQL query or using the RepositoryConnection API methods directly. Here we will use the latter:
RepositoryResult<Statement> statements = conn.getStatements(null, null, null);
We use the getStatements() method to retrieve data from the repository. The three arguments (all null) are for the subject, predicate, and object we wish to match. Since they are all set to null, this will retrieve all statements.
The object returned by getStatements() is a RepositoryResult, which implements both Iteration and Iterable: it allows you to process the result in a streaming fashion, using its hasNext() and next() methods, as well as using Java’s for-each loop to iterate over it. Almost all the methods that retrieve data from a Repository return such a streaming/iterating object: they are very useful for processsing very large result sets, because they do not require that the entire result is kept in memory all at the same time.
It is important to always call close() on this kind of result object when you are done with them, to avoid memory leaks and other problems. Since the RepositoryResult is also a AutoCloseable, you can use a try-with-resources construction (similar to what we used for opening the connection) to handle this.
However, since our Repository only contains 2 statements, we can safely just transfer the entire result into a Java Collection. RDF4J has the Model interface for this, which is an extension of the standard Java collections that has some special tricks up its sleeve for dealing with RDF data, but which you can also use in the same manner as a Set or a List. We convert our result to a Model as follows:
Model model = QueryResults.asModel(statements);
The QueryResults utility retrieves all the statements from the supplied result, and puts them in a Model. It also automatically closes the result object for you.
Finally, we want to print out our data to the console. This too is something you can do in numerous ways, but let’s just say we would like to print out our data in Turtle format. Here’s how:
Rio.write(model, System.out, RDFFormat.TURTLE);
Rio (which stands for “RDF I/O”) is the RDF4J parser/writer toolkit. We can just pass it our model, specify the outputstream, and the format in which we’d like it written, and Rio does the rest. Your code should now look like this:

You now have created your first Semantic Web application! After saving, just right-click on HelloRDF4J.java (in the project explorer) and select Run as -> Java application. You will see the following output in the Console:
<http://example.org/john> a <http://xmlns.com/foaf/0.1/Person> ;
         <http://www.w3.org/2000/01/rdf-schema#label> "John" .
As you can see, it contains exactly the two statements that we added earlier, in Turtle syntax.
However, it is a bit ugly, using all those long IRIs. We can make the output a bit prettier, by adding some namespace definitions to our Model:
model.setNamespace(RDF.NS);
model.setNamespace(RDFS.NS);
model.setNamespace(FOAF.NS);
model.setNamespace(ex);
Add these lines to your code, directly after where you have created the model. Then run your program again. You will now see the following:

That’s it! Obviously there is still loads to learn about how to use RDF4J effectively, but you’ve got the basics under control now: you can set up a new project using RDF4J,  and have seen some basic ways to add, write, and retrieve data. The rest is up to you. Good sources of further documentation are the Getting Started tutorial, Programming with RDF4J, and of course the API Javadoc.

  

     
      
        
          

  Table of Contents

  
  
    Setting up your environment
      
        Tweaking Eclipse preferences
      
    
    Creating a new project
    Defining the POM
    Configuring the Java compiler
    Programming our first Semantic Web application\n\n\n\nStarting a New Maven Project in Eclipse
    

  
  If you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.

    
    
You don't have to use Apache Maven or Eclipse IDE if you want to work with RDF4J. These are simply very useful tools for quickly getting a Java project started. Maven is good because it allows you to just define which libraries you want to use and never worry about any further third-party libraries you might need, and Eclipse IDE is good because it has good integration with Maven, code completion features, and is just generally a great Java development environment. But you can work with RDF4J just as well in a different build tool or IDE.



In this tutorial, we assume that you have a basic understanding of programming in Java, and have at least an inkling of what RDF is. However, we do not assume that you know how to use either RDF4J, Maven, or Eclipse, so we’ll go through it all one step at a time. If anything is already sufficiently familiar to you, you are of course free to skip ahead!
Setting up your environment
Before we kick off, you will need to install the Eclipse IDE. In this tutorial, we will use Eclipse for Java Developers version 4.18.0 (2020-12), but it shouldn’t matter too much which exact version you have installed. Select either the “Eclipse for Java developers” or “Eclipse for Java EE developers” installation option.
Eclipse IDE comes with a Maven plugin (called m2e) already installed. We will use this plugin to work with Maven. You are of course free to also install the Maven command line tools, but we won’t be using those directly in this tutorial.
When starting Eclipse for the first time, you will be asked for a workspace directory – this will be where Eclipse stores all project data as well as some configuration information. Feel free to pick/create a directory of your choice, or just accept the default.
Once Eclipse is started (and any welcome messages have been closed), you will be presented with a screen that should look roughly like this:

Tweaking Eclipse preferences
Before we start creating our actual project in Eclipse, there are a few preferences that we should tweak. This step is not required, but it will make things a little easier in the rest of this tutorial.
From the menu, select Eclipse -> Preferences. The Preferences dialog pops up. On the left, select Maven. You will see the Maven configuration settings. The options “Download Artifact Sources” and “Download Artifact JavaDoc” are unchecked by default. Check them both, then click OK.

The reason we want this, by the way, is that having either the sources or the Javadoc available really helps when you use code autocompletion in Eclipse – Eclipse will automatically read the Javadoc and provide the documentation in a little tooltip. As said: not required, but very useful.
Creating a new project
Now that we’re all set up, we can kick things off by creating a new project. Select the File menu, then New -> New Project... . A dialog will appear. In this dialog, select the option Maven Project:

Once you click Next, you will be presented with a screen to create a new Maven Project. Make you sure the option ‘Create a simple project’ is checked:

Click Next again. In the following screen you define further details of your Maven project, such as group and artifact ids, version number, and so on. For this tutorial,  you only have to fill in three fields:

group id – typically you use something like a Java package name for this. We will use org.example.
artifact id –  name of the maven artifact if you publish our project as one. We will use rdf4j-getting-started.
name – the project name. We will use HelloRDF4J .


Once this has been filled in you can click Finish. We now have our first Eclipse project, huzzah!
Defining the POM
So we have a project. That’s great, but it’s still empty: we haven’t added any code or necessary libraries (such as the RDF4J libraries) yet.
We will remedy the library-situation first. In Maven, requirements for libraries (we’re talking about jar files here) are specified in a special project configuration file called pom.xml, often referred to as “the POM”. Open your project and you will see that a file with that name is already present. Doubleclick it to open it in the POM editor:

As you can see, the POM already contains some information – in fact the information that we added when we created our project. However, we need to add more information about our project. Specifically, we want to add which external Java libraries we want to include in our project. In Maven, you do this by specifying dependencies.
The first dependency we will add is for RDF4J itself. RDF4J is not a single library, but consists of a large collection of separate modules, allowing you to pick and choose which functionality you need. To make handling of all the various modules and the dependencies they all need easier, we will first add a “Bill Of Materials” (BOM) dependency. The easiest way to do this is to edit the XML source code directly. Switch to the ‘pom.xml’ tab of the POM editor, and add the following XML fragment:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.6.0</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
It should look like this after you’ve added this and saved the file:

Next we need to add the actual RDF4J modules we want to use. As said, this is where you could pick and choose the specific modules (parsers, databases, etc) that you want. For this tutorial, however, we’ll be lazy and just add the entire core framework. We do this by adding the rdf4j-storage dependency, as follows:
<dependencies>
  <dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
  </dependency>
</dependencies>
Notice that because we already have a BOM added, we don’t need to specify a version number for this dependency. Once you have done this and save the file, Eclipse will automatically begin downloading the RDF4J libraries. This may take a little while, just let it do its thing. Once it’s complete, your POM should look like this:

We are going to add one more dependency, namely for a logging framework. RDF4J uses the SLF4J logging API, which requires a logging implementation, so we’ll provide one, in this case slf4j-simple (which is, as the name implies, a very simple logging library that by default just logs to the console). Add a new dependency, with group id org.slf4j, artifact id slf4j-simple. Again we don’t need to specify a version number: the RDF4J Bill of Materials already specifies which version of slf4j is compatible with RDF4J:
<dependency>
  <groupId>org.slf4j</groupId>
  <artifactId>slf4j-simple</artifactId>
  <scope>runtime</scope>
</dependency>
Once you have saved your changes to the POM, your project should be automatically refreshed by Eclipse again, and you should see a new section called ‘Maven dependencies’ in the Package Explorer (If your project does not automatically refresh and you don’t see the above section, right-click on your project in the Package Explorer, and select Maven -> Update Project...). This section, when opened, shows a list of jar files which can now be used in your project:

As you can see, this list contains not just all the RDF4J libraries, but also a lot of other libraries that RDF4J depends on.

    
    
If you are interested in having more control over which libraries are and aren't included, please see Setting up your development environment.



Configuring the Java compiler
Unfortunately the default Maven archetype that we used to create our new project uses a very old Java compiler (1.5) by default. Since RDF4J requires Java 8 at a minimum (we actually recommend Java 11 for better performance), we will need to change this.
Copy-paste (or type if you prefer) the following section into the xml file (put it just above the dependencyManagement section we added in earlier). This will set the version to Java 11.
<properties>
  <maven.compiler.source>11</maven.compiler.source>
  <maven.compiler.target>11</maven.compiler.target>
</properties>
Once you have done this, and saved your POM, you will need to manually update your Maven project for Eclipse to accept the changes. You do this by right-clicking on your project (in the project explorer), and then selecting ‘Maven’ -> ‘Update Project…':

Programming our first Semantic Web application
We’re done preparing, we can start programming! Right-click on src/main/java and select New -> Class. In the dialog shown, set the package name to org.example, the class name to HelloRDF4J, and make sure the option public static void main (String[] args) is checked:

Click ‘Finish’ and Eclipse will create the new class and automatically open it in an editor.
Since we will want to work with RDF data, we will start by creating something to keep that RDF data in. In RDF4J, RDF data is usually stored in a Repository. There many different types of Repository, both for local access as well as for accessing remote services. In this tutorial, we will create a simple local repository, namely one that uses an in-memory store, without any sort of persistent storage functionality (so the data will be lost when the program exits). Add the following code to your main method:
Repository rep = new SailRepository(new MemoryStore());
Once you have done this and saved the file, you will notice some red lines appearing:

This is Eclipse telling you that there is something wrong with your code. In this case, the problem is that several import statements are missing (note that Eclipse tells you what is wrong in detail as well in the ‘Problems’ tab underneath the editor). We’ll need to add those import statements. You can add each import manually of course, but luckily Eclipse has a shortcut. Hit Ctrl+Shift+O and Eclipse should automatically resolve all missing imports for you. It will pop up a dialog if there is more than one possibility for a particular import, which will like happen for the Repository interface. Just pick the one from org.eclipse.rdf4j:

Hit ‘Finish’, then save your code.
Now that we have created our repository, we are going to put some data in it, and then get that data out again and print it to the console.
Adding data can be done in numerous ways. In this tutorial, we will be creating a few RDF statements directly in Java (rather than, say, loading them from a file). To do this, we need a namespace that we can use to create any new IRIs we need. With this namespace, we can create a new IRI. We will create an identifier for a person called “John”, about whom we will later add some data to our repository:
Namespace ex = Values.namespace("ex", "http://example.org/");
IRI john = Values.iri(ex, "john");
We use a static factory(the Values class) to easily create new Namespaces, IRIs and other types of values.
We have now created a IRI, but have not yet added any data to our repository. To do this, we first need to open a RepositoryConnection. Such a connection allows us to add, remove, and retrieve data from our Repository. We do this as follows:
RepositoryConnection conn = rep.getConnection();
It is important to keep track of open connections and to close them again when we are finished with them, to free up any resources the connection may keep hold of. RDF4J connections are “AutoCloseable”, which means we can let Java handle the closing of the connection when we’re done with it, rather than having to deal with this ourselves. We use a try-with-resources construction for this, like so:
try (RepositoryConnection conn = rep.getConnection()) {
}
Your code should now look as follows:

Using the connection, we can start adding statements to our repository. We will add two triples, one to assert that John is a Person, and one to assert that John’s name is “John”:
conn.add(john, RDF.TYPE, FOAF.PERSON);
conn.add(john, RDFS.LABEL, Values.literal("John"));
Note how for well-known RDF vocabularies, such as RDF, RDFS, and FOAF, RDF4J provides constants that you can easily reuse to create/read data with. Also, you see another use of the Values static factory here, this time to create a Literal object.
Once we have added our data, we can retrieve it back from the repository again. You can do this in several ways, with a SPARQL query or using the RepositoryConnection API methods directly. Here we will use the latter:
RepositoryResult<Statement> statements = conn.getStatements(null, null, null);
We use the getStatements() method to retrieve data from the repository. The three arguments (all null) are for the subject, predicate, and object we wish to match. Since they are all set to null, this will retrieve all statements.
The object returned by getStatements() is a RepositoryResult, which implements both Iteration and Iterable: it allows you to process the result in a streaming fashion, using its hasNext() and next() methods, as well as using Java’s for-each loop to iterate over it. Almost all the methods that retrieve data from a Repository return such a streaming/iterating object: they are very useful for processsing very large result sets, because they do not require that the entire result is kept in memory all at the same time.
It is important to always call close() on this kind of result object when you are done with them, to avoid memory leaks and other problems. Since the RepositoryResult is also a AutoCloseable, you can use a try-with-resources construction (similar to what we used for opening the connection) to handle this.
However, since our Repository only contains 2 statements, we can safely just transfer the entire result into a Java Collection. RDF4J has the Model interface for this, which is an extension of the standard Java collections that has some special tricks up its sleeve for dealing with RDF data, but which you can also use in the same manner as a Set or a List. We convert our result to a Model as follows:
Model model = QueryResults.asModel(statements);
The QueryResults utility retrieves all the statements from the supplied result, and puts them in a Model. It also automatically closes the result object for you.
Finally, we want to print out our data to the console. This too is something you can do in numerous ways, but let’s just say we would like to print out our data in Turtle format. Here’s how:
Rio.write(model, System.out, RDFFormat.TURTLE);
Rio (which stands for “RDF I/O”) is the RDF4J parser/writer toolkit. We can just pass it our model, specify the outputstream, and the format in which we’d like it written, and Rio does the rest. Your code should now look like this:

You now have created your first Semantic Web application! After saving, just right-click on HelloRDF4J.java (in the project explorer) and select Run as -> Java application. You will see the following output in the Console:
<http://example.org/john> a <http://xmlns.com/foaf/0.1/Person> ;
         <http://www.w3.org/2000/01/rdf-schema#label> "John" .
As you can see, it contains exactly the two statements that we added earlier, in Turtle syntax.
However, it is a bit ugly, using all those long IRIs. We can make the output a bit prettier, by adding some namespace definitions to our Model:
model.setNamespace(RDF.NS);
model.setNamespace(RDFS.NS);
model.setNamespace(FOAF.NS);
model.setNamespace(ex);
Add these lines to your code, directly after where you have created the model. Then run your program again. You will now see the following:

That’s it! Obviously there is still loads to learn about how to use RDF4J effectively, but you’ve got the basics under control now: you can set up a new project using RDF4J,  and have seen some basic ways to add, write, and retrieve data. The rest is up to you. Good sources of further documentation are the Getting Started tutorial, Programming with RDF4J, and of course the API Javadoc.

  

     
      
        
          

  Table of Contents

  
  
    Setting up your environment
      
        Tweaking Eclipse preferences
      
    
    Creating a new project
    Defining the POM
    Configuring the Java compiler
    Programming our first Semantic Web application\n\n\n\nStarting a New Maven Project in Eclipse
    

  
  If you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.

    
    
You don't have to use Apache Maven or Eclipse IDE if you want to work with RDF4J. These are simply very useful tools for quickly getting a Java project started. Maven is good because it allows you to just define which libraries you want to use and never worry about any further third-party libraries you might need, and Eclipse IDE is good because it has good integration with Maven, code completion features, and is just generally a great Java development environment. But you can work with RDF4J just as well in a different build tool or IDE.



In this tutorial, we assume that you have a basic understanding of programming in Java, and have at least an inkling of what RDF is. However, we do not assume that you know how to use either RDF4J, Maven, or Eclipse, so we’ll go through it all one step at a time. If anything is already sufficiently familiar to you, you are of course free to skip ahead!
Setting up your environment
Before we kick off, you will need to install the Eclipse IDE. In this tutorial, we will use Eclipse for Java Developers version 4.18.0 (2020-12), but it shouldn’t matter too much which exact version you have installed. Select either the “Eclipse for Java developers” or “Eclipse for Java EE developers” installation option.
Eclipse IDE comes with a Maven plugin (called m2e) already installed. We will use this plugin to work with Maven. You are of course free to also install the Maven command line tools, but we won’t be using those directly in this tutorial.
When starting Eclipse for the first time, you will be asked for a workspace directory – this will be where Eclipse stores all project data as well as some configuration information. Feel free to pick/create a directory of your choice, or just accept the default.
Once Eclipse is started (and any welcome messages have been closed), you will be presented with a screen that should look roughly like this:

Tweaking Eclipse preferences
Before we start creating our actual project in Eclipse, there are a few preferences that we should tweak. This step is not required, but it will make things a little easier in the rest of this tutorial.
From the menu, select Eclipse -> Preferences. The Preferences dialog pops up. On the left, select Maven. You will see the Maven configuration settings. The options “Download Artifact Sources” and “Download Artifact JavaDoc” are unchecked by default. Check them both, then click OK.

The reason we want this, by the way, is that having either the sources or the Javadoc available really helps when you use code autocompletion in Eclipse – Eclipse will automatically read the Javadoc and provide the documentation in a little tooltip. As said: not required, but very useful.
Creating a new project
Now that we’re all set up, we can kick things off by creating a new project. Select the File menu, then New -> New Project... . A dialog will appear. In this dialog, select the option Maven Project:

Once you click Next, you will be presented with a screen to create a new Maven Project. Make you sure the option ‘Create a simple project’ is checked:

Click Next again. In the following screen you define further details of your Maven project, such as group and artifact ids, version number, and so on. For this tutorial,  you only have to fill in three fields:

group id – typically you use something like a Java package name for this. We will use org.example.
artifact id –  name of the maven artifact if you publish our project as one. We will use rdf4j-getting-started.
name – the project name. We will use HelloRDF4J .


Once this has been filled in you can click Finish. We now have our first Eclipse project, huzzah!
Defining the POM
So we have a project. That’s great, but it’s still empty: we haven’t added any code or necessary libraries (such as the RDF4J libraries) yet.
We will remedy the library-situation first. In Maven, requirements for libraries (we’re talking about jar files here) are specified in a special project configuration file called pom.xml, often referred to as “the POM”. Open your project and you will see that a file with that name is already present. Doubleclick it to open it in the POM editor:

As you can see, the POM already contains some information – in fact the information that we added when we created our project. However, we need to add more information about our project. Specifically, we want to add which external Java libraries we want to include in our project. In Maven, you do this by specifying dependencies.
The first dependency we will add is for RDF4J itself. RDF4J is not a single library, but consists of a large collection of separate modules, allowing you to pick and choose which functionality you need. To make handling of all the various modules and the dependencies they all need easier, we will first add a “Bill Of Materials” (BOM) dependency. The easiest way to do this is to edit the XML source code directly. Switch to the ‘pom.xml’ tab of the POM editor, and add the following XML fragment:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.6.0</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
It should look like this after you’ve added this and saved the file:

Next we need to add the actual RDF4J modules we want to use. As said, this is where you could pick and choose the specific modules (parsers, databases, etc) that you want. For this tutorial, however, we’ll be lazy and just add the entire core framework. We do this by adding the rdf4j-storage dependency, as follows:
<dependencies>
  <dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
  </dependency>
</dependencies>
Notice that because we already have a BOM added, we don’t need to specify a version number for this dependency. Once you have done this and save the file, Eclipse will automatically begin downloading the RDF4J libraries. This may take a little while, just let it do its thing. Once it’s complete, your POM should look like this:

We are going to add one more dependency, namely for a logging framework. RDF4J uses the SLF4J logging API, which requires a logging implementation, so we’ll provide one, in this case slf4j-simple (which is, as the name implies, a very simple logging library that by default just logs to the console). Add a new dependency, with group id org.slf4j, artifact id slf4j-simple. Again we don’t need to specify a version number: the RDF4J Bill of Materials already specifies which version of slf4j is compatible with RDF4J:
<dependency>
  <groupId>org.slf4j</groupId>
  <artifactId>slf4j-simple</artifactId>
  <scope>runtime</scope>
</dependency>
Once you have saved your changes to the POM, your project should be automatically refreshed by Eclipse again, and you should see a new section called ‘Maven dependencies’ in the Package Explorer (If your project does not automatically refresh and you don’t see the above section, right-click on your project in the Package Explorer, and select Maven -> Update Project...). This section, when opened, shows a list of jar files which can now be used in your project:

As you can see, this list contains not just all the RDF4J libraries, but also a lot of other libraries that RDF4J depends on.

    
    
If you are interested in having more control over which libraries are and aren't included, please see Setting up your development environment.



Configuring the Java compiler
Unfortunately the default Maven archetype that we used to create our new project uses a very old Java compiler (1.5) by default. Since RDF4J requires Java 8 at a minimum (we actually recommend Java 11 for better performance), we will need to change this.
Copy-paste (or type if you prefer) the following section into the xml file (put it just above the dependencyManagement section we added in earlier). This will set the version to Java 11.
<properties>
  <maven.compiler.source>11</maven.compiler.source>
  <maven.compiler.target>11</maven.compiler.target>
</properties>
Once you have done this, and saved your POM, you will need to manually update your Maven project for Eclipse to accept the changes. You do this by right-clicking on your project (in the project explorer), and then selecting ‘Maven’ -> ‘Update Project…':

Programming our first Semantic Web application
We’re done preparing, we can start programming! Right-click on src/main/java and select New -> Class. In the dialog shown, set the package name to org.example, the class name to HelloRDF4J, and make sure the option public static void main (String[] args) is checked:

Click ‘Finish’ and Eclipse will create the new class and automatically open it in an editor.
Since we will want to work with RDF data, we will start by creating something to keep that RDF data in. In RDF4J, RDF data is usually stored in a Repository. There many different types of Repository, both for local access as well as for accessing remote services. In this tutorial, we will create a simple local repository, namely one that uses an in-memory store, without any sort of persistent storage functionality (so the data will be lost when the program exits). Add the following code to your main method:
Repository rep = new SailRepository(new MemoryStore());
Once you have done this and saved the file, you will notice some red lines appearing:

This is Eclipse telling you that there is something wrong with your code. In this case, the problem is that several import statements are missing (note that Eclipse tells you what is wrong in detail as well in the ‘Problems’ tab underneath the editor). We’ll need to add those import statements. You can add each import manually of course, but luckily Eclipse has a shortcut. Hit Ctrl+Shift+O and Eclipse should automatically resolve all missing imports for you. It will pop up a dialog if there is more than one possibility for a particular import, which will like happen for the Repository interface. Just pick the one from org.eclipse.rdf4j:

Hit ‘Finish’, then save your code.
Now that we have created our repository, we are going to put some data in it, and then get that data out again and print it to the console.
Adding data can be done in numerous ways. In this tutorial, we will be creating a few RDF statements directly in Java (rather than, say, loading them from a file). To do this, we need a namespace that we can use to create any new IRIs we need. With this namespace, we can create a new IRI. We will create an identifier for a person called “John”, about whom we will later add some data to our repository:
Namespace ex = Values.namespace("ex", "http://example.org/");
IRI john = Values.iri(ex, "john");
We use a static factory(the Values class) to easily create new Namespaces, IRIs and other types of values.
We have now created a IRI, but have not yet added any data to our repository. To do this, we first need to open a RepositoryConnection. Such a connection allows us to add, remove, and retrieve data from our Repository. We do this as follows:
RepositoryConnection conn = rep.getConnection();
It is important to keep track of open connections and to close them again when we are finished with them, to free up any resources the connection may keep hold of. RDF4J connections are “AutoCloseable”, which means we can let Java handle the closing of the connection when we’re done with it, rather than having to deal with this ourselves. We use a try-with-resources construction for this, like so:
try (RepositoryConnection conn = rep.getConnection()) {
}
Your code should now look as follows:

Using the connection, we can start adding statements to our repository. We will add two triples, one to assert that John is a Person, and one to assert that John’s name is “John”:
conn.add(john, RDF.TYPE, FOAF.PERSON);
conn.add(john, RDFS.LABEL, Values.literal("John"));
Note how for well-known RDF vocabularies, such as RDF, RDFS, and FOAF, RDF4J provides constants that you can easily reuse to create/read data with. Also, you see another use of the Values static factory here, this time to create a Literal object.
Once we have added our data, we can retrieve it back from the repository again. You can do this in several ways, with a SPARQL query or using the RepositoryConnection API methods directly. Here we will use the latter:
RepositoryResult<Statement> statements = conn.getStatements(null, null, null);
We use the getStatements() method to retrieve data from the repository. The three arguments (all null) are for the subject, predicate, and object we wish to match. Since they are all set to null, this will retrieve all statements.
The object returned by getStatements() is a RepositoryResult, which implements both Iteration and Iterable: it allows you to process the result in a streaming fashion, using its hasNext() and next() methods, as well as using Java’s for-each loop to iterate over it. Almost all the methods that retrieve data from a Repository return such a streaming/iterating object: they are very useful for processsing very large result sets, because they do not require that the entire result is kept in memory all at the same time.
It is important to always call close() on this kind of result object when you are done with them, to avoid memory leaks and other problems. Since the RepositoryResult is also a AutoCloseable, you can use a try-with-resources construction (similar to what we used for opening the connection) to handle this.
However, since our Repository only contains 2 statements, we can safely just transfer the entire result into a Java Collection. RDF4J has the Model interface for this, which is an extension of the standard Java collections that has some special tricks up its sleeve for dealing with RDF data, but which you can also use in the same manner as a Set or a List. We convert our result to a Model as follows:
Model model = QueryResults.asModel(statements);
The QueryResults utility retrieves all the statements from the supplied result, and puts them in a Model. It also automatically closes the result object for you.
Finally, we want to print out our data to the console. This too is something you can do in numerous ways, but let’s just say we would like to print out our data in Turtle format. Here’s how:
Rio.write(model, System.out, RDFFormat.TURTLE);
Rio (which stands for “RDF I/O”) is the RDF4J parser/writer toolkit. We can just pass it our model, specify the outputstream, and the format in which we’d like it written, and Rio does the rest. Your code should now look like this:

You now have created your first Semantic Web application! After saving, just right-click on HelloRDF4J.java (in the project explorer) and select Run as -> Java application. You will see the following output in the Console:
<http://example.org/john> a <http://xmlns.com/foaf/0.1/Person> ;
         <http://www.w3.org/2000/01/rdf-schema#label> "John" .
As you can see, it contains exactly the two statements that we added earlier, in Turtle syntax.
However, it is a bit ugly, using all those long IRIs. We can make the output a bit prettier, by adding some namespace definitions to our Model:
model.setNamespace(RDF.NS);
model.setNamespace(RDFS.NS);
model.setNamespace(FOAF.NS);
model.setNamespace(ex);
Add these lines to your code, directly after where you have created the model. Then run your program again. You will now see the following:

That’s it! Obviously there is still loads to learn about how to use RDF4J effectively, but you’ve got the basics under control now: you can set up a new project using RDF4J,  and have seen some basic ways to add, write, and retrieve data. The rest is up to you. Good sources of further documentation are the Getting Started tutorial, Programming with RDF4J, and of course the API Javadoc.

  

     
      
        
          

  Table of Contents

  
  
    Setting up your environment
      
        Tweaking Eclipse preferences
      
    
    Creating a new project
    Defining the POM
    Configuring the Java compiler
    Programming our first Semantic Web application\n\n\n\nStarting a New Maven Project in Eclipse
    

  
  If you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.

    
    
You don't have to use Apache Maven or Eclipse IDE if you want to work with RDF4J. These are simply very useful tools for quickly getting a Java project started. Maven is good because it allows you to just define which libraries you want to use and never worry about any further third-party libraries you might need, and Eclipse IDE is good because it has good integration with Maven, code completion features, and is just generally a great Java development environment. But you can work with RDF4J just as well in a different build tool or IDE.



In this tutorial, we assume that you have a basic understanding of programming in Java, and have at least an inkling of what RDF is. However, we do not assume that you know how to use either RDF4J, Maven, or Eclipse, so we’ll go through it all one step at a time. If anything is already sufficiently familiar to you, you are of course free to skip ahead!
Setting up your environment
Before we kick off, you will need to install the Eclipse IDE. In this tutorial, we will use Eclipse for Java Developers version 4.18.0 (2020-12), but it shouldn’t matter too much which exact version you have installed. Select either the “Eclipse for Java developers” or “Eclipse for Java EE developers” installation option.
Eclipse IDE comes with a Maven plugin (called m2e) already installed. We will use this plugin to work with Maven. You are of course free to also install the Maven command line tools, but we won’t be using those directly in this tutorial.
When starting Eclipse for the first time, you will be asked for a workspace directory – this will be where Eclipse stores all project data as well as some configuration information. Feel free to pick/create a directory of your choice, or just accept the default.
Once Eclipse is started (and any welcome messages have been closed), you will be presented with a screen that should look roughly like this:

Tweaking Eclipse preferences
Before we start creating our actual project in Eclipse, there are a few preferences that we should tweak. This step is not required, but it will make things a little easier in the rest of this tutorial.
From the menu, select Eclipse -> Preferences. The Preferences dialog pops up. On the left, select Maven. You will see the Maven configuration settings. The options “Download Artifact Sources” and “Download Artifact JavaDoc” are unchecked by default. Check them both, then click OK.

The reason we want this, by the way, is that having either the sources or the Javadoc available really helps when you use code autocompletion in Eclipse – Eclipse will automatically read the Javadoc and provide the documentation in a little tooltip. As said: not required, but very useful.
Creating a new project
Now that we’re all set up, we can kick things off by creating a new project. Select the File menu, then New -> New Project... . A dialog will appear. In this dialog, select the option Maven Project:

Once you click Next, you will be presented with a screen to create a new Maven Project. Make you sure the option ‘Create a simple project’ is checked:

Click Next again. In the following screen you define further details of your Maven project, such as group and artifact ids, version number, and so on. For this tutorial,  you only have to fill in three fields:

group id – typically you use something like a Java package name for this. We will use org.example.
artifact id –  name of the maven artifact if you publish our project as one. We will use rdf4j-getting-started.
name – the project name. We will use HelloRDF4J .


Once this has been filled in you can click Finish. We now have our first Eclipse project, huzzah!
Defining the POM
So we have a project. That’s great, but it’s still empty: we haven’t added any code or necessary libraries (such as the RDF4J libraries) yet.
We will remedy the library-situation first. In Maven, requirements for libraries (we’re talking about jar files here) are specified in a special project configuration file called pom.xml, often referred to as “the POM”. Open your project and you will see that a file with that name is already present. Doubleclick it to open it in the POM editor:

As you can see, the POM already contains some information – in fact the information that we added when we created our project. However, we need to add more information about our project. Specifically, we want to add which external Java libraries we want to include in our project. In Maven, you do this by specifying dependencies.
The first dependency we will add is for RDF4J itself. RDF4J is not a single library, but consists of a large collection of separate modules, allowing you to pick and choose which functionality you need. To make handling of all the various modules and the dependencies they all need easier, we will first add a “Bill Of Materials” (BOM) dependency. The easiest way to do this is to edit the XML source code directly. Switch to the ‘pom.xml’ tab of the POM editor, and add the following XML fragment:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.6.0</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
It should look like this after you’ve added this and saved the file:

Next we need to add the actual RDF4J modules we want to use. As said, this is where you could pick and choose the specific modules (parsers, databases, etc) that you want. For this tutorial, however, we’ll be lazy and just add the entire core framework. We do this by adding the rdf4j-storage dependency, as follows:
<dependencies>
  <dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
  </dependency>
</dependencies>
Notice that because we already have a BOM added, we don’t need to specify a version number for this dependency. Once you have done this and save the file, Eclipse will automatically begin downloading the RDF4J libraries. This may take a little while, just let it do its thing. Once it’s complete, your POM should look like this:

We are going to add one more dependency, namely for a logging framework. RDF4J uses the SLF4J logging API, which requires a logging implementation, so we’ll provide one, in this case slf4j-simple (which is, as the name implies, a very simple logging library that by default just logs to the console). Add a new dependency, with group id org.slf4j, artifact id slf4j-simple. Again we don’t need to specify a version number: the RDF4J Bill of Materials already specifies which version of slf4j is compatible with RDF4J:
<dependency>
  <groupId>org.slf4j</groupId>
  <artifactId>slf4j-simple</artifactId>
  <scope>runtime</scope>
</dependency>
Once you have saved your changes to the POM, your project should be automatically refreshed by Eclipse again, and you should see a new section called ‘Maven dependencies’ in the Package Explorer (If your project does not automatically refresh and you don’t see the above section, right-click on your project in the Package Explorer, and select Maven -> Update Project...). This section, when opened, shows a list of jar files which can now be used in your project:

As you can see, this list contains not just all the RDF4J libraries, but also a lot of other libraries that RDF4J depends on.

    
    
If you are interested in having more control over which libraries are and aren't included, please see Setting up your development environment.



Configuring the Java compiler
Unfortunately the default Maven archetype that we used to create our new project uses a very old Java compiler (1.5) by default. Since RDF4J requires Java 8 at a minimum (we actually recommend Java 11 for better performance), we will need to change this.
Copy-paste (or type if you prefer) the following section into the xml file (put it just above the dependencyManagement section we added in earlier). This will set the version to Java 11.
<properties>
  <maven.compiler.source>11</maven.compiler.source>
  <maven.compiler.target>11</maven.compiler.target>
</properties>
Once you have done this, and saved your POM, you will need to manually update your Maven project for Eclipse to accept the changes. You do this by right-clicking on your project (in the project explorer), and then selecting ‘Maven’ -> ‘Update Project…':

Programming our first Semantic Web application
We’re done preparing, we can start programming! Right-click on src/main/java and select New -> Class. In the dialog shown, set the package name to org.example, the class name to HelloRDF4J, and make sure the option public static void main (String[] args) is checked:

Click ‘Finish’ and Eclipse will create the new class and automatically open it in an editor.
Since we will want to work with RDF data, we will start by creating something to keep that RDF data in. In RDF4J, RDF data is usually stored in a Repository. There many different types of Repository, both for local access as well as for accessing remote services. In this tutorial, we will create a simple local repository, namely one that uses an in-memory store, without any sort of persistent storage functionality (so the data will be lost when the program exits). Add the following code to your main method:
Repository rep = new SailRepository(new MemoryStore());
Once you have done this and saved the file, you will notice some red lines appearing:

This is Eclipse telling you that there is something wrong with your code. In this case, the problem is that several import statements are missing (note that Eclipse tells you what is wrong in detail as well in the ‘Problems’ tab underneath the editor). We’ll need to add those import statements. You can add each import manually of course, but luckily Eclipse has a shortcut. Hit Ctrl+Shift+O and Eclipse should automatically resolve all missing imports for you. It will pop up a dialog if there is more than one possibility for a particular import, which will like happen for the Repository interface. Just pick the one from org.eclipse.rdf4j:

Hit ‘Finish’, then save your code.
Now that we have created our repository, we are going to put some data in it, and then get that data out again and print it to the console.
Adding data can be done in numerous ways. In this tutorial, we will be creating a few RDF statements directly in Java (rather than, say, loading them from a file). To do this, we need a namespace that we can use to create any new IRIs we need. With this namespace, we can create a new IRI. We will create an identifier for a person called “John”, about whom we will later add some data to our repository:
Namespace ex = Values.namespace("ex", "http://example.org/");
IRI john = Values.iri(ex, "john");
We use a static factory(the Values class) to easily create new Namespaces, IRIs and other types of values.
We have now created a IRI, but have not yet added any data to our repository. To do this, we first need to open a RepositoryConnection. Such a connection allows us to add, remove, and retrieve data from our Repository. We do this as follows:
RepositoryConnection conn = rep.getConnection();
It is important to keep track of open connections and to close them again when we are finished with them, to free up any resources the connection may keep hold of. RDF4J connections are “AutoCloseable”, which means we can let Java handle the closing of the connection when we’re done with it, rather than having to deal with this ourselves. We use a try-with-resources construction for this, like so:
try (RepositoryConnection conn = rep.getConnection()) {
}
Your code should now look as follows:

Using the connection, we can start adding statements to our repository. We will add two triples, one to assert that John is a Person, and one to assert that John’s name is “John”:
conn.add(john, RDF.TYPE, FOAF.PERSON);
conn.add(john, RDFS.LABEL, Values.literal("John"));
Note how for well-known RDF vocabularies, such as RDF, RDFS, and FOAF, RDF4J provides constants that you can easily reuse to create/read data with. Also, you see another use of the Values static factory here, this time to create a Literal object.
Once we have added our data, we can retrieve it back from the repository again. You can do this in several ways, with a SPARQL query or using the RepositoryConnection API methods directly. Here we will use the latter:
RepositoryResult<Statement> statements = conn.getStatements(null, null, null);
We use the getStatements() method to retrieve data from the repository. The three arguments (all null) are for the subject, predicate, and object we wish to match. Since they are all set to null, this will retrieve all statements.
The object returned by getStatements() is a RepositoryResult, which implements both Iteration and Iterable: it allows you to process the result in a streaming fashion, using its hasNext() and next() methods, as well as using Java’s for-each loop to iterate over it. Almost all the methods that retrieve data from a Repository return such a streaming/iterating object: they are very useful for processsing very large result sets, because they do not require that the entire result is kept in memory all at the same time.
It is important to always call close() on this kind of result object when you are done with them, to avoid memory leaks and other problems. Since the RepositoryResult is also a AutoCloseable, you can use a try-with-resources construction (similar to what we used for opening the connection) to handle this.
However, since our Repository only contains 2 statements, we can safely just transfer the entire result into a Java Collection. RDF4J has the Model interface for this, which is an extension of the standard Java collections that has some special tricks up its sleeve for dealing with RDF data, but which you can also use in the same manner as a Set or a List. We convert our result to a Model as follows:
Model model = QueryResults.asModel(statements);
The QueryResults utility retrieves all the statements from the supplied result, and puts them in a Model. It also automatically closes the result object for you.
Finally, we want to print out our data to the console. This too is something you can do in numerous ways, but let’s just say we would like to print out our data in Turtle format. Here’s how:
Rio.write(model, System.out, RDFFormat.TURTLE);
Rio (which stands for “RDF I/O”) is the RDF4J parser/writer toolkit. We can just pass it our model, specify the outputstream, and the format in which we’d like it written, and Rio does the rest. Your code should now look like this:

You now have created your first Semantic Web application! After saving, just right-click on HelloRDF4J.java (in the project explorer) and select Run as -> Java application. You will see the following output in the Console:
<http://example.org/john> a <http://xmlns.com/foaf/0.1/Person> ;
         <http://www.w3.org/2000/01/rdf-schema#label> "John" .
As you can see, it contains exactly the two statements that we added earlier, in Turtle syntax.
However, it is a bit ugly, using all those long IRIs. We can make the output a bit prettier, by adding some namespace definitions to our Model:
model.setNamespace(RDF.NS);
model.setNamespace(RDFS.NS);
model.setNamespace(FOAF.NS);
model.setNamespace(ex);
Add these lines to your code, directly after where you have created the model. Then run your program again. You will now see the following:

That’s it! Obviously there is still loads to learn about how to use RDF4J effectively, but you’ve got the basics under control now: you can set up a new project using RDF4J,  and have seen some basic ways to add, write, and retrieve data. The rest is up to you. Good sources of further documentation are the Getting Started tutorial, Programming with RDF4J, and of course the API Javadoc.

  

     
      
        
          

  Table of Contents

  
  
    Setting up your environment
      
        Tweaking Eclipse preferences
      
    
    Creating a new project
    Defining the POM
    Configuring the Java compiler
    Programming our first Semantic Web application\n\n\n\nStarting a New Maven Project in Eclipse
    

  
  If you are new to RDF4J, or to tools like Eclipse IDE or Apache Maven, this tutorial will help you get started.

    
    
You don't have to use Apache Maven or Eclipse IDE if you want to work with RDF4J. These are simply very useful tools for quickly getting a Java project started. Maven is good because it allows you to just define which libraries you want to use and never worry about any further third-party libraries you might need, and Eclipse IDE is good because it has good integration with Maven, code completion features, and is just generally a great Java development environment. But you can work with RDF4J just as well in a different build tool or IDE.



In this tutorial, we assume that you have a basic understanding of programming in Java, and have at least an inkling of what RDF is. However, we do not assume that you know how to use either RDF4J, Maven, or Eclipse, so we’ll go through it all one step at a time. If anything is already sufficiently familiar to you, you are of course free to skip ahead!
Setting up your environment
Before we kick off, you will need to install the Eclipse IDE. In this tutorial, we will use Eclipse for Java Developers version 4.18.0 (2020-12), but it shouldn’t matter too much which exact version you have installed. Select either the “Eclipse for Java developers” or “Eclipse for Java EE developers” installation option.
Eclipse IDE comes with a Maven plugin (called m2e) already installed. We will use this plugin to work with Maven. You are of course free to also install the Maven command line tools, but we won’t be using those directly in this tutorial.
When starting Eclipse for the first time, you will be asked for a workspace directory – this will be where Eclipse stores all project data as well as some configuration information. Feel free to pick/create a directory of your choice, or just accept the default.
Once Eclipse is started (and any welcome messages have been closed), you will be presented with a screen that should look roughly like this:

Tweaking Eclipse preferences
Before we start creating our actual project in Eclipse, there are a few preferences that we should tweak. This step is not required, but it will make things a little easier in the rest of this tutorial.
From the menu, select Eclipse -> Preferences. The Preferences dialog pops up. On the left, select Maven. You will see the Maven configuration settings. The options “Download Artifact Sources” and “Download Artifact JavaDoc” are unchecked by default. Check them both, then click OK.

The reason we want this, by the way, is that having either the sources or the Javadoc available really helps when you use code autocompletion in Eclipse – Eclipse will automatically read the Javadoc and provide the documentation in a little tooltip. As said: not required, but very useful.
Creating a new project
Now that we’re all set up, we can kick things off by creating a new project. Select the File menu, then New -> New Project... . A dialog will appear. In this dialog, select the option Maven Project:

Once you click Next, you will be presented with a screen to create a new Maven Project. Make you sure the option ‘Create a simple project’ is checked:

Click Next again. In the following screen you define further details of your Maven project, such as group and artifact ids, version number, and so on. For this tutorial,  you only have to fill in three fields:

group id – typically you use something like a Java package name for this. We will use org.example.
artifact id –  name of the maven artifact if you publish our project as one. We will use rdf4j-getting-started.
name – the project name. We will use HelloRDF4J .


Once this has been filled in you can click Finish. We now have our first Eclipse project, huzzah!
Defining the POM
So we have a project. That’s great, but it’s still empty: we haven’t added any code or necessary libraries (such as the RDF4J libraries) yet.
We will remedy the library-situation first. In Maven, requirements for libraries (we’re talking about jar files here) are specified in a special project configuration file called pom.xml, often referred to as “the POM”. Open your project and you will see that a file with that name is already present. Doubleclick it to open it in the POM editor:

As you can see, the POM already contains some information – in fact the information that we added when we created our project. However, we need to add more information about our project. Specifically, we want to add which external Java libraries we want to include in our project. In Maven, you do this by specifying dependencies.
The first dependency we will add is for RDF4J itself. RDF4J is not a single library, but consists of a large collection of separate modules, allowing you to pick and choose which functionality you need. To make handling of all the various modules and the dependencies they all need easier, we will first add a “Bill Of Materials” (BOM) dependency. The easiest way to do this is to edit the XML source code directly. Switch to the ‘pom.xml’ tab of the POM editor, and add the following XML fragment:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.6.0</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
It should look like this after you’ve added this and saved the file:

Next we need to add the actual RDF4J modules we want to use. As said, this is where you could pick and choose the specific modules (parsers, databases, etc) that you want. For this tutorial, however, we’ll be lazy and just add the entire core framework. We do this by adding the rdf4j-storage dependency, as follows:
<dependencies>
  <dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-storage</artifactId>
    <type>pom</type>
  </dependency>
</dependencies>
Notice that because we already have a BOM added, we don’t need to specify a version number for this dependency. Once you have done this and save the file, Eclipse will automatically begin downloading the RDF4J libraries. This may take a little while, just let it do its thing. Once it’s complete, your POM should look like this:

We are going to add one more dependency, namely for a logging framework. RDF4J uses the SLF4J logging API, which requires a logging implementation, so we’ll provide one, in this case slf4j-simple (which is, as the name implies, a very simple logging library that by default just logs to the console). Add a new dependency, with group id org.slf4j, artifact id slf4j-simple. Again we don’t need to specify a version number: the RDF4J Bill of Materials already specifies which version of slf4j is compatible with RDF4J:
<dependency>
  <groupId>org.slf4j</groupId>
  <artifactId>slf4j-simple</artifactId>
  <scope>runtime</scope>
</dependency>
Once you have saved your changes to the POM, your project should be automatically refreshed by Eclipse again, and you should see a new section called ‘Maven dependencies’ in the Package Explorer (If your project does not automatically refresh and you don’t see the above section, right-click on your project in the Package Explorer, and select Maven -> Update Project...). This section, when opened, shows a list of jar files which can now be used in your project:

As you can see, this list contains not just all the RDF4J libraries, but also a lot of other libraries that RDF4J depends on.

    
    
If you are interested in having more control over which libraries are and aren't included, please see Setting up your development environment.



Configuring the Java compiler
Unfortunately the default Maven archetype that we used to create our new project uses a very old Java compiler (1.5) by default. Since RDF4J requires Java 8 at a minimum (we actually recommend Java 11 for better performance), we will need to change this.
Copy-paste (or type if you prefer) the following section into the xml file (put it just above the dependencyManagement section we added in earlier). This will set the version to Java 11.
<properties>
  <maven.compiler.source>11</maven.compiler.source>
  <maven.compiler.target>11</maven.compiler.target>
</properties>
Once you have done this, and saved your POM, you will need to manually update your Maven project for Eclipse to accept the changes. You do this by right-clicking on your project (in the project explorer), and then selecting ‘Maven’ -> ‘Update Project…':

Programming our first Semantic Web application
We’re done preparing, we can start programming! Right-click on src/main/java and select New -> Class. In the dialog shown, set the package name to org.example, the class name to HelloRDF4J, and make sure the option public static void main (String[] args) is checked:

Click ‘Finish’ and Eclipse will create the new class and automatically open it in an editor.
Since we will want to work with RDF data, we will start by creating something to keep that RDF data in. In RDF4J, RDF data is usually stored in a Repository. There many different types of Repository, both for local access as well as for accessing remote services. In this tutorial, we will create a simple local repository, namely one that uses an in-memory store, without any sort of persistent storage functionality (so the data will be lost when the program exits). Add the following code to your main method:
Repository rep = new SailRepository(new MemoryStore());
Once you have done this and saved the file, you will notice some red lines appearing:

This is Eclipse telling you that there is something wrong with your code. In this case, the problem is that several import statements are missing (note that Eclipse tells you what is wrong in detail as well in the ‘Problems’ tab underneath the editor). We’ll need to add those import statements. You can add each import manually of course, but luckily Eclipse has a shortcut. Hit Ctrl+Shift+O and Eclipse should automatically resolve all missing imports for you. It will pop up a dialog if there is more than one possibility for a particular import, which will like happen for the Repository interface. Just pick the one from org.eclipse.rdf4j:

Hit ‘Finish’, then save your code.
Now that we have created our repository, we are going to put some data in it, and then get that data out again and print it to the console.
Adding data can be done in numerous ways. In this tutorial, we will be creating a few RDF statements directly in Java (rather than, say, loading them from a file). To do this, we need a namespace that we can use to create any new IRIs we need. With this namespace, we can create a new IRI. We will create an identifier for a person called “John”, about whom we will later add some data to our repository:
Namespace ex = Values.namespace("ex", "http://example.org/");
IRI john = Values.iri(ex, "john");
We use a static factory(the Values class) to easily create new Namespaces, IRIs and other types of values.
We have now created a IRI, but have not yet added any data to our repository. To do this, we first need to open a RepositoryConnection. Such a connection allows us to add, remove, and retrieve data from our Repository. We do this as follows:
RepositoryConnection conn = rep.getConnection();
It is important to keep track of open connections and to close them again when we are finished with them, to free up any resources the connection may keep hold of. RDF4J connections are “AutoCloseable”, which means we can let Java handle the closing of the connection when we’re done with it, rather than having to deal with this ourselves. We use a try-with-resources construction for this, like so:
try (RepositoryConnection conn = rep.getConnection()) {
}
Your code should now look as follows:

Using the connection, we can start adding statements to our repository. We will add two triples, one to assert that John is a Person, and one to assert that John’s name is “John”:
conn.add(john, RDF.TYPE, FOAF.PERSON);
conn.add(john, RDFS.LABEL, Values.literal("John"));
Note how for well-known RDF vocabularies, such as RDF, RDFS, and FOAF, RDF4J provides constants that you can easily reuse to create/read data with. Also, you see another use of the Values static factory here, this time to create a Literal object.
Once we have added our data, we can retrieve it back from the repository again. You can do this in several ways, with a SPARQL query or using the RepositoryConnection API methods directly. Here we will use the latter:
RepositoryResult<Statement> statements = conn.getStatements(null, null, null);
We use the getStatements() method to retrieve data from the repository. The three arguments (all null) are for the subject, predicate, and object we wish to match. Since they are all set to null, this will retrieve all statements.
The object returned by getStatements() is a RepositoryResult, which implements both Iteration and Iterable: it allows you to process the result in a streaming fashion, using its hasNext() and next() methods, as well as using Java’s for-each loop to iterate over it. Almost all the methods that retrieve data from a Repository return such a streaming/iterating object: they are very useful for processsing very large result sets, because they do not require that the entire result is kept in memory all at the same time.
It is important to always call close() on this kind of result object when you are done with them, to avoid memory leaks and other problems. Since the RepositoryResult is also a AutoCloseable, you can use a try-with-resources construction (similar to what we used for opening the connection) to handle this.
However, since our Repository only contains 2 statements, we can safely just transfer the entire result into a Java Collection. RDF4J has the Model interface for this, which is an extension of the standard Java collections that has some special tricks up its sleeve for dealing with RDF data, but which you can also use in the same manner as a Set or a List. We convert our result to a Model as follows:
Model model = QueryResults.asModel(statements);
The QueryResults utility retrieves all the statements from the supplied result, and puts them in a Model. It also automatically closes the result object for you.
Finally, we want to print out our data to the console. This too is something you can do in numerous ways, but let’s just say we would like to print out our data in Turtle format. Here’s how:
Rio.write(model, System.out, RDFFormat.TURTLE);
Rio (which stands for “RDF I/O”) is the RDF4J parser/writer toolkit. We can just pass it our model, specify the outputstream, and the format in which we’d like it written, and Rio does the rest. Your code should now look like this:

You now have created your first Semantic Web application! After saving, just right-click on HelloRDF4J.java (in the project explorer) and select Run as -> Java application. You will see the following output in the Console:
<http://example.org/john> a <http://xmlns.com/foaf/0.1/Person> ;
         <http://www.w3.org/2000/01/rdf-schema#label> "John" .
As you can see, it contains exactly the two statements that we added earlier, in Turtle syntax.
However, it is a bit ugly, using all those long IRIs. We can make the output a bit prettier, by adding some namespace definitions to our Model:
model.setNamespace(RDF.NS);
model.setNamespace(RDFS.NS);
model.setNamespace(FOAF.NS);
model.setNamespace(ex);
Add these lines to your code, directly after where you have created the model. Then run your program again. You will now see the following:

That’s it! Obviously there is still loads to learn about how to use RDF4J effectively, but you’ve got the basics under control now: you can set up a new project using RDF4J,  and have seen some basic ways to add, write, and retrieve data. The rest is up to you. Good sources of further documentation are the Getting Started tutorial, Programming with RDF4J, and of course the API Javadoc.

  

     
      
        
          

  Table of Contents

  
  
    Setting up your environment
      
        Tweaking Eclipse preferences
      
    
    Creating a new project
    Defining the POM
    Configuring the Java compiler
    Programming our first Semantic Web application\n\n\n\nCreating Custom SPARQL Functions
    

  
  In this short tutoral, we’ll create a simple custom function and add it RDF4J’s SPARQL engine.
The SPARQL query language is extensible by nature: it allows you to add your own custom functions if the standard set of operators is not sufficient for your needs. The RDF4J SPARQL engine has been designed with this extensibility in mind: you can define your own custom function and use it as part of your SPARQL queries.
Here, we are going to implement a boolean function that detects if some string literal is a palindrome.
The palindrome function
Suppose we have the following RDF data:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix ex: <http://example.org/> .

ex:a rdfs:label "step on no pets" .
ex:b rdfs:label "go on, try it" .
We would like to be able to formulate a SPARQL query that allows us to retrieve all resources that have a palindrome as their label:
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX cfn: <http://example.org/custom-function/>
SELECT ?x ?label
WHERE {
   ?x rdfs:label ?label .
   FILTER(cfn:palindrome(str(?label)))
}
The expected result of this query, given the above data, would be:



x
label




ex:a
"step on no pets"



Unfortunately, the function cfn:palindrome is not a standard SPARQL function, so this query won’t work: the RDF4J SPARQL engine will simply report an error.
We could of course retrieve all label values in the database and then do some checking ourselves on these values, to detect if they’re palindromes. However if we add a custom function instead, we remove the need to scan over the entire database: the SPARQL engine itself can determine if a value is a valid palindrome or not, which removes the need for us to loop over all possible values.
There’s two basic steps in adding custom functions to RDF4J:

implementing a Java class for the function;
creating a JAR file with your function code in it and an Service Provider Interface (SPI) configuration.

Implementing the custom function as a Java class
In the RDF4J SPARQL engine, functions are expected to implement the Function
 interface.
package org.eclipser.rdf4j.examples.function;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

public class PalindromeFunction implements Function { }
The Function interface defines two methods: evaluate() and getURI(). The latter of these is a simple method that returns a string representation of the URI of the function:
// define a constant for the namespace of our custom function
public static final String NAMESPACE = "http://example.org/custom-function/";

/**
 * return the URI 'http://example.org/custom-function/palindrome' as a String
 */
public String getURI() {
    return NAMESPACE + "palindrome";
}
The real proof of the pudding is in the evaluate() method: this is where the function logic is implemented. In other words, in this method we check the incoming value to see if it is, first of all, a valid argument for the function, and second of all, a palindrome, and return the result.
Example 1
 show how we put everything together:
package org.eclipse.rdf4j.examples.function;

import static org.eclipse.rdf4j.model.util.Values.literal;

import org.eclipse.rdf4j.model.Literal;
import org.eclipse.rdf4j.model.Value;
import org.eclipse.rdf4j.query.algebra.evaluation.ValueExprEvaluationException;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

/**
 * An example custom SPARQL function that detects palindromes
 *
 * @author Jeen Broekstra
 */
public class PalindromeFunction implements Function {

    // define a constant for the namespace of our custom function
    public static final String NAMESPACE = "http://example.org/custom-function/";

    /**
     * return the URI 'http://example.org/custom-function/palindrome' as a
     * String
     */
    public String getURI() {
	return NAMESPACE + "palindrome";
    }

    /**
     * Executes the palindrome function.
     *
     * @return A boolean literal representing true if the input argument is a
     *         palindrome, false otherwise.
     * @throws ValueExprEvaluationException
     *         if more than one argument is supplied or if the supplied argument
     *         is not a literal.
     */
    public Value evaluate(TripleSource tripleSource, Value... args)
	throws ValueExprEvaluationException
	{
	    // our palindrome function expects only a single argument, so throw an error
	    // if there's more than one
	    if (args.length != 1) {
		throw new ValueExprEvaluationException(
			"palindrome function requires"
			+ "exactly 1 argument, got "
			+ args.length);
	    }
	    Value arg = args[0];
	    // check if the argument is a literal, if not, we throw an error
	    if (!(arg instanceof Literal)) {
		throw new ValueExprEvaluationException(
			"invalid argument (literal expected): " + arg);
	    }

	    // get the actual string value that we want to check for palindrome-ness.
	    String label = ((Literal)arg).getLabel();
	    // we invert our string
	    String inverted = "";
	    for (int i = label.length() - 1; i >= 0; i--) {
		inverted += label.charAt(i);
	    }
	    // a string is a palindrome if it is equal to its own inverse
	    boolean palindrome = inverted.equalsIgnoreCase(label);

	    // a function is always expected to return a Value object, so we
	    // return our boolean result as a Literal
	    return literal(palindrome);
	}
}
You are completely free to implement your function logic: in the above example, we have created a function that only returns true or false, but since the actual return type of an RDF4J function is Value
, you can create functions that return string literals, numbers, dates, or even IRIs or blank nodes.
In addition, the evaluate method accepts a TripleSource as input parameter, which you can use to inspect the underlying database, and query it for further information (for a simple/silly example see the Existing Palindrome function
, which in addition to checking that the argument is a palindrome, also checks if that palindrome already exists in the database).
There are two important things to keep in mind though:

the evaluate() method is invoked for every single solution in the query result. So you should make sure that the implementation of your function is not overly complex and memory-intensive.
RDF4J treats functions as singletons. This means that you should not “keep state” as part of your function; for example storing intermediate results in a private object field. This state will carry over between different uses of the function and even between different queries using it, making your results inconsistent.

Once we have created the Java class for our function, we need some way to make the RDF4J SPARQL engine aware of it. This is where the Service Provider Interface (SPI) comes into play.
Creating an SPI configuration
RDF4J’s set of SPARQL functions is dynamically determined through the use of a java.util.ServiceLoader class. Specifically, RDF4J has a class called FunctionRegistry
 which keeps track of all implementations of the Function interface. Java’s SPI mechanism depends on the presence of configuration files in the JAR files that contain service implementations. This configuration file is expected to be present in the directory META-INF/services in your JAR file.
In the case of the SPARQL function registry, the name of this configuration file should be org.eclipse.rdf4j.query.algebra.evaluation.function.Function (in other words, the file name is equal to the fully-qualified name of the service interface we are providing an implementation for). The contents are really quite simple: an SPI configuration is a text file, containing the fully-qualified names of each Java class that provides an SPI implementation, one on each line. So in our case, the contents of the file would be:
org.eclipse.rdf4j.example.function.PalindromeFunction
Apart from this configuration file, your JAR file should of course also contain the actual compiled class. All of this is fairly easy to do, for example from your Eclipse project:

create a directory META-INF and a subdirectory META-INF/services within the src directory of your project (or, if you use Maven, within src/main/resources) See our example resources dir for an example;
Add a text file named org.eclipse.rdf4j.query.algebra.evaluation.function.Function to this new directory. Make sure it contains a single line with the fully qualified name of your custom function class (in our example, that’s org.eclipse.rdf4j.example.function.PalindromeFunction);
Use Eclipse’s export function (or alternatively Maven’s package command) to create a JAR file (select the project, click ‘File’ -> ‘Export’ -> ‘JAR file’). Make sure the JAR file produced contains your compiled code and the sevice registry config file.

Once you have a proper JAR file, you need to add it the runtime classpath of your RDF4J project (or if you’re aiming to use this in an RDF4J Server, add it to the RDF4J Server webapp classpath and restart). After that, you’re done: RDF4J should automatically pick up your new custom function, you can from now on use it in your SPARQL queries.
Further reading

Introduction to the Service Provider Interface - Oracle Java documentation.

If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    The palindrome function
    Implementing the custom function as a Java class
    Creating an SPI configuration
    Further reading\n\n\n\nInterface Value



All Superinterfaces:
Serializable


All Known Subinterfaces:
BNode, IRI, Literal, LmdbResource, LmdbValue, MemValue, NativeResource, NativeValue, Resource, Triple


All Known Implementing Classes:
AbstractBNode, AbstractIRI, AbstractLiteral, AbstractTriple, BooleanLiteral, BooleanMemLiteral, CalendarLiteral, CalendarMemLiteral, CorruptIRI, CorruptIRIOrBNode, CorruptLiteral, CorruptUnknownValue, CorruptValue, DecimalLiteral, DecimalMemLiteral, IntegerLiteral, IntegerMemLiteral, InternedIRI, LmdbBNode, LmdbIRI, LmdbLiteral, MemBNode, MemIRI, MemLiteral, MemResource, MemTriple, NativeBNode, NativeIRI, NativeLiteral, NumericLiteral, NumericMemLiteral, SimpleBNode, SimpleIRI, SimpleLiteral, SimpleTriple



public interface Value
extends Serializable
The supertype of all RDF model objects (URIs, blank nodes and literals).







Method Summary

All MethodsInstance MethodsAbstract MethodsDefault Methods


Modifier and Type
Method
Description
default boolean
isBNode()

Check if the object is an instance of the given type.

default boolean
isIRI()

Check if the object is an instance of the given type.

default boolean
isLiteral()

Check if the object is an instance of the given type.

default boolean
isResource()

Check if the object is an instance of the given type.

default boolean
isTriple()

Check if the object is an instance of the given type.

String
stringValue()

Returns the String-value of a Value object.













Method Details



isBNode

default boolean isBNode()
Check if the object is an instance of the given type. Typically 2x than using instanceof.
 
 For implementers: This default implementation is overridden in the repsective sub-interface.

Returns:
true if instance of BNode






isIRI

default boolean isIRI()
Check if the object is an instance of the given type. Typically 2x than using instanceof.
 
 For implementers: This default implementation is overridden in the repsective sub-interface.

Returns:
true if instance of IRI






isResource

default boolean isResource()
Check if the object is an instance of the given type. Typically 2x than using instanceof.
 
 For implementers: This default implementation is overridden in the repsective sub-interface.

Returns:
true if instance of Resource






isLiteral

default boolean isLiteral()
Check if the object is an instance of the given type. Typically 2x than using instanceof.
 
 For implementers: This default implementation is overridden in the repsective sub-interface.

Returns:
true if instance of Literal






isTriple

default boolean isTriple()
Check if the object is an instance of the given type. Typically 2x than using instanceof.
 
 For implementers: This default implementation is overridden in the repsective sub-interface.

Returns:
true if instance of Triple






stringValue

String stringValue()
Returns the String-value of a Value object. This returns either a Literal's label, a
 IRI's URI or a BNode's ID.











Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nClass FunctionRegistry

java.lang.Object
org.eclipse.rdf4j.common.lang.service.ServiceRegistry<String,Function>
org.eclipse.rdf4j.query.algebra.evaluation.function.FunctionRegistry





public class FunctionRegistry
extends ServiceRegistry<String,Function>
A ServiceRegistry for implementations of the Function interface. Functions are registered by their
 IRI.

Author:
Arjohn Kampman








Field Summary

Fields inherited from class org.eclipse.rdf4j.common.lang.service.ServiceRegistry
logger, services





Constructor Summary
Constructors

Constructor
Description
FunctionRegistry()
 






Method Summary

All MethodsStatic MethodsInstance MethodsConcrete Methods


Modifier and Type
Method
Description
static FunctionRegistry
getInstance()

Gets the default FunctionRegistry.

protected String
getKey(Function function)

Gets the key for the specified service.





Methods inherited from class org.eclipse.rdf4j.common.lang.service.ServiceRegistry
add, get, getAll, getKeys, has, remove

Methods inherited from class java.lang.Object
clone, equals, finalize, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait









Constructor Details



FunctionRegistry

public FunctionRegistry()









Method Details



getInstance

public static FunctionRegistry getInstance()
Gets the default FunctionRegistry.

Returns:
The default registry.






getKey

protected String getKey(Function function)
Description copied from class: ServiceRegistry
Gets the key for the specified service.

Specified by:
getKey in class ServiceRegistry<String,Function>
Parameters:
function - The service to get the key for.
Returns:
The key for the specified service.












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nCreating Custom SPARQL Functions
    

  
  In this short tutoral, we’ll create a simple custom function and add it RDF4J’s SPARQL engine.
The SPARQL query language is extensible by nature: it allows you to add your own custom functions if the standard set of operators is not sufficient for your needs. The RDF4J SPARQL engine has been designed with this extensibility in mind: you can define your own custom function and use it as part of your SPARQL queries.
Here, we are going to implement a boolean function that detects if some string literal is a palindrome.
The palindrome function
Suppose we have the following RDF data:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix ex: <http://example.org/> .

ex:a rdfs:label "step on no pets" .
ex:b rdfs:label "go on, try it" .
We would like to be able to formulate a SPARQL query that allows us to retrieve all resources that have a palindrome as their label:
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX cfn: <http://example.org/custom-function/>
SELECT ?x ?label
WHERE {
   ?x rdfs:label ?label .
   FILTER(cfn:palindrome(str(?label)))
}
The expected result of this query, given the above data, would be:



x
label




ex:a
"step on no pets"



Unfortunately, the function cfn:palindrome is not a standard SPARQL function, so this query won’t work: the RDF4J SPARQL engine will simply report an error.
We could of course retrieve all label values in the database and then do some checking ourselves on these values, to detect if they’re palindromes. However if we add a custom function instead, we remove the need to scan over the entire database: the SPARQL engine itself can determine if a value is a valid palindrome or not, which removes the need for us to loop over all possible values.
There’s two basic steps in adding custom functions to RDF4J:

implementing a Java class for the function;
creating a JAR file with your function code in it and an Service Provider Interface (SPI) configuration.

Implementing the custom function as a Java class
In the RDF4J SPARQL engine, functions are expected to implement the Function
 interface.
package org.eclipser.rdf4j.examples.function;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

public class PalindromeFunction implements Function { }
The Function interface defines two methods: evaluate() and getURI(). The latter of these is a simple method that returns a string representation of the URI of the function:
// define a constant for the namespace of our custom function
public static final String NAMESPACE = "http://example.org/custom-function/";

/**
 * return the URI 'http://example.org/custom-function/palindrome' as a String
 */
public String getURI() {
    return NAMESPACE + "palindrome";
}
The real proof of the pudding is in the evaluate() method: this is where the function logic is implemented. In other words, in this method we check the incoming value to see if it is, first of all, a valid argument for the function, and second of all, a palindrome, and return the result.
Example 1
 show how we put everything together:
package org.eclipse.rdf4j.examples.function;

import static org.eclipse.rdf4j.model.util.Values.literal;

import org.eclipse.rdf4j.model.Literal;
import org.eclipse.rdf4j.model.Value;
import org.eclipse.rdf4j.query.algebra.evaluation.ValueExprEvaluationException;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

/**
 * An example custom SPARQL function that detects palindromes
 *
 * @author Jeen Broekstra
 */
public class PalindromeFunction implements Function {

    // define a constant for the namespace of our custom function
    public static final String NAMESPACE = "http://example.org/custom-function/";

    /**
     * return the URI 'http://example.org/custom-function/palindrome' as a
     * String
     */
    public String getURI() {
	return NAMESPACE + "palindrome";
    }

    /**
     * Executes the palindrome function.
     *
     * @return A boolean literal representing true if the input argument is a
     *         palindrome, false otherwise.
     * @throws ValueExprEvaluationException
     *         if more than one argument is supplied or if the supplied argument
     *         is not a literal.
     */
    public Value evaluate(TripleSource tripleSource, Value... args)
	throws ValueExprEvaluationException
	{
	    // our palindrome function expects only a single argument, so throw an error
	    // if there's more than one
	    if (args.length != 1) {
		throw new ValueExprEvaluationException(
			"palindrome function requires"
			+ "exactly 1 argument, got "
			+ args.length);
	    }
	    Value arg = args[0];
	    // check if the argument is a literal, if not, we throw an error
	    if (!(arg instanceof Literal)) {
		throw new ValueExprEvaluationException(
			"invalid argument (literal expected): " + arg);
	    }

	    // get the actual string value that we want to check for palindrome-ness.
	    String label = ((Literal)arg).getLabel();
	    // we invert our string
	    String inverted = "";
	    for (int i = label.length() - 1; i >= 0; i--) {
		inverted += label.charAt(i);
	    }
	    // a string is a palindrome if it is equal to its own inverse
	    boolean palindrome = inverted.equalsIgnoreCase(label);

	    // a function is always expected to return a Value object, so we
	    // return our boolean result as a Literal
	    return literal(palindrome);
	}
}
You are completely free to implement your function logic: in the above example, we have created a function that only returns true or false, but since the actual return type of an RDF4J function is Value
, you can create functions that return string literals, numbers, dates, or even IRIs or blank nodes.
In addition, the evaluate method accepts a TripleSource as input parameter, which you can use to inspect the underlying database, and query it for further information (for a simple/silly example see the Existing Palindrome function
, which in addition to checking that the argument is a palindrome, also checks if that palindrome already exists in the database).
There are two important things to keep in mind though:

the evaluate() method is invoked for every single solution in the query result. So you should make sure that the implementation of your function is not overly complex and memory-intensive.
RDF4J treats functions as singletons. This means that you should not “keep state” as part of your function; for example storing intermediate results in a private object field. This state will carry over between different uses of the function and even between different queries using it, making your results inconsistent.

Once we have created the Java class for our function, we need some way to make the RDF4J SPARQL engine aware of it. This is where the Service Provider Interface (SPI) comes into play.
Creating an SPI configuration
RDF4J’s set of SPARQL functions is dynamically determined through the use of a java.util.ServiceLoader class. Specifically, RDF4J has a class called FunctionRegistry
 which keeps track of all implementations of the Function interface. Java’s SPI mechanism depends on the presence of configuration files in the JAR files that contain service implementations. This configuration file is expected to be present in the directory META-INF/services in your JAR file.
In the case of the SPARQL function registry, the name of this configuration file should be org.eclipse.rdf4j.query.algebra.evaluation.function.Function (in other words, the file name is equal to the fully-qualified name of the service interface we are providing an implementation for). The contents are really quite simple: an SPI configuration is a text file, containing the fully-qualified names of each Java class that provides an SPI implementation, one on each line. So in our case, the contents of the file would be:
org.eclipse.rdf4j.example.function.PalindromeFunction
Apart from this configuration file, your JAR file should of course also contain the actual compiled class. All of this is fairly easy to do, for example from your Eclipse project:

create a directory META-INF and a subdirectory META-INF/services within the src directory of your project (or, if you use Maven, within src/main/resources) See our example resources dir for an example;
Add a text file named org.eclipse.rdf4j.query.algebra.evaluation.function.Function to this new directory. Make sure it contains a single line with the fully qualified name of your custom function class (in our example, that’s org.eclipse.rdf4j.example.function.PalindromeFunction);
Use Eclipse’s export function (or alternatively Maven’s package command) to create a JAR file (select the project, click ‘File’ -> ‘Export’ -> ‘JAR file’). Make sure the JAR file produced contains your compiled code and the sevice registry config file.

Once you have a proper JAR file, you need to add it the runtime classpath of your RDF4J project (or if you’re aiming to use this in an RDF4J Server, add it to the RDF4J Server webapp classpath and restart). After that, you’re done: RDF4J should automatically pick up your new custom function, you can from now on use it in your SPARQL queries.
Further reading

Introduction to the Service Provider Interface - Oracle Java documentation.

If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    The palindrome function
    Implementing the custom function as a Java class
    Creating an SPI configuration
    Further reading\n\n\n\nCreating Custom SPARQL Functions
    

  
  In this short tutoral, we’ll create a simple custom function and add it RDF4J’s SPARQL engine.
The SPARQL query language is extensible by nature: it allows you to add your own custom functions if the standard set of operators is not sufficient for your needs. The RDF4J SPARQL engine has been designed with this extensibility in mind: you can define your own custom function and use it as part of your SPARQL queries.
Here, we are going to implement a boolean function that detects if some string literal is a palindrome.
The palindrome function
Suppose we have the following RDF data:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix ex: <http://example.org/> .

ex:a rdfs:label "step on no pets" .
ex:b rdfs:label "go on, try it" .
We would like to be able to formulate a SPARQL query that allows us to retrieve all resources that have a palindrome as their label:
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX cfn: <http://example.org/custom-function/>
SELECT ?x ?label
WHERE {
   ?x rdfs:label ?label .
   FILTER(cfn:palindrome(str(?label)))
}
The expected result of this query, given the above data, would be:



x
label




ex:a
"step on no pets"



Unfortunately, the function cfn:palindrome is not a standard SPARQL function, so this query won’t work: the RDF4J SPARQL engine will simply report an error.
We could of course retrieve all label values in the database and then do some checking ourselves on these values, to detect if they’re palindromes. However if we add a custom function instead, we remove the need to scan over the entire database: the SPARQL engine itself can determine if a value is a valid palindrome or not, which removes the need for us to loop over all possible values.
There’s two basic steps in adding custom functions to RDF4J:

implementing a Java class for the function;
creating a JAR file with your function code in it and an Service Provider Interface (SPI) configuration.

Implementing the custom function as a Java class
In the RDF4J SPARQL engine, functions are expected to implement the Function
 interface.
package org.eclipser.rdf4j.examples.function;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

public class PalindromeFunction implements Function { }
The Function interface defines two methods: evaluate() and getURI(). The latter of these is a simple method that returns a string representation of the URI of the function:
// define a constant for the namespace of our custom function
public static final String NAMESPACE = "http://example.org/custom-function/";

/**
 * return the URI 'http://example.org/custom-function/palindrome' as a String
 */
public String getURI() {
    return NAMESPACE + "palindrome";
}
The real proof of the pudding is in the evaluate() method: this is where the function logic is implemented. In other words, in this method we check the incoming value to see if it is, first of all, a valid argument for the function, and second of all, a palindrome, and return the result.
Example 1
 show how we put everything together:
package org.eclipse.rdf4j.examples.function;

import static org.eclipse.rdf4j.model.util.Values.literal;

import org.eclipse.rdf4j.model.Literal;
import org.eclipse.rdf4j.model.Value;
import org.eclipse.rdf4j.query.algebra.evaluation.ValueExprEvaluationException;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

/**
 * An example custom SPARQL function that detects palindromes
 *
 * @author Jeen Broekstra
 */
public class PalindromeFunction implements Function {

    // define a constant for the namespace of our custom function
    public static final String NAMESPACE = "http://example.org/custom-function/";

    /**
     * return the URI 'http://example.org/custom-function/palindrome' as a
     * String
     */
    public String getURI() {
	return NAMESPACE + "palindrome";
    }

    /**
     * Executes the palindrome function.
     *
     * @return A boolean literal representing true if the input argument is a
     *         palindrome, false otherwise.
     * @throws ValueExprEvaluationException
     *         if more than one argument is supplied or if the supplied argument
     *         is not a literal.
     */
    public Value evaluate(TripleSource tripleSource, Value... args)
	throws ValueExprEvaluationException
	{
	    // our palindrome function expects only a single argument, so throw an error
	    // if there's more than one
	    if (args.length != 1) {
		throw new ValueExprEvaluationException(
			"palindrome function requires"
			+ "exactly 1 argument, got "
			+ args.length);
	    }
	    Value arg = args[0];
	    // check if the argument is a literal, if not, we throw an error
	    if (!(arg instanceof Literal)) {
		throw new ValueExprEvaluationException(
			"invalid argument (literal expected): " + arg);
	    }

	    // get the actual string value that we want to check for palindrome-ness.
	    String label = ((Literal)arg).getLabel();
	    // we invert our string
	    String inverted = "";
	    for (int i = label.length() - 1; i >= 0; i--) {
		inverted += label.charAt(i);
	    }
	    // a string is a palindrome if it is equal to its own inverse
	    boolean palindrome = inverted.equalsIgnoreCase(label);

	    // a function is always expected to return a Value object, so we
	    // return our boolean result as a Literal
	    return literal(palindrome);
	}
}
You are completely free to implement your function logic: in the above example, we have created a function that only returns true or false, but since the actual return type of an RDF4J function is Value
, you can create functions that return string literals, numbers, dates, or even IRIs or blank nodes.
In addition, the evaluate method accepts a TripleSource as input parameter, which you can use to inspect the underlying database, and query it for further information (for a simple/silly example see the Existing Palindrome function
, which in addition to checking that the argument is a palindrome, also checks if that palindrome already exists in the database).
There are two important things to keep in mind though:

the evaluate() method is invoked for every single solution in the query result. So you should make sure that the implementation of your function is not overly complex and memory-intensive.
RDF4J treats functions as singletons. This means that you should not “keep state” as part of your function; for example storing intermediate results in a private object field. This state will carry over between different uses of the function and even between different queries using it, making your results inconsistent.

Once we have created the Java class for our function, we need some way to make the RDF4J SPARQL engine aware of it. This is where the Service Provider Interface (SPI) comes into play.
Creating an SPI configuration
RDF4J’s set of SPARQL functions is dynamically determined through the use of a java.util.ServiceLoader class. Specifically, RDF4J has a class called FunctionRegistry
 which keeps track of all implementations of the Function interface. Java’s SPI mechanism depends on the presence of configuration files in the JAR files that contain service implementations. This configuration file is expected to be present in the directory META-INF/services in your JAR file.
In the case of the SPARQL function registry, the name of this configuration file should be org.eclipse.rdf4j.query.algebra.evaluation.function.Function (in other words, the file name is equal to the fully-qualified name of the service interface we are providing an implementation for). The contents are really quite simple: an SPI configuration is a text file, containing the fully-qualified names of each Java class that provides an SPI implementation, one on each line. So in our case, the contents of the file would be:
org.eclipse.rdf4j.example.function.PalindromeFunction
Apart from this configuration file, your JAR file should of course also contain the actual compiled class. All of this is fairly easy to do, for example from your Eclipse project:

create a directory META-INF and a subdirectory META-INF/services within the src directory of your project (or, if you use Maven, within src/main/resources) See our example resources dir for an example;
Add a text file named org.eclipse.rdf4j.query.algebra.evaluation.function.Function to this new directory. Make sure it contains a single line with the fully qualified name of your custom function class (in our example, that’s org.eclipse.rdf4j.example.function.PalindromeFunction);
Use Eclipse’s export function (or alternatively Maven’s package command) to create a JAR file (select the project, click ‘File’ -> ‘Export’ -> ‘JAR file’). Make sure the JAR file produced contains your compiled code and the sevice registry config file.

Once you have a proper JAR file, you need to add it the runtime classpath of your RDF4J project (or if you’re aiming to use this in an RDF4J Server, add it to the RDF4J Server webapp classpath and restart). After that, you’re done: RDF4J should automatically pick up your new custom function, you can from now on use it in your SPARQL queries.
Further reading

Introduction to the Service Provider Interface - Oracle Java documentation.

If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    The palindrome function
    Implementing the custom function as a Java class
    Creating an SPI configuration
    Further reading\n\n\n\nCreating Custom SPARQL Functions
    

  
  In this short tutoral, we’ll create a simple custom function and add it RDF4J’s SPARQL engine.
The SPARQL query language is extensible by nature: it allows you to add your own custom functions if the standard set of operators is not sufficient for your needs. The RDF4J SPARQL engine has been designed with this extensibility in mind: you can define your own custom function and use it as part of your SPARQL queries.
Here, we are going to implement a boolean function that detects if some string literal is a palindrome.
The palindrome function
Suppose we have the following RDF data:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix ex: <http://example.org/> .

ex:a rdfs:label "step on no pets" .
ex:b rdfs:label "go on, try it" .
We would like to be able to formulate a SPARQL query that allows us to retrieve all resources that have a palindrome as their label:
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX cfn: <http://example.org/custom-function/>
SELECT ?x ?label
WHERE {
   ?x rdfs:label ?label .
   FILTER(cfn:palindrome(str(?label)))
}
The expected result of this query, given the above data, would be:



x
label




ex:a
"step on no pets"



Unfortunately, the function cfn:palindrome is not a standard SPARQL function, so this query won’t work: the RDF4J SPARQL engine will simply report an error.
We could of course retrieve all label values in the database and then do some checking ourselves on these values, to detect if they’re palindromes. However if we add a custom function instead, we remove the need to scan over the entire database: the SPARQL engine itself can determine if a value is a valid palindrome or not, which removes the need for us to loop over all possible values.
There’s two basic steps in adding custom functions to RDF4J:

implementing a Java class for the function;
creating a JAR file with your function code in it and an Service Provider Interface (SPI) configuration.

Implementing the custom function as a Java class
In the RDF4J SPARQL engine, functions are expected to implement the Function
 interface.
package org.eclipser.rdf4j.examples.function;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

public class PalindromeFunction implements Function { }
The Function interface defines two methods: evaluate() and getURI(). The latter of these is a simple method that returns a string representation of the URI of the function:
// define a constant for the namespace of our custom function
public static final String NAMESPACE = "http://example.org/custom-function/";

/**
 * return the URI 'http://example.org/custom-function/palindrome' as a String
 */
public String getURI() {
    return NAMESPACE + "palindrome";
}
The real proof of the pudding is in the evaluate() method: this is where the function logic is implemented. In other words, in this method we check the incoming value to see if it is, first of all, a valid argument for the function, and second of all, a palindrome, and return the result.
Example 1
 show how we put everything together:
package org.eclipse.rdf4j.examples.function;

import static org.eclipse.rdf4j.model.util.Values.literal;

import org.eclipse.rdf4j.model.Literal;
import org.eclipse.rdf4j.model.Value;
import org.eclipse.rdf4j.query.algebra.evaluation.ValueExprEvaluationException;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

/**
 * An example custom SPARQL function that detects palindromes
 *
 * @author Jeen Broekstra
 */
public class PalindromeFunction implements Function {

    // define a constant for the namespace of our custom function
    public static final String NAMESPACE = "http://example.org/custom-function/";

    /**
     * return the URI 'http://example.org/custom-function/palindrome' as a
     * String
     */
    public String getURI() {
	return NAMESPACE + "palindrome";
    }

    /**
     * Executes the palindrome function.
     *
     * @return A boolean literal representing true if the input argument is a
     *         palindrome, false otherwise.
     * @throws ValueExprEvaluationException
     *         if more than one argument is supplied or if the supplied argument
     *         is not a literal.
     */
    public Value evaluate(TripleSource tripleSource, Value... args)
	throws ValueExprEvaluationException
	{
	    // our palindrome function expects only a single argument, so throw an error
	    // if there's more than one
	    if (args.length != 1) {
		throw new ValueExprEvaluationException(
			"palindrome function requires"
			+ "exactly 1 argument, got "
			+ args.length);
	    }
	    Value arg = args[0];
	    // check if the argument is a literal, if not, we throw an error
	    if (!(arg instanceof Literal)) {
		throw new ValueExprEvaluationException(
			"invalid argument (literal expected): " + arg);
	    }

	    // get the actual string value that we want to check for palindrome-ness.
	    String label = ((Literal)arg).getLabel();
	    // we invert our string
	    String inverted = "";
	    for (int i = label.length() - 1; i >= 0; i--) {
		inverted += label.charAt(i);
	    }
	    // a string is a palindrome if it is equal to its own inverse
	    boolean palindrome = inverted.equalsIgnoreCase(label);

	    // a function is always expected to return a Value object, so we
	    // return our boolean result as a Literal
	    return literal(palindrome);
	}
}
You are completely free to implement your function logic: in the above example, we have created a function that only returns true or false, but since the actual return type of an RDF4J function is Value
, you can create functions that return string literals, numbers, dates, or even IRIs or blank nodes.
In addition, the evaluate method accepts a TripleSource as input parameter, which you can use to inspect the underlying database, and query it for further information (for a simple/silly example see the Existing Palindrome function
, which in addition to checking that the argument is a palindrome, also checks if that palindrome already exists in the database).
There are two important things to keep in mind though:

the evaluate() method is invoked for every single solution in the query result. So you should make sure that the implementation of your function is not overly complex and memory-intensive.
RDF4J treats functions as singletons. This means that you should not “keep state” as part of your function; for example storing intermediate results in a private object field. This state will carry over between different uses of the function and even between different queries using it, making your results inconsistent.

Once we have created the Java class for our function, we need some way to make the RDF4J SPARQL engine aware of it. This is where the Service Provider Interface (SPI) comes into play.
Creating an SPI configuration
RDF4J’s set of SPARQL functions is dynamically determined through the use of a java.util.ServiceLoader class. Specifically, RDF4J has a class called FunctionRegistry
 which keeps track of all implementations of the Function interface. Java’s SPI mechanism depends on the presence of configuration files in the JAR files that contain service implementations. This configuration file is expected to be present in the directory META-INF/services in your JAR file.
In the case of the SPARQL function registry, the name of this configuration file should be org.eclipse.rdf4j.query.algebra.evaluation.function.Function (in other words, the file name is equal to the fully-qualified name of the service interface we are providing an implementation for). The contents are really quite simple: an SPI configuration is a text file, containing the fully-qualified names of each Java class that provides an SPI implementation, one on each line. So in our case, the contents of the file would be:
org.eclipse.rdf4j.example.function.PalindromeFunction
Apart from this configuration file, your JAR file should of course also contain the actual compiled class. All of this is fairly easy to do, for example from your Eclipse project:

create a directory META-INF and a subdirectory META-INF/services within the src directory of your project (or, if you use Maven, within src/main/resources) See our example resources dir for an example;
Add a text file named org.eclipse.rdf4j.query.algebra.evaluation.function.Function to this new directory. Make sure it contains a single line with the fully qualified name of your custom function class (in our example, that’s org.eclipse.rdf4j.example.function.PalindromeFunction);
Use Eclipse’s export function (or alternatively Maven’s package command) to create a JAR file (select the project, click ‘File’ -> ‘Export’ -> ‘JAR file’). Make sure the JAR file produced contains your compiled code and the sevice registry config file.

Once you have a proper JAR file, you need to add it the runtime classpath of your RDF4J project (or if you’re aiming to use this in an RDF4J Server, add it to the RDF4J Server webapp classpath and restart). After that, you’re done: RDF4J should automatically pick up your new custom function, you can from now on use it in your SPARQL queries.
Further reading

Introduction to the Service Provider Interface - Oracle Java documentation.

If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    The palindrome function
    Implementing the custom function as a Java class
    Creating an SPI configuration
    Further reading\n\n\n\nCreating Custom SPARQL Functions
    

  
  In this short tutoral, we’ll create a simple custom function and add it RDF4J’s SPARQL engine.
The SPARQL query language is extensible by nature: it allows you to add your own custom functions if the standard set of operators is not sufficient for your needs. The RDF4J SPARQL engine has been designed with this extensibility in mind: you can define your own custom function and use it as part of your SPARQL queries.
Here, we are going to implement a boolean function that detects if some string literal is a palindrome.
The palindrome function
Suppose we have the following RDF data:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix ex: <http://example.org/> .

ex:a rdfs:label "step on no pets" .
ex:b rdfs:label "go on, try it" .
We would like to be able to formulate a SPARQL query that allows us to retrieve all resources that have a palindrome as their label:
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX cfn: <http://example.org/custom-function/>
SELECT ?x ?label
WHERE {
   ?x rdfs:label ?label .
   FILTER(cfn:palindrome(str(?label)))
}
The expected result of this query, given the above data, would be:



x
label




ex:a
"step on no pets"



Unfortunately, the function cfn:palindrome is not a standard SPARQL function, so this query won’t work: the RDF4J SPARQL engine will simply report an error.
We could of course retrieve all label values in the database and then do some checking ourselves on these values, to detect if they’re palindromes. However if we add a custom function instead, we remove the need to scan over the entire database: the SPARQL engine itself can determine if a value is a valid palindrome or not, which removes the need for us to loop over all possible values.
There’s two basic steps in adding custom functions to RDF4J:

implementing a Java class for the function;
creating a JAR file with your function code in it and an Service Provider Interface (SPI) configuration.

Implementing the custom function as a Java class
In the RDF4J SPARQL engine, functions are expected to implement the Function
 interface.
package org.eclipser.rdf4j.examples.function;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

public class PalindromeFunction implements Function { }
The Function interface defines two methods: evaluate() and getURI(). The latter of these is a simple method that returns a string representation of the URI of the function:
// define a constant for the namespace of our custom function
public static final String NAMESPACE = "http://example.org/custom-function/";

/**
 * return the URI 'http://example.org/custom-function/palindrome' as a String
 */
public String getURI() {
    return NAMESPACE + "palindrome";
}
The real proof of the pudding is in the evaluate() method: this is where the function logic is implemented. In other words, in this method we check the incoming value to see if it is, first of all, a valid argument for the function, and second of all, a palindrome, and return the result.
Example 1
 show how we put everything together:
package org.eclipse.rdf4j.examples.function;

import static org.eclipse.rdf4j.model.util.Values.literal;

import org.eclipse.rdf4j.model.Literal;
import org.eclipse.rdf4j.model.Value;
import org.eclipse.rdf4j.query.algebra.evaluation.ValueExprEvaluationException;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

/**
 * An example custom SPARQL function that detects palindromes
 *
 * @author Jeen Broekstra
 */
public class PalindromeFunction implements Function {

    // define a constant for the namespace of our custom function
    public static final String NAMESPACE = "http://example.org/custom-function/";

    /**
     * return the URI 'http://example.org/custom-function/palindrome' as a
     * String
     */
    public String getURI() {
	return NAMESPACE + "palindrome";
    }

    /**
     * Executes the palindrome function.
     *
     * @return A boolean literal representing true if the input argument is a
     *         palindrome, false otherwise.
     * @throws ValueExprEvaluationException
     *         if more than one argument is supplied or if the supplied argument
     *         is not a literal.
     */
    public Value evaluate(TripleSource tripleSource, Value... args)
	throws ValueExprEvaluationException
	{
	    // our palindrome function expects only a single argument, so throw an error
	    // if there's more than one
	    if (args.length != 1) {
		throw new ValueExprEvaluationException(
			"palindrome function requires"
			+ "exactly 1 argument, got "
			+ args.length);
	    }
	    Value arg = args[0];
	    // check if the argument is a literal, if not, we throw an error
	    if (!(arg instanceof Literal)) {
		throw new ValueExprEvaluationException(
			"invalid argument (literal expected): " + arg);
	    }

	    // get the actual string value that we want to check for palindrome-ness.
	    String label = ((Literal)arg).getLabel();
	    // we invert our string
	    String inverted = "";
	    for (int i = label.length() - 1; i >= 0; i--) {
		inverted += label.charAt(i);
	    }
	    // a string is a palindrome if it is equal to its own inverse
	    boolean palindrome = inverted.equalsIgnoreCase(label);

	    // a function is always expected to return a Value object, so we
	    // return our boolean result as a Literal
	    return literal(palindrome);
	}
}
You are completely free to implement your function logic: in the above example, we have created a function that only returns true or false, but since the actual return type of an RDF4J function is Value
, you can create functions that return string literals, numbers, dates, or even IRIs or blank nodes.
In addition, the evaluate method accepts a TripleSource as input parameter, which you can use to inspect the underlying database, and query it for further information (for a simple/silly example see the Existing Palindrome function
, which in addition to checking that the argument is a palindrome, also checks if that palindrome already exists in the database).
There are two important things to keep in mind though:

the evaluate() method is invoked for every single solution in the query result. So you should make sure that the implementation of your function is not overly complex and memory-intensive.
RDF4J treats functions as singletons. This means that you should not “keep state” as part of your function; for example storing intermediate results in a private object field. This state will carry over between different uses of the function and even between different queries using it, making your results inconsistent.

Once we have created the Java class for our function, we need some way to make the RDF4J SPARQL engine aware of it. This is where the Service Provider Interface (SPI) comes into play.
Creating an SPI configuration
RDF4J’s set of SPARQL functions is dynamically determined through the use of a java.util.ServiceLoader class. Specifically, RDF4J has a class called FunctionRegistry
 which keeps track of all implementations of the Function interface. Java’s SPI mechanism depends on the presence of configuration files in the JAR files that contain service implementations. This configuration file is expected to be present in the directory META-INF/services in your JAR file.
In the case of the SPARQL function registry, the name of this configuration file should be org.eclipse.rdf4j.query.algebra.evaluation.function.Function (in other words, the file name is equal to the fully-qualified name of the service interface we are providing an implementation for). The contents are really quite simple: an SPI configuration is a text file, containing the fully-qualified names of each Java class that provides an SPI implementation, one on each line. So in our case, the contents of the file would be:
org.eclipse.rdf4j.example.function.PalindromeFunction
Apart from this configuration file, your JAR file should of course also contain the actual compiled class. All of this is fairly easy to do, for example from your Eclipse project:

create a directory META-INF and a subdirectory META-INF/services within the src directory of your project (or, if you use Maven, within src/main/resources) See our example resources dir for an example;
Add a text file named org.eclipse.rdf4j.query.algebra.evaluation.function.Function to this new directory. Make sure it contains a single line with the fully qualified name of your custom function class (in our example, that’s org.eclipse.rdf4j.example.function.PalindromeFunction);
Use Eclipse’s export function (or alternatively Maven’s package command) to create a JAR file (select the project, click ‘File’ -> ‘Export’ -> ‘JAR file’). Make sure the JAR file produced contains your compiled code and the sevice registry config file.

Once you have a proper JAR file, you need to add it the runtime classpath of your RDF4J project (or if you’re aiming to use this in an RDF4J Server, add it to the RDF4J Server webapp classpath and restart). After that, you’re done: RDF4J should automatically pick up your new custom function, you can from now on use it in your SPARQL queries.
Further reading

Introduction to the Service Provider Interface - Oracle Java documentation.

If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    The palindrome function
    Implementing the custom function as a Java class
    Creating an SPI configuration
    Further reading\n\n\n\nCreating Custom SPARQL Functions
    

  
  In this short tutoral, we’ll create a simple custom function and add it RDF4J’s SPARQL engine.
The SPARQL query language is extensible by nature: it allows you to add your own custom functions if the standard set of operators is not sufficient for your needs. The RDF4J SPARQL engine has been designed with this extensibility in mind: you can define your own custom function and use it as part of your SPARQL queries.
Here, we are going to implement a boolean function that detects if some string literal is a palindrome.
The palindrome function
Suppose we have the following RDF data:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix ex: <http://example.org/> .

ex:a rdfs:label "step on no pets" .
ex:b rdfs:label "go on, try it" .
We would like to be able to formulate a SPARQL query that allows us to retrieve all resources that have a palindrome as their label:
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX cfn: <http://example.org/custom-function/>
SELECT ?x ?label
WHERE {
   ?x rdfs:label ?label .
   FILTER(cfn:palindrome(str(?label)))
}
The expected result of this query, given the above data, would be:



x
label




ex:a
"step on no pets"



Unfortunately, the function cfn:palindrome is not a standard SPARQL function, so this query won’t work: the RDF4J SPARQL engine will simply report an error.
We could of course retrieve all label values in the database and then do some checking ourselves on these values, to detect if they’re palindromes. However if we add a custom function instead, we remove the need to scan over the entire database: the SPARQL engine itself can determine if a value is a valid palindrome or not, which removes the need for us to loop over all possible values.
There’s two basic steps in adding custom functions to RDF4J:

implementing a Java class for the function;
creating a JAR file with your function code in it and an Service Provider Interface (SPI) configuration.

Implementing the custom function as a Java class
In the RDF4J SPARQL engine, functions are expected to implement the Function
 interface.
package org.eclipser.rdf4j.examples.function;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

public class PalindromeFunction implements Function { }
The Function interface defines two methods: evaluate() and getURI(). The latter of these is a simple method that returns a string representation of the URI of the function:
// define a constant for the namespace of our custom function
public static final String NAMESPACE = "http://example.org/custom-function/";

/**
 * return the URI 'http://example.org/custom-function/palindrome' as a String
 */
public String getURI() {
    return NAMESPACE + "palindrome";
}
The real proof of the pudding is in the evaluate() method: this is where the function logic is implemented. In other words, in this method we check the incoming value to see if it is, first of all, a valid argument for the function, and second of all, a palindrome, and return the result.
Example 1
 show how we put everything together:
package org.eclipse.rdf4j.examples.function;

import static org.eclipse.rdf4j.model.util.Values.literal;

import org.eclipse.rdf4j.model.Literal;
import org.eclipse.rdf4j.model.Value;
import org.eclipse.rdf4j.query.algebra.evaluation.ValueExprEvaluationException;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

/**
 * An example custom SPARQL function that detects palindromes
 *
 * @author Jeen Broekstra
 */
public class PalindromeFunction implements Function {

    // define a constant for the namespace of our custom function
    public static final String NAMESPACE = "http://example.org/custom-function/";

    /**
     * return the URI 'http://example.org/custom-function/palindrome' as a
     * String
     */
    public String getURI() {
	return NAMESPACE + "palindrome";
    }

    /**
     * Executes the palindrome function.
     *
     * @return A boolean literal representing true if the input argument is a
     *         palindrome, false otherwise.
     * @throws ValueExprEvaluationException
     *         if more than one argument is supplied or if the supplied argument
     *         is not a literal.
     */
    public Value evaluate(TripleSource tripleSource, Value... args)
	throws ValueExprEvaluationException
	{
	    // our palindrome function expects only a single argument, so throw an error
	    // if there's more than one
	    if (args.length != 1) {
		throw new ValueExprEvaluationException(
			"palindrome function requires"
			+ "exactly 1 argument, got "
			+ args.length);
	    }
	    Value arg = args[0];
	    // check if the argument is a literal, if not, we throw an error
	    if (!(arg instanceof Literal)) {
		throw new ValueExprEvaluationException(
			"invalid argument (literal expected): " + arg);
	    }

	    // get the actual string value that we want to check for palindrome-ness.
	    String label = ((Literal)arg).getLabel();
	    // we invert our string
	    String inverted = "";
	    for (int i = label.length() - 1; i >= 0; i--) {
		inverted += label.charAt(i);
	    }
	    // a string is a palindrome if it is equal to its own inverse
	    boolean palindrome = inverted.equalsIgnoreCase(label);

	    // a function is always expected to return a Value object, so we
	    // return our boolean result as a Literal
	    return literal(palindrome);
	}
}
You are completely free to implement your function logic: in the above example, we have created a function that only returns true or false, but since the actual return type of an RDF4J function is Value
, you can create functions that return string literals, numbers, dates, or even IRIs or blank nodes.
In addition, the evaluate method accepts a TripleSource as input parameter, which you can use to inspect the underlying database, and query it for further information (for a simple/silly example see the Existing Palindrome function
, which in addition to checking that the argument is a palindrome, also checks if that palindrome already exists in the database).
There are two important things to keep in mind though:

the evaluate() method is invoked for every single solution in the query result. So you should make sure that the implementation of your function is not overly complex and memory-intensive.
RDF4J treats functions as singletons. This means that you should not “keep state” as part of your function; for example storing intermediate results in a private object field. This state will carry over between different uses of the function and even between different queries using it, making your results inconsistent.

Once we have created the Java class for our function, we need some way to make the RDF4J SPARQL engine aware of it. This is where the Service Provider Interface (SPI) comes into play.
Creating an SPI configuration
RDF4J’s set of SPARQL functions is dynamically determined through the use of a java.util.ServiceLoader class. Specifically, RDF4J has a class called FunctionRegistry
 which keeps track of all implementations of the Function interface. Java’s SPI mechanism depends on the presence of configuration files in the JAR files that contain service implementations. This configuration file is expected to be present in the directory META-INF/services in your JAR file.
In the case of the SPARQL function registry, the name of this configuration file should be org.eclipse.rdf4j.query.algebra.evaluation.function.Function (in other words, the file name is equal to the fully-qualified name of the service interface we are providing an implementation for). The contents are really quite simple: an SPI configuration is a text file, containing the fully-qualified names of each Java class that provides an SPI implementation, one on each line. So in our case, the contents of the file would be:
org.eclipse.rdf4j.example.function.PalindromeFunction
Apart from this configuration file, your JAR file should of course also contain the actual compiled class. All of this is fairly easy to do, for example from your Eclipse project:

create a directory META-INF and a subdirectory META-INF/services within the src directory of your project (or, if you use Maven, within src/main/resources) See our example resources dir for an example;
Add a text file named org.eclipse.rdf4j.query.algebra.evaluation.function.Function to this new directory. Make sure it contains a single line with the fully qualified name of your custom function class (in our example, that’s org.eclipse.rdf4j.example.function.PalindromeFunction);
Use Eclipse’s export function (or alternatively Maven’s package command) to create a JAR file (select the project, click ‘File’ -> ‘Export’ -> ‘JAR file’). Make sure the JAR file produced contains your compiled code and the sevice registry config file.

Once you have a proper JAR file, you need to add it the runtime classpath of your RDF4J project (or if you’re aiming to use this in an RDF4J Server, add it to the RDF4J Server webapp classpath and restart). After that, you’re done: RDF4J should automatically pick up your new custom function, you can from now on use it in your SPARQL queries.
Further reading

Introduction to the Service Provider Interface - Oracle Java documentation.

If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    The palindrome function
    Implementing the custom function as a Java class
    Creating an SPI configuration
    Further reading\n\n\n\nCreating Custom SPARQL Functions
    

  
  In this short tutoral, we’ll create a simple custom function and add it RDF4J’s SPARQL engine.
The SPARQL query language is extensible by nature: it allows you to add your own custom functions if the standard set of operators is not sufficient for your needs. The RDF4J SPARQL engine has been designed with this extensibility in mind: you can define your own custom function and use it as part of your SPARQL queries.
Here, we are going to implement a boolean function that detects if some string literal is a palindrome.
The palindrome function
Suppose we have the following RDF data:
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix ex: <http://example.org/> .

ex:a rdfs:label "step on no pets" .
ex:b rdfs:label "go on, try it" .
We would like to be able to formulate a SPARQL query that allows us to retrieve all resources that have a palindrome as their label:
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX cfn: <http://example.org/custom-function/>
SELECT ?x ?label
WHERE {
   ?x rdfs:label ?label .
   FILTER(cfn:palindrome(str(?label)))
}
The expected result of this query, given the above data, would be:



x
label




ex:a
"step on no pets"



Unfortunately, the function cfn:palindrome is not a standard SPARQL function, so this query won’t work: the RDF4J SPARQL engine will simply report an error.
We could of course retrieve all label values in the database and then do some checking ourselves on these values, to detect if they’re palindromes. However if we add a custom function instead, we remove the need to scan over the entire database: the SPARQL engine itself can determine if a value is a valid palindrome or not, which removes the need for us to loop over all possible values.
There’s two basic steps in adding custom functions to RDF4J:

implementing a Java class for the function;
creating a JAR file with your function code in it and an Service Provider Interface (SPI) configuration.

Implementing the custom function as a Java class
In the RDF4J SPARQL engine, functions are expected to implement the Function
 interface.
package org.eclipser.rdf4j.examples.function;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

public class PalindromeFunction implements Function { }
The Function interface defines two methods: evaluate() and getURI(). The latter of these is a simple method that returns a string representation of the URI of the function:
// define a constant for the namespace of our custom function
public static final String NAMESPACE = "http://example.org/custom-function/";

/**
 * return the URI 'http://example.org/custom-function/palindrome' as a String
 */
public String getURI() {
    return NAMESPACE + "palindrome";
}
The real proof of the pudding is in the evaluate() method: this is where the function logic is implemented. In other words, in this method we check the incoming value to see if it is, first of all, a valid argument for the function, and second of all, a palindrome, and return the result.
Example 1
 show how we put everything together:
package org.eclipse.rdf4j.examples.function;

import static org.eclipse.rdf4j.model.util.Values.literal;

import org.eclipse.rdf4j.model.Literal;
import org.eclipse.rdf4j.model.Value;
import org.eclipse.rdf4j.query.algebra.evaluation.ValueExprEvaluationException;
import org.eclipse.rdf4j.query.algebra.evaluation.function.Function;

/**
 * An example custom SPARQL function that detects palindromes
 *
 * @author Jeen Broekstra
 */
public class PalindromeFunction implements Function {

    // define a constant for the namespace of our custom function
    public static final String NAMESPACE = "http://example.org/custom-function/";

    /**
     * return the URI 'http://example.org/custom-function/palindrome' as a
     * String
     */
    public String getURI() {
	return NAMESPACE + "palindrome";
    }

    /**
     * Executes the palindrome function.
     *
     * @return A boolean literal representing true if the input argument is a
     *         palindrome, false otherwise.
     * @throws ValueExprEvaluationException
     *         if more than one argument is supplied or if the supplied argument
     *         is not a literal.
     */
    public Value evaluate(TripleSource tripleSource, Value... args)
	throws ValueExprEvaluationException
	{
	    // our palindrome function expects only a single argument, so throw an error
	    // if there's more than one
	    if (args.length != 1) {
		throw new ValueExprEvaluationException(
			"palindrome function requires"
			+ "exactly 1 argument, got "
			+ args.length);
	    }
	    Value arg = args[0];
	    // check if the argument is a literal, if not, we throw an error
	    if (!(arg instanceof Literal)) {
		throw new ValueExprEvaluationException(
			"invalid argument (literal expected): " + arg);
	    }

	    // get the actual string value that we want to check for palindrome-ness.
	    String label = ((Literal)arg).getLabel();
	    // we invert our string
	    String inverted = "";
	    for (int i = label.length() - 1; i >= 0; i--) {
		inverted += label.charAt(i);
	    }
	    // a string is a palindrome if it is equal to its own inverse
	    boolean palindrome = inverted.equalsIgnoreCase(label);

	    // a function is always expected to return a Value object, so we
	    // return our boolean result as a Literal
	    return literal(palindrome);
	}
}
You are completely free to implement your function logic: in the above example, we have created a function that only returns true or false, but since the actual return type of an RDF4J function is Value
, you can create functions that return string literals, numbers, dates, or even IRIs or blank nodes.
In addition, the evaluate method accepts a TripleSource as input parameter, which you can use to inspect the underlying database, and query it for further information (for a simple/silly example see the Existing Palindrome function
, which in addition to checking that the argument is a palindrome, also checks if that palindrome already exists in the database).
There are two important things to keep in mind though:

the evaluate() method is invoked for every single solution in the query result. So you should make sure that the implementation of your function is not overly complex and memory-intensive.
RDF4J treats functions as singletons. This means that you should not “keep state” as part of your function; for example storing intermediate results in a private object field. This state will carry over between different uses of the function and even between different queries using it, making your results inconsistent.

Once we have created the Java class for our function, we need some way to make the RDF4J SPARQL engine aware of it. This is where the Service Provider Interface (SPI) comes into play.
Creating an SPI configuration
RDF4J’s set of SPARQL functions is dynamically determined through the use of a java.util.ServiceLoader class. Specifically, RDF4J has a class called FunctionRegistry
 which keeps track of all implementations of the Function interface. Java’s SPI mechanism depends on the presence of configuration files in the JAR files that contain service implementations. This configuration file is expected to be present in the directory META-INF/services in your JAR file.
In the case of the SPARQL function registry, the name of this configuration file should be org.eclipse.rdf4j.query.algebra.evaluation.function.Function (in other words, the file name is equal to the fully-qualified name of the service interface we are providing an implementation for). The contents are really quite simple: an SPI configuration is a text file, containing the fully-qualified names of each Java class that provides an SPI implementation, one on each line. So in our case, the contents of the file would be:
org.eclipse.rdf4j.example.function.PalindromeFunction
Apart from this configuration file, your JAR file should of course also contain the actual compiled class. All of this is fairly easy to do, for example from your Eclipse project:

create a directory META-INF and a subdirectory META-INF/services within the src directory of your project (or, if you use Maven, within src/main/resources) See our example resources dir for an example;
Add a text file named org.eclipse.rdf4j.query.algebra.evaluation.function.Function to this new directory. Make sure it contains a single line with the fully qualified name of your custom function class (in our example, that’s org.eclipse.rdf4j.example.function.PalindromeFunction);
Use Eclipse’s export function (or alternatively Maven’s package command) to create a JAR file (select the project, click ‘File’ -> ‘Export’ -> ‘JAR file’). Make sure the JAR file produced contains your compiled code and the sevice registry config file.

Once you have a proper JAR file, you need to add it the runtime classpath of your RDF4J project (or if you’re aiming to use this in an RDF4J Server, add it to the RDF4J Server webapp classpath and restart). After that, you’re done: RDF4J should automatically pick up your new custom function, you can from now on use it in your SPARQL queries.
Further reading

Introduction to the Service Provider Interface - Oracle Java documentation.

If you require any further help, you can contact us to get support. We welcome your feedback.

  

     
      
        
          

  Table of Contents

  
  
    The palindrome function
    Implementing the custom function as a Java class
    Creating an SPI configuration
    Further reading\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nCreating SPARQL Queries With the SparqlBuilder
    

  
  RDF4J SparqlBuilder is a fluent Java API used to programmatically create SPARQL query strings.
It is based on the Spanqit query builder developed by Anqit Praqash, and has been slightly modified to allow tighter integration with the rest of the RDF4J framework.
SparqlBuilder allows the following SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:name ?name }
ORDER BY ?name
LIMIT 5
OFFSET 10

to be created as simply as:
query
    .prefix(FOAF.NS)
    .select(name)
    .where(x.has(FOAF.NAME, name))
    .orderBy(name)
    .limit(5)
    .offset(10);
The RDF4J SparqlBuilder is based on the SPARQL 1.1 Query Recommendation and the
SPARQL 1.1 Update Receommendation. Almost all features of SPARQL 1.1 are
supported, excluding some current known limitations.
This document assumes the reader is already familiar with the SPARQL query language. Please refer to the above specification if not.
Getting SparqlBuilder
Obtain SparqlBuilder by adding the following dependency to your maven pom file:
<dependency>
    <groupId>org.eclipse.rdf4j</groupId>
    <artifactId>rdf4j-sparqlbuilder</artifactId>
    <version>${rdf4j.version}</version>
</dependency>
Queries
The Queries class provides static methods to instantiate the various query objects. For example:
SelectQuery selectQuery = Queries.SELECT();
ConstructQuery constructQuery = Queries.CONSTRUCT();
Query objects provide methods to set or add the various elements appropriate for the type of query:
Prefix ex;
Variable product;
TriplePattern personWroteBook, personAuthoredBook;

// ...

selectQuery.prefix(ex).select(product).where(product.isA(ex.iri("book"));
constructQuery.prefix(ex).construct(personWroteBook).where(personAuthoredBook);
Elements
SPARQL elements are created using various static factory classes. Most core elements of a query are created by the static SparqlBuilder class:
import org.eclipse.rdf4j.model.vocabulary.FOAF;

Variable price = SparqlBuilder.var("price");
System.out.println(price.getQueryString()); // ==> ?price

Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
System.out.println(foaf.getQueryString()); // ==> PREFIX foaf: <http://xmlns.com/foaf/0.1/>
Other factory classes include the Queries class mentioned in the previous section, as well as the Expressions, GraphPatterns, and Rdf classes.
All query elements created by SparqlBuilder implement the QueryElement interface, which provides the getQueryString() method. This can be used to get the String representing the SPARQL syntax of any element.
Graph Patterns
SparqlBuilder uses three classes to represent the SPARQL graph patterns, all of which implement the GraphPattern interface:

The TriplePattern class represents triple patterns.
The GraphPatternNotTriple class represents collections of graph patterns.
The SubSelect class represents a SPARQL sub query.

Graph patterns are created by the more aptly named GraphPatterns class.
Triple Patterns
Use GraphPatterns#tp() to create a TriplePattern instance:
Prefix dc = SparqlBuilder.prefix("dc", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, dc.iri("author"), Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
or, using RDF4J Model object and vocabulary constants directly:
Prefix dc = SparqlBuilder.prefix(DC.NS);
Variable book = SparqlBuilder.var("book");

TriplePattern triple = GraphPatterns.tp(book, DC.AUTHOR, Rdf.literalOf("J.R.R. Tolkien"));
System.out.println(triple.getQueryString()); // ==> ?book dc:author "J.R.R. Tolkien"
In almost all places, SparqlBuilder allows either RDF4J Model objects or its own interfaces to be used. You can freely mix this.
A TriplePattern instance can also be created from the has() and isA() shortcut methods of RdfSubject, and be expanded to contain multiple triples with the same subject via predicate-object lists and object lists:
Prefix foaf = SparqlBuilder.prefix(FOAF.NS);
Variable x = SparqlBuilder.var("x"), name = SparqlBuilder.var("name");

TriplePattern triple = x.has(FOAF.NICK, Rdf.literalOf("Alice"), Rdf.literalOf("Alice_"))
    .andHas(FOAF.NAME, name);
System.out.println(triple.getQueryString());
// ===> ?x foaf:nick "Alice_", "Alice" ;
//	   foaf:name ?name .
Property Paths
Property paths can be generated with a lambda expression that uses a builder pattern
wherever a predicate (IRI) can be passed:
SelectQuery query = Queries
    .SELECT(name)
    .prefix(FOAF.NS)
    .where(x.has(p -> p.pred(FOAF.ACCOUNT)
                .then(FOAF.MBOX),
            name))
yields
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?x foaf:account / foaf:mbox ?name . }
Here are a few examples and the path they yield



property path builder code
property path




p -> p.pred(FOAF.ACCOUNT).then(FOAF.MBOX)
foaf:account / foaf:mbox


p -> p.pred(RDF.TYPE).then(RDFS.SUBCLASSOF).zeroOrMore()
rdf:type / rdfs:subClassOf *


p -> p.pred(EX.MOTHER_OF).or(EX.FATHER_OF).oneOrMore()
( ex:motherOf | ex:fatherOf ) +


p -> p.pred(EX.MOTHER_OF).or(p1 -> p1.pred(EX.FATHER_OF).zeroOrOne())
ex:motherOf | ( ex:fatherOf ? )


p -> p.negProp().pred(RDF.TYPE).invPred(RDF.TYPE)
!( rdf:type | ^ rdf:type )



Compound graph patterns
Three methods in GraphPatterns exist to create GraphPatternNotTriple instances. GraphPatterns#and() creates a group graph pattern, consisting of the GraphPattern instances passed as parameters:
Variable mbox = SparqlBuilder.var("mbox"), x = SparqlBuilder.var("x");
GraphPatternNotTriple groupPattern =
GraphPatterns.and(x.has(FOAF.NAME), name), x.has(FOAF.MBOX, mbox);
System.out.println(groupPattern.getQueryString());
// ==> { ?x foaf:mbox ?mbox . ?x foaf:name ?name }
GraphPatterns#union() creates an alternative graph pattern, taking the union of the provided GraphPattern instances:
Prefix dc10 = SparqlBuilder.prefix("dc10", iri("http://purl.org/dc/elements/1.0/")),
	dc11 = SparqlBuilder.prefix("dc11", iri("http://purl.org/dc/elements/1.1/"));
Variable book = SparqlBuilder.var("book"), title = SparqlBuilder.var("title");

GraphPatternNotTriple union = GraphPatterns.union(book.has(dc10.iri("title"), title),
	book.has(dc11.iri("title"), title);
System.out.println(union.getQueryString());
// ==> { ?book dc10:title ?title } UNION { ?book dc11:title ?title }
GraphPatterns#optional() creates an optional group graph pattern, consisting of the passed in GraphPatterns:
GraphPatternNotTriple optionalPattern = GraphPatterns.optional(GraphPatterns.tp(x, foaf.iri("mbox"), mbox));
System.out.println(optionalPattern.getQueryString());
// ==> OPTIONAL { ?x foaf:mbox ?mbox }
Sub-select
Finally, GraphPatterns#select() creates an instance of a SubSelect, which represents a SPARQL subquery:
SubSelect subQuery = GraphPatterns.select();
Query Constraints
You can create SPARQL query constraints using the Expressions class which provides static methods to create Expression objects representing SPARQL’s built-in expressions:
Variable name = SparqlBuilder.var("name");
Expression<?> regexExpression = Expressions.regex(name, "Smith");
System.out.println(regexExpression.getQueryString());
// ==> REGEX( ?name, "Smith" )
Expressions take Operand instances as arguments. Operand is implemented by the types you would expect (Variable, the RDF model interface RdfValue, and Expression itself). Where possible, Expressions has included wrappers to take appropriate Java primitives as parameters as well:
Variable price = SparqlBuilder.var("price");
Expression<?> priceLimit = Expressions.lt(price, 100);
System.out.println(priceLimit.getQueryString());
// ==> ?price < 100
For those places where a wrapper has not (yet) been created, RdfLiterals can be used instead. The Rdf class provides factory methods to create StringLiteral, NumericLiteral, and BooleanLiteral instances:
Variable price = SparqlBuilder.var("price");
ExpressionOperand discount = Rdf.literalOf(0.9);
Expression<?> discountedPrice = Expressions.multiply(price, discount);
System.out.println(discountedPrice.getQueryString());
// ==> ( ?price * 0.9 )
The RDF Model
SparqlBuilder provides a collection of interfaces to model RDF objects in the org.eclipse.rdf4j.sparqlbuilder.rdf package. Instances of these interfaces can be used when needed and as expected while creating SPARQL queries. The Rdf class contains static factory methods to create RDF objects for use with SparqlBuilder, including IRI’s, blank nodes, and RDF literals.
Currently SparqlBuilder’s set of interfaces for RDF model objects is different from the main RDF4J model interfaces. This will be further integrated in future releases.
Examples
For more detailed examples of how to use SparqlBuilder, check out the JUnit tests located within the project.
Known Limitations
Currently, the following SPARQL 1.1 features have not yet been implemented in SparqlBuilder:

Values Block
RDF Collection Syntax
DESCRIBE and ASK Queries


  

     
      
        
          

  Table of Contents

  
  
    Getting SparqlBuilder
    Queries
    Elements
    Graph Patterns
      
        Triple Patterns
        Property Paths
        Compound graph patterns
        Sub-select
      
    
    Query Constraints
    The RDF Model
    Examples
    Known Limitations\n\n\n\nProgramming With RDF4J
    

  
  
    
        
          
          Setting up your development environmentThis chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
          
          The RDF Model APIThe RDF Model API is the core of the RDF4J framework. It provides the basic building blocks for manipulating RDF data in Java.
          
          The Repository APIThe Repository API is the central access point for RDF4J-compatible RDF databases (a.k.a. triplestores), as well as for SPARQL endpoints. This is what you use to execute SPARQL queries and update your data.
          
          Parsing and Writing RDF with RioThe RDF4J framework includes a set of parsers and writers for RDF called Rio. Rio (“RDF I/O”) is a toolkit that can be used independently from the rest of RDF4J.
          
          The LMDB StoreNew in RDF4J 4.0

Experimental

The RDF4J LMDB Store is a new SAIL database, using the Symas Lightning
Memory-Mapped Database: a fast embeddable
key-value database using memory-mapped IO for great performance and stability.
          
          Full-text indexing with the Lucene SAILThe LuceneSail enables you to add full text search of RDF literals to find subject resources to any Sail stack.
          
          Reasoning and Validation with SPINThe SPARQL Inferencing Notation (SPIN) is a way to represent a wide range of business rules on top of an RDF dataset. These rules can be anything from constraint validation to inferred property value calculation.
          
          Validation with SHACLThe SHapes Constraint Language (SHACL) is a language for validating RDF graphs.
          
          Federation with FedXFedX provides transparent federation of multiple SPARQL endpoints under a single virtual endpoint.
          
          Integration with SpringThe rdf4j-spring
 module allows for using an RDF4J repository as the data backend of a spring application.
          
          GeoSPARQLRDF4J offers an extended algebra for partial GeoSPARQL support. When enabled, this offers additional geospatial functionality as part of the SPARQL engine, on top of any RDF4J repository, using the well-known Spatial4J and JTS libraries for geospatial reasoning.
          
          RDF-star and SPARQL-starRDF4J has (experimental) support for RDF-star and SPARQL-star.
          
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nIndex of /javadoc/latest/org/eclipse/rdf4j/spring\n\n\nProgramming With RDF4J
    

  
  
    
        
          
          Setting up your development environmentThis chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
          
          The RDF Model APIThe RDF Model API is the core of the RDF4J framework. It provides the basic building blocks for manipulating RDF data in Java.
          
          The Repository APIThe Repository API is the central access point for RDF4J-compatible RDF databases (a.k.a. triplestores), as well as for SPARQL endpoints. This is what you use to execute SPARQL queries and update your data.
          
          Parsing and Writing RDF with RioThe RDF4J framework includes a set of parsers and writers for RDF called Rio. Rio (“RDF I/O”) is a toolkit that can be used independently from the rest of RDF4J.
          
          The LMDB StoreNew in RDF4J 4.0

Experimental

The RDF4J LMDB Store is a new SAIL database, using the Symas Lightning
Memory-Mapped Database: a fast embeddable
key-value database using memory-mapped IO for great performance and stability.
          
          Full-text indexing with the Lucene SAILThe LuceneSail enables you to add full text search of RDF literals to find subject resources to any Sail stack.
          
          Reasoning and Validation with SPINThe SPARQL Inferencing Notation (SPIN) is a way to represent a wide range of business rules on top of an RDF dataset. These rules can be anything from constraint validation to inferred property value calculation.
          
          Validation with SHACLThe SHapes Constraint Language (SHACL) is a language for validating RDF graphs.
          
          Federation with FedXFedX provides transparent federation of multiple SPARQL endpoints under a single virtual endpoint.
          
          Integration with SpringThe rdf4j-spring
 module allows for using an RDF4J repository as the data backend of a spring application.
          
          GeoSPARQLRDF4J offers an extended algebra for partial GeoSPARQL support. When enabled, this offers additional geospatial functionality as part of the SPARQL engine, on top of any RDF4J repository, using the well-known Spatial4J and JTS libraries for geospatial reasoning.
          
          RDF-star and SPARQL-starRDF4J has (experimental) support for RDF-star and SPARQL-star.
          
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nProgramming With RDF4J
    

  
  
    
        
          
          Setting up your development environmentThis chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
          
          The RDF Model APIThe RDF Model API is the core of the RDF4J framework. It provides the basic building blocks for manipulating RDF data in Java.
          
          The Repository APIThe Repository API is the central access point for RDF4J-compatible RDF databases (a.k.a. triplestores), as well as for SPARQL endpoints. This is what you use to execute SPARQL queries and update your data.
          
          Parsing and Writing RDF with RioThe RDF4J framework includes a set of parsers and writers for RDF called Rio. Rio (“RDF I/O”) is a toolkit that can be used independently from the rest of RDF4J.
          
          The LMDB StoreNew in RDF4J 4.0

Experimental

The RDF4J LMDB Store is a new SAIL database, using the Symas Lightning
Memory-Mapped Database: a fast embeddable
key-value database using memory-mapped IO for great performance and stability.
          
          Full-text indexing with the Lucene SAILThe LuceneSail enables you to add full text search of RDF literals to find subject resources to any Sail stack.
          
          Reasoning and Validation with SPINThe SPARQL Inferencing Notation (SPIN) is a way to represent a wide range of business rules on top of an RDF dataset. These rules can be anything from constraint validation to inferred property value calculation.
          
          Validation with SHACLThe SHapes Constraint Language (SHACL) is a language for validating RDF graphs.
          
          Federation with FedXFedX provides transparent federation of multiple SPARQL endpoints under a single virtual endpoint.
          
          Integration with SpringThe rdf4j-spring
 module allows for using an RDF4J repository as the data backend of a spring application.
          
          GeoSPARQLRDF4J offers an extended algebra for partial GeoSPARQL support. When enabled, this offers additional geospatial functionality as part of the SPARQL engine, on top of any RDF4J repository, using the well-known Spatial4J and JTS libraries for geospatial reasoning.
          
          RDF-star and SPARQL-starRDF4J has (experimental) support for RDF-star and SPARQL-star.
          
      
    

  

     
      
        
          
  About

  
  Eclipse RDF4J™ is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nSetting Up Your Development Environment
    

  
  This chapter gives you some pointers on how to install the RDF4J libraries and how to initialize your project.
Before you can get started programming with RDF4J, you will need to set up your development environment, download the necessary, libraries, and so on.
Using Apache Maven
By far the most flexible way to include RDF4J in your project, is to use Maven. Apache Maven is a software management tool that helps you by offering things like library version management and dependency management (which is very useful because it means that once you decide you need a particular RDF4J library, Maven automatically downloads all the libraries that your library of choice requires in turn). For details on how to start using Maven, take a look at the Apache Maven website. If you are familiar with Maven, here are a few pointers to help set up your maven project.
Maven Repository
RDF4J is available from the Central Repository, which means you don’t need to add an additional repository configuration to your project.
The BOM (Bill Of Materials)
A problem in larger projects is a thing called ‘version mismatch’: one part of your project uses version 1.0 of a particular RDF4J artifact, and another part uses 1.0.2 of the same (or a slightly different) artifact, and because they share dependencies you get duplicate libraries on your classpath.
To help simplify this, RDF4J provides a BOM (Bill Of Materials) for you to include in your project. A BOM is basically a list of related artifacts and their versions. The advantage of including a BOM in your project is that you declare the version of RDF4J only once, and then can rely on all specific RDF4J artifact dependencies to use the correct version.
To include the BOM in your project, add the following to your project root pom:
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.eclipse.rdf4j</groupId>
      <artifactId>rdf4j-bom</artifactId>
      <version>3.0.4</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

After you have done this, you can simply include any RDF4J artifact as a normal dependency, but you can leave out the version number in that dependency. The included BOM ensures that all included RDF4J artifacts throughout your project will use version 3.0.4.
Which Maven Artifact
The groupId for all RDF4J core artifacts is org.eclipse.rdf4j. To include a maven dependency in your project that automatically gets you the entire RDF4J core framework, you can use artifactId rdf4j-storage:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-storage</artifactId>
  <type>pom</type>
</dependency>

This dependency includes all parsers, writers, core interfaces, databases and reasoners provided by RDF4J.
If you don’t need the database implementations and reasoners, but just want to use the client APIs and parser/writer utilities, you can instead use the rdf4j-client dependency:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-client</artifactId>
  <type>pom</type>
</dependency>

This will include the Model and Repository APIs, all Rio parsers and writers, and clients for connecting to remote RDF4J Servers or remote SPARQL endpoints.
You can fine-tune your dependencies further if you wish, so that you don’t include more than you need. Here are some typical scenarios and the dependencies that go with it. Of course, it’s up to you to vary on these basic scenarios and figure exactly which components you need (and if you don’t want to bother you can always just use the ‘everything and the kitchen sink’ rdf4j-storage dependency).
Simple local storage and querying of RDF
If you require functionality for quick in-memory storage and querying of RDF, you will need to include dependencies on the SAIL repository module (artifactId rdf4j-repository-sail) and the in-memory storage backend module (artifactId rdf4j-sail-memory):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sail</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-memory</artifactId>
</dependency>

A straightforward variation on this scenario is of course if you decide you need a more scalable persistent storage instead of (or alongside) simple in-memory storage. In this case, you can include the native store:
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-sail-nativerdf</artifactId>
</dependency>

Parsing / writing RDF files
The RDF4J parser toolkit is called Rio, and it is split in several modules: one for its main API (rdf4j-rio-api), and one for each specific syntax format. If you require functionality to parse or write an RDF file, you will need to include a dependency on any of the parsers for that you will want to use. For example, if you need an RDF/XML syntax parser and a Turtle syntax writer, include the following two dependencies (you do not need to include the API dependency explicitly since each parser implementation depends on it already):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-rdfxml</artifactId>
</dependency>
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-rio-turtle</artifactId>
</dependency>

Accessing a remote RDF4J Server
If your project only needs functionality to query/manipulate a remotely running RDF4J Server, you can stick to just including the HTTPRepository module (rdf4j-repository-http):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-http</artifactId>
</dependency>

Accessing a SPARQL endpoint
If you want to have functionality to query a remote SPARQL endpoint, such as DBPedia, you can use the SPARQLRepository module  (rdf4j-repository-sparql):
<dependency>
  <groupId>org.eclipse.rdf4j</groupId>
  <artifactId>rdf4j-repository-sparql</artifactId>
</dependency>

Using the onejar or SDK distribution
If you are not familiar with Apache Maven, an alternative way to get started with using the RDF4J libraries is to download the RDF4J onejar library and include it in your classpath.
The RDF4J onejar contains all of RDF4J’s own functionality. However, it does not contain any of the third-party libraries on which RDF4J depends, which means that if you use the onejar, you will, in addition, need to download and install these third-party libraries (if your project does not already use them, as most of these libraries are pretty common).
It is important to note that the RDF4J framework consists of a set of libraries: RDF4J is not a monolithic piece of software, you can pick and choose which parts you want and which ones you don’t. In those cases where you don’t care about picking and choosing and just want to get on with it, the onejar is a good choice.
If, however, you want a little more control over what is included, you can download the complete SDK and select (from the lib directory) those libraries that you require. The SDK distribution contains all RDF4J libraries as individual jar files, and in addition it also contains all the third-party libraries you need work with RDF4J.
NOTE: we are considering changing the onejar distribution in a future release, to have it include all third-party dependencies, including a logger implementation, as a “quick start” option for RDF4J. We welcome your feedback on this at Github issue #1791.
Logging: SLF4J initialization
Before you begin using RDF4J , one important configuration step needs to be taken: the initialization and configuration of a logging framework.
RDF4J uses the Simple Logging Facade for Java (SLF4J), which is a framework for abstracting from the actual logging implementation. SLF4J allows you, as a user of the RDF4J framework, to plug in your own favorite logging implementation. SLF4J supports the most popular logging implementations such as Java Logging, Apache Commons Logging, Logback, log4j, etc. See the SLF4J website for more info.
What you need to do is to decide which logging implementation you are going to use and include the appropriate SLF4J logger adapter in your classpath. For example, if you decide to use Apache Log4J, you need to include the SFL4J-Log4J adapter in your classpath. The SLF4J release packages includes adapters for various logging implementations; just download the SLF4J release package and include the appropriate adapter in your classpath (or, when using Maven, set the appropriate dependency); slf4j-log4j12-<version>.jar, for example.
One thing to keep in mind when configuring logging is that SLF4J expects only a single logger implementation on the classpath. Thus, you should choose only a single logger. In addition, if parts of your code depend on projects that use other logging frameworks directly, you can include a Legacy Bridge which makes sure calls to the legacy logger get redirected to SLF4J (and from there on, to your logger of choice).
In particular, when working with RDF4J’s HTTPRepository or SPARQLRepository libraries, you should include the jcl-over-slf4j legacy bridge. This is because RDF4J internally uses the Apache Commons HttpClient, which relies on JCL (Jakarta Commons Logging). You can do without this if your own app is a webapp, to be deployed in e.g. Tomcat, but otherwise, your application will probably show a lot of debug log messages on standard output, starting with something like:
DEBUG httpclient.wire.header

When you set this up correctly, you can have a single logger configuration for your entire project, and you will be able to control both this kind of logging by third party libraries and by RDF4J itself using this single config.
The RDF4J framework itself does not prescribe a particular logger implementation (after all, that’s the whole point of SLF4J, that you get to choose your preferred logger). However, several of the applications included in RDF4J (such as RDF4J Server, Workbench, and the command line console) do use a logger implementation. The server and console application both use logback, which is the successor to log4j and a native implementation of SLF4J. The Workbench uses java.util.logging instead.

  

     
      
        
          

  Table of Contents

  
  
    Using Apache Maven
      
        Maven Repository
        The BOM (Bill Of Materials)
        Which Maven Artifact
        Simple local storage and querying of RDF
        Parsing / writing RDF files
        Accessing a remote RDF4J Server
        Accessing a SPARQL endpoint
      
    
    Using the onejar or SDK distribution
    Logging: SLF4J initialization\n\n\n\nThe RDF Model API
    

  
  The RDF Model API is the core of the RDF4J framework. It provides the basic building blocks for manipulating RDF data in Java.
RDF Building Blocks: IRIs, literals, blank nodes and statements
The core of the RDF4J framework is the RDF Model API (see the Model API Javadoc), defined in package org.eclipse.rdf4j.model. This API defines how the building blocks of RDF (statements, IRIs, blank nodes, literals, and models) are represented.
RDF statements are represented by the Statement
 interface. Each Statement has a subject, predicate, object and (optionally) a context. Each of these 4 items is a Value
. The Value interface is further specialized into Resource
, and Literal
. Resource represents any RDF value that is either a BNode
 or an IRI
. Literal represents RDF literal values (strings, dates, integer numbers, and so on).
Creating new building blocks: the Values and Statements factory methods
 New in RDF4J 3.5 

To create new values and statements, you can use the Values
  and Statements
 static factory methods, which provide easy creation of new IRIs, Literals, BNodes, Triples and Statements based on a variety of different input objects.
import static org.eclipse.rdf4j.model.util.Statements.statement;
import static org.eclipse.rdf4j.model.util.Values.iri;
import static org.eclipse.rdf4j.model.util.Values.literal;

IRI bob = iri("http://example.org/bob");
IRI nameProp = iri("http://example.org/name");
Literal bobsName = literal("Bob");
Literal bobsAge = literal(42);

Statement st = statement(bob, nameProp, bobsName, null);
Using a ValueFactory
If you want more control than the static factory methods provide, you can also use a ValueFactory
 instance. You can obtain one from Values
, or you can directly use a singleton ValueFactory implementation called SimpleValueFactory
:
import org.eclipse.rdf4j.model.ValueFactory;
import org.eclipse.rdf4j.model.impl.SimpleValueFactory;

ValueFactory factory = SimpleValueFactory.getInstance();
For performance reasons, the SimpleValueFactory provides only basic input validation. The ValidatingValueFactory
 is stricter, albeit somewhat slower (though this should not be noticeable unless you are working with very significant amounts of data).
You can also obtain a ValueFactory from the Repository
 you are working with, and in fact, this is the recommend approach. For more information about this see the Repository API documentation.
Regardless of how you obtain your ValueFactory, once you have it, you can use it to create new IRIs, Literals, and Statements:
IRI bob = iri(factory, "http://example.org/bob");
IRI name = iri(factory,"http://example.org/name");
Literal bobsName = literal(factory, "Bob");
Statement nameStatement = statement(factory, bob, name, bobsName);
Or if you prefer, using the Valuefactory directly:
IRI bob = factory.createIRI("http://example.org/bob");
IRI name = factory.createIRI("http://example.org/name");
Literal bobsName = factory.createLiteral("Bob");
Statement nameStatement = factory.createStatement(bob, name, bobsName);
The Model API also provides pre-defined IRIs for several well-known vocabularies, such as RDF, RDFS, OWL, DC (Dublin Core), FOAF (Friend-of-a-Friend), and more. These constants can all be found in the org.eclipse.rdf4j.model.vocabulary package, and can be quite handy in quick creation of RDF statements (or in querying a Repository, as we shall see later):
Statement typeStatement = Values.statement(bob, RDF.TYPE, FOAF.PERSON);
The Model interface
The above interfaces and classes show how we can create the individual building blocks that make up an RDF model. However, an actual collection of RDF data is just that: a collection. In order to deal with collections of RDF statements, we can use the org.eclipse.rdf4j.model.Model
 interface.
Model is an extension of the default Java Collection class java.util.Set<Statement>. This means that you can use a Model like any other Java collection in your code:
// create a new Model to put statements in
Model model = DynamicModelFactory.createEmptyModel();
// add an RDF statement
model.add(typeStatement);
// add another RDF statement by simply providing subject, predicate, and object.
model.add(bob, name, bobsName);

// iterate over every statement in the Model
for (Statement statement: model) {
	   ...
}
In addition, however, Model offers a number of useful methods to quickly get subsets of statements and otherwise search/filter your collection of statements. For example, to quickly iterate over all statements that make a resource an instance of the class foaf:Person, you can do:
for (Statement typeStatement: model.filter(null, RDF.TYPE, FOAF.PERSON)) {
  // ...
}
Even more convenient is that you can quickly retrieve the building blocks that make up the statements. For example, to immediately iterate over all subject-resources that are of type foaf:Person and then retrieve each person’s name, you can do something like the following:
for (Resource person: model.filter(null, RDF.TYPE, FOAF.PERSON).subjects()) {
  // get the name of the person (if it exists)
  Optional<Literal> name = Models.objectLiteral(model.filter(person, FOAF.NAME, null));
}
The filter() method returns a Model again. However, the Model returned by this method is still backed by the original Model. Thus, changes that you make to this returned Model will automatically be reflected in the original Model as well.
RDF4J provides three default implementations of the Model interface: org.eclipse.rdf4j.model.impl.DynamicModel
, org.eclipse.rdf4j.model.impl.LinkedHashModel
, and org.eclipse.rdf4j.model.impl.TreeModel
. The difference between them is in their performance for different kinds of lookups and insertion patterns (see their respective javadoc entries for details). These differences are only really noticeable when dealing with quite large collections of statements, however.
Building RDF Models with the ModelBuilder
Since version 2.1, RDF4J provides a ModelBuilder
 utility. The ModelBuilder provides a fluent API to quickly and efficiently create RDF models programmatically.
Here’s a simple code example that demonstrates how to quickly create an RDF graph with some FOAF data:
ModelBuilder builder = new ModelBuilder();

// set some namespaces
builder.setNamespace("ex", "http://example.org/").setNamespace(FOAF.NS);

builder.namedGraph("ex:graph1")      // add a new named graph to the model
       .subject("ex:john")        // add  several statements about resource ex:john
	 .add(FOAF.NAME, "John")  // add the triple (ex:john, foaf:name "John") to the named graph
	 .add(FOAF.AGE, 42)
	 .add(FOAF.MBOX, "john@example.org");

// add a triple to the default graph
builder.defaultGraph().add("ex:graph1", RDF.TYPE, "ex:Graph");

// return the Model object
Model m = builder.build();
The ModelBuilder offers several conveniences:

you can specify a subject/predicate IRI as a prefixed name string (for example “ex:john”), so you don’t have to use a ValueFactory to create an IRI object first.
you can add a literal object as a String, an int, or several other supported Java primitive types.
the subject() method makes it easier to take a resource-centric view when building an RDF Model.

Quickly accessing data with the Models utility
The Models
 utility class offers a number of useful methods for convenient access and manipulation of data in a Model object. We have already shown some examples of its use in previous sections. For example, to retrieve the value of the foaf:name properties for all resources of type foaf:Person:
for (Resource person: model.filter(null, RDF.TYPE, FOAF.PERSON).subjects()) {
  // get the name of the person (if it exists)
  Optional<Literal> name = Models.objectLiteral(model.filter(person, FOAF.NAME, null));
}
The Models.objectLiteral method retrieves an arbitrary object literal value from the statements in the supplied Model. Since the supplied Model is filtered to only contain the foaf:name statements for the given person, the resulting object literal value is the name value for this person. Note that if the model happens to contain more than one name value for this person, this will just return an arbitrary one.
The Models utility provides variants for retrieving different types of object values: Models.object() retrieves a Value, Models.objectResource() a Resource, Models.objectIRI() an IRI.
Property-centric access
To provide quicker access to a property’s value(s), the Models class offers some further shortcuts that bypass the need to first filter the Model. For example, to retrieve the name literal, we can replace the `objectLiteral call from the previous example like so:
for (Resource person: model.filter(null, RDF.TYPE, FOAF.PERSON).subjects()) {
  // get the name of the person (if it exists)
  Optional<Literal> name = Models.getPropertyLiteral(model, person, FOAF.NAME);
}
Models also provides methods that allow retrieving all values, instead of one arbitrary one:
for (Resource person: model.filter(null, RDF.TYPE, FOAF.PERSON).subjects()) {
  // get all name-values of the person
  Set<Literal> names = Models.getPropertyLiterals(model, person, FOAF.NAME);
}
For both retrieval types, Models also provides variants that retrieve other value types such as IRIs. The Models
 javadoc is worth exploring for a complete overview of all methods.
In addition to retrieving values in a property-centric manner, Models also provides a setProperty method, which can be used to quickly give a resoure’s property a new value. For example:
Literal newName = vf.createLiteral("John");
Models.setProperty(person, FOAF.NAME, newName);
This will remove any existing name-properties for the given person, and set it to the single new value “John”.
RDF Collections
To model closed lists of items, RDF provides a Collection vocabulary . RDF Collections are represented as a list of items using a Lisp-like structure. The list starts with a head resource (typically a blank node), which is connected to the first collection member via the rdf:first relation. The head resource is then connected to the rest of the list via an rdf:rest relation. The last resource in the list is marked using the rdf:nil node.
As an example, a list containing three values, “A”, “B”, and “C” looks like this as an RDF Collection:

Here, the blank node _:n1 is the head resource of the list. In this example it is declared an instance of rdf:List, however this is not required for the collection to be considered well-formed. For each collection member, a new node is added (linked to the previous node via the rdf:rest property), and the actual member value is linked to to this node via the rdf:first property. The last member member of the list is marked by the fact that the value of its rdf:rest property is set to rdf:nil.
Working with this kind of structure directly is rather cumbersome. To make life a little easier, rdf4j provides several utilities to convert between Java Collections and RDF Collections.
Converting to/from Java Collections
As an example, suppose we wish to add the above list of three string literals as a property value for the property ex:favoriteLetters of ex:John .
The RDFCollections
 utility allows us to do this, as follows:
import static org.eclipse.rdf4j.model.util.Values.bnode;
import static org.eclipse.rdf4j.model.util.Values.iri;
import static org.eclipse.rdf4j.model.util.Values.literal;

String ns = "http://example.org/";
// IRI for ex:favoriteLetters
IRI favoriteLetters = iri(ns, "favoriteLetters");
// IRI for ex:John
IRI john = iri(ns, "John");
// create a list of letters
List<Literal> letters = Arrays.asList(new Literal[] { literal("A"), literal("B"), literal("C") });
// create a head resource for our list
Resource head = bnode();
// convert our list and add it to a newly-created Model
Model aboutJohn = RDFCollections.asRDF(letters, head, new LinkedHashModel());
// set the ex:favoriteLetters property to link to the head of the list
aboutJohn.add(john, favoriteLetters, head);
Of course, we can also convert back:
Model aboutJohn = ... ; // our Model about John
// get the value of the ex:favoriteLetters property
Resource node = Models.objectResource(aboutJohn.filter(john, favoriteLetters, null)).orElse(null);
// Convert its collection back to an ArrayList of values
if(node != null) {
	 List<Value> values = RDFCollections.asValues(aboutJohn, node, new ArrayList<Value>());
	 // you may need to cast back to Literal.
	 Literal a = (Literal)values.get(0);
}
Extracting, copying, or deleting an RDF Collection
To extract an RDF Collection from the model which contains it, we can do the following:
Model aboutJohn = ...; // our model
// get the value of the ex:favoriteLetters property
Resource node = Models.objectResource(aboutJohn.filter(john, favoriteLetters, null)).orElse(null);
// get the RDF Collection in a separate model
if (node != null) {
	 Model rdfList = RDFCollections.getCollection(aboutJohn, node, new LinkedHashModel());
}
As you can see, instead of converting the RDF Collection to a Java List of values, we get back another Model object from this, containing a copy of the RDF statements that together form the RDF Collection. This is useful in cases where your original Model contains more data than just the RDF Collection, and you want to isolate the collection.
Once you have this copy of your Collection, you can use it to add it somewhere else, or to remove the collection from your Model:
// remove the collection from our model about John
aboutJohn.removeAll(rdfList);
// finally remove the triple that linked John to the collection
aboutJohn.remove(john, favoriteLetters, node);
Actually, deleting can be done more efficiently than this. Rather than first creating a completely new copy of the RDF Collection only to then delete it, we can use a streaming approach instead:
// extract the collection from our model in streaming fashion and remove each triple from the model
RDFCollections.extract(aboutJohn, node, st -> aboutJohn.remove(st));
// remove the statement that linked john to the collection
aboutJohn.remove(john, favoriteLetters, node);
Working with rdf:Alt, rdf:Bag, rdf:Seq
(new since 3.3.0)
The RDF container classes rdf:Alt, rdf:Bag, and rdf:Seq can also be used to model sets or lists of items in RDF. RDF containers look like this:
   urn:myBag -rdf:type--> rdf:Bag
     |
     +---rdf:_1--> "A"
     |
     +---rdf:_2--> "B"
     |
     +---rdf:_3--> "C"
RDF4J offers utility conversion functions very similar to the utilities for RDF Collections: the  RDFContainers
 class.
For example, to create the above RDF container, we can do this:
List<Literal> letters = Arrays.asList(new Literal[] { literal("A"), literal("B"), literal("C") });
IRI myBag = iri("urn:myBag");
Model letterBag = RDFContainers.toRDF(RDF.BAG, letters, myBag, new TreeModel());
and to convert back to a java collection:
List<Value> newList = RDFContainers.toValues(RDF.BAG, letterBag, myBag, new ArrayList<>());


  

     
      
        
          

  Table of Contents

  
  
    RDF Building Blocks: IRIs, literals, blank nodes and statements
      
        Creating new building blocks: the Values and Statements factory methods
        Using a ValueFactory
      
    
    The Model interface
    Building RDF Models with the ModelBuilder
    Quickly accessing data with the Models utility
      
        Property-centric access
      
    
    RDF Collections
      
        Converting to/from Java Collections
        Extracting, copying, or deleting an RDF Collection
      
    
    Working with rdf:Alt, rdf:Bag, rdf:Seq\n\n\n\nInterface Resource



All Superinterfaces:
Serializable, Value


All Known Subinterfaces:
BNode, IRI, LmdbResource, NativeResource, Triple


All Known Implementing Classes:
AbstractBNode, AbstractIRI, AbstractTriple, CorruptIRI, CorruptIRIOrBNode, InternedIRI, LmdbBNode, LmdbIRI, MemBNode, MemIRI, MemResource, MemTriple, NativeBNode, NativeIRI, SimpleBNode, SimpleIRI, SimpleTriple



public interface Resource
extends Value
The supertype of all RDF resources (IRIs and blank nodes).







Method Summary

All MethodsInstance MethodsDefault Methods


Modifier and Type
Method
Description
default boolean
isResource()

Check if the object is an instance of the given type.





Methods inherited from interface org.eclipse.rdf4j.model.Value
isBNode, isIRI, isLiteral, isTriple, stringValue









Method Details



isResource

default boolean isResource()
Description copied from interface: Value
Check if the object is an instance of the given type. Typically 2x than using instanceof.
 
 For implementers: This default implementation is overridden in the repsective sub-interface.

Specified by:
isResource in interface Value
Returns:
true if instance of Resource












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface Literal



All Superinterfaces:
Serializable, Value


All Known Implementing Classes:
AbstractLiteral, BooleanLiteral, BooleanMemLiteral, CalendarLiteral, CalendarMemLiteral, CorruptLiteral, CorruptUnknownValue, DecimalLiteral, DecimalMemLiteral, IntegerLiteral, IntegerMemLiteral, LmdbLiteral, MemLiteral, NativeLiteral, NumericLiteral, NumericMemLiteral, SimpleLiteral



public interface Literal
extends Value
An RDF-1.1 literal consisting of a label (the lexical value), a datatype, and optionally a language tag.

 
 Value accessor methods (for instance, booleanValue()) map literal lexical values conforming to the syntax of
 a supported XML Schema 1.1 datatype to a corresponding Java object.
 

Author:
Arjohn Kampman
See Also:


RDF-1.1 Concepts and Abstract Syntax
RDF 1.1 Concepts and Abstract Syntax - §5.1
      The XML Schema Built-in Datatypes
XML Schema Definition Language (XSD) 1.1 Part 2: Datatypes










Method Summary

All MethodsInstance MethodsAbstract MethodsDefault Methods


Modifier and Type
Method
Description
boolean
booleanValue()

Returns the boolean value of this literal.

byte
byteValue()

Returns the byte value of this literal.

XMLGregorianCalendar
calendarValue()

Returns the XMLGregorianCalendar value of this literal.

BigDecimal
decimalValue()

Returns the decimal value of this literal.

double
doubleValue()

Returns the double value of this literal.

boolean
equals(Object other)

Compares this literal to another object.

float
floatValue()

Returns the float value of this literal.

CoreDatatype
getCoreDatatype()

CoreDatatype is an interface for natively supported datatypes in RDF4J.

IRI
getDatatype()

Gets the datatype for this literal.

String
getLabel()

Gets the label (the lexical value) of this literal.

Optional<String>
getLanguage()

Gets the language tag for this literal, normalized to lower case.

int
hashCode()

Computes the hash code of this literal.

BigInteger
integerValue()

Returns the integer value of this literal.

int
intValue()

Returns the int value of this literal.

default boolean
isLiteral()

Check if the object is an instance of the given type.

long
longValue()

Returns the long value of this literal.

short
shortValue()

Returns the short value of this literal.

default TemporalAccessor
temporalAccessorValue()

Retrieves the temporal accessor value of this literal.

default TemporalAmount
temporalAmountValue()

Retrieves the temporal amount value of this literal.





Methods inherited from interface org.eclipse.rdf4j.model.Value
isBNode, isIRI, isResource, isTriple, stringValue









Method Details



isLiteral

default boolean isLiteral()
Description copied from interface: Value
Check if the object is an instance of the given type. Typically 2x than using instanceof.
 
 For implementers: This default implementation is overridden in the repsective sub-interface.

Specified by:
isLiteral in interface Value
Returns:
true if instance of Literal






getLabel

String getLabel()
Gets the label (the lexical value) of this literal.

Returns:
The literal's label.






getLanguage

Optional<String> getLanguage()
Gets the language tag for this literal, normalized to lower case.

Returns:
The language tag for this literal, or Optional.empty() if it doesn't have one.






getDatatype

IRI getDatatype()
Gets the datatype for this literal.
 
 If getLanguage() returns a non-empty value than this must return
 rdf:langString. If no datatype was
 assigned to this literal by the creator, then this method must return
 xsd:string.

Returns:
The datatype for this literal.






booleanValue

boolean booleanValue()
Returns the boolean value of this literal.

Returns:
The boolean value of the literal.
Throws:
IllegalArgumentException - If the literal's label cannot be represented by a boolean .






byteValue

byte byteValue()
Returns the byte value of this literal.

Returns:
The byte value of the literal.
Throws:
NumberFormatException - If the literal cannot be represented by a byte.






shortValue

short shortValue()
Returns the short value of this literal.

Returns:
The short value of the literal.
Throws:
NumberFormatException - If the literal's label cannot be represented by a short.






intValue

int intValue()
Returns the int value of this literal.

Returns:
The int value of the literal.
Throws:
NumberFormatException - If the literal's label cannot be represented by a int.






longValue

long longValue()
Returns the long value of this literal.

Returns:
The long value of the literal.
Throws:
NumberFormatException - If the literal's label cannot be represented by to a long .






integerValue

BigInteger integerValue()
Returns the integer value of this literal.

Returns:
The integer value of the literal.
Throws:
NumberFormatException - If the literal's label is not a valid integer.






decimalValue

BigDecimal decimalValue()
Returns the decimal value of this literal.

Returns:
The decimal value of the literal.
Throws:
NumberFormatException - If the literal's label is not a valid decimal.






floatValue

float floatValue()
Returns the float value of this literal.

Returns:
The float value of the literal.
Throws:
NumberFormatException - If the literal's label cannot be represented by a float.






doubleValue

double doubleValue()
Returns the double value of this literal.

Returns:
The double value of the literal.
Throws:
NumberFormatException - If the literal's label cannot be represented by a double.






temporalAccessorValue

default TemporalAccessor temporalAccessorValue()
                                        throws DateTimeException
Retrieves the temporal accessor value of this literal.

 
 A temporal accessor representation can be given for literals whose label conforms to the syntax of the following
 XML Schema 1.1 date/time datatypes:
 

 

 xsd:dateTime,
 xsd:time,
 xsd:date,

 xsd:gYearMonth,
 xsd:gYear,
 xsd:gMonthDay,
 xsd:gDay,
 xsd:gMonth.

 

 
 Temporal accessor representations may be converted to specific java.time values like
 OffsetDateTime using target static factory methods, for instance
 OffsetDateTime.from(literal.temporalAccessorValue()).
 

 
 Note however that java.time doesn't include dedicated classes for some legal XML Schema date/time values,
 like offset dates (for instance, 2020-11-16+01:00) and xsd:gDay (for instance, ---16).
 

Returns:
the temporal accessor value of this literal
Throws:
DateTimeException - if this literal cannot be represented by a TemporalAccessor value
Since:
3.5.0
See Also:


The Java™ Tutorials – Trail: Date Time








temporalAmountValue

default TemporalAmount temporalAmountValue()
                                    throws DateTimeException
Retrieves the temporal amount value of this literal.

 
 A temporal amount representation can be given for literals whose label conforms to the syntax of the
 XML Schema 2
 xsd:duration datatype.
 

 
 The adoption of the XML Schema 2 definition is a known deviation
 from the RDF 1.1 standard;
 well-formedness rules are relaxed to consider all duration components as optional and freely mixable.
 

 
 Temporal amount representations may be converted to specific java.time values like Duration using
 target static factory methods, for instance Duration.from(literal.temporalAmountValue()).
 

 
 Note however that java.time doesn't include dedicated classes for legal XML Schema duration values
 including both date and time components (for instance, P1YT23H).
 

Returns:
the temporal amount value of this literal
Throws:
DateTimeException - if this literal cannot be represented by a TemporalAmount value
Since:
3.5.0
See Also:


The Java™ Tutorials – Trail: Date Time








calendarValue

XMLGregorianCalendar calendarValue()
Returns the XMLGregorianCalendar value of this literal. A calendar representation can be given for
 literals whose label conforms to the syntax of the following XML
 Schema datatypes: dateTime, time, date, gYearMonth,
 gMonthDay, gYear, gMonth or gDay.

Returns:
The calendar value of the literal.
Throws:
IllegalArgumentException - If the literal cannot be represented by a XMLGregorianCalendar.






getCoreDatatype

CoreDatatype getCoreDatatype()
CoreDatatype is an interface for natively supported datatypes in RDF4J. This includes, among others, the XML
 Schema datatypes and rdf:langString. CoreDatatypes are implemented as enums and more performant and convenient to
 work with than IRI-based datatypes. The constant 

invalid @link
{@link CoreDatatype#NONE)

} is used to represent a datatype that
 is not one of the supported core datatypes.

Returns:
The CoreDatatype or 

invalid @link
{@link CoreDatatype#NONE)

} if the datatype matches none of the core datatypes. This
         method will not return null.






equals

boolean equals(Object other)
Compares this literal to another object.

Overrides:
equals in class Object
Parameters:
other - the object to compare this literal to
Returns:
true, if the other object is an instance of Literal and if their labels, language tags and datatypes are equal






hashCode

int hashCode()
Computes the hash code of this literal.

Overrides:
hashCode in class Object
Returns:
a hash code for this literal computed as getLabel().hashCode()












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface BNode



All Superinterfaces:
Resource, Serializable, Value


All Known Implementing Classes:
AbstractBNode, CorruptIRIOrBNode, LmdbBNode, MemBNode, NativeBNode, SimpleBNode



public interface BNode
extends Resource
An RDF-1.1 blank node (aka bnode, aka anonymous node). A blank node has an identifier to be able to
 compare it to other blank nodes internally. Please note that, conceptually, blank node equality can only be
 determined by examining the statements that refer to them.

See Also:


RDF-1.1 Concepts and Abstract Syntax










Method Summary

All MethodsInstance MethodsAbstract MethodsDefault Methods


Modifier and Type
Method
Description
boolean
equals(Object o)

Compares this blank node to another object.

String
getID()

Retrieves this blank node's identifier.

int
hashCode()

Computes the hash code of this blank node.

default boolean
isBNode()

Check if the object is an instance of the given type.





Methods inherited from interface org.eclipse.rdf4j.model.Resource
isResource

Methods inherited from interface org.eclipse.rdf4j.model.Value
isIRI, isLiteral, isTriple, stringValue









Method Details



isBNode

default boolean isBNode()
Description copied from interface: Value
Check if the object is an instance of the given type. Typically 2x than using instanceof.
 
 For implementers: This default implementation is overridden in the repsective sub-interface.

Specified by:
isBNode in interface Value
Returns:
true if instance of BNode






getID

String getID()
Retrieves this blank node's identifier.

Returns:
A blank node identifier.






equals

boolean equals(Object o)
Compares this blank node to another object.

Overrides:
equals in class Object
Parameters:
o - the object to compare this blank node to
Returns:
true, if the other object is an instance of BNode and their IDs are
         equal; false, otherwise.






hashCode

int hashCode()
Computes the hash code of this blank node.

Overrides:
hashCode in class Object
Returns:
a hash code for this blank node computed as getID().hashCode()












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface IRI



All Superinterfaces:
Resource, Serializable, Value


All Known Implementing Classes:
AbstractIRI, CorruptIRI, CorruptIRIOrBNode, InternedIRI, LmdbIRI, MemIRI, NativeIRI, SimpleIRI



public interface IRI
extends Resource
An Internationalized Resource Identifier (IRI). IRIs may contain characters from the Universal Character Set
 (Unicode/ISO 10646), including Chinese or Japanese kanji, Korean, Cyrillic characters, and so forth. It is defined by
 RFC 3987.
 
 An IRI can be split into a namespace part and a local name part, which are derived from an IRI string by splitting it
 in two using the following algorithm:
 
 Split after the first occurrence of the '#' character,
 If this fails, split after the last occurrence of the '/' character,
 If this fails, split after the last occurrence of the ':' character.
 
 The last step should never fail as every legal (full) IRI contains at least one ':' character to separate the scheme
 from the rest of the IRI. The implementation should check this upon object creation.

Author:
Jeen Broekstra
See Also:


RFC 3987










Method Summary

All MethodsInstance MethodsAbstract MethodsDefault Methods


Modifier and Type
Method
Description
boolean
equals(Object o)

Compares this IRI to another object.

String
getLocalName()

Gets the local name part of this IRI.

String
getNamespace()

Gets the namespace part of this IRI.

int
hashCode()

Computes the hash code of this IRI.

default boolean
isIRI()

Check if the object is an instance of the given type.





Methods inherited from interface org.eclipse.rdf4j.model.Resource
isResource

Methods inherited from interface org.eclipse.rdf4j.model.Value
isBNode, isLiteral, isTriple, stringValue









Method Details



isIRI

default boolean isIRI()
Description copied from interface: Value
Check if the object is an instance of the given type. Typically 2x than using instanceof.
 
 For implementers: This default implementation is overridden in the repsective sub-interface.

Specified by:
isIRI in interface Value
Returns:
true if instance of IRI






getNamespace

String getNamespace()
Gets the namespace part of this IRI.
 
 The namespace is defined as per the algorithm described in the class documentation.

Returns:
the namespace of this IRI






getLocalName

String getLocalName()
Gets the local name part of this IRI.
 
 The local name is defined as per the algorithm described in the class documentation.

Returns:
the local name of this IRI






equals

boolean equals(Object o)
Compares this IRI to another object.

Overrides:
equals in class Object
Parameters:
o - the object to compare this IRI to
Returns:
true, if the other object is an instance of IRI and their string values are equal; false, otherwise






hashCode

int hashCode()
Computes the hash code of this IRI.

Overrides:
hashCode in class Object
Returns:
a hash code for this IRI computed as Value.stringValue().hashCode()












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nClass Statements

java.lang.Object
org.eclipse.rdf4j.model.util.Statements




public class Statements
extends Object
Utility methods for working with Statement objects, including conversion to/from RDF-star
 triple objects.

Author:
Jeen Broekstra








Field Summary
Fields

Modifier and Type
Field
Description
static Function<Triple,Resource>
TRIPLE_BNODE_MAPPER

A Function that maps Triple to BNode consistently.







Constructor Summary
Constructors

Constructor
Description
Statements()
 






Method Summary

All MethodsStatic MethodsConcrete MethodsDeprecated Methods


Modifier and Type
Method
Description
static void
consume(ValueFactory vf,
 Resource subject,
 IRI predicate,
 Value object,
 Consumer<Statement> consumer,
 Resource... contexts)

Creates one or more Statement objects with the given subject, predicate and object, one for each given
 context, and sends each created statement to the supplied Consumer.

static void
convertRDFStarToReification(Statement st,
 Consumer<Statement> consumer)

Converts the supplied RDF-star statement to RDF reification statements, and sends the resultant statements to the
 supplied consumer.

static void
convertRDFStarToReification(ValueFactory vf,
 Function<Triple,Resource> reifiedIdMapper,
 Statement st,
 Consumer<Statement> consumer)

Converts the supplied RDF-star statement to RDF reification statements, and sends the resultant statements to the
 supplied consumer.

static void
convertRDFStarToReification(ValueFactory vf,
 Statement st,
 Consumer<Statement> consumer)

Converts the supplied RDF-star statement to RDF reification statements, and sends the resultant statements to the
 supplied consumer.

static <C extends Collection<Statement>>C
create(ValueFactory vf,
 Resource subject,
 IRI predicate,
 Value object,
 C collection,
 Resource... contexts)

Creates one or more Statement objects with the given subject, predicate and object, one for each given
 context.

static boolean
isSameTriple(Statement st1,
 Statement st2)

Checks if the two statements represent the same triple (that is, they have equal subject, predicate, and object).

static Statement
statement(Resource subject,
 IRI predicate,
 Value object,
 Resource context)

Create a Statement from the supplied subject, predicate, object and context.

static Statement
statement(Triple triple)

Create a Statement from the supplied RDF-star triple

static Statement
statement(Triple triple,
 Resource context)

Create a Statement from the supplied RDF-star triple and context.

static Statement
statement(ValueFactory vf,
 Resource subject,
 IRI predicate,
 Value object,
 Resource context)

Create a Statement from the supplied subject, predicate, object and context.

static Statement
statement(ValueFactory vf,
 Triple triple,
 Resource context)

Create a Statement from the supplied RDF-star triple and context.

static Statement
stripContext(Statement statement)

Strips the context (if any) from the supplied statement and returns a statement with the same subject, predicate
 and object, but with no assigned context.

static Statement
stripContext(ValueFactory vf,
 Statement statement)

Strips the context (if any) from the supplied statement and returns a statement with the same subject, predicate
 and object, but with no assigned context.

static Statement
toStatement(Triple triple)

Deprecated.
Use statement(Triple) instead


static Statement
toStatement(Triple triple,
 Resource context)

Deprecated.
since 3.7.0 - use statement(Triple, Resource) instead


static Statement
toStatement(ValueFactory vf,
 Triple triple,
 Resource context)

Deprecated.
Use statement(ValueFactory, Triple, Resource) instead


static Triple
toTriple(Statement statement)

Deprecated.
Use Values.triple(Statement) instead


static Triple
toTriple(ValueFactory vf,
 Statement statement)

Deprecated.
Use Values.triple(ValueFactory, Statement) instead






Methods inherited from class java.lang.Object
clone, equals, finalize, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait









Field Details



TRIPLE_BNODE_MAPPER

@Experimental
public static Function<Triple,Resource> TRIPLE_BNODE_MAPPER
A Function that maps Triple to BNode consistently. Multiple
 invocations for the same Triple will return the same BNode.
 
 The current implementation creates a BNode by encoding the string representation
 of the Triple using base64 URL-safe encoding.









Constructor Details



Statements

public Statements()









Method Details



consume

public static void consume(ValueFactory vf,
 Resource subject,
 IRI predicate,
 Value object,
 Consumer<Statement> consumer,
 Resource... contexts)
Creates one or more Statement objects with the given subject, predicate and object, one for each given
 context, and sends each created statement to the supplied Consumer. If no context is supplied, only a
 single statement (without any assigned context) is created.

Parameters:
vf - the ValueFactory to use for creating statements.
subject - the subject of each statement. May not be null.
predicate - the predicate of each statement. May not be null.
object - the object of each statement. May not be null.
consumer - the Consumer function for the produced statements.
contexts - the context(s) for which to produce statements. This argument is an optional vararg: leave it
                  out completely to produce a single statement without context.






create

public static <C extends Collection<Statement>> C create(ValueFactory vf,
 Resource subject,
 IRI predicate,
 Value object,
 C collection,
 Resource... contexts)
Creates one or more Statement objects with the given subject, predicate and object, one for each given
 context. If no context is supplied, only a single statement (without any assigned context) is created.

Parameters:
vf - the ValueFactory to use for creating statements.
subject - the subject of each statement. May not be null.
predicate - the predicate of each statement. May not be null.
object - the object of each statement. May not be null.
collection - the collection of Statements to which the newly created Statements will be added. May not be
                   null.
contexts - the context(s) for which to produce statements. This argument is an optional vararg: leave it
                   out completely to produce a single statement without context.
Returns:
the input collection of Statements, with the newly created Statements added.






stripContext

public static Statement stripContext(Statement statement)
Strips the context (if any) from the supplied statement and returns a statement with the same subject, predicate
 and object, but with no assigned context.

Parameters:
statement - the statement to strip the context from
Returns:
a statement without context
Since:
3.1.0






stripContext

public static Statement stripContext(ValueFactory vf,
 Statement statement)
Strips the context (if any) from the supplied statement and returns a statement with the same subject, predicate
 and object, but with no assigned context.

Parameters:
vf - the ValueFactory to use for creating a new Statement.
statement - the statement to strip the context from.
Returns:
a statement without context
Since:
3.1.0






toTriple

@Deprecated(since="3.5.0")
public static Triple toTriple(Statement statement)
Deprecated.
Use Values.triple(Statement) instead

Create an RDF-star triple from the supplied Statement

Parameters:
statement - a statement to convert to an RDF-star triple
Returns:
an RDF-star triple with the same subject, predicate and object as the input statement.
Since:
3.4.0






toTriple

@Deprecated(since="3.5.0")
public static Triple toTriple(ValueFactory vf,
 Statement statement)
Deprecated.
Use Values.triple(ValueFactory, Statement) instead

Create an RDF-star triple from the supplied Statement

Parameters:
vf - the ValueFactory to use for creating the Triple object.
statement - a statement to convert to an RDF-star triple
Returns:
an RDF-star triple with the same subject, predicate and object as the input statement.
Since:
3.4.0






toStatement

public static Statement toStatement(Triple triple)
Deprecated.
Use statement(Triple) instead

Create a Statement from the supplied RDF-star triple

Parameters:
triple - an RDF-star triple to convert to a Statement.
Returns:
an Statement with the same subject, predicate and object as the input triple, and no context.
Since:
3.4.0






statement

public static Statement statement(Triple triple)
Create a Statement from the supplied RDF-star triple

Parameters:
triple - an RDF-star triple to convert to a Statement.
Returns:
an Statement with the same subject, predicate and object as the input triple, and no context.
Since:
3.4.0






statement

public static Statement statement(Triple triple,
 Resource context)
Create a Statement from the supplied RDF-star triple and context.

Parameters:
triple - an RDF-star triple to convert to a Statement.
context - the context to assign to the Statement.
Returns:
an Statement with the same subject, predicate and object as the input triple, and having the
         supplied context.
Since:
3.7.0






toStatement

public static Statement toStatement(Triple triple,
 Resource context)
Deprecated.
since 3.7.0 - use statement(Triple, Resource) instead

Create a Statement from the supplied RDF-star triple and context.

Parameters:
triple - an RDF-star triple to convert to a Statement.
context - the context to assign to the Statement.
Returns:
an Statement with the same subject, predicate and object as the input triple, and having the
         supplied context.
Since:
3.4.0






toStatement

public static Statement toStatement(ValueFactory vf,
 Triple triple,
 Resource context)
Deprecated.
Use statement(ValueFactory, Triple, Resource) instead

Create a Statement from the supplied RDF-star triple and context.

Parameters:
vf - the ValueFactory to use for creating the Statement object.
triple - an RDF-star triple to convert to a Statement.
context - the context to assign to the Statement. May be null to indicate no context.
Returns:
an Statement with the same subject, predicate and object as the input triple, and having the
         supplied context.
Since:
3.4.0






statement

public static Statement statement(ValueFactory vf,
 Triple triple,
 Resource context)
Create a Statement from the supplied RDF-star triple and context.

Parameters:
vf - the ValueFactory to use for creating the Statement object.
triple - an RDF-star triple to convert to a Statement.
context - the context to assign to the Statement. May be null to indicate no context.
Returns:
an Statement with the same subject, predicate and object as the input triple, and having the
         supplied context.
Since:
3.7.0






statement

public static Statement statement(Resource subject,
 IRI predicate,
 Value object,
 Resource context)
Create a Statement from the supplied subject, predicate, object and context.

Parameters:
subject - the statement subject
predicate - the statement predicate
object - the statement object
context - the context to assign to the Statement. May be null to indicate no context.
Returns:
an Statement with the same subject, predicate and object as the input triple, and having the
         supplied context.
Throws:
NullPointerException - if any of subject, predicate, or object are null.
Since:
3.5.0






statement

public static Statement statement(ValueFactory vf,
 Resource subject,
 IRI predicate,
 Value object,
 Resource context)
Create a Statement from the supplied subject, predicate, object and context.

Parameters:
vf - the ValueFactory to use for creating the Statement object.
subject - the statement subject
predicate - the statement predicate
object - the statement object
context - the context to assign to the Statement. May be null to indicate no context.
Returns:
an Statement with the same subject, predicate and object as the input triple, and having the
         supplied context.
Throws:
NullPointerException - if any of vf, subject, predicate, or object are null.
Since:
3.5.0






isSameTriple

public static boolean isSameTriple(Statement st1,
 Statement st2)
Checks if the two statements represent the same triple (that is, they have equal subject, predicate, and object).
 Context information is disregarded.

Parameters:
st1 - the first statement to compare. May not be null.
st2 - the second statement to compare. May not be null.
Returns:
true iff the subject, predicate and object of st1 and st2 are equal,
         false otherwise.
Since:
2.0
See Also:


Statement.equals(Object)








convertRDFStarToReification

@Experimental
public static void convertRDFStarToReification(Statement st,
 Consumer<Statement> consumer)
Converts the supplied RDF-star statement to RDF reification statements, and sends the resultant statements to the
 supplied consumer. If the supplied statement is not RDF-star it will be sent to the consumer as is.
 
 The statements needed to represent reification will use blank nodes.

Parameters:
st - the Statement to convert.
consumer - the Consumer function for the produced statements.






convertRDFStarToReification

@Experimental
public static void convertRDFStarToReification(ValueFactory vf,
 Statement st,
 Consumer<Statement> consumer)
Converts the supplied RDF-star statement to RDF reification statements, and sends the resultant statements to the
 supplied consumer. If the supplied statement is not RDF-star it will be sent to the consumer as is.
 
 The statements needed to represent reification will use blank nodes.
 
 The supplied value factory is used to create all new statements and blank nodes.

Parameters:
vf - the ValueFactory to use for creating statements.
st - the Statement to convert.
consumer - the Consumer function for the produced statements.






convertRDFStarToReification

@Experimental
public static void convertRDFStarToReification(ValueFactory vf,
 Function<Triple,Resource> reifiedIdMapper,
 Statement st,
 Consumer<Statement> consumer)
Converts the supplied RDF-star statement to RDF reification statements, and sends the resultant statements to the
 supplied consumer. If the supplied statement is not RDF-star it will be sent to the consumer as is.
 
 The supplied value factory is used to create all new statements.
 
 The supplied mapper function maps a Triple to a Resource and is used to create the ID of the RDF
 reification statement corresponding to the converted triple. The function must return the same value for
 identical triples in order to produce consistent results between invocations. See TRIPLE_BNODE_MAPPER.

Parameters:
vf - the ValueFactory to use for creating statements.
reifiedIdMapper - the mapper Function from Triple to Resource.
st - the Statement to convert,
consumer - the Consumer function for the produced statements.












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nInterface ValueFactory



All Known Implementing Classes:
AbstractValueFactory, MemValueFactory, SimpleValueFactory, ValidatingValueFactory, ValueStore



public interface ValueFactory
A factory for creating IRIs, blank nodes, literals and statements based on the RDF-1.1 Concepts and Abstract Syntax, a W3C Recommendation.

Author:
Arjohn Kampman
See Also:


RDF-1.1 Concepts and Abstract Syntax










Method Summary

All MethodsInstance MethodsAbstract MethodsDefault Methods


Modifier and Type
Method
Description
BNode
createBNode()

Creates a new bNode.

BNode
createBNode(String nodeID)

Creates a new blank node with the given node identifier.

IRI
createIRI(String iri)

Creates a new IRI from the supplied string-representation.

IRI
createIRI(String namespace,
 String localName)

Creates a new IRI from the supplied namespace and local name.

Literal
createLiteral(boolean value)

Creates a new xsd:boolean-typed literal representing the specified value.

Literal
createLiteral(byte value)

Creates a new xsd:byte-typed literal representing the specified value.

Literal
createLiteral(double value)

Creates a new xsd:double-typed literal representing the specified value.

Literal
createLiteral(float value)

Creates a new xsd:float-typed literal representing the specified value.

Literal
createLiteral(int value)

Creates a new xsd:int-typed literal representing the specified value.

Literal
createLiteral(long value)

Creates a new xsd:long-typed literal representing the specified value.

Literal
createLiteral(short value)

Creates a new xsd:short-typed literal representing the specified value.

Literal
createLiteral(String label)

Creates a new literal with the supplied label.

Literal
createLiteral(String label,
 String language)

Creates a new literal with the supplied label and language attribute.

Literal
createLiteral(String label,
 CoreDatatype datatype)

Creates a new literal with the supplied label and datatype.

Literal
createLiteral(String label,
 IRI datatype)

Creates a new literal with the supplied label and datatype.

Literal
createLiteral(String label,
 IRI datatype,
 CoreDatatype coreDatatype)

Creates a new literal with the supplied label and datatype.

Literal
createLiteral(BigDecimal bigDecimal)

Creates a new literal representing the specified bigDecimal that is typed as an xsd:decimal.

Literal
createLiteral(BigInteger bigInteger)

Creates a new literal representing the specified bigInteger that is typed as an xsd:integer.

default Literal
createLiteral(TemporalAccessor value)

Creates a new literal representing a temporal accessor value.

default Literal
createLiteral(TemporalAmount value)

Creates a new literal representing a temporal amount value.

Literal
createLiteral(Date date)

Creates a new literal representing the specified date that is typed using the appropriate XML Schema date/time
 datatype.

Literal
createLiteral(XMLGregorianCalendar calendar)

Creates a new literal representing the specified calendar that is typed using the appropriate XML Schema
 date/time datatype.

Statement
createStatement(Resource subject,
 IRI predicate,
 Value object)

Creates a new statement with the supplied subject, predicate and object.

Statement
createStatement(Resource subject,
 IRI predicate,
 Value object,
 Resource context)

Creates a new statement with the supplied subject, predicate and object and associated context.

default Triple
createTriple(Resource subject,
 IRI predicate,
 Value object)

Creates a new RDF-star triple with the supplied subject, predicate and object.













Method Details



createIRI

IRI createIRI(String iri)
Creates a new IRI from the supplied string-representation.

Parameters:
iri - A string-representation of a IRI.
Returns:
An object representing the IRI.
Throws:
IllegalArgumentException - If the supplied string does not resolve to a legal (absolute) IRI.






createIRI

IRI createIRI(String namespace,
 String localName)
Creates a new IRI from the supplied namespace and local name. Calling this method is funtionally equivalent to
 calling createIRI(namespace+localName), but allows the ValueFactory to reuse supplied
 namespace and local name strings whenever possible. Note that the values returned by IRI.getNamespace()
 and IRI.getLocalName() are not necessarily the same as the values that are supplied to this method.

Parameters:
namespace - The IRI's namespace.
localName - The IRI's local name.
Throws:
IllegalArgumentException - If the supplied namespace and localname do not resolve to a legal (absolute)
                                  IRI.






createBNode

BNode createBNode()
Creates a new bNode.

Returns:
An object representing the bNode.






createBNode

BNode createBNode(String nodeID)
Creates a new blank node with the given node identifier.

Parameters:
nodeID - The blank node identifier.
Returns:
An object representing the blank node.






createLiteral

Literal createLiteral(String label)
Creates a new literal with the supplied label. The return value of Literal.getDatatype() for the returned
 object must be xsd:string.

Parameters:
label - The literal's label, must not be null.
Returns:
A literal for the specified value.






createLiteral

Literal createLiteral(String label,
 String language)
Creates a new literal with the supplied label and language attribute. The return value of
 Literal.getDatatype() for the returned object must be
 rdf:langString.

Parameters:
label - The literal's label, must not be null.
language - The literal's language attribute, must not be null.
Returns:
A literal for the specified value and language attribute.






createLiteral

Literal createLiteral(String label,
 IRI datatype)
Creates a new literal with the supplied label and datatype.

Parameters:
label - The literal's label, must not be null.
datatype - The literal's datatype. If it is null, the datatype
                 xsd:string will be assigned to this
                 literal.
Returns:
A literal for the specified value and type.






createLiteral

Literal createLiteral(String label,
 CoreDatatype datatype)
Creates a new literal with the supplied label and datatype.

Parameters:
label - The literal's label, must not be null.
datatype - The literal's datatype. It may not be null.






createLiteral

Literal createLiteral(String label,
 IRI datatype,
 CoreDatatype coreDatatype)
Creates a new literal with the supplied label and datatype.

Parameters:
label - The literal's label, must not be null.
datatype - The literal's datatype. If it is null, the datatype
                 xsd:string will be assigned to this
                 literal.






createLiteral

Literal createLiteral(boolean value)
Creates a new xsd:boolean-typed literal representing the specified value.

Parameters:
value - The value for the literal.
Returns:
An xsd:boolean-typed literal for the specified value.






createLiteral

Literal createLiteral(byte value)
Creates a new xsd:byte-typed literal representing the specified value.

Parameters:
value - The value for the literal.
Returns:
An xsd:byte-typed literal for the specified value.






createLiteral

Literal createLiteral(short value)
Creates a new xsd:short-typed literal representing the specified value.

Parameters:
value - The value for the literal.
Returns:
An xsd:short-typed literal for the specified value.






createLiteral

Literal createLiteral(int value)
Creates a new xsd:int-typed literal representing the specified value.

Parameters:
value - The value for the literal.
Returns:
An xsd:int-typed literal for the specified value.






createLiteral

Literal createLiteral(long value)
Creates a new xsd:long-typed literal representing the specified value.

Parameters:
value - The value for the literal.
Returns:
An xsd:long-typed literal for the specified value.






createLiteral

Literal createLiteral(float value)
Creates a new xsd:float-typed literal representing the specified value.

Parameters:
value - The value for the literal.
Returns:
An xsd:float-typed literal for the specified value.






createLiteral

Literal createLiteral(double value)
Creates a new xsd:double-typed literal representing the specified value.

Parameters:
value - The value for the literal.
Returns:
An xsd:double-typed literal for the specified value.






createLiteral

Literal createLiteral(BigDecimal bigDecimal)
Creates a new literal representing the specified bigDecimal that is typed as an xsd:decimal.

Parameters:
bigDecimal - The value for the literal.
Returns:
An xsd:decimal-typed literal for the specified value.






createLiteral

Literal createLiteral(BigInteger bigInteger)
Creates a new literal representing the specified bigInteger that is typed as an xsd:integer.

Parameters:
bigInteger - The value for the literal.
Returns:
An xsd:integer-typed literal for the specified value.






createLiteral

default Literal createLiteral(TemporalAccessor value)
Creates a new literal representing a temporal accessor value.

Parameters:
value - the temporal accessor value for the literal
Returns:
a literal representing the specified temporal accessor value with the appropriate
         XML Schema date/time datatype
Throws:
NullPointerException - if value is null
IllegalArgumentException - if value cannot be represented by an XML Schema date/time datatype
Since:
3.5.0






createLiteral

default Literal createLiteral(TemporalAmount value)
Creates a new literal representing a temporal amount value.

Parameters:
value - the temporal amount value for the literal
Returns:
a literal representing the specified temporal amount value with the appropriate
         XML Schema duration datatype
Throws:
NullPointerException - if value is null
IllegalArgumentException - if value cannot be represented by an XML Schema duration datatype
Since:
3.5.0






createLiteral

Literal createLiteral(XMLGregorianCalendar calendar)
Creates a new literal representing the specified calendar that is typed using the appropriate XML Schema
 date/time datatype.

Parameters:
calendar - The value for the literal.
Returns:
A typed literal for the specified calendar.






createLiteral

Literal createLiteral(Date date)
Creates a new literal representing the specified date that is typed using the appropriate XML Schema date/time
 datatype.

Parameters:
date - The value for the literal.
Returns:
A typed literal for the specified date.






createStatement

Statement createStatement(Resource subject,
 IRI predicate,
 Value object)
Creates a new statement with the supplied subject, predicate and object.

Parameters:
subject - The statement's subject.
predicate - The statement's predicate.
object - The statement's object.
Returns:
The created statement.






createStatement

Statement createStatement(Resource subject,
 IRI predicate,
 Value object,
 Resource context)
Creates a new statement with the supplied subject, predicate and object and associated context.

Parameters:
subject - The statement's subject.
predicate - The statement's predicate.
object - The statement's object.
context - The statement's context.
Returns:
The created statement.






createTriple

default Triple createTriple(Resource subject,
 IRI predicate,
 Value object)
Creates a new RDF-star triple with the supplied subject, predicate and object.

Parameters:
subject - The statement's subject.
predicate - The statement's predicate.
object - The statement's object.
Returns:
The created triple.
Since:
3.2.0












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\nClass SimpleValueFactory

java.lang.Object
org.eclipse.rdf4j.model.base.AbstractValueFactory
org.eclipse.rdf4j.model.impl.SimpleValueFactory




All Implemented Interfaces:
ValueFactory


Direct Known Subclasses:
ValueStore



public class SimpleValueFactory
extends AbstractValueFactory
Default implementation of the ValueFactory interface.

Author:
Arjohn Kampman








Constructor Summary
Constructors

Modifier
Constructor
Description
protected 
SimpleValueFactory()

Hidden constructor to enforce singleton pattern.







Method Summary

All MethodsStatic MethodsInstance MethodsConcrete Methods


Modifier and Type
Method
Description
BNode
createBNode()

Creates a new bNode.

BNode
createBNode(String nodeID)

Creates a new blank node with the given node identifier.

protected Literal
createFPLiteral(Number value,
 CoreDatatype.XSD datatype)
 
protected Literal
createFPLiteral(Number value,
 IRI datatype)

Calls createNumericLiteral(Number, IRI) with the supplied value and datatype as parameters.

protected Literal
createIntegerLiteral(Number value,
 CoreDatatype.XSD datatype)
 
protected Literal
createIntegerLiteral(Number value,
 IRI datatype)

Calls createNumericLiteral(Number, IRI) with the supplied value and datatype as parameters.

IRI
createIRI(String iri)

Creates a new IRI from the supplied string-representation.

IRI
createIRI(String namespace,
 String localName)

Creates a new IRI from the supplied namespace and local name.

Literal
createLiteral(boolean b)

Creates a new xsd:boolean-typed literal representing the specified value.

Literal
createLiteral(byte value)

Calls createIntegerLiteral(Number, IRI) with the supplied value and XSD.BYTE as parameters.

Literal
createLiteral(double value)

Calls createFPLiteral(Number, IRI) with the supplied value and XSD.DOUBLE as parameters.

Literal
createLiteral(float value)

Calls createFPLiteral(Number, IRI) with the supplied value and XSD.FLOAT as parameters.

Literal
createLiteral(int value)

Calls createIntegerLiteral(Number, IRI) with the supplied value and XSD.INT as parameters.

Literal
createLiteral(long value)

Calls createIntegerLiteral(Number, IRI) with the supplied value and XSD.LONG as parameters.

Literal
createLiteral(short value)

Calls createIntegerLiteral(Number, IRI) with the supplied value and XSD.SHORT as parameters.

Literal
createLiteral(String value)

Creates a new literal with the supplied label.

Literal
createLiteral(String value,
 String language)

Creates a new literal with the supplied label and language attribute.

Literal
createLiteral(String value,
 IRI datatype)

Creates a new literal with the supplied label and datatype.

Literal
createLiteral(BigDecimal bigDecimal)

Creates a new literal representing the specified bigDecimal that is typed as an xsd:decimal.

Literal
createLiteral(BigInteger bigInteger)

Creates a new literal representing the specified bigInteger that is typed as an xsd:integer.

Literal
createLiteral(Date date)

Converts the supplied Date to a XMLGregorianCalendar, then calls
 ValueFactory.createLiteral(XMLGregorianCalendar).

Literal
createLiteral(XMLGregorianCalendar calendar)

Calls ValueFactory.createLiteral(String, IRI) with the String-value of the supplied calendar and the
 appropriate datatype as parameters.

protected Literal
createNumericLiteral(Number number,
 CoreDatatype datatype)
 
protected Literal
createNumericLiteral(Number number,
 IRI datatype)

Creates specific optimized subtypes of SimpleLiteral for numeric datatypes.

Statement
createStatement(Resource subject,
 IRI predicate,
 Value object)

Creates a new statement with the supplied subject, predicate and object.

Statement
createStatement(Resource subject,
 IRI predicate,
 Value object,
 Resource context)

Creates a new statement with the supplied subject, predicate and object and associated context.

Triple
createTriple(Resource subject,
 IRI predicate,
 Value object)

Creates a new RDF-star triple with the supplied subject, predicate and object.

static SimpleValueFactory
getInstance()

Provide a single shared instance of a SimpleValueFactory.





Methods inherited from class org.eclipse.rdf4j.model.base.AbstractValueFactory
createLiteral, createLiteral, createLiteral, createLiteral

Methods inherited from class java.lang.Object
clone, equals, finalize, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait









Constructor Details



SimpleValueFactory

protected SimpleValueFactory()
Hidden constructor to enforce singleton pattern.









Method Details



getInstance

public static SimpleValueFactory getInstance()
Provide a single shared instance of a SimpleValueFactory.

Returns:
a singleton instance of SimpleValueFactory.






createIRI

public IRI createIRI(String iri)
Description copied from interface: ValueFactory
Creates a new IRI from the supplied string-representation.

Specified by:
createIRI in interface ValueFactory
Overrides:
createIRI in class AbstractValueFactory
Parameters:
iri - A string-representation of a IRI.
Returns:
An object representing the IRI.






createIRI

public IRI createIRI(String namespace,
 String localName)
Description copied from interface: ValueFactory
Creates a new IRI from the supplied namespace and local name. Calling this method is funtionally equivalent to
 calling createIRI(namespace+localName), but allows the ValueFactory to reuse supplied
 namespace and local name strings whenever possible. Note that the values returned by IRI.getNamespace()
 and IRI.getLocalName() are not necessarily the same as the values that are supplied to this method.

Specified by:
createIRI in interface ValueFactory
Overrides:
createIRI in class AbstractValueFactory
Parameters:
namespace - The IRI's namespace.
localName - The IRI's local name.






createBNode

public BNode createBNode(String nodeID)
Description copied from interface: ValueFactory
Creates a new blank node with the given node identifier.

Specified by:
createBNode in interface ValueFactory
Overrides:
createBNode in class AbstractValueFactory
Parameters:
nodeID - The blank node identifier.
Returns:
An object representing the blank node.






createLiteral

public Literal createLiteral(String value)
Description copied from interface: ValueFactory
Creates a new literal with the supplied label. The return value of Literal.getDatatype() for the returned
 object must be xsd:string.

Specified by:
createLiteral in interface ValueFactory
Overrides:
createLiteral in class AbstractValueFactory
Parameters:
value - The literal's label, must not be null.
Returns:
A literal for the specified value.






createLiteral

public Literal createLiteral(String value,
 String language)
Description copied from interface: ValueFactory
Creates a new literal with the supplied label and language attribute. The return value of
 Literal.getDatatype() for the returned object must be
 rdf:langString.

Specified by:
createLiteral in interface ValueFactory
Overrides:
createLiteral in class AbstractValueFactory
Parameters:
value - The literal's label, must not be null.
language - The literal's language attribute, must not be null.
Returns:
A literal for the specified value and language attribute.






createLiteral

public Literal createLiteral(boolean b)
Description copied from interface: ValueFactory
Creates a new xsd:boolean-typed literal representing the specified value.

Specified by:
createLiteral in interface ValueFactory
Overrides:
createLiteral in class AbstractValueFactory
Parameters:
b - The value for the literal.
Returns:
An xsd:boolean-typed literal for the specified value.






createLiteral

public Literal createLiteral(String value,
 IRI datatype)
Description copied from interface: ValueFactory
Creates a new literal with the supplied label and datatype.

Specified by:
createLiteral in interface ValueFactory
Overrides:
createLiteral in class AbstractValueFactory
Parameters:
value - The literal's label, must not be null.
datatype - The literal's datatype. If it is null, the datatype
                 xsd:string will be assigned to this
                 literal.
Returns:
A literal for the specified value and type.






createStatement

public Statement createStatement(Resource subject,
 IRI predicate,
 Value object)
Description copied from interface: ValueFactory
Creates a new statement with the supplied subject, predicate and object.

Specified by:
createStatement in interface ValueFactory
Overrides:
createStatement in class AbstractValueFactory
Parameters:
subject - The statement's subject.
predicate - The statement's predicate.
object - The statement's object.
Returns:
The created statement.






createStatement

public Statement createStatement(Resource subject,
 IRI predicate,
 Value object,
 Resource context)
Description copied from interface: ValueFactory
Creates a new statement with the supplied subject, predicate and object and associated context.

Specified by:
createStatement in interface ValueFactory
Overrides:
createStatement in class AbstractValueFactory
Parameters:
subject - The statement's subject.
predicate - The statement's predicate.
object - The statement's object.
context - The statement's context.
Returns:
The created statement.






createTriple

public Triple createTriple(Resource subject,
 IRI predicate,
 Value object)
Description copied from interface: ValueFactory
Creates a new RDF-star triple with the supplied subject, predicate and object.

Specified by:
createTriple in interface ValueFactory
Overrides:
createTriple in class AbstractValueFactory
Parameters:
subject - The statement's subject.
predicate - The statement's predicate.
object - The statement's object.
Returns:
The created triple.






createBNode

public BNode createBNode()
Description copied from interface: ValueFactory
Creates a new bNode.

Specified by:
createBNode in interface ValueFactory
Overrides:
createBNode in class AbstractValueFactory
Returns:
An object representing the bNode.






createLiteral

public Literal createLiteral(byte value)
Calls createIntegerLiteral(Number, IRI) with the supplied value and XSD.BYTE as parameters.

Specified by:
createLiteral in interface ValueFactory
Overrides:
createLiteral in class AbstractValueFactory
Parameters:
value - The value for the literal.
Returns:
An xsd:byte-typed literal for the specified value.






createLiteral

public Literal createLiteral(short value)
Calls createIntegerLiteral(Number, IRI) with the supplied value and XSD.SHORT as parameters.

Specified by:
createLiteral in interface ValueFactory
Overrides:
createLiteral in class AbstractValueFactory
Parameters:
value - The value for the literal.
Returns:
An xsd:short-typed literal for the specified value.






createLiteral

public Literal createLiteral(int value)
Calls createIntegerLiteral(Number, IRI) with the supplied value and XSD.INT as parameters.

Specified by:
createLiteral in interface ValueFactory
Overrides:
createLiteral in class AbstractValueFactory
Parameters:
value - The value for the literal.
Returns:
An xsd:int-typed literal for the specified value.






createLiteral

public Literal createLiteral(long value)
Calls createIntegerLiteral(Number, IRI) with the supplied value and XSD.LONG as parameters.

Specified by:
createLiteral in interface ValueFactory
Overrides:
createLiteral in class AbstractValueFactory
Parameters:
value - The value for the literal.
Returns:
An xsd:long-typed literal for the specified value.






createIntegerLiteral

protected Literal createIntegerLiteral(Number value,
 IRI datatype)
Calls createNumericLiteral(Number, IRI) with the supplied value and datatype as parameters.





createIntegerLiteral

protected Literal createIntegerLiteral(Number value,
 CoreDatatype.XSD datatype)





createLiteral

public Literal createLiteral(float value)
Calls createFPLiteral(Number, IRI) with the supplied value and XSD.FLOAT as parameters.

Specified by:
createLiteral in interface ValueFactory
Overrides:
createLiteral in class AbstractValueFactory
Parameters:
value - The value for the literal.
Returns:
An xsd:float-typed literal for the specified value.






createLiteral

public Literal createLiteral(double value)
Calls createFPLiteral(Number, IRI) with the supplied value and XSD.DOUBLE as parameters.

Specified by:
createLiteral in interface ValueFactory
Overrides:
createLiteral in class AbstractValueFactory
Parameters:
value - The value for the literal.
Returns:
An xsd:double-typed literal for the specified value.






createLiteral

public Literal createLiteral(BigInteger bigInteger)
Description copied from interface: ValueFactory
Creates a new literal representing the specified bigInteger that is typed as an xsd:integer.

Specified by:
createLiteral in interface ValueFactory
Overrides:
createLiteral in class AbstractValueFactory
Parameters:
bigInteger - The value for the literal.
Returns:
An xsd:integer-typed literal for the specified value.






createLiteral

public Literal createLiteral(BigDecimal bigDecimal)
Description copied from interface: ValueFactory
Creates a new literal representing the specified bigDecimal that is typed as an xsd:decimal.

Specified by:
createLiteral in interface ValueFactory
Overrides:
createLiteral in class AbstractValueFactory
Parameters:
bigDecimal - The value for the literal.
Returns:
An xsd:decimal-typed literal for the specified value.






createFPLiteral

protected Literal createFPLiteral(Number value,
 IRI datatype)
Calls createNumericLiteral(Number, IRI) with the supplied value and datatype as parameters.





createFPLiteral

protected Literal createFPLiteral(Number value,
 CoreDatatype.XSD datatype)





createNumericLiteral

protected Literal createNumericLiteral(Number number,
 IRI datatype)
Creates specific optimized subtypes of SimpleLiteral for numeric datatypes.





createNumericLiteral

protected Literal createNumericLiteral(Number number,
 CoreDatatype datatype)





createLiteral

public Literal createLiteral(XMLGregorianCalendar calendar)
Calls ValueFactory.createLiteral(String, IRI) with the String-value of the supplied calendar and the
 appropriate datatype as parameters.

Specified by:
createLiteral in interface ValueFactory
Overrides:
createLiteral in class AbstractValueFactory
Parameters:
calendar - The value for the literal.
Returns:
A typed literal for the specified calendar.
See Also:


XMLGregorianCalendar.toXMLFormat()
XMLGregorianCalendar.getXMLSchemaType()
XMLDatatypeUtil.qnameToCoreDatatype(javax.xml.namespace.QName)








createLiteral

public Literal createLiteral(Date date)
Converts the supplied Date to a XMLGregorianCalendar, then calls
 ValueFactory.createLiteral(XMLGregorianCalendar).

Specified by:
createLiteral in interface ValueFactory
Overrides:
createLiteral in class AbstractValueFactory
Parameters:
date - The value for the literal.
Returns:
A typed literal for the specified date.












Copyright © 2015–2025 Eclipse Foundation. All rights reserved.\n\n\n\n