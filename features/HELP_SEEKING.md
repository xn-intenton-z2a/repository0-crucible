# HELP_SEEKING

## Overview
Help-seeking is the trait that enables the agent to recognize its own limitations and reach out for external assistance. For an autonomous agent, “help” can come in various forms: consulting a more knowledgeable AI (like calling an OpenAI API for guidance), searching documentation or knowledge bases, or even alerting a human operator for input. This capability is crucial for keeping the agent aligned and effective – rather than getting stuck or making faulty assumptions, the agent can proactively seek information or confirmation. In our context, implementing help-seeking means the agent can handle queries it doesn’t internally know how to solve by leveraging an external AI model to get answers, and it can escalate issues when automated methods fail. This ensures that as complexity grows, the agent remains robust, using external knowledge to supplement its own logic.

## Implementation Details
1. **Identify When to Seek Help:** Determine the conditions under which the agent should invoke an external helper. Initially, we can trigger help-seeking via a special command pattern. For example, if `payload.command` begins with `"help:"` or `"ask:"`, the agent will treat everything after that prefix as a query for external assistance. This provides an explicit way for users (or higher-level agent logic) to request the agent to fetch an answer or solution it doesn’t have built-in.
2. **Integrate OpenAI API (External AI):** Leverage the existing OpenAI dependency to answer the query. In `src/lib/main.js`, within the agenticHandler, handle the help query by calling the OpenAI API:
  - Use the OpenAI SDK (`OpenAIApi`) to create a completion or chat completion. For example, if using a chat model, construct a prompt with the user’s question. A simple approach is: system message like “You are an expert assistant.” and user message with the query string extracted from `payload.command`.
  - Call `await openai.createChatCompletion({ model: "gpt-3.5-turbo", messages: [...] })` (or a comparable completion method) using the API key from config. Parse the response to extract the assistant’s answer text.
  - Note: In tests and development, ensure this call is mocked or not executed against real API unless configured. The test suite already sets up a mock for `openai` which returns a dummy response. Our code should handle that dummy format (which appears to return a JSON string in `choices[0].message.content`). If the content looks like JSON, parse it; otherwise, treat it as plain text.
3. **Return the Acquired Help:** Format the agent’s response to include the help information. For example, return an object `{ status: "success", answer: <string> }` where `<string>` is the answer from the external source. The agent effectively becomes a relay for information in this scenario. Also increment `callCount` for this as it is a handled command. If the external API fails or returns nothing useful, handle errors gracefully: log an error and maybe return a message like “Unable to get help at this time.”
4. **Error Escalation (Future hook):** In addition to explicit help commands, consider modifying error handling to utilize help-seeking. For instance, if after implementing other features the agent encounters a command it truly cannot process or an error it doesn’t know how to fix, it could automatically attempt a help query (perhaps asking, “How do I handle X error?”). Implementing this fully may be complex, so for now just include a placeholder or note in logs that suggests using help. (This can be mentioned as a comment or simple conditional: in the catch of `agenticHandler`, after logging the error, you might output `logInfo("Consider using help-seeking for: " + error.message)` as a hint.)
5. **Unit Tests for Help Query:** Add tests to verify the help-seeking behavior:
  - When `agenticHandler` is called with a command like `"help: What is 2+2?"`, the function should invoke the OpenAI API. Thanks to the test’s mock, it will receive a dummy response. Verify that the returned object contains an `answer` field (or similarly named field) with the expected content from the dummy (e.g., `"dummy success"` from the mock). For instance:
    ```js
    const resp = await agenticLib.agenticHandler({ command: "help: test question" });
    expect(resp.status).toBe("success");
    expect(resp.answer).toMatch(/dummy success/);
    ```
  - Test that normal commands (not starting with "help:") do not inadvertently call the API. This can be done by spying on the OpenAI client method: for a regular payload, ensure the spy is not called, whereas for a help payload it is called.
  - If implementing any error escalation, simulate a scenario that triggers it and verify the agent attempts a help call or logs the suggestion.
6. **Documentation:** Update the README to include a **Help-Seeking** feature description. Explain that the agent can tap into external knowledge when asked. Provide usage examples, such as: `{"command": "help: <question>"}` will cause the agent to query an AI model for an answer to `<question>`. Also mention the intended purpose: the agent uses this to overcome its built-in limitations. If applicable, note that in future the agent might automatically use this ability when it encounters an unfamiliar task or persistent error.

## Long-Term Direction
The help-seeking capability can expand into a robust safety net and research tool for the agent. In the future, the agent could have multiple channels for help: querying documentation or forums (via APIs), asking other specialized agents, or creating a GitHub issue to get human feedback on a problem. It could incorporate a feedback mechanism where if a solution from the AI assistant doesn’t work, it tries a different query or source. Help-seeking also ties in with self-improvement: the agent might ask for code review or best practices from an AI when improving itself. As it becomes more advanced, the agent could maintain a knowledge base of solutions – so after seeking help once, it remembers the answer for next time (integrating with the Memory trait). Ultimately, a well-developed help-seeking trait ensures that even as the agent tackles new domains or unexpected challenges, it remains adaptable and does not operate in a vacuum. It will continuously learn from external wisdom and involve humans only when absolutely necessary, striking a balance between autonomy and safe oversight.
